[
  {
    "objectID": "posts/ipynb/python_pingouin.html",
    "href": "posts/ipynb/python_pingouin.html",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "",
    "text": "Pingouin은 간단하지만 완전한 통계 기능를 위해 설계되었습니다. 예를 들어 기존의 SciPy 패키지의 ttest_ind 함수는 T-value과 p-value만 알려주지만 Pingouin의 ttest 함수는 T-value, p-value뿐만 아니라 자유도, 효과 크기(Cohen ’s d), 95% 신뢰 구간, 통계적 검정력등을 동시에 출력합니다. 아래의 목록은 할 수 있는 대표적인 분석입니다.\n더 자세한 것은 공식 홈페이지를 참고하세요."
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#one-sample-t-test",
    "href": "posts/ipynb/python_pingouin.html#one-sample-t-test",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.1 1.1. One-sample T-test",
    "text": "2.1 1.1. One-sample T-test\n\n모집단의 평균은 4로 가정\n\n\nmu = 4\nx = [5.5, 2.4, 6.8, 9.6, 4.2]\npg.ttest(x, mu)\n\n\n\n\n\n\n\n\nT\ndof\ntail\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n1.397391\n4\ntwo-sided\n0.234824\n[-1.68, 5.08]\n0.624932\n0.766\n0.191796\n\n\n\n\n\n\n\n자유도(dof)는 4, T-value(T)는 1.3973 이며 p-Value가 일반적인 기준(0.05) 이상이기 때문에 표본 x의 평균은 모집단의 평균과 차이가 없다(귀무가설)고 볼 수 있다."
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#paired-t-test",
    "href": "posts/ipynb/python_pingouin.html#paired-t-test",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.2 1.2. Paired T-test",
    "text": "2.2 1.2. Paired T-test\n꼬리를 one-sided로 설정하면 pingouin이 알아서 꼬리의 방향을 알려줍니다. 아래 코드의 경우 T-value가 음수이기 때문에 꼬리의 방향이 less로 표현됩니다.\n\npre = [5.5, 2.4, 6.8, 9.6, 4.2]\npost = [6.4, 3.4, 6.4, 11.0, 4.8]\npg.ttest(pre, post, paired=True, tail=\"one-sided\")\n\n\n\n\n\n\n\n\nT\ndof\ntail\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-2.307832\n4\nless\n0.041114\n[-inf, -0.05]\n0.250801\n3.122\n0.12048\n\n\n\n\n\n\n\n꼬리의 방향이 less라는 것은 표본 x의 평균이 표본 y의 평균보다 작다는 것을 뜻합니다.\n일부러 꼬리의 방향을 반대(greater)로 한 대립 가설을 확인해 봅니다.\n\npg.ttest(pre, post, paired=True, tail=\"greater\")\n\n\n\n\n\n\n\n\nT\ndof\ntail\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-2.307832\n4\ngreater\n0.958886\n[-1.35, inf]\n0.250801\n0.32\n0.016865\n\n\n\n\n\n\n\np-Value가 엉망인것을 알 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#two-sample-t-test",
    "href": "posts/ipynb/python_pingouin.html#two-sample-t-test",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.3 1.3. Two-sample T-test",
    "text": "2.3 1.3. Two-sample T-test\n\n2.3.1 1.3.1. 표본 크기가 같은 경우\n\nx = np.random.normal(loc=7, size=20)\ny = np.random.normal(loc=4, size=20)\npg.ttest(x, y, correction=\"auto\")\n\n\n\n\n\n\n\n\nT\ndof\ntail\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n10.151246\n38\ntwo-sided\n2.245760e-12\n[2.48, 3.71]\n3.210106\n2.223e+09\n1.0\n\n\n\n\n\n\n\n\n\n2.3.2 1.3.2. 표본 크기가 다른경우\n\nx = np.random.normal(loc=7, size=20)\ny = np.random.normal(loc=4, size=15)\npg.ttest(x, y, correction=\"auto\")\n\n\n\n\n\n\n\n\nT\ndof\ntail\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n8.352442\n24.033207\ntwo-sided\n1.443438e-08\n[2.21, 3.65]\n2.995554\n5.808e+06\n1.0"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#pearsons-correlation",
    "href": "posts/ipynb/python_pingouin.html#pearsons-correlation",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.4 1.4. Pearson’s correlation",
    "text": "2.4 1.4. Pearson’s correlation\n\nmean, cov, n = [4, 5], [(1, 0.6), (0.6, 1)], 30\nx, y = np.random.multivariate_normal(mean, cov, n).T\npg.corr(x, y)\n\n\n\n\n\n\n\n\nn\nr\nCI95%\nr2\nadj_r2\np-val\nBF10\npower\n\n\n\n\npearson\n30\n0.63893\n[0.36, 0.81]\n0.408231\n0.364397\n0.000145\n220.85\n0.978466"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#robust-correlation",
    "href": "posts/ipynb/python_pingouin.html#robust-correlation",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.5 1.5. Robust correlation",
    "text": "2.5 1.5. Robust correlation\n\n# 표본 x에 아웃라이어 추가\nx[5] = 18\n# Use the robust Shepherd's pi correlation\npg.corr(x, y, method=\"shepherd\")\n\n\n\n\n\n\n\n\nn\noutliers\nr\nCI95%\nr2\nadj_r2\np-val\npower\n\n\n\n\nshepherd\n30\n3\n0.391331\n[0.04, 0.66]\n0.15314\n0.090409\n0.043538\n0.586546"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#데이터의-정규성-테스트",
    "href": "posts/ipynb/python_pingouin.html#데이터의-정규성-테스트",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.6 1.6. 데이터의 정규성 테스트",
    "text": "2.6 1.6. 데이터의 정규성 테스트\npingouin.normality()함수를 pandas의 데이터 프레임형식에 사용할 수 있습니다.\n\n# 일변량 정규성(Univariate normality)\npg.normality(x)\n\n\n\n\n\n\n\n\nW\npval\nnormal\n\n\n\n\n0\n0.485009\n3.733778e-09\nFalse\n\n\n\n\n\n\n\n\n# 다변량 정규성(Multivariate normality)\npg.multivariate_normality(np.column_stack((x, y)))\n\n(False, 6.620602006901788e-07)"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#q-q-plot-시각화",
    "href": "posts/ipynb/python_pingouin.html#q-q-plot-시각화",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.7 1.7. Q-Q plot 시각화",
    "text": "2.7 1.7. Q-Q plot 시각화\n\nx = np.random.normal(size=50)\nax = pg.qqplot(x, dist=\"norm\")"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#one-way-anova",
    "href": "posts/ipynb/python_pingouin.html#one-way-anova",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.8 1.8. One-way ANOVA",
    "text": "2.8 1.8. One-way ANOVA\n기본 내장되어 있는 데이터프레임(mixed_anova)을 사용합니다.\n\n# Read an example dataset\ndf = pg.read_dataset(\"mixed_anova\")\ndf.tail()\n\n\n\n\n\n\n\n\nScores\nTime\nGroup\nSubject\n\n\n\n\n175\n6.176981\nJune\nMeditation\n55\n\n\n176\n8.523692\nJune\nMeditation\n56\n\n\n177\n6.522273\nJune\nMeditation\n57\n\n\n178\n4.990568\nJune\nMeditation\n58\n\n\n179\n7.822986\nJune\nMeditation\n59\n\n\n\n\n\n\n\n\n# Run the ANOVA\naov = pg.anova(data=df, dv=\"Scores\", between=\"Group\", detailed=True)\naov\n\n\n\n\n\n\n\n\nSource\nSS\nDF\nMS\nF\np-unc\nnp2\n\n\n\n\n0\nGroup\n5.459963\n1\n5.459963\n5.243656\n0.0232\n0.028616\n\n\n1\nWithin\n185.342729\n178\n1.041251\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#repeated-measures-anova",
    "href": "posts/ipynb/python_pingouin.html#repeated-measures-anova",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.9 1.9. Repeated measures ANOVA",
    "text": "2.9 1.9. Repeated measures ANOVA\n\npg.rm_anova(data=df, dv=\"Scores\", within=\"Time\", subject=\"Subject\", detailed=True)\n\n\n\n\n\n\n\n\nSource\nSS\nDF\nMS\nF\np-unc\nnp2\neps\n\n\n\n\n0\nTime\n7.628428\n2\n3.814214\n3.912796\n0.022629\n0.062194\n0.998751\n\n\n1\nError\n115.027023\n118\n0.974805\nNaN\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#post-hoc-tests-corrected-for-multiple-comparisons",
    "href": "posts/ipynb/python_pingouin.html#post-hoc-tests-corrected-for-multiple-comparisons",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.10 1.10. Post-hoc tests corrected for multiple-comparisons",
    "text": "2.10 1.10. Post-hoc tests corrected for multiple-comparisons\n\n# FDR-corrected post hocs with Hedges'g effect size\nposthoc = pg.pairwise_ttests(\n    data=df,\n    dv=\"Scores\",\n    within=\"Time\",\n    subject=\"Subject\",\n    parametric=True,\n    padjust=\"fdr_bh\",\n    effsize=\"hedges\",\n)\n\nposthoc\n\n\n\n\n\n\n\n\nContrast\nA\nB\nPaired\nParametric\nT\ndof\nTail\np-unc\np-corr\np-adjust\nBF10\nhedges\n\n\n\n\n0\nTime\nAugust\nJanuary\nTrue\nTrue\n-1.740370\n59.0\ntwo-sided\n0.087008\n0.130512\nfdr_bh\n0.582\n-0.327583\n\n\n1\nTime\nAugust\nJune\nTrue\nTrue\n-2.743238\n59.0\ntwo-sided\n0.008045\n0.024134\nfdr_bh\n4.232\n-0.482547\n\n\n2\nTime\nJanuary\nJune\nTrue\nTrue\n-1.023620\n59.0\ntwo-sided\n0.310194\n0.310194\nfdr_bh\n0.232\n-0.169520"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#two-way-mixed-anova",
    "href": "posts/ipynb/python_pingouin.html#two-way-mixed-anova",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.11 1.11. Two-way mixed ANOVA",
    "text": "2.11 1.11. Two-way mixed ANOVA\n\n# Compute the two-way mixed ANOVA and export to a .csv file\naov = pg.mixed_anova(\n    data=df,\n    dv=\"Scores\",\n    between=\"Group\",\n    within=\"Time\",\n    subject=\"Subject\",\n    correction=False,\n    effsize=\"np2\",\n)\naov\n\n\n\n\n\n\n\n\nSource\nSS\nDF1\nDF2\nMS\nF\np-unc\nnp2\neps\n\n\n\n\n0\nGroup\n5.459963\n1\n58\n5.459963\n5.051709\n0.028420\n0.080120\nNaN\n\n\n1\nTime\n7.628428\n2\n116\n3.814214\n4.027394\n0.020369\n0.064929\n0.998751\n\n\n2\nInteraction\n5.167192\n2\n116\n2.583596\n2.727996\n0.069545\n0.044922\nNaN"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#bland-altman-plot",
    "href": "posts/ipynb/python_pingouin.html#bland-altman-plot",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.12 1.12. Bland-Altman plot",
    "text": "2.12 1.12. Bland-Altman plot\n\nmean, cov = [10, 11], [[1, 0.8], [0.8, 1]]\nx, y = np.random.multivariate_normal(mean, cov, 30).T\nax = pg.plot_blandaltman(x, y)"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#paired-t-test-검정력-시각화",
    "href": "posts/ipynb/python_pingouin.html#paired-t-test-검정력-시각화",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.13 1.13. paired T-test 검정력 시각화",
    "text": "2.13 1.13. paired T-test 검정력 시각화\nT-검정의 표본 크기와 효과 크기(Cohen’d)에 따른 검정력 곡선을 시각화합니다.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"ticks\", context=\"notebook\", font_scale=1.2)\nd = 0.5  # Fixed effect size\nn = np.arange(5, 80, 5)  # Incrementing sample size\n# Compute the achieved power\npwr = pg.power_ttest(d=d, n=n, contrast=\"paired\", tail=\"two-sided\")\nplt.plot(n, pwr, \"ko-.\")\nplt.axhline(0.8, color=\"r\", ls=\":\")\nplt.xlabel(\"Sample size\")\nplt.ylabel(\"Power (1 - type II error)\")\nplt.title(\"Achieved power of a paired T-test\")\nsns.despine()"
  },
  {
    "objectID": "posts/ipynb/python_pingouin.html#paired-plot",
    "href": "posts/ipynb/python_pingouin.html#paired-plot",
    "title": "파이썬 Pingouin으로 통계 분석하기",
    "section": "2.14 1.14. Paired plot",
    "text": "2.14 1.14. Paired plot\nmixed_anova데이터셋을 가지고 명상이 학교 성적에 미치는 영향에 대한 시각화를 해본다.\n\ndf = pg.read_dataset(\"mixed_anova\").query(\"Group == 'Meditation' and Time != 'January'\")\n\npg.plot_paired(data=df, dv=\"Scores\", within=\"Time\", subject=\"Subject\")"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_06.html",
    "href": "posts/ipynb/scanpy_workshop_06.html",
    "title": "Scanpy로 scRNA-seq 분석 06",
    "section": "",
    "text": "세포 유형의 발달 궤적을 분석하고 시각화하는 것은 생물학 연구에서 매우 중요한 과정입니다. 특히 단일 세포 전사체 분석(single-cell transcriptomics)의 발전으로, 개별 세포의 발달 경로와 상태 변화를 더 정밀하게 추적할 수 있게 되었습니다. 이러한 분석을 위해 다양한 기법들이 사용되는데, 그 중에서도 PAGA(Partition-based Graph Abstraction)는 매우 유용한 도구로 자리 잡고 있습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_06.html#데이터-불러오기",
    "href": "posts/ipynb/scanpy_workshop_06.html#데이터-불러오기",
    "title": "Scanpy로 scRNA-seq 분석 06",
    "section": "3.1 데이터 불러오기",
    "text": "3.1 데이터 불러오기\n다운로드한 trajectory_seurat_filtered.h5ad파일을 불러옵니다.\n\nadata = sc.read_h5ad(\"./input/trajectory/trajectory_seurat_filtered.h5ad\")\nadata\n\nAnnData object with n_obs × n_vars = 5828 × 3585\n    obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'batches', 'dataset', 'nCount', 'nUMI', 'pMito', 'pRibo', 'pHb', 'pChY', 'pChX', 'pnonXY', 'pPCG', 'pNCG', 'pMito_UMIs', 'pRibo_UMIs', 'pHb_UMIs', 'pChY_UMIs', 'pChX_UMIs', 'pnonXY_UMIs', 'pPCG_UMIs', 'pNCG_UMIs', 'SEL_nCount', 'SEL_nUMI', 'SEL_pMito', 'SEL_pRibo', 'SEL_pHb', 'SEL_pChY', 'SEL_pChX', 'SEL_pnonXY', 'SEL_pPCG', 'SEL_pNCG', 'SEL_pMito_UMIs', 'SEL_pRibo_UMIs', 'SEL_pHb_UMIs', 'SEL_pChY_UMIs', 'SEL_pChX_UMIs', 'SEL_pnonXY_UMIs', 'SEL_pPCG_UMIs', 'SEL_pNCG_UMIs', 'S.Score', 'G2M.Score', 'Phase', 'metadata_clusters', 'clusters', 'outlier', 'subgroups', 'clusters_use'\n    var: 'features'\n    obsm: 'X_harmony', 'X_harmony_Phase', 'X_pca', 'X_umap', 'X_umap3d'\n    varm: 'PCs'\n\n\n위 출력을 통해 adata파일에 유맵과 클러스터 정보가 들어 있다는 것을 확인 할 수 있습니다. 이 정보를 다시 사용해도 되지만 여기서는 scanpy를 사용해 다시 분석할 예정입니다.\n\n# X 행렬에 로그 정규화된 카운트값이 들어있는지 확인합니다.\nprint(adata.X[:5, :5])\n\n  (0, 4)    0.11622072805743532\n  (2, 1)    0.09413397843954842\n  (3, 1)    0.08438841021254412\n  (3, 4)    0.08438841021254412\n  (4, 1)    0.14198147850903975"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_06.html#데이터-탐색하기",
    "href": "posts/ipynb/scanpy_workshop_06.html#데이터-탐색하기",
    "title": "Scanpy로 scRNA-seq 분석 06",
    "section": "3.2 데이터 탐색하기",
    "text": "3.2 데이터 탐색하기\nUMAP 플랏을 그려서 간단하게 살펴봅니다.\n데이터 집합을 분석할 때는 무슨 일이 일어나고 있는지, 데이터에서 보이는 클러스터는 무엇이며, 가장 중요한 것은 클러스터가 서로 어떻게 연관되어 있는지 이해하는 것이 중요합니다. 그럼, 데이터를 조금 살펴봅시다. 이 표의 도움을 받아 데이터 집합에서 이러한 핵심 마커를 나타내는 클러스터 번호를 적어 보세요."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_06.html#다시-umap-그리기",
    "href": "posts/ipynb/scanpy_workshop_06.html#다시-umap-그리기",
    "title": "Scanpy로 scRNA-seq 분석 06",
    "section": "3.3 다시 UMAP 그리기",
    "text": "3.3 다시 UMAP 그리기\nScanpy를 사용해 클러스터링과 유맵을 다시 실행합니다. 데이터에 이미 포함된 차원감소 데이터인 X_harmony_Phase를 시작점으로 UMAP을 새로 그립니다.\n\n# 먼저 기존에 들어있던 umap 정보를 다른 이름으로 저장합니다.\nadata.obsm[\"X_umap_old\"] = adata.obsm[\"X_umap\"]\n\nsc.pp.neighbors(adata, n_pcs=30, n_neighbors=20, use_rep=\"X_harmony_Phase\")\nsc.tl.umap(adata, min_dist=0.4, spread=3)\n# sc.tl.umap(adata, min_dist=0.6, spread=1.5)\n# sc.pl.umap(\n#     adata,\n#     color=[\"clusters\"],\n#     legend_loc=\"on data\",\n#     legend_fontsize=\"small\",\n#     edges=True,\n# )\n\n# 다시 클러스터링 하기\nsc.tl.leiden(adata, key_added=\"leiden_1.0\", resolution=1.0)  # default resolution in 1.0\nsc.tl.leiden(adata, key_added=\"leiden_1.2\", resolution=1.2)  # default resolution in 1.0\nsc.tl.leiden(adata, key_added=\"leiden_1.4\", resolution=1.4)  # default resolution in 1.0\n\nsc.pl.umap(\n    adata,\n    color=[\"leiden_1.0\", \"leiden_1.2\", \"leiden_1.4\", \"clusters\"],\n    legend_loc=\"on data\",\n    legend_fontsize=\"xx-small\",\n    ncols=2,\n)\n\n\n\n\n\n\n\n\n클러스터로 분류는 되어있지만 각각이 어떤 세포 유형인지는 알 수 없는 아래 표의 마커 유전자 정보를 가지고 세포유형을 지정하도록 하겠습니다.\n\n\n\nMarker\nCell Type\n\n\n\n\nCd34\nHSC progenitor\n\n\nMs4a1\nB cell lineage\n\n\nCd3e\nT cell lineage\n\n\nLtf\nGranulocyte lineage\n\n\nCst3\nMonocyte lineage\n\n\nMcpt8\nMast Cell lineage\n\n\nAlas2\nRBC lineage\n\n\nSiglech\nDendritic cell lineage\n\n\nC1qc\nMacrophage cell lineage\n\n\nPf4\nMegakaryocyte cell lineage\n\n\n\n\nmarkers = [\n    \"Cd34\",\n    \"Alas2\",\n    \"Pf4\",\n    \"Mcpt8\",\n    \"Ltf\",\n    \"Cst3\",\n    \"Siglech\",\n    \"C1qc\",\n    \"Ms4a1\",\n    \"Cd3e\",\n]\n\nsc.pl.umap(adata, color=markers, use_raw=False, ncols=3)\n\n\n\n\n\n\n\n\n위 시각화 결과를 참고해 leiden_1.4 클러스터링 결과에 세포 유형을 지정합니다.\n\n# 클러스터 이름 변경을 위한 매핑 딕셔너리\ncluster_mapping = {\n    \"10\": \"10_megakaryo\",  # Pf4\n    \"17\": \"17_macro\",  # C1qc\n    \"11\": \"11_eryth\",  # Alas2\n    \"18\": \"18_dend\",  # Siglech\n    \"13\": \"13_mast\",  # Mcpt8\n    \"0\": \"0_mono\",  # Cts3\n    \"1\": \"1_gran\",  # Ltf\n    \"9\": \"9_gran\",  # (\"1_gran\"과 동일하지만 다른 클러스터)\n    \"14\": \"14_TC\",  # Cd3e\n    \"16\": \"16_BC\",  # Ms4a1\n    \"8\": \"8_progen\",  # Cd34\n    \"4\": \"4_progen\",  # (another progenitor cluster)\n    \"5\": \"5_progen\",  # (another progenitor cluster)\n}\n\n# 일관성을 위해 leiden_1.4 열을 문자열로 변환하는 코드\nannot = pd.DataFrame(adata.obs[\"leiden_1.4\"].astype(\"string\"))\n\n# 매핑을 적용하여 클러스터 이름을 바꾸고 나머지는 변경하지 않습니다.\nannot[\"annot\"] = annot[\"leiden_1.4\"].map(cluster_mapping).fillna(annot[\"leiden_1.4\"])\n\n# 새 주석으로 데이터 객체를 업데이트합니다.\nadata.obs[\"annot\"] = annot[\"annot\"].astype(\"category\")\n\n# 새 주석을 사용해 UMAP 플롯하기\nsc.pl.umap(\n    adata,\n    color=[\"annot\", \"leiden_1.4\"],\n    legend_loc=\"on data\",\n    ncols=2,\n    legend_fontsize=\"xx-small\",\n)\n# 어노테이션의 값 개수를 표시합니다.\n# print(annot[\"annot\"].value_counts())"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_06.html#그래프-엣지-필터링",
    "href": "posts/ipynb/scanpy_workshop_06.html#그래프-엣지-필터링",
    "title": "Scanpy로 scRNA-seq 분석 06",
    "section": "4.1 그래프 엣지 필터링",
    "text": "4.1 그래프 엣지 필터링\n먼저 엣지와 UMAP을 겹쳐서 좀 더 자세히 살펴봅시다.\n\nwith plt.rc_context({\"figure.figsize\": (6, 6)}):\n    sc.pl.umap(\n        adata,\n        edges=True,\n        color=\"annot\",\n        legend_loc=\"on data\",\n        legend_fontsize=\"xx-small\",\n    )\n\n\n\n\n\n\n\n\n위 그림에서 알 수 있듯이, 관련 없는 클러스터 사이의 엣지가 보입니다. 따라서 더 적은 수의 이웃(n_neighbors=5)로 다시 그려 보겠습니다.\n\nsc.pp.neighbors(adata, n_neighbors=5, use_rep=\"X_harmony_Phase\", n_pcs=30)\n\nwith plt.rc_context({\"figure.figsize\": (6, 6)}):\n    sc.pl.umap(\n        adata,\n        edges=True,\n        color=\"annot\",\n        legend_loc=\"on data\",\n        legend_fontsize=\"xx-small\",\n    )\n\n\n\n\n\n\n\n\n불필요한 그래프 엣지가 훨씬 줄어들었습니다. 이제 다시 PAGA를 실행합니다.\n\nsc.tl.draw_graph(adata, init_pos=\"X_umap\")\n\nwith plt.rc_context({\"figure.figsize\": (6, 6)}):\n    sc.pl.draw_graph(adata, color=\"annot\", legend_loc=\"on data\", legend_fontsize=\"xx-small\")\n\n\n\n\n\n\n\n\n\nsc.tl.paga(adata, groups=\"annot\")\nwith plt.rc_context({\"figure.figsize\": (5, 5)}):\n    sc.pl.paga(adata, color=\"annot\", edge_width_scale=0.3)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_06.html#paga-초기화해서-임베딩하기",
    "href": "posts/ipynb/scanpy_workshop_06.html#paga-초기화해서-임베딩하기",
    "title": "Scanpy로 scRNA-seq 분석 06",
    "section": "4.2 PAGA 초기화해서 임베딩하기",
    "text": "4.2 PAGA 초기화해서 임베딩하기\nPAGA 레이아웃의 다른 시작 위치를 사용하여 그래프를 다시 그릴 수 있습니다. 다음은 UMAP에서도 가능합니다.\n\nsc.tl.draw_graph(adata, init_pos=\"paga\")\n\nwith plt.rc_context({\"figure.figsize\": (5, 5)}):\n    sc.pl.draw_graph(adata, color=[\"annot\"], legend_loc=\"on data\", legend_fontsize=\"xx-small\")\n\n\n\n\n\n\n\n\n이제 모든 마커 유전자를 의미 있는 레이아웃으로 볼 수 있습니다.\n\nwith plt.rc_context({\"figure.figsize\": (3, 3)}):\n    sc.pl.draw_graph(adata, color=markers, use_raw=False)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_06.html#두-그래프-비교",
    "href": "posts/ipynb/scanpy_workshop_06.html#두-그래프-비교",
    "title": "Scanpy로 scRNA-seq 분석 06",
    "section": "4.3 두 그래프 비교",
    "text": "4.3 두 그래프 비교\n\nwith plt.rc_context({\"figure.figsize\": (5, 5)}):\n    sc.pl.paga_compare(\n        adata,\n        threshold=0.03,\n        title=\"\",\n        right_margin=0.2,\n        size=10,\n        edge_width_scale=0.5,\n        legend_fontsize=10,\n        fontsize=10,\n        frameon=False,\n        edges=True,\n    )\n\n# with plt.rc_context({\"figure.figsize\": (5, 5)}):\n#     plt.show()"
  },
  {
    "objectID": "posts/ipynb/scanpy_QC.html#배경ambient-rna-제거",
    "href": "posts/ipynb/scanpy_QC.html#배경ambient-rna-제거",
    "title": "scRNA-seq 데이터 QC하기",
    "section": "3.1 배경(Ambient) RNA 제거",
    "text": "3.1 배경(Ambient) RNA 제거\n\n\n\n배경 RNA는 세포의 기본 mRNA와 함께 카운트되어 혼동을 일으킵니다.\n\n\ndroplet 기술을 사용한 scRNA-seq의 경우, 세포가 용해되면서 나온 mRNA가 세포들과 함께 시퀀싱될 수 있습니다. 이런 현상을 배경(Ambient) RNA 오염이라 하며 다운스트림 분석에서 해석을 왜곡할 수 있습니다.\n배경 RNA 오염은 사용한 scRNA-seq 기술과 분석한 세포의 유형에 따라 달라지기 때문에 데이터를 적절하게 보정하는 것이 중요합니다. 현재 알려진 방법으로는 SoupX[Young and Behjati, 2020] 및 DecontX[Yang et al., 2020]와 같은 방법이 있는데 이는 세포 용해액의 구성을 추정해 카운트 행렬의 값을 수정하는 원리입니다.\nSoupX는 R언어로 작성되어 있음으로 아래의 코드와 같이 파이썬의 rpy2 모듈을 사용해야 합니다.\n\n%%R\nlibrary(SoupX)\n\n\n    WARNING: The R package \"reticulate\" only fixed recently\n    an issue that caused a segfault when used with rpy2:\n    https://github.com/rstudio/reticulate/pull/1188\n    Make sure that you use a version of that package that includes\n    the fix.\n    \n\n\nSoupX로 더 나은 결과를 얻기 위해서는 클러스터링 정보가 필요합니다.[Young and Behjati, 2020] 먼저 AnnData 객체의 복사본을 만들어 정규화하고, 클러스터링을 진행 합니다. 클러스터링 정보는 soupx_groups 객체로 저장해 사용합니다.\n\nadata_pp = adata.copy()\nsc.pp.normalize_per_cell(adata_pp)\nsc.pp.log1p(adata_pp)\nsc.pp.pca(adata_pp)\nsc.pp.neighbors(adata_pp)\nsc.tl.leiden(adata_pp, key_added=\"soupx_groups\")\n\n# 클러스터를 .obs에 soupx_groups로 추가하고 벡터로 저장합니다.\nsoupx_groups = adata_pp.obs[\"soupx_groups\"]\n\n# AnnData 객체의 복사본을 삭제해 메모리를 절약합니다.\ndel adata_pp\n\n# 세포, 유전자, 필터링된 카운터 행렬을 저장합니다.\n# SoupX에는 모양 특징 x 바코드 행렬이 필요하므로 transpose해야 합니다.\ncells = adata.obs_names\ngenes = adata.var_names\ndata = adata.X.T\n\ncellranger에서 얻은 raw_feature_bc_matrix.h5를 data_tod 객체로 저장해 SoupX의 인풋으로 사용합니다.\n\nadata_raw = sc.read_10x_h5(\n    filename=data_path + \"raw_feature_bc_matrix.h5\",\n)\nadata_raw.var_names_make_unique()\ndata_tod = adata_raw.X.T\n# 객체를 삭제해 메모리를 절약합니다.\ndel adata_raw\n\n이제 SoupX를 실행하기 위한 모든 준비가 완료되었습니다. SoupChannel 객체를 만들고 메타데이터를 데이터 프레임 형식으로 추가합니다. 그런다음 autoEstCont함수를 사용해 배경 RNA 오염 비율을 계산하고 adjustCounts로 카운트 행렬을 보정합니다.다\n\n%%R -i data -i data_tod -i genes -i cells -i soupx_groups -o out \n\n# specify row and column names of data\nrownames(data) = genes\ncolnames(data) = cells\n# ensure correct sparse format for table of counts and table of droplets\ndata &lt;- as(data, \"sparseMatrix\")\ndata_tod &lt;- as(data_tod, \"sparseMatrix\")\n\n# Generate SoupChannel Object for SoupX \nsc = SoupChannel(data_tod, data, calcSoupProfile = FALSE)\n\n# Add extra meta data to the SoupChannel object\nsoupProf = data.frame(row.names = rownames(data), est = rowSums(data)/sum(data), counts = rowSums(data))\nsc = setSoupProfile(sc, soupProf)\n# Set cluster information in SoupChannel\nsc = setClusters(sc, soupx_groups)\n\n# Estimate contamination fraction\nsc  = autoEstCont(sc, doPlot=TRUE)\n# Infer corrected table of counts and rount to integer\nout = adjustCounts(sc, roundToInt = TRUE)\n\n\n\n\n\n\n\n\n이후의 모든 분석 단계에 보정된 카운트 행렬을 사용하고자 soupX_counts 레이어로 .X을 덮어씌웁니다.\n\nadata.layers[\"counts\"] = adata.X\nadata.layers[\"soupX_counts\"] = out.T\nadata.X = adata.layers[\"soupX_counts\"]\n\n추가로 최소 20개 이상의 세포에서 검출되지 않는 유전자는 필터링합니다.\n\nprint(f\"Total number of genes: {adata.n_vars}\")\n\n# Min 20 cells - filters out 0 count genes\nsc.pp.filter_genes(adata, min_cells=20)\nprint(f\"Number of genes after cell filter: {adata.n_vars}\")\n\nTotal number of genes: 32285\nNumber of genes after cell filter: 14534"
  },
  {
    "objectID": "posts/ipynb/scanpy_QC.html#이중체doublet-검출",
    "href": "posts/ipynb/scanpy_QC.html#이중체doublet-검출",
    "title": "scRNA-seq 데이터 QC하기",
    "section": "3.2 이중체(Doublet) 검출",
    "text": "3.2 이중체(Doublet) 검출\n\n\n\n세포 쌍을 무작위로 샘플링하고 유전자 발현 프로필을 평균화하여 염색체 수를 얻고 PCA 공간에 투영합니다. 그런 다음 kNN으로 Doublet점수를 계산해 검출합니다.\n\n\n이중체는 두 개 이상의 세포가 하나의 droplet에 포함된 것입니다. 이런 기술적인 문제가 있기 때문에 카운터 행렬에서 ‘세포’ 대신 ’바코드’라는 용어를 사용하는 것입니다. 이중체는 동일한 세포 유형(그러나 다른 개체에서 유래한)에 의해 형성된 동형 이중체와 그렇지 않은 이형 이중체가 있습니다. 동형 이중체는 카운트 행렬의 정보로 구별이 어렵고 셀 해싱이나 SNP로만 식별할 수 있기 때문에 여기서 다루는 이중체 검출의 목표가 아닙니다.\n반면에 이형 이중체는 잘못 분류될 가능성이 높고 다운스트림 결과를 왜곡할 수 있음으로 이를 식별하는 것은 중요합니다. 일반적으로 이중체 검출은 전처리 초기 단계에서 진행합니다. 이형 이중체 검출을 위한 여러 소프트웨어 패키지들이 있는데 scDblFinder는 현재 가장 검출 정확도가 높고 우수한 계산 효율성 및 안정성을 가진 도구로 평가됩니다.[Xi and Li, 2021] 따라서 여기에서는 scDblFinder 사용합니다.\n\n%%R\nlibrary(Seurat)\nlibrary(scater)\nlibrary(scDblFinder)\nlibrary(BiocParallel)\n\n\ndata_mat = adata.X.T\n\nscDblFinder는 입력 값으로 SingleCellExperiment객체를 사용하므로 아래 예시 코드와 같이 AnnData객체를 변환해야 합니다. 코드를 실행하면 다음의 열이 결과로 추가됩니다.\n\nscDblFinder.score: Doublet 점수(높을수록 셀이 이중일 가능성이 높음)\nscDblFinder.class: 분류(Doublet 또는 Singlet)\n\n\n%%R -i data_mat -o doublet_score -o doublet_class\n\nset.seed(42)\nsce = scDblFinder(\n    SingleCellExperiment(list(counts=data_mat), )\n)\n\ndoublet_score = sce$scDblFinder.score\ndoublet_class = sce$scDblFinder.class\n\n\n# scDblFinder 결과를 AnnData 객체의 .obs에 추가합니다.\nadata.obs[\"scDblFinder_score\"] = doublet_score\nadata.obs[\"scDblFinder_class\"] = doublet_class\nadata.obs.scDblFinder_class.value_counts()\n\nscDblFinder_class\nsinglet    10125\ndoublet     1701\nName: count, dtype: int64\n\n\nDoublet 점수를 시각화해 scDblFinder_class이 올바른지 확인합니다.\nplt.hist(doublet_score, bins=20)\nplt.title(\"Doublet score\")\nplt.show()\n\n\n\n\n\n\n\n추후에 진행될 다운스트림 분석 과정에서 더 많은 또는 더 적은 셀을 필터링하는 것이 필요할 수 있습니다. 따라서 지금 당장 식별된 이중체를 삭제하는 것은 좋은 생각이 아닙니다. 지금까지 진행한 품질 관리 절차를 통해 얼마나 많은 바코드와 유전자가 필터링 되었는지 확인하고 파일로 저장하겠습니다.\n\n\n바코드 수: 13486 -&gt; 11826(0.9)\n유전자 수: 32285 -&gt; 14534(0.5)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_04.html",
    "href": "posts/ipynb/scanpy_workshop_04.html",
    "title": "Scanpy로 scRNA-seq 분석 04",
    "section": "",
    "text": "클러스터링으로 scRNA-seq 데이터를 여러 그룹으로 나눈 뒤에는 해당 그룹이 어떤 세포인지 아는 것은 분석 결과를 해석할 때 중요합니다. 세포 유형을 예측하는 방법에는 크게 두 가지 방법이 있습니다. 레퍼런스 scRNA-seq 데이터를 사용해 유사도를 비교하는 방법과 알려진 마커 유전자를 사용해 직접 세포 유형을 지정하는 방법입니다. 레퍼런스 데이터는 이미 세포 유형의 전사체 표현 데이터를 말합니다. 이 데이터를 사용하여 클러스터링된 세포들의 전사체 표현 패턴을 레퍼런스 데이터와 비교하여 가장 유사한 세포 유형을 예측합니다. 여기서는 가장 간단한 ingest 방법을 사용해봅니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_04.html#비교-데이터-불러오기",
    "href": "posts/ipynb/scanpy_workshop_04.html#비교-데이터-불러오기",
    "title": "Scanpy로 scRNA-seq 분석 04",
    "section": "2.1 비교 데이터 불러오기",
    "text": "2.1 비교 데이터 불러오기\nscanpy.datasets함수를 사용해 레퍼런스로 사용할 pbmc3k데이터를 불러옵니다. 여기서 주의할 점은 레퍼런스 데이터와 실험 데이터는 비슷한 실험 조건을 가져야 한다는 것입니다. 예를 들어, 레퍼런스 데이터가 10X Genomics의 Chromium 플랫폼으로 생성되었다면, 실험 데이터도 동일한 플랫폼으로 생성되어야 합니다. 이는 플랫폼 간의 차이로 인해 발생하는 바이어스를 최소화하기 위함입니다.\n\nadata_ref = sc.datasets.pbmc3k_processed()\nadata_ref.obs[\"sample\"] = \"pbmc3k\"\nadata_ref\n\nAnnData object with n_obs × n_vars = 2638 × 1838\n    obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain', 'sample'\n    var: 'n_cells'\n    uns: 'draw_graph', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'\n    obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_draw_graph_fr'\n    varm: 'PCs'\n    obsp: 'distances', 'connectivities'\n\n\n\nprint(adata_ref.shape)\nadata_ref.obs.head()\n\n(2638, 1838)\n\n\n\n\n\n\n\n\n\nn_genes\npercent_mito\nn_counts\nlouvain\nsample\n\n\nindex\n\n\n\n\n\n\n\n\n\nAAACATACAACCAC-1\n781\n0.030178\n2419.0\nCD4 T cells\npbmc3k\n\n\nAAACATTGAGCTAC-1\n1352\n0.037936\n4903.0\nB cells\npbmc3k\n\n\nAAACATTGATCAGC-1\n1131\n0.008897\n3147.0\nCD4 T cells\npbmc3k\n\n\nAAACCGTGCTTCCG-1\n960\n0.017431\n2639.0\nCD14+ Monocytes\npbmc3k\n\n\nAAACCGTGTATGCG-1\n522\n0.012245\n980.0\nNK cells\npbmc3k\n\n\n\n\n\n\n\n\nsc.pl.umap(adata_ref, color=[\"louvain\"])"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_04.html#분석-데이터-불러오기",
    "href": "posts/ipynb/scanpy_workshop_04.html#분석-데이터-불러오기",
    "title": "Scanpy로 scRNA-seq 분석 04",
    "section": "2.2 분석 데이터 불러오기",
    "text": "2.2 분석 데이터 불러오기\n클러스터링 단계에서 저장된 코로나19 데이터 개체를 읽어와 보겠습니다.\n\npath_file = \"./output/covid/results/scanpy_covid_qc_dr_sc_cl.h5ad\"\nadata = sc.read_h5ad(path_file)\nadata.obs[\"sample\"] = \"covid\"\nadata\n\nAnnData object with n_obs × n_vars = 7227 × 19094\n    obs: 'type', 'sample', 'batch', 'n_counts', 'leiden_1.0', 'leiden_0.6', 'leiden_0.4', 'leiden_1.4', 'louvain_1.0', 'louvain_0.6', 'louvain_0.4', 'louvain_1.4', 'kmeans5', 'kmeans10', 'kmeans15', 'hclust_5', 'hclust_10', 'hclust_15'\n    var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std', 'highly_variable_nbatches', 'highly_variable_intersection'\n    uns: 'hclust_10_colors', 'hclust_15_colors', 'hclust_5_colors', 'hvg', 'kmeans10_colors', 'kmeans15_colors', 'kmeans5_colors', 'leiden_0.4', 'leiden_0.4_colors', 'leiden_0.6', 'leiden_0.6_colors', 'leiden_1.0', 'leiden_1.0_colors', 'leiden_1.4', 'leiden_1.4_colors', 'log1p', 'louvain_0.4', 'louvain_0.4_colors', 'louvain_0.6', 'louvain_0.6_colors', 'louvain_1.0', 'louvain_1.0_colors', 'louvain_1.4', 'louvain_1.4_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sample_colors', 't-test', 't-test_ov', 'tsne', 'umap', 'wilcoxon'\n    obsm: 'Scanorama', 'X_pca', 'X_tsne', 'X_umap'\n    varm: 'PCs'\n    layers: 'counts', 'log1p'\n    obsp: 'connectivities', 'distances'\n\n\n\n# adata.uns[\"log1p\"][\"base\"] = None\nprint(adata.shape)\n\n(7227, 19094)\n\n\n\nsc.pl.umap(adata, color=[\"louvain_0.6\"])"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_04.html#ingest-사용해-예측하기",
    "href": "posts/ipynb/scanpy_workshop_04.html#ingest-사용해-예측하기",
    "title": "Scanpy로 scRNA-seq 분석 04",
    "section": "2.3 Ingest 사용해 예측하기",
    "text": "2.3 Ingest 사용해 예측하기\n세포 유형 예측을 위한 또 다른 방법은 Ingest이며, 자세한 내용은 링크을 참조하세요. 먼저 두 데이터 집합에 동일한 유전자가 있는지 확인합니다.\n\nvar_names = adata_ref.var_names.intersection(adata.var_names)\nprint(f\"타겟 데이터의 유전자 수: {adata.shape[1]}\")\nprint(f\"레퍼런스 데이터의 유전자 수: {adata_ref.shape[1]}\")\nprint(f\"타겟과 레퍼런스 데이터에서 중첩되는 유전자 수: {len(var_names)}\")\n\n타겟 데이터의 유전자 수: 19094\n레퍼런스 데이터의 유전자 수: 1838\n타겟과 레퍼런스 데이터에서 중첩되는 유전자 수: 1674\n\n\n먼저 두 데이터 세트에 대해 동일한 유전자 세트를 사용하여 pca와 umap을 다시 실행해야 합니다. 데이터 세트에 대해 동일한 유전자 세트로 다시 실행해야 합니다.\n\nadata_ref_ = adata_ref[:, var_names]\nadata_target_ = adata[:, var_names]\n\nsc.pp.pca(adata_ref_)\nsc.pp.neighbors(adata_ref_)\nsc.tl.umap(adata_ref_)\nsc.pp.pca(adata_target_)\nsc.pp.neighbors(adata_target_)\nsc.tl.umap(adata_target_)\nsc.tl.ingest(adata_target_, adata_ref_, obs=\"louvain\")\n\n# to fix colors\nadata_target_.uns[\"louvain_colors\"] = adata_ref_.uns[\"louvain_colors\"]\nsc.pl.umap(adata_target_, color=[\"louvain\", \"louvain_0.6\"], wspace=0.5)\n\n\n\n\n\n\n\n\n\nadata_concat = adata_ref_.concatenate(adata_target_, batch_categories=[\"ref\", \"target\"])\nadata_concat.obs.louvain = adata_concat.obs.louvain.astype(\"category\")\n# fix category ordering\nadata_concat.obs.louvain.cat.reorder_categories(\n    adata_ref.obs.louvain.cat.categories,\n)\n# fix category colors\nadata_concat.uns[\"louvain_colors\"] = adata_ref.uns[\"louvain_colors\"]\nsc.pl.umap(adata_concat, color=[\"batch\", \"louvain\"])\n\n\n\n\n\n\n\n\n임베딩에서 그룹의 Density plot 하위 집합을 부분적으로 시각화하기\n\nsc.tl.embedding_density(adata_concat, groupby=\"batch\")\nsc.pl.embedding_density(adata_concat, groupby=\"batch\", ncols=2)\n\n\n\n\n\n\n\n\n각각의 leiden_0.6 클러스터에 어떤 세포 유형들이 들어 있는지 막대 그래프를 그려봅니다.\n\ndef plot_stacked_bar(data, index_col, columns_col, legend_position):\n    tmp = pd.crosstab(data.obs[index_col], data.obs[columns_col], normalize=\"index\")\n    ax = tmp.plot.bar(stacked=True, grid=False, width=0.8)\n    ax.legend(bbox_to_anchor=legend_position, loc=\"upper right\", frameon=False)\n    ax.set_xlabel(index_col)\n    ax.set_ylabel(\"Fraction of cells\")\n    ax.set_title(columns_col)\n    return ax\n\n\nax = plot_stacked_bar(adata_concat, \"louvain_0.6\", \"louvain\", (1.6, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\npred_ingest = adata_concat[adata_concat.obs[\"batch\"] == \"target\"].obs[\"louvain\"]\n# 인덱스에서 '-target' 제거\npred_ingest.index = pred_ingest.index.str.replace(\"-target\", \"\", regex=False)\n\nadata.obs[\"pred_ingest\"] = pred_ingest\n\nsc.pl.umap(adata, color=[\"louvain_0.6\", \"pred_ingest\"], ncols=2)\n\n\n\n\n\n\n\n\n위 시각화 결과를 보면 어떤 클러스터는 명확한 예측 결과를 보여지만 다른 클러스터에서는 여러 세포유형의 섞여있는 것으로 보입니다. 이런 경우에는 해당 클러스터에 속한 세포들이 어떤 유전자를 발현하는지 살펴보고 생물학적 지식을 활용해 결정해야 합니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_04.html#마커-유전자를-사용해-예측하기",
    "href": "posts/ipynb/scanpy_workshop_04.html#마커-유전자를-사용해-예측하기",
    "title": "Scanpy로 scRNA-seq 분석 04",
    "section": "2.4 마커 유전자를 사용해 예측하기",
    "text": "2.4 마커 유전자를 사용해 예측하기\n세포 유형을 예측하는데 가장 많이 사용되는 방법은 알려진 마커 유전자 목록과 비교해보는 방법입니다. 이 방법을 위해서는 신뢰할 수 있고 분석중인 샘플과 연관된 마커 유전자 목록이 필요합니다. 여기서는 사람 세포의 마커 유전자 목록을 불러와 사용하겠습니다.\n\n# 사람 세포 마커 유전자 목록 불러오기\npath_file = \"./input/human_cell_markers.txt\"\ndf = pd.read_table(path_file)\nprint(df.shape)\n\n(2868, 15)\n\n\n파일에는 2868개의 유전자가 포함되어 있네요. 세포 유형 예측을 위해 약간의 수정을 통해 gene_dict 객체를 만들어 줍니다.\n\n# 세포 유형별 유전자 수에 대한 필터링\ndf[\"nG\"] = df.geneSymbol.str.split(\",\").str.len()\n\ndf = df[df[\"nG\"] &gt; 5]\ndf = df[df[\"nG\"] &lt; 100]\ndf = df[df[\"cancerType\"] == \"Normal\"]\n\ndf.index = df.cellName\ngene_dict = df.geneSymbol.str.split(\",\").to_dict()\n\ndf.head()\n\n\n\n\n\n\n\n\nspeciesType\ntissueType\nUberonOntologyID\ncancerType\ncellType\ncellName\nCellOntologyID\ncellMarker\ngeneSymbol\ngeneID\nproteinName\nproteinID\nmarkerResource\nPMID\nCompany\nnG\n\n\n\n\n13\nHuman\nLiver\nUBERON_0002107\nNormal\nNormal cell\nKupffer cell\nCL_0000091\nCD11b, CD14, CD18, CD32, CD68\nITGAM, CD14, ITGB2, [FCGR2A, FCGR2B, FCGR2C], ...\n3684, 929, 3689, [2212, 2213, 9103], 968\nITAM, CD14, ITB2, [FCG2A, FCG2B, FCG2C], CD68\nP11215, P08571, P05107, [P12318, P31994, P3199...\nExperiment\n17692868\nNaN\n7.0\n\n\n22\nHuman\nPeripheral blood\nUBERON_0005408\nNormal\nNormal cell\nMonocyte\nCL_0000576\nCD11b, CD16, CD19, CD2, CD3, CD56, CD94, NKG2A\nITGAM, [FCGR3A, FCGR3B], CD19, CD2, [CD3D, CD3...\n3684, [2214, 2215], 930, 914, [915, 916, 917],...\nITAM, [FCG3A, FCG3B], CD19, CD2, [CD3D, CD3E, ...\nP11215, [P08637, O75015], P15391, P06729, [P04...\nExperiment\n29610856\nNaN\n11.0\n\n\n23\nHuman\nBlood\nUBERON_0000178\nNormal\nNormal cell\nNatural killer cell\nCL_0000623\nCD11b, CD16, CD19, CD2, CD3, CD56, CD94, NKG2A\nITGAM, [FCGR3A, FCGR3B], CD19, CD2, [CD3D, CD3...\n3684, [2214, 2215], 930, 914, [915, 916, 917],...\nITAM, [FCG3A, FCG3B], CD19, CD2, [CD3D, CD3E, ...\nP11215, [P08637, O75015], P15391, P06729, [P04...\nExperiment\n29610856\nNaN\n11.0\n\n\n25\nHuman\nGingiva\nUBERON_0001828\nNormal\nNormal cell\nMesenchymal stem cell\nCL_0000134\nCD105, CD166, CD29, CD44, CD49e, CD73, CD90\nENG, ALCAM, ITGB1, CD44, ITGA5, NT5E, THY1\n2022, 214, 3688, 960, 3678, 4907, 7070\nEGLN, CD166, ITB1, CD44, ITA5, 5NTD, THY1\nP17813, Q13740, P05556, P16070, P08648, P21589...\nExperiment\n29604386\nNaN\n7.0\n\n\n28\nHuman\nPeripheral blood\nUBERON_0005408\nNormal\nNormal cell\nMonocyte\nCL_0000576\nCD14, CD19, CD20, CD27, CD3, CD38, IgA, IgG\nCD14, CD19, MS4A1, CD27, [CD3D, CD3E, CD3G], C...\n929, 930, 931, 939, [915, 916, 917], 952, 973,...\nCD14, CD19, CD20, CD27, [CD3D, CD3E, CD3G], CD...\nP08571, P15391, P11836, P26842, [P04234, P0776...\nExperiment\n29579044\nNaN\n10.0\n\n\n\n\n\n\n\n이제 gseapy.enrichr 함수를 사용해 위에서 만든 gene_dict과 louvain_0.6의 DEG 목록과 비교해 세포 유형을 예측합니다.\n\n# 클러스터별로 DEG 분석\nsc.tl.rank_genes_groups(adata, \"louvain_0.6\", method=\"wilcoxon\", key_added=\"wilcoxon\")\n\ngsea_res = {}\npred = {}\n\nfor cl in adata.obs[\"louvain_0.6\"].cat.categories.tolist():\n    glist = (\n        sc.get.rank_genes_groups_df(adata, group=cl, key=\"wilcoxon\")[\"names\"]\n        .squeeze()\n        .str.strip()\n        .tolist()\n    )\n    enr_res = gseapy.enrichr(\n        gene_list=glist[:300],\n        organism=\"Human\",\n        gene_sets=gene_dict,\n        background=adata.layers[\"counts\"].shape[1],\n        cutoff=1,\n    )\n    if enr_res.results.shape[0] == 0:\n        pred[cl] = \"Unass\"\n    else:\n        enr_res.results.sort_values(by=\"P-value\", axis=0, ascending=True, inplace=True)\n        gsea_res[cl] = enr_res\n        pred[cl] = enr_res.results[\"Term\"][0]\n\npred\n\n{'0': 'Circulating fetal cell',\n '1': 'CD4+ T cell',\n '2': 'CD4+ T cell',\n '3': 'CD8+ T cell',\n '4': 'CD1C+_A dendritic cell',\n '5': 'B cell',\n '6': 'CD16+ dendritic cell',\n '7': 'B cell',\n '8': 'Circulating fetal cell',\n '9': 'Circulating fetal cell',\n '10': 'Circulating fetal cell',\n '11': 'Circulating fetal cell'}\n\n\n예측된 결과를 시각화로 살펴봅니다.\n\nprediction = [pred[x] for x in adata.obs[\"louvain_0.6\"]]\nadata.obs[\"GS_overlap_pred\"] = prediction\n\nsc.pl.umap(adata, color=\"GS_overlap_pred\")"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_04.html#세포유형-지정하기",
    "href": "posts/ipynb/scanpy_workshop_04.html#세포유형-지정하기",
    "title": "Scanpy로 scRNA-seq 분석 04",
    "section": "2.5 세포유형 지정하기",
    "text": "2.5 세포유형 지정하기\n위에서 얻은 결과를 통해 세포 유형을 지정하고 시각화 해봅니다.\n\n# 임의로 주석\ncluster_annotations = {\n    \"0\": \"CD14+ Monocyte\",\n    \"1\": \"CD4T cell\",\n    \"2\": \"CD4T cell\",\n    \"3\": \"CD8T cell\",\n    \"4\": \"NK cell\",\n    \"5\": \"B cell\",\n    \"6\": \"CD14+ Monocyte\",\n    \"7\": \"B cell\",\n    \"8\": \"FCGR3A+ Monocyte\",\n    \"9\": \"CD4T cell\",\n    \"10\": \"Circulating fetal cell\",\n    \"11\": \"Circulating fetal cell\",\n}\n\nadata.obs[\"cell_type\"] = adata.obs[\"louvain_0.6\"].map(cluster_annotations)\n\n# 주석 확인\nsc.pl.umap(adata, color=[\"louvain_0.6\", \"cell_type\"])"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_04.html#over-representation-analysis-ora",
    "href": "posts/ipynb/scanpy_workshop_04.html#over-representation-analysis-ora",
    "title": "Scanpy로 scRNA-seq 분석 04",
    "section": "3.1 Over-representation analysis (ORA)",
    "text": "3.1 Over-representation analysis (ORA)\nORA는 주어진 유전자 목록에서 특정 기능이나 경로에 속하는 유전자들이 과발현 되는지를 평가하는 방법입니다. ORA는 특정 생물학적 기능이나 경로와 관련된 유전자들이 관심 있는 유전자 목록에서 통계적으로 유의미하게 많이 포함되어 있는지 확인합니다. 이를 통해 특정 생물학적 기능이나 경로가 실험 조건에서 중요한 역할을 하는지 파악할 수 있습니다. ORA을 하면 유의미성을 나타내는 p-value와 함께 과도하게 나타나는 유전자 집합 결과를 얻습니다. 다만 유전자 발현 수준이나 경로의 구조를 고려하지 않는다는 한계점이 있습니다.\n이제 예시로 코로나 환자의 DEG를 사용하여 면역 반응, 염증 경로, 바이러스 복제와 관련된 경로들이 유의미하게 변화하는지를 살펴보겠습니다.\n\nsc.tl.rank_genes_groups(adata, \"type\", method=\"wilcoxon\", key_added=\"wilcoxon\")\nsc.pl.rank_genes_groups(\n    adata,\n    n_genes=25,\n    sharey=False,\n    key=\"wilcoxon\",\n    ncols=3,\n)\n\n\n\n\n\n\n\n\n앞서 구한 DEG 목록을 gseapy에 넘겨서 ORA 분석을 진행합니다. 많이 사용되는 몇가지 데이터베이스 목록은 아래와 같습니다.\n\nGO_Biological_Process_2018\nKEGG_2019_Human\nKEGG_2019_Mouse\nWikiPathways_2019_Human\nWikiPathways_2019_Mouse\n\n\n# 가능한 데이터베이스 : ‘Human’, ‘Mouse’, ‘Yeast’, ‘Fly’, ‘Fish’, ‘Worm’\ngene_set_names = gseapy.get_library_name(organism=\"Human\")\n\n# ?gseapy.enrichr\nglist = (\n    sc.get.rank_genes_groups_df(\n        adata, group=\"Covid\", key=\"wilcoxon\", log2fc_min=0.25, pval_cutoff=0.05\n    )[\"names\"]\n    .squeeze()\n    .str.strip()\n    .tolist()\n)\n\n간단한 막대그래프를 사용해 결과를 시각화합니다.\n\ndef plot_ora(\n    glist,\n    gene_set=\"GO_Biological_Process_2018\",\n    size=(4, 4),\n    color=\"darkred\",\n    truncate=True,\n):\n    enr_res = gseapy.enrichr(\n        gene_list=glist,\n        organism=\"Human\",\n        gene_sets=gene_set,\n        cutoff=0.5,\n    )\n    if truncate:\n        # True 시 뒤의 코드명 일부 제거.\n        enr_res.res2d[\"Term\"] = enr_res.res2d[\"Term\"].apply(lambda x: \" \".join(x.split(\" \")[:-1]))\n    ax = gseapy.barplot(\n        enr_res.res2d,\n        title=gene_set,\n        figsize=size,\n        color=color,\n    )\n    ax.grid(False)\n    return ax\n\n\nax = plot_ora(\n    glist,\n    gene_set=\"GO_Biological_Process_2018\",\n    size=(4, 4),\n    color=\"darkred\",\n    truncate=True,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nax = plot_ora(glist, gene_set=\"KEGG_2019_Human\", size=(5, 4), color=\"darkred\", truncate=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nax = plot_ora(\n    glist,\n    gene_set=\"WikiPathways_2019_Human\",\n    size=(4, 4),\n    color=\"darkred\",\n    truncate=True,\n)\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_04.html#functional-class-scoring-fcs-분석",
    "href": "posts/ipynb/scanpy_workshop_04.html#functional-class-scoring-fcs-분석",
    "title": "Scanpy로 scRNA-seq 분석 04",
    "section": "3.2 Functional Class Scoring (FCS) 분석",
    "text": "3.2 Functional Class Scoring (FCS) 분석\nORA외에도 Functional Class Scoring(FCS) 분석을 진행 할 수 있습니다. FCS는 유전자 집합 내 발현 변화 또한 고려한 방법입니다. FCS 분석 중 유명한 방법이 유전자 세트 강화 분석(GSEA)입니다. GSEA는 각 유전자에 발현 변화(예: 로그 폴드 변화)를 기준으로 점수를 부여하고, 이 점수를 유전자 집합 내에서 합산하거나 평균을 구한 후, 무작위 분포(퍼뮤테이션 기반 또는 파라메트릭)와 비교합니다. 그 결과 (일반적으로 폴드 변화를 기반으로) 순위가 매겨진 유전자 목록을 점수화하고 순열 테스트를 계산하여 특정 유전자 세트가 상향 조절된 유전자에 더 많이 존재하는지, 하향 조절된 유전자에 더 많이 존재하는지, 아니면 차등 조절되지 않는지를 확인할 수 있습니다. 이 방법의 장점은 발현 수준을 고려하기 때문에 미묘한 효과도 포착할 수 있다는 것입니다. 다만 집합내 유전자들이 서로 독립이라는 가정해야 한다는 한계점이 있습니다.\nGSEA를 수행하기 위해서는 모든 차등 발현 유전자(DEG)와 그 로그 폴드 체인지를 포함한 테이블이 필요합니다.\n\n# Available databases : ‘Human’, ‘Mouse’, ‘Yeast’, ‘Fly’, ‘Fish’, ‘Worm’\ngene_set_names = gseapy.get_library_name(organism=\"Human\")\n\ngene_rank = sc.get.rank_genes_groups_df(adata, group=\"Covid\", key=\"wilcoxon\")[\n    [\"names\", \"logfoldchanges\"]\n]\ngene_rank.sort_values(by=[\"logfoldchanges\"], inplace=True, ascending=False)\n\n# 계산_qc_metrics는 유전자당 세포 수를 계산합니다.\nsc.pp.calculate_qc_metrics(adata, percent_top=None, log1p=False, inplace=True)\n\n# 최소 30개 이상의 세포에서 발현되는 유전자를 필터링합니다.\ngene_rank = gene_rank[gene_rank[\"names\"].isin(adata.var_names[adata.var.n_cells_by_counts &gt; 30])]\n\ngene_rank\n\n\n\n\n\n\n\n\nnames\nlogfoldchanges\n\n\n\n\n2800\nCCL7\n24.208344\n\n\n2868\nIL1A\n24.117874\n\n\n2360\nCYP19A1\n23.754789\n\n\n2903\nPRTN3\n23.376261\n\n\n1107\nNXF3\n9.236358\n\n\n...\n...\n...\n\n\n16567\nENHO\n-4.197762\n\n\n15497\nLYNX1\n-4.719684\n\n\n15466\nC5orf17\n-5.105195\n\n\n17113\nLYPD2\n-5.713926\n\n\n18516\nAC004556.1\n-26.306053\n\n\n\n\n13553 rows × 2 columns\n\n\n\n다음으로 GSEA를 실행합니다. 그러면 여러 경로에 대한 정보가 포함된 테이블이 생성됩니다. 이 테이블을 p-value 또는 정규화된 강화 점수(NES) 기준으로 정렬하고 필터링하여 상위 경로만 시각화할 수 있습니다. 이 분석은 실험 조건에서 중요한 역할을 하는 주요 경로를 식별하고 분석할 수 있습니다.\n\nres = gseapy.prerank(rnk=gene_rank, gene_sets=\"KEGG_2021_Human\")\n\nterms = res.res2d.Term\nprint(terms[:10])\n\n0                              IL-17 signaling pathway\n1                                 Rheumatoid arthritis\n2    Viral protein interaction with cytokine and cy...\n3               Cytokine-cytokine receptor interaction\n4    AGE-RAGE signaling pathway in diabetic complic...\n5                          Chemokine signaling pathway\n6                                           Amoebiasis\n7               Fluid shear stress and atherosclerosis\n8                           Osteoclast differentiation\n9                                            Pertussis\nName: Term, dtype: object\n\n\n\ngseapy.gseaplot(rank_metric=res.ranking, term=terms[0], **res.results[terms[0]])\nplt.show()\n\n\n\n\n\n\n\n\n위 결과를 보면 KEGG 데이터베이스의 IL-17 signaling pathway 경로가 과발현 되어 있다는 것을 알 수있습니다. IL-17은 염증 반응을 조절하는 중요한 사이토카인입니다. 이 경로의 상향 조절은 강한 염증 반응을 나타내며, 면역계가 병원체와 싸우는 과정에서 중요한 역할을 합니다. 따라서 이런 경로의 상향 조절은 염증, 면역 반응, 감염 및 대사질환과 관련된 중요한 생물학적 변화를 시사합니다.\n마지막으로, 다른 분석을 위해 데이터를 저장해 보겠습니다.\n\nsave_file = \"./output/covid/results/scanpy_covid_annot.h5ad\"\nadata.write_h5ad(save_file, compression=\"gzip\")"
  },
  {
    "objectID": "posts/ipynb/scanpy_Id2symbol.html#pybiomart",
    "href": "posts/ipynb/scanpy_Id2symbol.html#pybiomart",
    "title": "Scanpy로 gene_id를 gene_symbol로 변경하기",
    "section": "1.1 pybiomart?",
    "text": "1.1 pybiomart?\npybiomart의 목적은 파이썬에서 BioMart 데이터베이스를 쉽게 접근할 수 있게 간단한 인터페이스를 제공하는 것으로 R에서 biomaRt와 유사한 기능을 제공하는 것을 목표로 합니다. 다음 콘다 명령어를 통해 설치할 수 있습니다.\nconda install pybiomart"
  },
  {
    "objectID": "posts/ipynb/scanpy_Id2symbol.html#anndata-객체에-대하여",
    "href": "posts/ipynb/scanpy_Id2symbol.html#anndata-객체에-대하여",
    "title": "Scanpy로 gene_id를 gene_symbol로 변경하기",
    "section": "2.1 AnnData 객체에 대하여",
    "text": "2.1 AnnData 객체에 대하여\nScanpy에서 사용되는 AnnData 객체는 개별 세포/유전자 주석 데이터를 저장하는 데이터 구조입니다.AnnData 객체는 크게 네 가지 주요 구성 요소로 구성됩니다.\n\nadata.X는 세포 x 유전자 레이아웃에서 유전자 발현 카운트 정보를 저장.\nadata.obs는 세포 주석을 가리키며, 세포 유형, 총 카운트, 클러스터 ID 등이 Pandas 데이터프레임 형태로 저장. 일반적으로 색인은 세포 바코드를 사용.\nadata.var는 유전자 주석을 가리키며, 유전자 ID, 유전자 심볼, 해당 유전자가 높은 변동성을 가지고 있는지 여부 등이 데이터프레임으로 저장.\nadata.uns는 세포나 유전자와 직접적으로 연결되지 않은 비구조적인 데이터를 포함. 예를 들면 KNN, PCA값 등이 있음."
  },
  {
    "objectID": "posts/ipynb/scanpy_Id2symbol.html#세포-주석-살펴보기",
    "href": "posts/ipynb/scanpy_Id2symbol.html#세포-주석-살펴보기",
    "title": "Scanpy로 gene_id를 gene_symbol로 변경하기",
    "section": "2.2 세포 주석 살펴보기",
    "text": "2.2 세포 주석 살펴보기\n\nadata.obs.head()\n\n\n\n\n\n\n\n\nSample Characteristic[organism]\nSample Characteristic Ontology Term[organism]\nSample Characteristic[individual]\nSample Characteristic Ontology Term[individual]\nSample Characteristic[ethnic group]\nSample Characteristic Ontology Term[ethnic group]\nSample Characteristic[sex]\nSample Characteristic Ontology Term[sex]\nSample Characteristic[age]\nSample Characteristic Ontology Term[age]\n...\nSample Characteristic[clinical history]\nSample Characteristic Ontology Term[clinical history]\nFactor Value[clinical history]\nFactor Value Ontology Term[clinical history]\nFactor Value[sampling time point]\nFactor Value Ontology Term[sampling time point]\nFactor Value[inferred cell type - ontology labels]\nFactor Value Ontology Term[inferred cell type - ontology labels]\nFactor Value[inferred cell type - authors labels]\nFactor Value Ontology Term[inferred cell type - authors labels]\n\n\n\n\nSAMEA7198212-AAAACCGCACAAGCCC\nHomo sapiens\nhttp://purl.obolibrary.org/obo/NCBITaxon_9606\npatient1\nNaN\nThai\nNaN\nmale\nhttp://purl.obolibrary.org/obo/PATO_0000384\n35 year\nNaN\n...\nsecondary DENV-4 infection\nNaN\nsecondary DENV-4 infection\nNaN\none day before defervescence\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSAMEA7198212-AAAACCGTCATTGCGA\nHomo sapiens\nhttp://purl.obolibrary.org/obo/NCBITaxon_9606\npatient1\nNaN\nThai\nNaN\nmale\nhttp://purl.obolibrary.org/obo/PATO_0000384\n35 year\nNaN\n...\nsecondary DENV-4 infection\nNaN\nsecondary DENV-4 infection\nNaN\none day before defervescence\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSAMEA7198212-AAAACTCCAAGGTGTG\nHomo sapiens\nhttp://purl.obolibrary.org/obo/NCBITaxon_9606\npatient1\nNaN\nThai\nNaN\nmale\nhttp://purl.obolibrary.org/obo/PATO_0000384\n35 year\nNaN\n...\nsecondary DENV-4 infection\nNaN\nsecondary DENV-4 infection\nNaN\none day before defervescence\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSAMEA7198212-AAAACTCCAGCGTCCA\nHomo sapiens\nhttp://purl.obolibrary.org/obo/NCBITaxon_9606\npatient1\nNaN\nThai\nNaN\nmale\nhttp://purl.obolibrary.org/obo/PATO_0000384\n35 year\nNaN\n...\nsecondary DENV-4 infection\nNaN\nsecondary DENV-4 infection\nNaN\none day before defervescence\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSAMEA7198212-AAAATGATCCTAGGGC\nHomo sapiens\nhttp://purl.obolibrary.org/obo/NCBITaxon_9606\npatient1\nNaN\nThai\nNaN\nmale\nhttp://purl.obolibrary.org/obo/PATO_0000384\n35 year\nNaN\n...\nsecondary DENV-4 infection\nNaN\nsecondary DENV-4 infection\nNaN\none day before defervescence\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 28 columns"
  },
  {
    "objectID": "posts/ipynb/R_seurat_RDS2mtx.html#umap-그리기",
    "href": "posts/ipynb/R_seurat_RDS2mtx.html#umap-그리기",
    "title": "RDS 객체를 10X MEX 형식으로 저장하기",
    "section": "4.1 UMAP 그리기",
    "text": "4.1 UMAP 그리기\nUMAP을 그리기 위해 아래 코드들을 실행합니다.\n\n# 데이터 정규화\nseurat_obj &lt;- NormalizeData(object = seurat_obj, verbose = FALSE)\n\n# 변수 특징 찾기\nseurat_obj &lt;- FindVariableFeatures(object = seurat_obj, verbose = FALSE)\n\n# 데이터 스케일링\nseurat_obj &lt;- ScaleData(object = seurat_obj, verbose = FALSE)\n\n# 주성분 분석 실행\nseurat_obj &lt;- RunPCA(\n    object = seurat_obj,\n    features = VariableFeatures(object = seurat_obj),\n    verbose = FALSE\n)\n\n# 이웃 찾기\nseurat_obj &lt;- FindNeighbors(\n    object = seurat_obj, dims = 1:10, verbose = FALSE\n)\n\n# 클러스터 찾기\nseurat_obj &lt;- FindClusters(\n    object = seurat_obj, resolution = 0.5, verbose = FALSE\n)\n\n# UMAP 실행\nseurat_obj &lt;- RunUMAP(\n    object = seurat_obj, dims = 1:10, verbose = FALSE\n)\n\n\nDimPlot(\n    object = seurat_obj,\n    label = TRUE,          # 각 데이터 포인트에 레이블 표시 여부\n    reduction = \"umap\"    # UMAP 데이터 사용\n) + NoLegend()            # 범례 숨기기\n\n\n\n\n\n\n\n\n메타 데이터도 살펴봅니다.\n\nhead(seurat_obj[[]])\n\n\nA data.frame: 6 × 6\n\n\n\norig.ident\nnCount_RNA\nnFeature_RNA\npercent.mt\nRNA_snn_res.0.5\nseurat_clusters\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\nAAACATACAACCAC-1\npbmc3k\n2419\n779\n3.0177759\n2\n2\n\n\nAAACATTGAGCTAC-1\npbmc3k\n4903\n1352\n3.7935958\n3\n3\n\n\nAAACATTGATCAGC-1\npbmc3k\n3147\n1129\n0.8897363\n2\n2\n\n\nAAACCGTGCTTCCG-1\npbmc3k\n2639\n960\n1.7430845\n1\n1\n\n\nAAACCGTGTATGCG-1\npbmc3k\n980\n521\n1.2244898\n6\n6\n\n\nAAACGCACTGGTAC-1\npbmc3k\n2163\n781\n1.6643551\n2\n2\n\n\n\n\n\nseurat_clusters들이 숫자로 되어 있습니다. 이것을 cell type annotation작을 통해 아래와 같이 값을 바꿔줍니다.\n\n# 클러스터 이름 재할당\nseurat_obj[[]] &lt;- seurat_obj[[]] %&gt;%\n  mutate(seurat_clusters = recode(\n    seurat_clusters,\n    `0` = \"Naive CD4 T\",\n    `1` = \"CD14+ Mono\",\n    `2` = \"Memory CD4 T\",\n    `3` = \"B\",\n    `4` = \"CD8 T\",\n    `5` = \"FCGR3A+ Mono\",\n    `6` = \"NK\",\n    `7` = \"DC\",\n    `8` = \"Platelet\"\n  ))\n\n# 수정된 클러스터 정보 확인\nhead(seurat_obj[[]])\n\n\nA data.frame: 6 × 6\n\n\n\norig.ident\nnCount_RNA\nnFeature_RNA\npercent.mt\nRNA_snn_res.0.5\nseurat_clusters\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\nAAACATACAACCAC-1\npbmc3k\n2419\n779\n3.0177759\n2\nMemory CD4 T\n\n\nAAACATTGAGCTAC-1\npbmc3k\n4903\n1352\n3.7935958\n3\nB\n\n\nAAACATTGATCAGC-1\npbmc3k\n3147\n1129\n0.8897363\n2\nMemory CD4 T\n\n\nAAACCGTGCTTCCG-1\npbmc3k\n2639\n960\n1.7430845\n1\nCD14+ Mono\n\n\nAAACCGTGTATGCG-1\npbmc3k\n980\n521\n1.2244898\n6\nNK\n\n\nAAACGCACTGGTAC-1\npbmc3k\n2163\n781\n1.6643551\n2\nMemory CD4 T\n\n\n\n\n\n다시 UMAP을 그려보겠습니다.\n\nDimPlot(\n  seurat_obj,\n  reduction = \"umap\",\n  label=TRUE,\n  group.by=\"seurat_clusters\") + NoLegend()\n\n\n\n\n\n\n\n\n이제 RDS파일로 객체를 저장합니다.\n\noutput_path &lt;- \"../output/pbmc3k/\"\n\n# Seurat 객체를 RDS 파일로 저장\nsaveRDS(\n    seurat_obj,\n    file = paste0(output_path, \"pbmc3k.rds\")\n)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_05.html",
    "href": "posts/ipynb/scanpy_workshop_05.html",
    "title": "Scanpy로 scRNA-seq 분석 05",
    "section": "",
    "text": "클러스터링으로 scRNA-seq 데이터를 여러 그룹으로 나눈 뒤에는 해당 그룹이 어떤 세포인지 아는 것은 분석 결과를 해석할 때 중요합니다. 세포 유형을 예측하는 방법에는 크게 두 가지 방법이 있습니다. 레퍼런스 scRNA-seq 데이터를 사용해 유사도를 비교하는 방법과 알려진 마커 유전자를 사용해 직접 세포 유형을 지정하는 방법입니다. 레퍼런스 데이터는 이미 세포 유형의 전사체 표현 데이터를 말합니다. 이 데이터를 사용하여 클러스터링된 세포들의 전사체 표현 패턴을 레퍼런스 데이터와 비교하여 가장 유사한 세포 유형을 예측합니다. 여기서는 가장 간단한 ingest 방법을 사용해봅니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_05.html#over-representation-analysis-ora",
    "href": "posts/ipynb/scanpy_workshop_05.html#over-representation-analysis-ora",
    "title": "Scanpy로 scRNA-seq 분석 05",
    "section": "7.1 Over-representation analysis (ORA)",
    "text": "7.1 Over-representation analysis (ORA)\nORA는 주어진 유전자 목록에서 특정 기능이나 경로에 속하는 유전자들이 과발현 되는지를 평가하는 방법입니다. ORA는 특정 생물학적 기능이나 경로와 관련된 유전자들이 관심 있는 유전자 목록에서 통계적으로 유의미하게 많이 포함되어 있는지 확인합니다. 이를 통해 특정 생물학적 기능이나 경로가 실험 조건에서 중요한 역할을 하는지 파악할 수 있습니다. ORA을 하면 유의미성을 나타내는 p-value와 함께 과도하게 나타나는 유전자 집합 결과를 얻습니다. 다만 유전자 발현 수준이나 경로의 구조를 고려하지 않는다는 한계점이 있습니다.\n이제 예시로 코로나 환자의 DEG를 사용하여 면역 반응, 염증 경로, 바이러스 복제와 관련된 경로들이 유의미하게 변화하는지를 살펴보겠습니다.\n\nsc.tl.rank_genes_groups(adata, \"type\", method=\"wilcoxon\", key_added=\"wilcoxon\")\nsc.pl.rank_genes_groups(\n    adata,\n    n_genes=25,\n    sharey=False,\n    key=\"wilcoxon\",\n    ncols=3,\n)\n\n\n\n\n\n\n\n\n앞서 구한 DEG 목록을 gseapy에 넘겨서 ORA 분석을 진행합니다. 많이 사용되는 몇가지 데이터베이스 목록은 아래와 같습니다.\n\nGO_Biological_Process_2018\nKEGG_2019_Human\nKEGG_2019_Mouse\nWikiPathways_2019_Human\nWikiPathways_2019_Mouse\n\n\n# 가능한 데이터베이스 : ‘Human’, ‘Mouse’, ‘Yeast’, ‘Fly’, ‘Fish’, ‘Worm’\ngene_set_names = gseapy.get_library_name(organism=\"Human\")\n\n# ?gseapy.enrichr\nglist = (\n    sc.get.rank_genes_groups_df(\n        adata, group=\"Covid\", key=\"wilcoxon\", log2fc_min=0.25, pval_cutoff=0.05\n    )[\"names\"]\n    .squeeze()\n    .str.strip()\n    .tolist()\n)\n\n간단한 막대그래프를 사용해 결과를 시각화합니다.\n\ndef plot_ora(\n    glist,\n    gene_set=\"GO_Biological_Process_2018\",\n    size=(4, 4),\n    color=\"darkred\",\n    truncate=True,\n):\n    enr_res = gseapy.enrichr(\n        gene_list=glist,\n        organism=\"Human\",\n        gene_sets=gene_set,\n        cutoff=0.5,\n    )\n    if truncate:\n        # True 시 뒤의 코드명 일부 제거.\n        enr_res.res2d[\"Term\"] = enr_res.res2d[\"Term\"].apply(lambda x: \" \".join(x.split(\" \")[:-1]))\n    ax = gseapy.barplot(\n        enr_res.res2d,\n        title=gene_set,\n        figsize=size,\n        color=color,\n    )\n    ax.grid(False)\n    return ax\n\n\nax = plot_ora(\n    glist,\n    gene_set=\"GO_Biological_Process_2018\",\n    size=(4, 4),\n    color=\"darkred\",\n    truncate=True,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nax = plot_ora(glist, gene_set=\"KEGG_2019_Human\", size=(5, 4), color=\"darkred\", truncate=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nax = plot_ora(\n    glist,\n    gene_set=\"WikiPathways_2019_Human\",\n    size=(4, 4),\n    color=\"darkred\",\n    truncate=True,\n)\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_05.html#functional-class-scoring-fcs-분석",
    "href": "posts/ipynb/scanpy_workshop_05.html#functional-class-scoring-fcs-분석",
    "title": "Scanpy로 scRNA-seq 분석 05",
    "section": "7.2 Functional Class Scoring (FCS) 분석",
    "text": "7.2 Functional Class Scoring (FCS) 분석\nORA외에도 Functional Class Scoring(FCS) 분석을 진행 할 수 있습니다. FCS는 유전자 집합 내 발현 변화 또한 고려한 방법입니다. FCS 분석 중 유명한 방법이 유전자 세트 강화 분석(GSEA)입니다. GSEA는 각 유전자에 발현 변화(예: 로그 폴드 변화)를 기준으로 점수를 부여하고, 이 점수를 유전자 집합 내에서 합산하거나 평균을 구한 후, 무작위 분포(퍼뮤테이션 기반 또는 파라메트릭)와 비교합니다. 그 결과 (일반적으로 폴드 변화를 기반으로) 순위가 매겨진 유전자 목록을 점수화하고 순열 테스트를 계산하여 특정 유전자 세트가 상향 조절된 유전자에 더 많이 존재하는지, 하향 조절된 유전자에 더 많이 존재하는지, 아니면 차등 조절되지 않는지를 확인할 수 있습니다. 이 방법의 장점은 발현 수준을 고려하기 때문에 미묘한 효과도 포착할 수 있다는 것입니다. 다만 집합내 유전자들이 서로 독립이라는 가정해야 한다는 한계점이 있습니다.\nGSEA를 수행하기 위해서는 모든 차등 발현 유전자(DEG)와 그 로그 폴드 체인지를 포함한 테이블이 필요합니다.\n\n# Available databases : ‘Human’, ‘Mouse’, ‘Yeast’, ‘Fly’, ‘Fish’, ‘Worm’\ngene_set_names = gseapy.get_library_name(organism=\"Human\")\n\ngene_rank = sc.get.rank_genes_groups_df(adata, group=\"Covid\", key=\"wilcoxon\")[\n    [\"names\", \"logfoldchanges\"]\n]\ngene_rank.sort_values(by=[\"logfoldchanges\"], inplace=True, ascending=False)\n\n# 계산_qc_metrics는 유전자당 세포 수를 계산합니다.\nsc.pp.calculate_qc_metrics(adata, percent_top=None, log1p=False, inplace=True)\n\n# 최소 30개 이상의 세포에서 발현되는 유전자를 필터링합니다.\ngene_rank = gene_rank[gene_rank[\"names\"].isin(adata.var_names[adata.var.n_cells_by_counts &gt; 30])]\n\ngene_rank\n\n\n\n\n\n\n\n\nnames\nlogfoldchanges\n\n\n\n\n2800\nCCL7\n24.208344\n\n\n2868\nIL1A\n24.117874\n\n\n2360\nCYP19A1\n23.754789\n\n\n2903\nPRTN3\n23.376261\n\n\n1107\nNXF3\n9.236358\n\n\n...\n...\n...\n\n\n16567\nENHO\n-4.197762\n\n\n15497\nLYNX1\n-4.719684\n\n\n15466\nC5orf17\n-5.105195\n\n\n17113\nLYPD2\n-5.713926\n\n\n18516\nAC004556.1\n-26.306053\n\n\n\n\n13553 rows × 2 columns\n\n\n\n다음으로 GSEA를 실행합니다. 그러면 여러 경로에 대한 정보가 포함된 테이블이 생성됩니다. 이 테이블을 p-value 또는 정규화된 강화 점수(NES) 기준으로 정렬하고 필터링하여 상위 경로만 시각화할 수 있습니다. 이 분석은 실험 조건에서 중요한 역할을 하는 주요 경로를 식별하고 분석할 수 있습니다.\n\nres = gseapy.prerank(rnk=gene_rank, gene_sets=\"KEGG_2021_Human\")\n\nterms = res.res2d.Term\nprint(terms[:10])\n\n0                              IL-17 signaling pathway\n1                                 Rheumatoid arthritis\n2    Viral protein interaction with cytokine and cy...\n3               Cytokine-cytokine receptor interaction\n4    AGE-RAGE signaling pathway in diabetic complic...\n5                          Chemokine signaling pathway\n6                                           Amoebiasis\n7               Fluid shear stress and atherosclerosis\n8                           Osteoclast differentiation\n9                                            Pertussis\nName: Term, dtype: object\n\n\n\ngseapy.gseaplot(rank_metric=res.ranking, term=terms[0], **res.results[terms[0]])\nplt.show()\n\n\n\n\n\n\n\n\n위 결과를 보면 KEGG 데이터베이스의 IL-17 signaling pathway 경로가 과발현 되어 있다는 것을 알 수있습니다. IL-17은 염증 반응을 조절하는 중요한 사이토카인입니다. 이 경로의 상향 조절은 강한 염증 반응을 나타내며, 면역계가 병원체와 싸우는 과정에서 중요한 역할을 합니다. 따라서 이런 경로의 상향 조절은 염증, 면역 반응, 감염 및 대사질환과 관련된 중요한 생물학적 변화를 시사합니다.\n마지막으로, 다른 분석을 위해 데이터를 저장해 보겠습니다.\n\nsave_file = \"./output/covid/results/scanpy_covid_annot.h5ad\"\nadata.write_h5ad(save_file, compression=\"gzip\")"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_07.html",
    "href": "posts/ipynb/scanpy_workshop_07.html",
    "title": "Scanpy로 scRNA-seq 분석 07",
    "section": "",
    "text": "공간 전사체 분석은 종양 연구에서 매우 중요한데 종양 미세환경 내의 세포들(종양 세포, 면역세포, 혈관세포 등)과 그들의 상호작용을 이해하는 데 도움이 되기 때문입니다. 특히 공간에서 일어나는 유전자 발현 패턴이 특정 치료제나 면역 치료에 대한 반응성을 예측할 수 있다고 믿어지기 때문에 더욱 더 중요해지고 있습니다. 이번 글에서는 공간 전사체(이하 Visium)데이터와 scRNA-seq 데이터를 결합하는 방법을 위주로 살펴보겠습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_07.html#스팟-필터링",
    "href": "posts/ipynb/scanpy_workshop_07.html#스팟-필터링",
    "title": "Scanpy로 scRNA-seq 분석 07",
    "section": "3.1 스팟 필터링",
    "text": "3.1 스팟 필터링\n미토콘드리아 판독률이 25% 미만이고 헤모글로빈(hb) 판독률이 20% 미만 그리고 검출된 유전자가 1,000개 이상인 모든 스팟을 선택합니다. 데이터에 적합한 필터링 기준은 사전 지식을 바탕으로 직접 판단해야 합니다.\n\nkeep = (\n    (adata.obs[\"pct_counts_hb\"] &lt; 20)\n    & (adata.obs[\"pct_counts_mt\"] &lt; 25)\n    & (adata.obs[\"n_genes_by_counts\"] &gt; 1000)\n)\nprint(sum(keep))\n\nadata = adata[keep, :]\n\n5749\n\n\n필터링한 결과에 대한 조직 절편을 다시 시각화 합니다.\n\nfor library in library_names:\n    print(library)\n    sc.pl.spatial(\n        adata[adata.obs.library_id == library, :],\n        library_id=library,\n        color=[\"total_counts\", \"n_genes_by_counts\", \"pct_counts_mt\", \"pct_counts_hb\"],\n        ncols=2,\n    )\n\nV1_Mouse_Brain_Sagittal_Anterior\n\n\n\n\n\n\n\n\n\nV1_Mouse_Brain_Sagittal_Posterior"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_07.html#많이-발현되는-유전자-확인",
    "href": "posts/ipynb/scanpy_workshop_07.html#많이-발현되는-유전자-확인",
    "title": "Scanpy로 scRNA-seq 분석 07",
    "section": "3.2 많이 발현되는 유전자 확인",
    "text": "3.2 많이 발현되는 유전자 확인\n데이터에서 가장 많이 발현되는 유전자가 무엇인지 살펴보겠습니다.\n\nwith plt.rc_context({\"figure.figsize\": (5, 4)}):\n    sc.pl.highest_expr_genes(adata, n_top=20)\n\n\n\n\n\n\n\n\n위 시각화 결과에서 알 수 있듯이 미토콘드리아 유전자는 가장 많이 발현되는 유전자 중 하나입니다. 또한 lncRNA,Bc1(Brain cytoplasmic RNA1)도 많이 발현됩니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_07.html#유전자-필터링하기",
    "href": "posts/ipynb/scanpy_workshop_07.html#유전자-필터링하기",
    "title": "Scanpy로 scRNA-seq 분석 07",
    "section": "3.3 유전자 필터링하기",
    "text": "3.3 유전자 필터링하기\n데이터에서 Bc1 유전자, 헤모글로빈 유전자(혈액 오염), 미토콘드리아 유전자를 제거합니다.\n\nmito_genes = adata.var_names.str.startswith(\"mt-\")\nhb_genes = adata.var_names.str.contains(\"^Hb.*-\")\n\nremove = np.add(mito_genes, hb_genes)\nremove[adata.var_names == \"Bc1\"] = True\nkeep = np.invert(remove)\nprint(f\"제거되는 데이터의 갯수: {sum(remove)}\")\n\nadata = adata[:, keep]\n\n제거되는 데이터의 갯수: 22"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_07.html#visium-데이터셋-하위-집합-만들기",
    "href": "posts/ipynb/scanpy_workshop_07.html#visium-데이터셋-하위-집합-만들기",
    "title": "Scanpy로 scRNA-seq 분석 07",
    "section": "8.1 Visium 데이터셋 하위 집합 만들기",
    "text": "8.1 Visium 데이터셋 하위 집합 만들기\n불러온 scRNA-seq 데이터는 마우스 대뇌 피질에 대한 것임으로 전체 Visium 데이터에서 대뇌 피질에 대한 부분만 분리해 사용합니다. 물론 전체 데이터를 사용해서 분석을 수행할 수 있지만 위양성이 발생할 수 있음으로 주의해서 해석해야 합니다.\n디컨볼루션을 위해서는 카운트 데이터가 필요하므로 앞서 생성한 counts_adata 객체에서 하위 집합을 만들 것입니다.\n\nlib_a = \"V1_Mouse_Brain_Sagittal_Anterior\"\n\ncounts_adata.obs[\"clusters\"] = adata.obs.clusters\n\nadata_anterior_subset = counts_adata[\n    (counts_adata.obs.library_id == lib_a) & (counts_adata.obsm[\"spatial\"][:, 1] &lt; 6000),\n    :,\n].copy()\n\n# 대뇌 피질 클러스터 선택\nadata_anterior_subset = adata_anterior_subset[\n    adata_anterior_subset.obs.clusters.isin([\"3\", \"5\", \"6\"]), :\n]\n\n# 올바른 구역만 있는지 확인\nwith plt.rc_context({\"figure.figsize\": (5, 5)}):\n    sc.pl.spatial(\n        adata_anterior_subset,\n        img_key=\"hires\",\n        library_id=lib_a,\n        color=[\"clusters\"],\n        # size=1.5,\n    )"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_07.html#디컨볼루션deconvolution",
    "href": "posts/ipynb/scanpy_workshop_07.html#디컨볼루션deconvolution",
    "title": "Scanpy로 scRNA-seq 분석 07",
    "section": "8.2 디컨볼루션(Deconvolution)",
    "text": "8.2 디컨볼루션(Deconvolution)\n디컨볼루션은 scRNA-seq 데이터를 사용해 벌크 RNA-seq 데이터셋에서 세포 유형의 비율를 추정하는 방법입니다. Visium 데이터도 일종의 작은 벌크 RNA-seq 데이터로 판단할 수 있기에 적용 할 수 있습니다. 디컨볼루션을 하는 방법에는 DWLS, cell2location, Tangram, Stereo-Seq, RCTD, SCDC 등이 알려져 있습니다. 여기서는 SCVI-tools 패키지에 구현된 Stereoscope을 사용합니다. 자세한 내용은 깃허브를 참조하세요.\n\n8.2.1 디컨볼루션을 위한 유전자 선택하기\n디컨볼루션 방법을 사용하기 위해서는 사전에 유전자 선택해야 하며 아래와 같은 여러 옵션이 있습니다.\n\nVisium 데이터에 가변 유전자를 사용.\nVisium 데이터와 scRNAseq 데이터 모두에 가변 유전자 사용.\nVisium 데이터의 클러스터 간에 DE 유전자를 사용.\n\n여기서는 scRNAseq 데이터의 클러스터당 상위 DE 유전자를 사용할 것입니다.\n\nsc.tl.rank_genes_groups(adata_cortex, \"subclass\", method=\"t-test\", n_genes=100, use_raw=False)\n\nsc.tl.filter_rank_genes_groups(adata_cortex, min_fold_change=1)\n\ngenes = sc.get.rank_genes_groups_df(adata_cortex, group=None)\n# genes\ndeg = genes.names.unique().tolist()\n# Visium 데이터에도 있는 유전자만 남깁니다.\ndeg = np.intersect1d(deg, adata_anterior_subset.var.index).tolist()\n# print(len(deg))\n\n# dotplot 그리기\nsc.pl.rank_genes_groups_dotplot(adata_cortex, n_genes=2)\n\n\n\n\n\n\n\n\n\n\n8.2.2 모델 훈련\n이제 scRNA-seq 데이터를 사용해 모델을 훈련해보겠습니다. 모든 데이터가 카운트 단위여야 한다는 것에 유의하세요. 또한 anndata 객체에 카운트 데이터가 복사본으로 count 레이어로 저장되어 있어야 합니다.\n\n# 카운트 데이터 복사하기\nsc_adata = adata_cortex.copy()\nsc_adata.X = adata_cortex.raw.X.copy()\n\n# 카운트 레이어 추가\nsc_adata.layers[\"counts\"] = sc_adata.X.copy()\n\n# DEG에 속한 유전자로 하위 집합 만들기\nsc_adata = sc_adata[:, deg].copy()\n\n# stereoscope 만들기\nRNAStereoscope.setup_anndata(sc_adata, layer=\"counts\", labels_key=\"subclass\")\n\n# 모델은 파일에 저장되므로 실행 속도가 느린 경우 train = False를 설정해 로컬에 저장된 모델을 읽습니다.\ntrain = True\nif train:\n    sc_model = RNAStereoscope(sc_adata)\n    sc_model.train(max_epochs=300)\n    sc_model.history[\"elbo_train\"][10:].plot()\n    sc_model.save(\"./data/spatial/visium/scanpy_scmodel\", overwrite=True)\nelse:\n    sc_model = RNAStereoscope.load(\"./data/spatial/visium/scanpy_scmodel\", sc_adata)\n    print(\"Loaded RNA model from file!\")\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\nEpoch 300/300: 100%|██████████| 300/300 [00:15&lt;00:00, 19.80it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.34e+7]\n\n\n`Trainer.fit` stopped: `max_epochs=300` reached.\n\n\nEpoch 300/300: 100%|██████████| 300/300 [00:15&lt;00:00, 19.86it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.34e+7]\n\n\n\n\n\n\n\n\n\n\n\n8.2.3 세포 유형 예측하기\n먼저 새로운 Visium 개체 st_adata를 만들고 카운트 데이터를 레이어로 만듭니다.\n\nst_adata = adata_anterior_subset.copy()\n\nst_adata.layers[\"counts\"] = st_adata.X.copy()\nst_adata = st_adata[:, deg].copy()\n\nSpatialStereoscope.setup_anndata(st_adata, layer=\"counts\")\n\ntrain = True\nif train:\n    spatial_model = SpatialStereoscope.from_rna_model(st_adata, sc_model)\n    spatial_model.train(max_epochs=2000)\n    spatial_model.history[\"elbo_train\"][10:].plot()\n    spatial_model.save(\"./data/spatial/visium/scanpy_stmodel\", overwrite=True)\nelse:\n    spatial_model = SpatialStereoscope.load(\"./data/spatial/visium/scanpy_stmodel\", st_adata)\n    print(\"Loaded Spatial model from file!\")\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\nEpoch 2000/2000: 100%|██████████| 2000/2000 [00:19&lt;00:00, 102.09it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6] \n\n\n`Trainer.fit` stopped: `max_epochs=2000` reached.\n\n\nEpoch 2000/2000: 100%|██████████| 2000/2000 [00:19&lt;00:00, 100.99it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]\n\n\n\n\n\n\n\n\n\n이제 scRNA-seq 데이터에서 가져온 세포 유형이 실제로 어떻게 예측되는지 시각화를 통해 살펴봅니다.\n\n# 모델의 예측 결과를 `.obs` 슬롯에 추가합니다.\nst_adata.obsm[\"deconvolution\"] = spatial_model.get_proportions()\n\n# 또한 .obsm에 데이터프레임 추가합니다.\nfor ct in st_adata.obsm[\"deconvolution\"].columns:\n    st_adata.obs[ct] = st_adata.obsm[\"deconvolution\"][ct]\n\n\nwith plt.rc_context({\"figure.figsize\": (6, 4)}):\n    sc.pl.spatial(\n        st_adata,\n        img_key=\"hires\",\n        color=[\"L2/3 IT\", \"L4\", \"L5 PT\", \"L6 CT\", \"Oligo\", \"Astro\"],\n        library_id=lib_a,\n        size=1.5,\n        ncols=2,\n    )\n\n\n\n\n\n\n\n\n또한 바이올린 플랏으로 클러스터간 예측된 세포 유형에 대해 시각화해봅니다.\n\nwith plt.rc_context({\"figure.figsize\": (5, 5)}):\n    sc.pl.violin(\n        st_adata,\n        [\"L2/3 IT\", \"L6 CT\", \"Oligo\", \"Astro\"],\n        jitter=0.4,\n        groupby=\"clusters\",\n        rotation=90,\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n디컨볼루션 결과는 예측 결과로 매개변수, 유전자 선택 등을 어떻게 조정하는지에 따라 다른 결과가 나올 수 있습니다. 그러니 항상 검증하는 단계가 필요합니다."
  },
  {
    "objectID": "posts/ipynb/python_sankeyDiagram.html",
    "href": "posts/ipynb/python_sankeyDiagram.html",
    "title": "파이썬으로 Sankey diagram그리기",
    "section": "",
    "text": "Sankey 다이어그램은 한 값 집합에서 다른 값 집합으로의 흐름을 시각화하는 도구로 열 손실에 비례하는 너비를 가진 화살표를 사용하여 증기 엔진 효율을 시각화했던 Sankey 선장의 이름을 따서 명명되었습니다. Sankey 다이어그램은 서로 다른 고객 세그먼트 간의 전환이나 흐름을 보여주는 데 효과적이며 노드(연결되는 항목)와 링크(연결)로 구성됩니다.\nSankey 다이어그램은 두 도메인 간의 다대다 매핑이나 여러 경로를 통해 트래픽이 이동하는 방식을 나타내는 데 유용합니다. 예를 들어, 대학과 전공 간의 관계를 시각화하거나 웹사이트 내에서 페이지 간의 트래픽 흐름을 보여줄 수 있습니다.\n\n1 기본 Sankey 다이어그램 그리기\n간단한 Sankey 다이어그램을 구현하는 방법을 이해하기 위해 Plotly를 사용해 기본적인 다이어그램을 만들어 보겠습니다. Plotly에서 Sankey 다이어그램은 세 개의 리스트로 정의됩니다. 세 가지 리스트는 source(출발점), target(도착점), values(값)입니다. Plotly는 각 노드를 0부터 시작하여 전체 노드 수에서 1을 뺀 숫자까지 인덱싱합니다. source와 target 리스트는 노드 간의 연결을 정의합니다. 아래 코드를 살펴보면 이해하기 더 쉬울 것입니다.\n\nimport plotly.graph_objects as go\nfrom typing import List, Dict\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n# Define node and link data\nlabels: List[str] = [\"A\", \"B\", \"X\", \"Y\", \"Z\"]\nsource_indices: List[int] = [0, 0, 0, 1, 1, 1]  # A -&gt; X, A -&gt; Y, A -&gt; Z, B -&gt; X, B -&gt; Y, B -&gt; Z\ntarget_indices: List[int] = [2, 3, 4, 2, 3, 4]  # X, Y, Z\nvalues: List[int] = [5, 7, 6, 2, 9, 4]           # Weights for each link\n\n# Define colors\ncolor_dict: Dict[str, str] = {\n    'A': 'rgba(252,65,94,0.7)', \n    'B': 'rgba(255,162,0,0.7)', \n    'X': 'rgba(55,178,255,0.7)',\n    'Y': 'rgba(200,200,200,0.7)',\n    'Z': 'rgba(200,200,200,0.7)'\n}\n\ncolor_dict_link: Dict[str, str] = {\n    'A': 'rgba(252,65,94,0.4)',\n    'B': 'rgba(255,162,0,0.4)',\n    'X': 'rgba(55,178,255,0.4)',\n    'Y': 'rgba(200,200,200,0.4)',\n    'Z': 'rgba(200,200,200,0.4)'\n}\n\n# Create node color list\nnode_colors: List[str] = [color_dict[label] for label in labels]\n\n# Create link color list based on source nodes\nlink_colors: List[str] = [\n    color_dict_link['A'] if source == 0 else color_dict_link['B'] if source == 1 else color_dict_link['X']\n    for source in source_indices\n]\n\n# Create Sankey diagram\nfig = go.Figure(data=[go.Sankey(\n    node=dict(\n        pad=15,\n        thickness=20,\n        line=dict(color=\"black\", width=0.5),\n        label=labels,\n        color=node_colors\n    ),\n    link=dict(\n        source=source_indices,\n        target=target_indices,\n        value=values,\n        color=link_colors\n    )\n)])\n\n# Update layout\nfig.update_layout(\n    title_text=\"Sankey Diagram with Custom Colors for A and B\",\n    font_size=10,\n    width=600,\n    height=400,\n)\n\n# Show diagram\nfig.show()\n\n                                                \n\n\n\n\n2 고급 Sankey 다이어그램 그리기\nSankey 다이어그램을 위해서는 먼저 데이터를 전처리하는 것부터 시작해야 합니다. 아래는 pandas를 사용해 데이터를 불러오고 노드와 링크 데이터를 만들고 시각화하는 코드입니다.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../../input/estimated-us-energy-cons.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nCategory\nSankey demo series (from)\nSankey demo series (to)\nSankey demo series (weight)\n\n\n\n\n0\nhighcharts-k3rhrdv-1\nNet Import\nElectricity & Heat\n0.14\n\n\n1\nhighcharts-k3rhrdv-2\nSolar\nElectricity & Heat\n1.28\n\n\n2\nhighcharts-k3rhrdv-3\nNuclear\nElectricity & Heat\n8.05\n\n\n3\nhighcharts-k3rhrdv-4\nHydro\nElectricity & Heat\n2.31\n\n\n4\nhighcharts-k3rhrdv-5\nWind\nElectricity & Heat\n3.84\n\n\n\n\n\n\n\n\n# 노드 및 링크 데이터 준비\n# 고유한 'to' 노드를 포함하여 모든 노드를 정의합니다.\nto_nodes = df[\"Sankey demo series (to)\"].unique().tolist()\nlabels = df[\"Sankey demo series (from)\"].tolist() + to_nodes\n\n# 출발 노드 인덱스 (source)\nsource_indices = df[\"Sankey demo series (from)\"].map(lambda x: labels.index(x)).tolist()\n\n# 도착 노드 인덱스 (target)\ntarget_indices = [labels.index(to_node) for to_node in df[\"Sankey demo series (to)\"]]\n\n# 링크의 가중치\nvalues = df[\"Sankey demo series (weight)\"].tolist()\n\n# 색상 정의\ncolor_dict = {\n    'Net Import': 'rgba(252,65,94,0.7)',   # Red\n    'Solar': 'rgba(255,162,0,0.7)',        # Orange\n    'Nuclear': 'rgba(55,178,255,0.7)',     # Light Blue\n    'Hydro': 'rgba(0,128,0,0.7)',          # Green\n    'Wind': 'rgba(75,0,130,0.7)',          # Indigo\n    'Geothermal': 'rgba(255,105,180,0.7)', # Hot Pink\n    'Natural Gas': 'rgba(255,215,0,0.7)',  # Gold\n    'Coal': 'rgba(105,105,105,0.7)',       # Dim Gray\n    'Biomass': 'rgba(139,69,19,0.7)',      # Saddle Brown\n    'Petroleum': 'rgba(173,216,230,0.7)',  # Pastel Blue\n    'Electricity & Heat': 'rgba(200,200,200,0.7)',  # Gray for target node\n    'Residential': 'rgba(173,216,230,0.7)', # Light Blue for Residential\n    'Commercial': 'rgba(144,238,144,0.7)',  # Light Green for Commercial\n    'Industrial': 'rgba(255,182,193,0.7)',   # Light Pink for Industrial\n    'Transportation': 'rgba(255,140,0,0.7)'  # Dark Orange for Transportation\n}\n\n# 노드 색상 리스트 생성\nnode_colors = [color_dict.get(label, 'rgba(200,200,200,0.7)') for label in labels]\n\n# 링크 색상 리스트 생성 (출발 노드에 따라 색상 결정)\nlink_colors = [color_dict[df[\"Sankey demo series (from)\"].iloc[i]] for i in range(len(df))]\n\n# Sankey 다이어그램 생성\nfig = go.Figure(data=[go.Sankey(\n    node=dict(\n        pad=15,\n        thickness=20,\n        line=dict(color=\"black\", width=0.5),\n        label=labels,\n        color=node_colors\n    ),\n    link=dict(\n        source=source_indices,\n        target=target_indices,\n        value=values,\n        color=link_colors  # 링크 색상 적용\n    )\n)])\n\n\n# 레이아웃 업데이트\nfig.update_layout(\n    title_text=\"Sankey Diagram for Energy Sources\",\n    font_size=10,\n    width=600, height=500,\n    )\n\n# 다이어그램 표시\nfig.show()\n\n                                                \n\n\n\n\n3 마치며\nSankey 다이어그램은 데이터 분석과 시각화에 있어 매우 유용한 도구입니다. 고객 세그먼트 간 전환, 웹사이트 트래픽 흐름, 에너지 소비 등 다양한 분야에서 활용될 수 있습니다. Python과 Plotly를 사용하면 복잡한 데이터셋도 쉽게 Sankey 다이어그램으로 표현할 수 있어, 데이터 기반의 의사결정과 인사이트 도출에 큰 도움이 될 것입니다. Sankey 다이어그램의 구현 방법을 익히고 실제 데이터에 적용해 보면서, 여러분의 데이터 시각화 스킬을 한 단계 더 발전시켜 보시기 바랍니다."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#scrna-seq과-10xgenomics",
    "href": "posts/ipynb/R_seurat_tutorials.html#scrna-seq과-10xgenomics",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "1.1 scRNA seq과 10xGenomics",
    "text": "1.1 scRNA seq과 10xGenomics\nscRNA-seq는 single-cell RNA sequencing의 줄임말로, 하나의 세포에서 mRNA를 측정하는 방법입니다. 이 기술은 기존 bulk RNA-seq 방법과는 달리 하나의 세포에서 RNA를 추출하여 분석합니다. 이를 통해, 개별 세포의 유전자 발현 패턴, 전사체 감지, 변형과 발현의 상호작용 등을 이해할 수 있습니다.\n10xGenomics는 scRNA-seq 분석에서 매우 인기있는 플랫폼으로 droplet-based 방법을 사용합니다. droplet-based 방법은 cell barcoding 및 unique molecular identifier(UMI)를 사용하여 RNA-seq 라이브러리를 생성하는 공정으로 사실상 현재 scRNA seq분야에 표준으로 사용됩니다."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#시퀀싱-데이터-준비",
    "href": "posts/ipynb/R_seurat_tutorials.html#시퀀싱-데이터-준비",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "2.1 시퀀싱 데이터 준비",
    "text": "2.1 시퀀싱 데이터 준비\n실험을 통해 다음과 같은 fastq 파일을 가지고 있다고 간주합니다. 여기에서는 학습 목적으로 아주 작은 데이터셋으로 진행합니다만 실제 데이터는 훨씬 큽니다.\n.(pbmc_1k_v3_fastqs)\n├── pbmc_1k_v3_S1_L001_I1_001.fastq.gz\n├── pbmc_1k_v3_S1_L001_R1_001.fastq.gz\n├── pbmc_1k_v3_S1_L001_R2_001.fastq.gz\n├── pbmc_1k_v3_S1_L002_I1_001.fastq.gz\n├── pbmc_1k_v3_S1_L002_R1_001.fastq.gz\n└── pbmc_1k_v3_S1_L002_R2_001.fastq.gz"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#cell-ranger-설치",
    "href": "posts/ipynb/R_seurat_tutorials.html#cell-ranger-설치",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "2.2 cell ranger 설치",
    "text": "2.2 cell ranger 설치\n여기에서는 설치 방법은 생략하고 공식 홈페이지 링크를 참조하시기 바랍니다."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#cell-ranger-실행",
    "href": "posts/ipynb/R_seurat_tutorials.html#cell-ranger-실행",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "2.3 cell ranger 실행",
    "text": "2.3 cell ranger 실행\ncellranger count --id=run_count_1kpbmcs \\\n   --fastqs=./pbmc_1k_v3_fastqs \\\n   --sample=pbmc_1k_v3 \\\n   --transcriptome=./refdata-gex-GRCh38-2020-A\n   --nosecondary\n위의 명령어를 통해 cell ranger를 실행할 수 있습니다. --id 는 생성되는 결과의 폴더명이며, --fastqs는 fastq 파일이 있는 폴더의 위치, --sample은 metadata에 들어가는 샘플 정보, --transcriptome는 참조 전사체의 위치 입니다.\n저의 경우는 10x Genomics 사이트에서 다음의 명령어로 다운로드 받았습니다. 참고로 참조 전사체는 실험에 사용된 시료에 따라 다르게 사용해야 합니다.\nwget https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-2020-A.tar.gz\ntar -zxvf refdata-gex-GRCh38-2020-A.tar.gz"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#cell-ranger-결과",
    "href": "posts/ipynb/R_seurat_tutorials.html#cell-ranger-결과",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "2.4 cell ranger 결과",
    "text": "2.4 cell ranger 결과\ncell ranger가 문제없이 작동했다면 out 폴더에 다음과 같은 파일과 폴더가 생겨납니다.\n.(out)\n├── analysis\n├── cloupe.cloupe\n├── filtered_feature_bc_matrix\n├── filtered_feature_bc_matrix.h5\n├── metrics_summary.csv\n├── molecule_info.h5\n├── possorted_genome_bam.bam\n├── possorted_genome_bam.bam.bai\n├── raw_feature_bc_matrix\n├── raw_feature_bc_matrix.h5\n└── web_summary.html\n이중에 Seurat 패키지가 필요로 하는 것은 filtered_feature_bc_matrix 폴더 입니다. 폴더안에는 다음과 같은 파일이 들어있습니다.\n.(filtered_feature_bc_matrix)\n├── barcodes.tsv.gz\n├── features.tsv.gz\n└── matrix.mtx.gz\n이걸로 모든 사전 준비가 완료되었습니다."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#사용할-패키기-불러오기",
    "href": "posts/ipynb/R_seurat_tutorials.html#사용할-패키기-불러오기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "2.5 사용할 패키기 불러오기",
    "text": "2.5 사용할 패키기 불러오기\n\n2.5.1 Seurat\nSeurat은 R 프로그래밍 언어로 작성된 scRNA-seq 데이터 분석을 위한 유명한 패키지 중 하나입니다. Seurat은 높은 차원의 scRNA-seq 데이터에서 유전자 발현 패턴을 탐색하고 이를 이용하여 세포 및 클러스터의 식별과 분석, 특성 제시, 시각화 등 다양한 분석 작업을 수행할 수 있습니다. Seurat은 다양한 데이터 전처리 및 정규화 기능과 함께 차원 축소, 클러스터링, 시각화, 서브셋 생성, 유전자 발현 분석, 세포간 상호작용 분석 등 다양한 분석 도구를 제공합니다. Seurat은 현재까지 업데이트와 기능 추가가 활발하게 이루어지고 있으며, scRNA-seq 분석에 필수적인 유틸리티 패키지 중 하나입니다.\n\n\n2.5.2 scDblFinder\nscDblFinder는 단일 세포 RNA 시퀀싱 데이터에서 더블렛(두 개의 세포가 동시에 캡처되어 하나의 세포로 보이는 것) 현상을 탐지하고 제거하기 위한 R 패키지입니다. 이 패키지는 UMI(Unique Molecular Identifier)를 기반으로하여 두 개 이상의 세포에서 동시에 탐지된 UMIs를 찾아서 더블렛으로 추정하고, 더블렛으로 추정된 셀을 제거합니다. 이를 통해 scRNA-seq 데이터의 정확도와 해석력을 높일 수 있습니다. 또한, scDblFinder는 Seurat 및 SingleCellExperiment 형식의 데이터를 지원하며, 다양한 분석 옵션을 제공하여 사용자가 데이터에 맞게 조정할 수 있습니다.\n\n\n2.5.3 tidyverse\ntidyverse는 데이터 분석에 필요한 필수 R 패키지들의 모음으로 데이터 처리, 시각화, 모델링, 프로그래밍 등의 다양한 작업을 수행하는 데 사용됩니다. 주요 패키지로는 ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats 등이 있습니다.\n\noptions(verbose=FALSE) # Seurat 함수들이 실행될 때 로그 메시지를 표시하지 않습니다.\noptions(tidyverse.quiet=TRUE) # tidyverse 패키지의 로그 메시지가 출력되지 않습니다.\noptions(warn=-1)\noptions(future.rng.onMisuse=\"ignore\")\n\nlibrary(Seurat)\nlibrary(tidyverse)\nlibrary(scDblFinder)\nlibrary(future) # Enable parallelization\nplan(\"multicore\", workers=30) # cpu core에 맞게 조절합니다.\n# plan()\n\n\n\n2.5.4 패키지 버전 확인\n\n2.5.4.1 사용한 Seurat 패키지의 버전\n\npackageVersion(\"Seurat\")\n\n[1] ‘4.3.0’\n\n\n\n\n2.5.4.2 사용한 scDblFinder 패키지의 버전\n\npackageVersion(\"scDblFinder\")\n\n[1] ‘1.12.0’"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#count-행렬은-어떻게-생겼을까",
    "href": "posts/ipynb/R_seurat_tutorials.html#count-행렬은-어떻게-생겼을까",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "4.1 count 행렬은 어떻게 생겼을까?",
    "text": "4.1 count 행렬은 어떻게 생겼을까?\n\n# 처음 5개의 세포에 있는 몇 가지 유전자를 확인해봅니다.\nseurat_obj[[\"RNA\"]]@counts[c(\"CD3D\", \"TCL1A\", \"MS4A1\"), 1:3]\n\n3 x 3 sparse Matrix of class \"dgCMatrix\"\n      AAACCCAAGGAGAGTA-1 AAACGCTTCAGCCCAG-1 AAAGAACAGACGACTG-1\nCD3D                   .                  .                  6\nTCL1A                  .                  9                  .\nMS4A1                  .                  5                  ."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#seurat-개체의-메타데이터는-어디에-저장될까",
    "href": "posts/ipynb/R_seurat_tutorials.html#seurat-개체의-메타데이터는-어디에-저장될까",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "4.2 Seurat 개체의 메타데이터는 어디에 저장될까?",
    "text": "4.2 Seurat 개체의 메타데이터는 어디에 저장될까?\nseurat_obj@meta.data 혹은 seurat_obj[[]]을 통해 메타데이터를 확인할 수 있습니다.\n\nhead(seurat_obj@meta.data, 5)\n\n\nA data.frame: 5 × 3\n\n\n\norig.ident\nnCount_RNA\nnFeature_RNA\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAAACCCAAGGAGAGTA-1\npbmc1k\n12861\n3871\n\n\nAAACGCTTCAGCCCAG-1\npbmc1k\n9432\n3234\n\n\nAAAGAACAGACGACTG-1\npbmc1k\n6520\n2631\n\n\nAAAGAACCAATGGCAG-1\npbmc1k\n4362\n2121\n\n\nAAAGAACGTCTGCAAT-1\npbmc1k\n9905\n3157"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#서열-데이터-품질-관리",
    "href": "posts/ipynb/R_seurat_tutorials.html#서열-데이터-품질-관리",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "5.1 서열 데이터 품질 관리",
    "text": "5.1 서열 데이터 품질 관리\nscRNA seq 데이터의 분석의 신뢰성을 얻기 위해서 데이터의 품질 관리는 필수이빈다. Seurat을 사용하면 품질관리(QC) 지표를 쉽게 탐색하고 사용자 정의 기준에 따라 세포를 필터링할 수 있습니다. 일반적으로 사용되는 QC 기준은 다음 세가지 입니다.\n\n각 세포에서 검출된 고유 유전자의 수.\n\n품질이 낮은 세포는 종종 유전자가 매우 적습니다.\n이중 또는 다중의 세포가 들어간 droplet에는 비정상적으로 유전자 수가 높습니다.\n\n각 세포에서 검출된 총 서열의 수(고유 유전자의 수와 밀접한 상관관계가 있음)\n미토콘드리아 게놈에 매핑되는 서열의 비율\n\n품질이 낮거나 죽어가는 세포에는 미토콘드리아 유전자가 많이 발견됩니다.\n\n\n\n5.1.1 QC 지표 시각화하기\n\n5.1.1.1 바이올린 플랏\n\nseurat_obj[[\"percent.mt\"]] &lt;- PercentageFeatureSet(seurat_obj, pattern=\"^MT-\") \n# mouse 시료의 경우 pattern을 \"^mt-\"로 변경해야 합니다.\n\nplot &lt;- VlnPlot(seurat_obj, features=c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol=3)\n\noptions(repr.plot.width=6, repr.plot.height=6)\nplot\n\n\n\n\n\n\n\n\n\n\n5.1.1.2 Scatter 플랏\n\nplot1 &lt;- FeatureScatter(seurat_obj, feature1=\"nCount_RNA\", feature2=\"percent.mt\")\nplot2 &lt;- FeatureScatter(seurat_obj, feature1=\"nCount_RNA\", feature2=\"nFeature_RNA\")\n\noptions(repr.plot.width=12, repr.plot.height=6)\nplot1 + plot2\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 QC 및 추가 분석을 위한 세포 선택하기\n위의 결과를 토대로 nFeature_RNA가 200개에서 6000개 사이이고 percent.mt가 20 이하인 세포들만 선택합니다.\n\nseurat_obj &lt;- subset(seurat_obj, subset=nFeature_RNA &gt; 200 & nFeature_RNA &lt; 6000 & percent.mt &lt; 20)\n\n아직 모든 QC 과정이 끝난 것은 아닙니다. PCA를 진행하고나서 scDblFinder 패키지를 사용해 추가적인 더블렛 데이터를 제거하겠습니다."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#데이터-정규화하기",
    "href": "posts/ipynb/R_seurat_tutorials.html#데이터-정규화하기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "5.2 데이터 정규화하기",
    "text": "5.2 데이터 정규화하기\nQC를 통해 일부 데이터를 제거한 다음 단계는 데이터를 정규화하는 것입니다. 여기서는 각 세포의 발현 값을 전체 발현으로 나누고 스케일 계수(기본적으로 10,000)를 곱한 다음 로그 변환하는 LogNormalize방법을 사용합니다. 이렇게 정규화된 값은 seurat_obj[[\"RNA\"]]@data에 저장됩니다.\n\nseurat_obj &lt;- NormalizeData(seurat_obj, verbose=FALSE)\nseurat_obj &lt;- FindVariableFeatures(seurat_obj, verbose=FALSE)\nseurat_obj &lt;- ScaleData(seurat_obj, verbose=FALSE)\nseurat_obj &lt;- RunPCA(seurat_obj, verbose=FALSE)\n\n\n5.2.1 Elbow 플랏 그리기\nSeurat에서 clustering을 수행하기 전에는 몇 개의 차원(dimension)을 사용할지 결정해야 합니다. 차원의 수는 PCA와 같은 차원 축소 기법을 사용하여 줄여진 차원의 수를 의미합니다. 그리고 이 차원의 수는 클러스터링 알고리즘에 사용됩니다.\n그러나 차원의 수가 너무 적거나 많으면 적절한 클러스터링이 어려울 수 있습니다. 차원이 적을 경우 정보 손실이 크게 발생하고, 차원이 많을 경우에는 불필요한 차원의 포함으로 인해 과적합(overfitting)이 발생할 가능성이 있습니다.\n따라서 적절한 차원의 수를 선택하기 위해 elbow plot을 사용합니다. elbow plot은 차원의 수를 x축으로, 해당 차원의 데이터를 잘 설명하는 정도(예: variance)를 y축으로 나타냅니다. 이 때, 차원의 수를 늘리면 y축 값은 점점 증가하게 됩니다. 그러나 어느 지점 이후로는 y값이 더 이상 크게 증가하지 않고 평평해지는 지점이 나타나는데, 이 지점이 elbow point입니다. 이 지점 이후로는 차원을 늘려도 데이터를 잘 설명하지 못하므로, elbow point를 기준으로 적절한 차원의 수를 선택합니다. 이를 통해 데이터의 차원을 축소할 때, 적절한 차원의 수를 선택하여 과적합을 방지하고 필요한 정보만을 추출할 수 있습니다.\n\nplot &lt;- ElbowPlot(seurat_obj)\n\noptions(repr.plot.width = 6, repr.plot.height = 6)\nplot"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#umap-그리기",
    "href": "posts/ipynb/R_seurat_tutorials.html#umap-그리기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "6.1 UMAP 그리기",
    "text": "6.1 UMAP 그리기\nUMAP은 Uniform Manifold Approximation and Projection의 약자로, scRNA-seq 데이터를 시각화하기 위한 비선형 차원 축소 방법 중 하나입니다. t-SNE와 유사한 기능을 가지고 있지만, 대규모 데이터셋에서 더욱 빠르고 정확한 임베딩을 제공합니다.\nUMAP은 데이터의 국부적인 구조를 보존하는데 초점을 둡니다. 즉, 비슷한 특성을 가진 데이터들이 서로 가깝게 묶이고, 서로 다른 특성을 가진 데이터들은 더 멀리 배치되도록 임베딩을 생성합니다. 이를 통해, scRNA-seq 데이터의 복잡한 구조를 파악하고 시각화할 수 있습니다.\n\nseurat_obj &lt;- FindNeighbors(seurat_obj, dims = 1:10, verbose = FALSE)\nseurat_obj &lt;- FindClusters(seurat_obj, resolution = 0.5, verbose = FALSE)\n# 일반적으로 resolution 값은 0.1 ~ 1.0 사이의 값을 많이 사용합니다. \n# 값이 작을수록 세분화된 군집을 얻을 수 있기 때문에\n# 세포의 종류나 상태 등을 더 세부적으로 파악하고자 할 때는 작은 값이 유용합니다. \n# 반면 큰 값은 대부분의 데이터를 하나의 군집으로 묶어줌으로써 전체적인 데이터 구조를 파악하는 데에 유용할 수 있습니다.\nseurat_obj &lt;- RunUMAP(seurat_obj, dims = 1:10, verbose = FALSE)\n\nplot &lt;- DimPlot(seurat_obj, reduction = \"umap\", label=TRUE)\n\noptions(repr.plot.width = 6, repr.plot.height = 6)\nplot\n\n\n\n\n\n\n\n\n위의 UMAP 플랏을 통해 총 10개의 cluster로 나누어 졌음을 알 수 있습니다. 이제 추가적인 QC를 진행해보겠습니다."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#scdblfinder를-사용해-doublet-제거-하기",
    "href": "posts/ipynb/R_seurat_tutorials.html#scdblfinder를-사용해-doublet-제거-하기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "6.2 scDblFinder를 사용해 doublet 제거 하기",
    "text": "6.2 scDblFinder를 사용해 doublet 제거 하기\nscDblFinder는 클러스터 정보를 기반으로 인공적으로 생성된 더블렛을 찾아냅니다. 아래 코드를 통해 더블렛을 찾고 UMAP 플랏에 표시해보겠습니다.\n\nsce &lt;- scDblFinder(GetAssayData(seurat_obj, slot = \"counts\"), clusters = Idents(seurat_obj))\n# scDblFinder 결과 점수를 다시 Seurat 객체로 옮깁니다.\nseurat_obj$scDblFinder.score &lt;- sce$scDblFinder.score\n\np &lt;- FeaturePlot(seurat_obj, \"scDblFinder.score\", pt.size = 0.1) \noptions(repr.plot.width = 6, repr.plot.height = 6)\np\n\nAssuming the input to be a matrix of counts or expected counts.\n\n11 clusters\n\nCreating ~5000 artificial doublets...\n\nas(&lt;dgeMatrix&gt;, \"dgCMatrix\") is deprecated since Matrix 1.5-0; do as(., \"CsparseMatrix\") instead\n\nDimensional reduction\n\nEvaluating kNN...\n\nTraining model...\n\niter=0, 32 cells excluded from training.\n\niter=1, 29 cells excluded from training.\n\niter=2, 30 cells excluded from training.\n\nThreshold found:0.519\n\n30 (2.6%) doublets called\n\n\n\n\n\n\n\n\n\n\nscDblFinder 결과에서 Threshold 값이 0.519 라는 것과 총 30(2.6%)개의 더블렛이 계산되었습니다. 다시 subset()함수를 사용해 scDblFinder 값이 0.519 이하인 세포만 고르는 QC 과정을 진행하겠습니다.\n\n6.2.1 더블렛 제거하기\n\n# 개체 메타 데이터의 값에 대한 하위 집합만들기\nseurat_obj &lt;- subset(x = seurat_obj, subset = scDblFinder.score &lt; 0.519 )\n\nDefaultAssay(seurat_obj) &lt;- \"RNA\"  # default assay is RNA\nseurat_obj &lt;- NormalizeData(seurat_obj, verbose = FALSE)\nseurat_obj &lt;- FindVariableFeatures(seurat_obj, verbose = FALSE)\nseurat_obj &lt;- ScaleData(seurat_obj, verbose = FALSE)\nseurat_obj &lt;- RunPCA(seurat_obj, verbose = FALSE)\nseurat_obj &lt;- FindNeighbors(seurat_obj, dims = 1:10, verbose = FALSE)\nseurat_obj &lt;- FindClusters(seurat_obj, resolution = 0.5, verbose = FALSE)\nseurat_obj &lt;- RunUMAP(seurat_obj, dims = 1:10, verbose = FALSE)\n\np &lt;- DimPlot(seurat_obj, label = TRUE)\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\np"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#marker-gene-찾기",
    "href": "posts/ipynb/R_seurat_tutorials.html#marker-gene-찾기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "7.1 Marker gene 찾기",
    "text": "7.1 Marker gene 찾기\n\nmarkers &lt;- FindAllMarkers(seurat_obj, only.pos = TRUE, verbose = FALSE)\nwrite_csv(markers, \"../output/pbmc1k_marker.csv\") # 결과를 csv 파일로 저장\nmarkers %&gt;% head()\n\n\nA data.frame: 6 × 7\n\n\n\np_val\navg_log2FC\npct.1\npct.2\np_val_adj\ncluster\ngene\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;chr&gt;\n\n\n\n\nS100A12\n4.929494e-211\n3.984628\n0.979\n0.047\n9.960536e-207\n0\nS100A12\n\n\nVCAN\n3.023544e-202\n4.236786\n1.000\n0.086\n6.109374e-198\n0\nVCAN\n\n\nCD14\n3.015410e-195\n2.387003\n0.954\n0.058\n6.092936e-191\n0\nCD14\n\n\nCSF3R\n2.588854e-192\n2.561836\n0.979\n0.080\n5.231038e-188\n0\nCSF3R\n\n\nMNDA\n1.513641e-189\n3.044325\n0.993\n0.104\n3.058462e-185\n0\nMNDA\n\n\nS100A8\n1.971209e-188\n5.905527\n0.996\n0.142\n3.983026e-184\n0\nS100A8\n\n\n\n\n\nFindAllMarkers() 결과는 데이터프레임입니다. avg_log2FC는 다른 클러스터와 비교해 발현량이 얼마나 차이나는지를 의미합니다. 해당 열을 가지고 각 클러스터당 상위 5개의 유전자 마커를 추려서 heatmap 을 그려봅니다."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#marker-gene-heatmap",
    "href": "posts/ipynb/R_seurat_tutorials.html#marker-gene-heatmap",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "7.2 Marker gene heatmap",
    "text": "7.2 Marker gene heatmap\n\ntop5 &lt;- markers %&gt;% group_by(cluster) %&gt;% top_n(n = 5, wt = avg_log2FC)\np &lt;- DoHeatmap(seurat_obj, features = top5$gene) + NoLegend()\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\np\n\n\n\n\n\n\n\n\n위의 heatmap은 멋져보이기는 하지만 그렇게 큰 정보를 제공해주지는 않습니다. 일반적으로 클러스터가 어떤 세포인지 알아내는 작업이 scRNA seq 분석방법에서 가장 중요한 부분이며 여러가지 접근법이 있지만 가장 덜 자동화된 부분이기도 합니다.\n가장 일반적인 방법은 FindAllMarkers() 함수를 이용해 얻은 각 클러스터의 유전자 마커와 참조 데이터 세트를 비교 하는 것입니다. 참조 데이터 세트란, 이미 잘 정의된 세포 유형들의 scRNA-seq 데이터로 현재 분석 중인 데이터의 클러스터들을 참조 데이터 세트와 비교하여 유사한 패턴을 가진 세포 유형을 찾아내는 것입니다. 이 작업은 시간이 아주 많이 필요하며 작업자에 따라 다른 결과가 나옵니다. 그래서 이번에는 ChatGPT를 사용하는 방법으로 해보겠습니다."
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#chatgpt-사용하기",
    "href": "posts/ipynb/R_seurat_tutorials.html#chatgpt-사용하기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "7.3 ChatGPT 사용하기",
    "text": "7.3 ChatGPT 사용하기\n\nChatGPT에 대한 자세한 설명은 생략합니다.\n\n프롬프터에 클러스터의 유전자 마커를 쉽게 입력하기 위해 다음의 코드를 사용합니다.\n\n# 빈 리스트 생성\nmarker_list &lt;- list()\ngene_number &lt;- 10 # 10개의 유전자만 찾아낼때\n\n# for loop으로 리스트에 값 추가\nfor (i in unique(Idents(seurat_obj))) {\n  marker_list[[paste0(\"cluster\",i)]] &lt;- markers %&gt;% group_by(cluster) %&gt;% top_n(gene_number, avg_log2FC) %&gt;%\n      ungroup() %&gt;% arrange(cluster, desc(avg_log2FC)) %&gt;% filter(cluster == i) %&gt;% .$gene\n}\n\n# 결과 출력\nmarker_list\n\n\n    $cluster0\n        \n'S100A8''S100A9''LYZ''VCAN''S100A12''FCN1''MNDA''CTSS''NAMPT''PLXDC2'\n\n    $cluster3\n        \n'IGHM''AFF3''IGHD''IGLC2''FCRL1''BANK1''TCL1A''BACH2''CD79A''LINC00926'\n\n    $cluster7\n        \n'GZMH''NKG7''CCL5''SGCD''SAMD3''CST7''GZMK''TOX''GZMA''KLRG1'\n\n    $cluster1\n        \n'INPP4B''IL7R''ANK3''CDC14A''SERINC5''BCL11B''IL32''RORA''CAMK4''TTC39C'\n\n    $cluster6\n        \n'IGKC''IGHA1''JCHAIN''IGHG3''IGHGP''IGHG1''IGHG2''BANK1''OSBPL10''MS4A1'\n\n    $cluster2\n        \n'LEF1''NELL2''TSHZ2''FHIT''CAMK4''PRKCA''BCL11B''PDE3B''TXK''TRABD2A'\n\n    $cluster5\n        \n'KLRB1''SLC4A10''IL4I1''GZMK''COLQ''ZBTB16''ADAM12''IL32''AL136456.1''AGAP1'\n\n    $cluster4\n        \n'TCF7L2''CST3''HLA-DPA1''FCGR3A''LST1''HLA-DPB1''AIF1''HLA-DRB1''HLA-DRA''IFI30'\n\n    $cluster8\n        \n'GNLY''NKG7''PRF1''KLRD1''GZMA''KLRF1''SPON2''FGFBP2''CST7''GZMB'\n\n    $cluster9\n        \n'PPBP''GP1BB''NRGN''PF4''CAVIN2''TUBB1''CLU''GNG11''RGS18''HIST1H2AC'\n\n\n\n\n위 코드의 출력을 가지고 ChatGPT에 입력했더니 다음과 같이 말했습니다.\n주어진 유전자 목록만으로는 각각의 클러스터의 정확한 세포 유형을 알 수 없습니다. 유전자는 여러 종류의 세포에서 발현될 수 있기 때문입니다. 유전자의 발현 패턴을 분석하거나 추가적인 실험 결과와 함께 고려해야만 각 클러스터의 세포 유형을 추론할 수 있습니다.\n\n그러나 일반적으로 각 클러스터에 대한 예상 세포 유형은 다음과 같습니다:\n\n$cluster0: 이 클러스터는 PBMC에서 neutrophils을 나타내는 것으로 추정됩니다.\n$cluster1: 이 클러스터는 PBMC에서 CD4 T cells을 나타내는 것으로 추정됩니다.\n$cluster2: 이 클러스터는 PBMC에서 B cells을 나타내는 것으로 추정됩니다.\n$cluster3: 이 클러스터는 PBMC에서 B cells을 나타내는 것으로 추정됩니다.\n$cluster4: 이 클러스터는 PBMC에서 monocytes을 나타내는 것으로 추정됩니다.\n$cluster5: 이 클러스터는 PBMC에서 NK cells을 나타내는 것으로 추정됩니다.\n$cluster6: 이 클러스터는 PBMC에서 B cells을 나타내는 것으로 추정됩니다.\n$cluster7: 이 클러스터는 PBMC에서 CD8 T cells을 나타내는 것으로 추정됩니다.\n$cluster8: 이 클러스터는 PBMC에서 NK cells을 나타내는 것으로 추정됩니다.\n$cluster9: 이 클러스터는 PBMC에서 platelets을 나타내는 것으로 추정됩니다.\n위의 결과를 통해 대부분의 클러스터 타입을 알 수 있지만, B cell이 3개 NK cell이 2개로 중복됩니다. 좀 더 세분화하기 위해 B cell의 유전자 마커를 물어봅니다.\nB cell은 다양한 sub type으로 분류될 수 있습니다. 일부 sub type들과 그에 따른 대표 마커들은 다음과 같습니다.\n\n- Naive B cell\n    CD19, CD20, CD27-, IgD+, IgM+\n- Memory B cell\n    CD19, CD20, CD27+, IgD-, IgM+/-\n- Plasma cell\n    CD19, CD20-, CD38++, CD138+, MUM1+, IRF4+, XBP1+\n- Germinal center B cell\n    CD19, CD20, CD77+, CD10+, BCL6+, AID+\n- Regulatory B cell (Breg)\n    CD19, CD20, CD24++, CD38++, CD25+, IL-10+\n- B1 B cell\n    CD19, CD20, CD27+, IgM+, CD43+\n- Marginal zone B cell\n    CD19, CD20, CD27-, CD43-, IgM+, IgD+, CD23-, CD21+\n\n각 sub type마다 대표적인 마커들이 있지만, 이것들만으로 모든 sub type을 완벽하게 분류하는 것은 어렵습니다. 따라서, sub type 분류는 보통 여러 마커를 조합하여 결정됩니다.\n이번에는 NK cell 세분화를 위한 유전자 마커를 물어봅니다.\nNK 세포는 크게 두 가지 subset으로 나눌 수 있습니다: CD56bright NK 세포와 CD56dim NK 세포입니다. CD56bright NK 세포는 주로 면역 조절에 관여하며, cytokine을 분비하는 역할을 합니다. 반면 CD56dim NK 세포는 주로 세포 독성에 관여하며, 인체 내에서 암세포와 바이러스에 대한 첫 번째 방어선 역할을 합니다.\n\nCD56bright NK 세포를 구별하는 유전자 마커로는 CD56(bright) (NCAM1)과 CD16 (FCGR3A)이 있습니다. 반면 CD56dim NK 세포를 구별하는 유전자 마커로는 CD56(dim) (NCAM1)과 KIR (Killer-cell immunoglobulin-like receptors)가 있습니다. KIR은 NK 세포의 분화 상태, 활성 상태, 대상 세포와의 상호 작용 등을 조절합니다.\n위의 결과와 pbmc1k_marker.csv 파일을 비교하면서 다음과 같이 클러스터의 이름을 지정합니다.\n\nChatGPT를 사용한 방법은 정확하지 않고 힌트정도만 알 수 있습니다. 실제 실험결과라면 SingleR 패키지 혹은 문헌 검색을 통해 직접 찾아보시기 바랍니다.\n\n그리고 UMAP 플랏을 그려서 결과를 확인합니다.\n\n# 참조를 위해 이전 ID 클래스(클러스터 레이블)를 저장합니다.\nseurat_obj[[\"old.ident\"]] &lt;- Idents(object = seurat_obj)\n\n# 레이블 변경하기\nseurat_obj &lt;- RenameIdents(\n    object = seurat_obj,\n    `0` = \"neutrophils\",\n    `1` = \"CD4+ T cells\",\n    `2` = \"naive B cells\",\n    `3` = \"Plasma cells\",\n    `4` = \"monocytes\",\n    `5` = \"CD56bright NK cells\",\n    `6` = \"memory  B cells\",\n    `7` = \"CD8 T cells\",\n    `8` = \"CD56dim NK cells\",\n    `9` = \"platelet\"\n    )\n\n\np &lt;- DimPlot(seurat_obj, reduction = \"umap\", label = TRUE, pt.size = 0.5) + NoLegend()\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\np"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#클러스터당-세포-수-확인",
    "href": "posts/ipynb/R_seurat_tutorials.html#클러스터당-세포-수-확인",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "8.1 클러스터당 세포 수 확인",
    "text": "8.1 클러스터당 세포 수 확인\n\ntable(Idents(seurat_obj))\n\n\n        neutrophils        CD4+ T cells       naive B cells        Plasma cells \n                284                 204                 169                 130 \n          monocytes CD56bright NK cells     memory  B cells         CD8 T cells \n                 87                  74                  60                  54 \n   CD56dim NK cells            platelet \n                 53                  15"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#클러스터당-세포-비율-확인",
    "href": "posts/ipynb/R_seurat_tutorials.html#클러스터당-세포-비율-확인",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "8.2 클러스터당 세포 비율 확인",
    "text": "8.2 클러스터당 세포 비율 확인\n\nprop.table(table(Idents(seurat_obj)))\n\n\n        neutrophils        CD4+ T cells       naive B cells        Plasma cells \n         0.25132743          0.18053097          0.14955752          0.11504425 \n          monocytes CD56bright NK cells     memory  B cells         CD8 T cells \n         0.07699115          0.06548673          0.05309735          0.04778761 \n   CD56dim NK cells            platelet \n         0.04690265          0.01327434"
  },
  {
    "objectID": "posts/ipynb/R_seurat_tutorials.html#rds-파일로-저장하기",
    "href": "posts/ipynb/R_seurat_tutorials.html#rds-파일로-저장하기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "8.3 RDS 파일로 저장하기",
    "text": "8.3 RDS 파일로 저장하기\nRDS 파일을 저장해두면 추후 분석에 위의 과정을 반복할 필요가 없습니다.\n\nsaveRDS(seurat_obj, file = \"../output/pbmc1k_final.rds\")\n\n지금 까지 살펴본 내용을 요약해보면 다음과 같습니다.\n\nPre-processing: 데이터 전처리를 통해 불필요한 변수 제거, 정규화 등을 수행합니다.\nDimensionality reduction: 차원 축소 기법을 사용해 데이터의 주요 구조를 파악합니다.\nClustering: 유사한 특성을 가진 데이터들을 그룹화합니다.\nCell type identification: 각 클러스터에 대해 유전자 발현 패턴 등을 비교하여 cell type을 추론합니다.\n\n이후 진행되는 scRNA-seq Downstream analysis는 여기서 얻은 결과를 기반으로 합니다. 따라서 여기서의 결과가 부정확하거나 신뢰성이 떨어지면 추가 분석에서 얻은 결과 또한 의미가 없습니다. 그러므로 분석에서 사용된 데이터의 품질, 분석 방법의 적절성, 도구의 성능 등을 철저히 검토하고 확실한 기준에 따라 분석을 수행하세요. 또한, 추후에 데이터나 분석 방법이 변경되는 경우 이전의 결과와의 비교를 통해 신뢰성을 유지할 수 있도록 관리하는 것도 잊지 말아야 합니다."
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#그룹화grouping",
    "href": "posts/ipynb/python_marsilea.html#그룹화grouping",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "2.1 그룹화(Grouping)",
    "text": "2.1 그룹화(Grouping)\n그룹화(Grouping)를 사용하면 데이터를 그룹으로 분류하고 각 그룹에 대한 시각화를 할 수 있습니다. 우리는 vsplit() 함수를 사용하여 캔버스를 세 그룹으로 나눕니다. labels 매개변수는 각 열에 대한 그룹을 지정합니다. order 매개변수는 플롯에 표시될 그룹의 순서를 지정합니다. 이제 그룹을 시각적으로 더 명확하게 만들기 위해 측면 플롯을 추가해 봅시다. 여기서 spacing은 캔버스 너비의 일부분을 나타냅니다."
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#추가-구성-요소로-그룹-주석-달기",
    "href": "posts/ipynb/python_marsilea.html#추가-구성-요소로-그룹-주석-달기",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "2.2 추가 구성 요소로 그룹 주석 달기",
    "text": "2.2 추가 구성 요소로 그룹 주석 달기\n우리는 add_top()을 사용하여 캔버스 상단에 Chunk 플롯을 추가합니다. Chunk 플롯은 그룹을 주석 처리하는 데 사용되는 주석 플롯입니다. 추가로 size 및 pad 매개변수를 사용하여 플롯의 크기와 플롯 간의 간격을 조정할 수 있습니다. 단위는 인치입니다.\n\n\n\n\n\n\nNote\n\n\n\n텍스트를 그리는 Chunk와 같은 플롯의 경우 플롯의 크기가 자동으로 텍스트에 맞게 조정되므로 플롯의 크기를 지정할 필요가 없습니다.\n\n\n\ncb.vsplit(labels=[\"c1\", \"c1\", \"c2\", \"c2\", \"c3\", \"c3\"], order=[\"c1\", \"c2\", \"c3\"], spacing=0.08)\n\ngroup_labels = mp.Chunk([\"c1\", \"c2\", \"c3\"], [\"#FF6D60\", \"#F7D060\", \"#F3E99F\"])\ncb.add_top(group_labels, size=0.2, pad=0.1)\ncb.render()"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#계층적-클러스터링",
    "href": "posts/ipynb/python_marsilea.html#계층적-클러스터링",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "2.3 계층적 클러스터링",
    "text": "2.3 계층적 클러스터링\nadd_dendrogram()를 사용하여 캔버스 하단에 덴드로그램을 추가합니다. 덴드로그램은 계층적 클러스터링 과정을 기록한 나무 모양의 다이어그램입니다. Marsilea에서는 클러스터링이 히트맵에만 국한되지 않고 다양한 시각화에서 수행될 수 있습니다.\n여기서 주목할 점은 그룹의 순서와 그룹 내 순서가 클러스터링 결과에 따라 자동으로 변경된다는 것입니다.\n\ncb.add_dendrogram(\"bottom\", colors=\"g\")\ncb.render()"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#하단-플롯-및-제목-추가",
    "href": "posts/ipynb/python_marsilea.html#하단-플롯-및-제목-추가",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "2.4 하단 플롯 및 제목 추가",
    "text": "2.4 하단 플롯 및 제목 추가\n우리는 메인 시각화에 더 많은 플롯을 추가할 수 있습니다. 여기서는 바 플롯을 하단에 추가하고 add_title()을 사용하여 맨 위에 제목을 추가합니다.\n\ncb.add_bottom(ma.plotter.Bar(data, color=\"#577D86\"), size=2, pad=0.1)\ncb.add_title(top=\"My First Marsilea Example\")\ncb.render()"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#저장하기",
    "href": "posts/ipynb/python_marsilea.html#저장하기",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "2.5 저장하기",
    "text": "2.5 저장하기\nsave()를 사용하여 파일로 저장할 수 있습니다.\ncb.save(\"my_first_marsilea_example.png\")\n또는 모든 맷플롯립 그림을 저장하는 방식과 동일하게 저장할 수 있습니다. .figure.로 figure 객체에 액세스할 수 있습니다. 잘려나가는 것을 피하기 위해 bbox_inches=\"tight\" 모드로 저장하는 것이 권장됩니다. 또는 캔버스의 여백을 늘릴 수도 있습니다.\ncb.figure.savefig(\"my_first_marsilea_example.png\", bbox_inches=\"tight\")"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#요약",
    "href": "posts/ipynb/python_marsilea.html#요약",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "2.6 요약",
    "text": "2.6 요약\nMarsilea에서 시각화를 제어하는 데 사용할 수 있는 메서드 목록입니다.\n\n메인 레이어에 추가: add_layer()\n측면에 추가:\n\n왼쪽: add_left()\n오른쪽: add_right()\n위쪽: add_top()\n아래쪽: add_bottom()\n\n그룹화:\n\n수직 그룹화: hsplit()\n수평 그룹화: vsplit()\n\n덴드로그램 추가: add_dendrogram()\n제목 추가: add_title()\n범례 추가: add_legends()\n플롯 저장: save()"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#데이터-불러오기",
    "href": "posts/ipynb/python_marsilea.html#데이터-불러오기",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "4.1 데이터 불러오기",
    "text": "4.1 데이터 불러오기\n데이터를 불러와서 각 아미노산의 높이를 계산합니다. 관련 정보는 위키를 참조하세요.\n\nfrom collections import Counter\n\nseq = ma.load_data(\"seq_align\")\nseq = seq.iloc[:, 130:175]\n\ncollect = []\nfor _, col in seq.items():\n    collect.append(Counter(col))\n\nhm = pd.DataFrame(collect)\ndel hm[\"-\"]\nhm = hm.T.fillna(0.0)\nhm.columns = seq.columns\nhm /= hm.sum(axis=0)\n\nn = hm.shape[1]\ns = 20\nEn = (1 / np.log(2)) * ((s - 1) / (2 * n))\n\nheights = []\nfor _, col in hm.items():\n    H = -(np.log2(col) * col).sum()\n    R = np.log2(20) - (H + En)\n    heights.append(col * R)\n\nlogo = pd.DataFrame(heights).T\nlogo.head()\n\n\n\n\n\n\n\n\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n...\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n\n\n\n\nF\n2.636345\n0.000000\n0.000000\n0.00000\n0.0\n0.000000\n0.000000\n0.0\n0.336123\n0.000000\n...\n0.0\n0.000000\n0.000000\n0.0\n0.979395\n0.00000\n0.00000\n0.00000\n0.00000\n0.0\n\n\nA\n0.659086\n2.636345\n0.000000\n0.00000\n0.0\n0.753879\n0.753879\n0.0\n0.336123\n1.344492\n...\n0.0\n0.000000\n0.376939\n0.0\n0.000000\n0.00000\n0.00000\n0.26218\n0.00000\n0.0\n\n\nT\n0.000000\n0.659086\n0.000000\n0.28342\n0.0\n0.000000\n0.000000\n0.0\n0.336123\n0.000000\n...\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n1.50868\n0.00000\n0.00000\n0.00000\n0.0\n\n\nN\n0.000000\n0.000000\n1.305860\n0.28342\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n...\n0.0\n2.066042\n0.000000\n0.0\n0.000000\n0.00000\n0.00000\n0.00000\n0.29169\n0.0\n\n\nM\n0.000000\n0.000000\n0.979395\n0.00000\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n...\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n0.00000\n0.25217\n0.00000\n0.00000\n0.0\n\n\n\n\n5 rows × 45 columns"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#색상-팔레트-및-데이터-준비",
    "href": "posts/ipynb/python_marsilea.html#색상-팔레트-및-데이터-준비",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "4.2 색상 팔레트 및 데이터 준비",
    "text": "4.2 색상 팔레트 및 데이터 준비\n\ncolor_encode = {\n    \"A\": \"#f76ab4\",\n    \"C\": \"#ff7f00\",\n    \"D\": \"#e41a1c\",\n    \"E\": \"#e41a1c\",\n    \"F\": \"#84380b\",\n    \"G\": \"#f76ab4\",\n    \"H\": \"#3c58e5\",\n    \"I\": \"#12ab0d\",\n    \"K\": \"#3c58e5\",\n    \"L\": \"#12ab0d\",\n    \"M\": \"#12ab0d\",\n    \"N\": \"#972aa8\",\n    \"P\": \"#12ab0d\",\n    \"Q\": \"#972aa8\",\n    \"R\": \"#3c58e5\",\n    \"S\": \"#ff7f00\",\n    \"T\": \"#ff7f00\",\n    \"V\": \"#12ab0d\",\n    \"W\": \"#84380b\",\n    \"Y\": \"#84380b\",\n    \"-\": \"white\",\n}\n\nmax_aa = []\nfreq = []\n\nfor _, col in hm.items():\n    ix = np.argmax(col)\n    max_aa.append(hm.index[ix])\n    freq.append(col[ix])\n\nposition = []\nmock_ticks = []\nfor i in seq.columns:\n    if int(i) % 10 == 0:\n        position.append(i)\n        mock_ticks.append(\"^\")\n    else:\n        position.append(\"\")\n        mock_ticks.append(\"\")\n\nprint(position)\n\n['130', '', '', '', '', '', '', '', '', '', '140', '', '', '', '', '', '', '', '', '', '150', '', '', '', '', '', '', '', '', '', '160', '', '', '', '', '', '', '', '', '', '170', '', '', '', '']"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#시각화",
    "href": "posts/ipynb/python_marsilea.html#시각화",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "4.3 시각화",
    "text": "4.3 시각화\n\nheight = 2\nwidth = height * seq.shape[1] / seq.shape[0]\n\nch = ma.CatHeatmap(seq.to_numpy(), palette=color_encode, height=height, width=width)\nch.add_layer(ma.plotter.TextMesh(seq.to_numpy()))\nch.add_top(ma.plotter.SeqLogo(logo, color_encode=color_encode), pad=0.1, size=2)\nch.add_left(ma.plotter.Labels(seq.index), pad=0.1)\nch.add_bottom(ma.plotter.Labels(mock_ticks, rotation=0), pad=0.1)\nch.add_bottom(ma.plotter.Labels(position, rotation=0))\nch.add_bottom(\n    ma.plotter.Numbers(freq, width=0.9, color=\"#FFB11B\", show_value=False),\n    name=\"freq_bar\",\n    size=2,\n)\nch.add_bottom(ma.plotter.Labels(max_aa, rotation=0), pad=0.1)\nch.render()\n\nch.get_ax(\"freq_bar\").set_axis_off()"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#데이터-불러오기-1",
    "href": "posts/ipynb/python_marsilea.html#데이터-불러오기-1",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "5.1 데이터 불러오기",
    "text": "5.1 데이터 불러오기\n\ndata = ma.load_data(\"les_miserables\")\nnodes = data[\"nodes\"]\nlinks = data[\"links\"]\n\nsizes = nodes[\"value\"].to_numpy().reshape(1, -1)\ncolors = nodes[\"group\"].to_numpy().reshape(1, -1)\ndata\n\n{'nodes':                name  group  value\n 0            Myriel      1     31\n 1          Napoleon      1      0\n 2   Mlle.Baptistine      1      9\n 3      Mme.Magloire      1      3\n 4      CountessdeLo      1      0\n ..              ...    ...    ...\n 72        Toussaint      5      0\n 73           Child1     10      3\n 74           Child2     10      0\n 75           Brujon      4      0\n 76    Mme.Hucheloup      8      0\n \n [77 rows x 3 columns],\n 'links':      source  target\n 0         1       0\n 1         2       0\n 2         3       0\n 3         3       2\n 4         4       0\n ..      ...     ...\n 249      76      66\n 250      76      63\n 251      76      62\n 252      76      48\n 253      76      58\n \n [254 rows x 2 columns]}"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#아크-다이어그램-그리기",
    "href": "posts/ipynb/python_marsilea.html#아크-다이어그램-그리기",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "5.2 아크 다이어그램 그리기",
    "text": "5.2 아크 다이어그램 그리기\n\npalette = {\n    0: \"#3C486B\",\n    1: \"#F0F0F0\",\n    2: \"#F9D949\",\n    3: \"#F45050\",\n    4: \"#F2E3DB\",\n    5: \"#41644A\",\n    6: \"#E86A33\",\n    7: \"#009FBD\",\n    8: \"#77037B\",\n    9: \"#4F4557\",\n    10: \"#B0DAFF\",\n}\n\nlink_colors = [palette[nodes.iloc[i].group] for i in links[\"source\"]]\n\nheight = 0.5\nwidth = height * len(nodes) / 3\n\nsh = ma.SizedHeatmap(\n    sizes,\n    colors,\n    palette=palette,\n    sizes=(10, 200),\n    frameon=False,\n    height=height,\n    width=width,\n)\nsh.add_bottom(mp.Labels(nodes[\"name\"], fontsize=8))\narc = mp.Arc(nodes.index, links.to_numpy(), colors=link_colors, lw=0.5, alpha=0.5)\nsh.add_top(arc, size=3)\nsh.add_title(top=\"Character relationships in Les Miserables\", align=\"right\", fontstyle=\"italic\")\nsh.render()"
  },
  {
    "objectID": "posts/ipynb/python_marsilea.html#데이터셋-불러오기-및-전처리",
    "href": "posts/ipynb/python_marsilea.html#데이터셋-불러오기-및-전처리",
    "title": "Marsilea로 복잡한 시각화 처리하기",
    "section": "6.1 데이터셋 불러오기 및 전처리",
    "text": "6.1 데이터셋 불러오기 및 전처리\n\nfrom legendkit import cat_legend\nfrom matplotlib.colors import LinearSegmentedColormap\n\nembryo = ma.load_data(\"mouse_embryo\")\n\nxmax = embryo[\"cell_x\"].max()\nymax = embryo[\"cell_y\"].max()\nxstart, xend = -xmax * 0.05, xmax * 1.05\nystart, yend = -ymax * 0.05, ymax * 1.05\n\nxrange = np.linspace(xstart, xend, 200)\nyrange = np.linspace(ystart, yend, 200)\n\nxmid = (xrange[1:] + xrange[:-1]) / 2\nymid = (yrange[1:] + yrange[:-1]) / 2\n\n\ndef get_xy_hist(ct):\n    x = embryo[embryo[\"cell_type\"] == ct][\"cell_x\"].to_numpy()\n    y = embryo[embryo[\"cell_type\"] == ct][\"cell_y\"].to_numpy()\n    xhist, _ = np.histogram(x, bins=xrange)\n    yhist, _ = np.histogram(y, bins=yrange)\n    return xhist, yhist\n\n\nembryo\n\n\n\n\n\n\n\n\ncell_x\ncell_y\ncell_type\ntimepoint\ncolors\n\n\n\n\n0\n133.599963\n316.800580\nCavity\nE12.5\n#6d32e6\n\n\n1\n132.733938\n316.300580\nCavity\nE12.5\n#6d32e6\n\n\n2\n131.867912\n315.800580\nBrain\nE12.5\n#bf024f\n\n\n3\n131.001887\n315.300580\nBrain\nE12.5\n#bf024f\n\n\n4\n130.135861\n314.800580\nBrain\nE12.5\n#bf024f\n\n\n...\n...\n...\n...\n...\n...\n\n\n51360\n4.928203\n243.666605\nCavity\nE12.5\n#6d32e6\n\n\n51361\n4.062178\n243.166605\nCavity\nE12.5\n#6d32e6\n\n\n51362\n3.196152\n242.666605\nCavity\nE12.5\n#6d32e6\n\n\n51363\n2.330127\n242.166605\nCavity\nE12.5\n#6d32e6\n\n\n51364\n1.464102\n241.666605\nCavity\nE12.5\n#6d32e6\n\n\n\n\n51365 rows × 5 columns\n\n\n\n각 세포 유형에 대한 색상을 지정하고 시각화합니다.\n\ncolormap = {\n    \"Cavity\": \"#6d32e6\",\n    \"Brain\": \"#bf024f\",\n    \"Meninges\": \"#d147a3\",\n    \"Choroid plexus\": \"#b3a726\",\n    \"Cartilage primordium\": \"#103a14\",\n    \"Jaw and tooth\": \"#ef833a\",\n    \"Connective tissue\": \"#b38b5c\",\n    \"Epidermis\": \"#35586d\",\n    \"Lung primordium\": \"#3cb44b\",\n    \"Sympathetic nerve\": \"#dfdce0\",\n    \"Liver\": \"#bd3add\",\n    \"Mucosal epithelium\": \"#0bd3b1\",\n    \"GI tract\": \"#ff4374\",\n    \"Mesentery\": \"#b74c11\",\n    \"Dorsal root ganglion\": \"#036df4\",\n    \"Muscle\": \"#dd7936\",\n    \"Mesothelium\": \"#5c5ca6\",\n    \"Blood vessel\": \"#be9b72\",\n    \"Urogenital ridge\": \"#d3245a\",\n    \"Heart\": \"#03fff4\",\n    \"Pancreas\": \"#f062f9\",\n    \"Kidney\": \"#62cfe8\",\n    \"Ovary\": \"#c923b1\",\n}\n\nwidth = 5\nheight = width * (yend - ystart) / (xend - xstart)\nb = ma.WhiteBoard(height=height, width=width)\n\ncell_types = [\"Brain\", \"Cartilage primordium\", \"Liver\", \"Heart\", \"GI tract\"]\nfor n in cell_types:\n    b.add_canvas(\"bottom\", size=0.2, pad=0.1, name=f\"{n}-x\")\n    b.add_canvas(\"right\", size=0.2, pad=0.1, name=f\"{n}-y\")\nb.render()\n\n# Draw cell\nax = b.get_main_ax()\npoints = ax.scatter(embryo[\"cell_x\"], embryo[\"cell_y\"], s=1, c=embryo[\"colors\"])\npoints.set_rasterized(True)\nax.set_xlim(xstart, xend)\nax.set_ylim(ystart, yend)\nax.set_title(\"Mouse Embryo E12.5\")\nax.set_axis_off()\n\ncolors = list(colormap.values())\nlabels = list(colormap.keys())\ncat_legend(colors=colors, labels=labels, ax=ax, loc=\"out left center\", fontsize=10)\n\nfor n in cell_types:\n    xh, yh = get_xy_hist(n)\n    cmap = LinearSegmentedColormap.from_list(n, [\"white\", colormap[n]])\n    x_ax = b.get_ax(f\"{n}-x\")\n    x_ax.pcolormesh(xh.reshape(1, -1), cmap=cmap)\n    x_ax.set_axis_off()\n    x_ax.text(0, 0.5, n, va=\"center\", ha=\"right\", transform=x_ax.transAxes)\n\n    y_ax = b.get_ax(f\"{n}-y\")\n    y_ax.pcolormesh(yh.reshape(-1, 1), cmap=cmap)\n    y_ax.set_axis_off()\n    y_ax.text(0.5, 0, n, va=\"top\", ha=\"center\", rotation=90, transform=y_ax.transAxes)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_02.html",
    "href": "posts/ipynb/scanpy_workshop_02.html",
    "title": "Scanpy로 scRNA-seq 분석 02",
    "section": "",
    "text": "이번 실습에서는 데이터의 시각화를 위한 차원축소 방법인 PCA, tSNE, UMAP등에 대해 배우고 배치 효과를 제거해서 여러 데이터셋을 통합하는 방법에 대해 살펴봅니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_02.html#bbknn",
    "href": "posts/ipynb/scanpy_workshop_02.html#bbknn",
    "title": "Scanpy로 scRNA-seq 분석 02",
    "section": "7.1 BBKNN",
    "text": "7.1 BBKNN\n먼저 BBKNN(Batch Balanced KNN)을 실행해 보겠습니다. BBKNN은 빠르고 직관적인 배치 효과 제거 도구로 scanpy 워크플로우에서 바로 사용할 수 있습니다. 또한 scanpy.pp.neighbors() 함수 대신에 사용할 수 있습니다. 더 자세한 내용은 BBKNN 문서를 참조하세요.\n\nadata_bbknn = adata.copy()\nneighbors_within_batch = 25 if adata.n_obs &gt; 100000 else 3\n\nbbknn.bbknn(adata_bbknn, batch_key=\"sample\", neighbors_within_batch=neighbors_within_batch)\n\nsc.tl.umap(adata_bbknn)\nsc.tl.tsne(adata_bbknn)\n\n이제 통합되지 않은 공간과 통합된 공간의 축소된 치수를 그릴 수 있습니다.\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 8), constrained_layout=True)\nsc.pl.tsne(adata_bbknn, color=\"sample\", title=\"BBKNN Corrected tsne\", ax=axs[0, 1], show=False)\nsc.pl.tsne(adata, color=\"sample\", title=\"Uncorrected tsne\", ax=axs[0, 0], show=False)\nsc.pl.umap(adata_bbknn, color=\"sample\", title=\"BBKNN Corrected umap\", ax=axs[1, 1], show=False)\nsc.pl.umap(adata, color=\"sample\", title=\"Uncorrected umap\", ax=axs[1, 0], show=False)\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_02.html#scanorama",
    "href": "posts/ipynb/scanpy_workshop_02.html#scanorama",
    "title": "Scanpy로 scRNA-seq 분석 02",
    "section": "7.2 Scanorama",
    "text": "7.2 Scanorama\nScanorama는 scRNA-seq 데이터 세트의 일괄 보정 및 통합을 사용할 수 있도록 설계되었습니다. Scanorama를 통한 보정의 결과는 다른 도구의 입력으로 사용하여 scRNA-seq 클러스터링, 시각화 및 분석에 사용할 수 있습니다. 이제 Scanorama도 사용해 보겠습니다. 먼저, 각 데이터 세트에서 개별 AnnData 객체를 만들어야 합니다.\n\n# 배치별로 개체를 분할합니다.\nbatches = adata.obs[\"sample\"].cat.categories.tolist()\nalldata = {}\nfor batch in batches:\n    alldata[batch] = adata[adata.obs[\"sample\"] == batch,]\n\n# 개별 데이터 집합을 처음에 정의한 가변 유전자로 하위 집합합니다.\nalldata2 = {}\nfor ds in alldata.keys():\n    print(ds)\n    alldata2[ds] = alldata[ds][:, var_genes]\n\n# AnnData 객체 목록으로 변환\nadatas = list(alldata2.values())\n\n# scanorama.integrate 실행\nscanorama.integrate_scanpy(adatas, dimred=50, verbose=False)\n\n# scanorama는 adatas의 각 데이터 집합에서 수정된 행렬을 adata.obsm에 추가합니다.\nadatas[0].obsm[\"X_scanorama\"].shape\n\ncovid_1\ncovid_15\ncovid_16\ncovid_17\nctrl_5\nctrl_13\nctrl_14\nctrl_19\nFound 4268 genes among all datasets\n\n\n(874, 50)\n\n\n\n# 모든 통합 행렬을 가져옵니다.\nscanorama_int = [ad.obsm[\"X_scanorama\"] for ad in adatas]\n\n# 하나의 행렬로 만듭니다.\nall_s = np.concatenate(scanorama_int)\nprint(all_s.shape)\n\n# AnnData 객체에 추가하려면 먼저 새 객체를 만듭니다.\nadata_sc = adata.copy()\nadata_sc.obsm[\"Scanorama\"] = all_s\n\n(7227, 50)\n\n\n\n# tsne 과 umap을 그립니다.\nsc.pp.neighbors(adata_sc, n_pcs=30, use_rep=\"Scanorama\")\nsc.tl.umap(adata_sc)\nsc.tl.tsne(adata_sc, n_pcs=30, use_rep=\"Scanorama\")\n\n이제 통합되지 않은 공간과 통합된 공간을 축소한 차원을 그릴 수 있습니다.\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 8), constrained_layout=True)\nsc.pl.tsne(adata, color=\"sample\", title=\"Uncorrected tsne\", ax=axs[0, 0], show=False)\nsc.pl.tsne(adata_sc, color=\"sample\", title=\"Scanorama tsne\", ax=axs[0, 1], show=False)\nsc.pl.umap(adata, color=\"sample\", title=\"Uncorrected umap\", ax=axs[1, 0], show=False)\nsc.pl.umap(adata_sc, color=\"sample\", title=\"Scanorama umap\", ax=axs[1, 1], show=False)\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_02.html#결과-살펴보기",
    "href": "posts/ipynb/scanpy_workshop_02.html#결과-살펴보기",
    "title": "Scanpy로 scRNA-seq 분석 02",
    "section": "7.3 결과 살펴보기",
    "text": "7.3 결과 살펴보기\n이제 세 가지 통합 방법을 나란히 사용하여 UMAP을 그려보겠습니다.\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 8), constrained_layout=True)\nsc.pl.umap(adata, color=\"sample\", title=\"Uncorrected\", ax=axs[0, 0], show=False)\nsc.pl.umap(adata_bbknn, color=\"sample\", title=\"BBKNN\", ax=axs[0, 1], show=False)\nsc.pl.umap(adata_sc, color=\"sample\", title=\"Scanorama\", ax=axs[1, 0], show=False)\naxs[-1, -1].axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n배치 효과 제거를 위해 사용할 수 있는 다양한 방법이 있으며, 각 방법에는 장단점이 있습니다. 가장 적합한 방법을 선택하는 데에는 데이터의 특성과 분석 목표에 따라 달라질 수 있습니다. 그러니 다양한 방법을 시도하여 데이터 특성에 가장 잘 맞는 방법을 찾는 것이 중요합니다.\n\n\n\n\n\n\nNote\n\n\n\n배치 효과가 잘 제거되었는지 어떻게 평가할 수 있을까요? 크게 두가지 방법이 있습니다.\n\n시각적 평가: UMAP 플롯을 통해 배치가 잘 섞여 있는지 시각적으로 확인합니다. 배치 간의 구분이 사라지면 배치 효과가 잘 제거된 것입니다.\n실루엣 스코어: 높은 실루엣 스코어는 잘 클러스터링된 데이터를 의미합니다. 배치 효과 제거 전후의 실루엣 스코어를 비교하여 배치 효과 제거의 효과를 평가합니다.\n\n\n\n시각적 평가를 통해 어떤 도구가 배치 효과 보정을 잘 했는지 살펴보면, 아무래도 Scanorma로 한 것이 좋아보입니다. 이후의 분석은 adata_sc 객체를 가지고 진행하겠습니다."
  },
  {
    "objectID": "posts/md/How_DS_works.html#데이터-검사하기",
    "href": "posts/md/How_DS_works.html#데이터-검사하기",
    "title": "어떻게 데이터 과학은 작동하는가?",
    "section": "5.1 데이터 검사하기",
    "text": "5.1 데이터 검사하기\n다음 단계는 데이터를 깊게 살펴보는 것입니다. 여기에는 두 가지 목적이 있습니다. 첫 번째는 잘못된 데이터를 발견하여 수정하거나 제거하는 것입니다. 다른 하나는 각 행과 열에 익숙해지는 것입니다. 이 단계를 건너뛰고 데이터를 최대한 활용 할 수는 없습니다.\n데이터의 한 열만 살펴보세요. 무엇에 레이블이 붙어 있나요? 값이 레이블에 맞는가요? 그 레이블이 여러분에게 어떤 의미가 있나요? 열의 의미에 대한 문서가 있나요? 어떻게 측정되었는지에 대한 문서가 있나요? 스스로에게 물어보세요.\n각각의 열에 대한 히스토그램을 그려보고 분포가 기능에 대해 알고 있는 것과 일치하는지도 확인 해보세요. 혹시 비정상적인 수치가 있나요? 그런 이상값이 의미가 있나요? 어떤 식으로든 조금이라도 이상해 보인다면 그 이유를 찾아보세요."
  },
  {
    "objectID": "posts/md/How_DS_works.html#데이터-수정하기",
    "href": "posts/md/How_DS_works.html#데이터-수정하기",
    "title": "어떻게 데이터 과학은 작동하는가?",
    "section": "5.2 데이터 수정하기",
    "text": "5.2 데이터 수정하기\n각각의 열을 살펴보면서 혹시 이름과 설명이 오해의 소지가 있거나 잘못된 것을 발견할 수도 있습니다. 혹은 일부 값이 잘못되었다는 사실을 발견할 수도 있습니다. 이 경우 세 가지 선택지가 있습니다. 첫 번째, 수정이 분명해 보이는 경우 값을 수정한다 (예: 키 72 미터를 72 인치로 변환). 두 번째, 수정이 분명하지 않은 경우 값을 삭제하고 누락된 상태로 둘 수 있습니다. 세 번째, 값이 중요한 정보인 경우 전체 행 또는 열을 제거할 수 있습니다. 이렇게 하면 잘못된 데이터로 모델을 학습시키는 것을 방지할 수 있습니다. 잘못된 데이터는 누락된 데이터보다 훨씬 더 큰 피해를 줍니다.\n가끔 바람직하지 않은 값이나 행을 제거하고 싶은 유혹이 있을 수 있습니다. 이러한 데이터는 여러분이 선호하는 이론을 뒷받침하지 않을 수 있습니다. 그러나 이렇게 하지는 마세요. 이것은 비윤리적이며 더 나쁜 것은 잘못된 답을 얻을 수 있다는 것입니다."
  },
  {
    "objectID": "posts/md/How_DS_works.html#결측치-다루기",
    "href": "posts/md/How_DS_works.html#결측치-다루기",
    "title": "어떻게 데이터 과학은 작동하는가?",
    "section": "5.3 결측치 다루기",
    "text": "5.3 결측치 다루기\n거의 모든 데이터 집합에는 누락된 값이 있습니다. 이것은 때로 잘못된 것으로 판명되어 삭제된 것이기도 하고 보통 서로 다른 소스에서 가져온 데이터를 합치는 경우에 생겨다는 경우가 있습니다. 어쨌거나 머신러닝에 사용하기 위해서는 결측치를 수정해야 합니다. 결측치를 수정하는 방법에는 여러 가지가 있습니다. 먼저 샘플을 보고 싶으시다면 Azure에서 결측치 다루기 를 확인하세요. 결론적으로 결측치를 수정하는 최선의 방법은 데이터 마다 다르다는 것입니다."
  },
  {
    "objectID": "posts/md/How_DS_works.html#footnotes",
    "href": "posts/md/How_DS_works.html#footnotes",
    "title": "어떻게 데이터 과학은 작동하는가?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA Pocket Guide to Data science (e2eml.school)↩︎\n유튜브 영상↩︎"
  },
  {
    "objectID": "posts/md/How_readPaper.html",
    "href": "posts/md/How_readPaper.html",
    "title": "학술 논문을 효율적으로 읽는 법",
    "section": "",
    "text": "데이터 과학 분야에서 머신 러닝, 딥 러닝, 새로운 데이터베이스 및 데이터 엔지니어링과 같은 분야에서 사용되는 기술들은 매우 복잡합니다. 게다가 새로운 기술들이 매주 쏟아져 나오고 있어서 최신 동향을 따라잡고 필요한 정보를 얻기 위해서는 꾸준히 논문을 읽고 공부해야 합니다. 다만 솔직히, 논문을 읽는 것은 쉬운 일이 아닙니다. 왜냐하면 논문은 독자들이 전문적인 지식을 갖추고 있다고 가정하고 쓰여지기 때문입니다. 그러나 논문을 읽는 것은 연습과 노력을 통해 향상될 수 있는 기술로 꾸준한 연습과 학습을 통해 논문을 이해하고 최신 지식을 습득하는 능력을 키워나갈 수 있습니다. 따라서 논문을 읽는 것을 두려워하지 말고 꾸준히 도전하고 연습하여 능력을 향상시켜 나가는 것이 중요합니다."
  },
  {
    "objectID": "posts/md/How_readPaper.html#논문-검색",
    "href": "posts/md/How_readPaper.html#논문-검색",
    "title": "학술 논문을 효율적으로 읽는 법",
    "section": "5.1 논문 검색",
    "text": "5.1 논문 검색\n\narXiv: 주로 과학, 수학, 공학, 컴퓨터 과학, 물리학, 통계학 등 다양한 분야의 논문을 자유롭게 다운로드하고 읽을 수 있습니다.\nPubMed: 미국 국립보건원 국립의학도서관 (NIH/NLM) 에서 제공하는 PubMed Central®(PMC) 은 생명과학 및 의학 관련 논문 전문 보관소입니다. PubMed 는 의학 및 생명 과학 분야의 논문을 검색하고 접근하기에 매우 유용한 도구입니다.\nGoogle Scholar: 구글의 검색 엔진을 활용하여 학술 문헌을 검색하는 도구입니다. 간단한 검색어를 입력하면 다양한 분야의 논문, 학위 논문, 책, 초록 등을 검색할 수 있습니다.\nSci-hub: 유료 학술 논문에 무료로 접근할 수 있게 해주는 웹사이트입니다. 저작권 문제로 논란의 여지가 있지만, 많은 연구자들이 이용하고 있습니다.\nSciencehubmutalaid: 연구자들이 서로 논문을 공유하는 사이트로 주로 유료 학술 논문을 위주로 읽고 싶은 논문에 대해서 요청을 하면 권한이 있는 다른 사람이 찾아 주고는 합니다.\nSCICSPACE: 논문 검색, 요약, 및 시각화 기능을 제공하는 플랫폼으로 LLM 기술을 사용해 문헌 조사 과정을 간소화하고 효율적으로 만듭니다."
  },
  {
    "objectID": "posts/md/How_readPaper.html#참고-문헌-관리",
    "href": "posts/md/How_readPaper.html#참고-문헌-관리",
    "title": "학술 논문을 효율적으로 읽는 법",
    "section": "5.2 참고 문헌 관리",
    "text": "5.2 참고 문헌 관리\n\nZotero: 오픈 소스 참고 문헌 관리 소프트웨어로, 웹 브라우저와 통합되어 온라인 자료를 쉽게 수집하고 관리할 수 있습니다.\nMendeley: 참고 문헌 관리와 학술 소셜 네트워크 기능을 결합한 도구로, PDF 주석 기능과 협업 기능이 강점입니다.\nPaperPile: 클라우드 기반의 참고 문헌 관리 도구로, Google Docs 와의 통합이 뛰어나 온라인 작업 환경에 적합합니다.\nEndnote: 광범위한 참고 문헌 관리 기능을 제공하는 소프트웨어로, 문헌 수집, 조직화, 인용 및 서지 작성을 효율적으로 수행할 수 있습니다. Microsoft Word 와의 통합이 뛰어나며, 다양한 인용 스타일을 지원합니다."
  },
  {
    "objectID": "posts/md/How_readPaper.html#footnotes",
    "href": "posts/md/How_readPaper.html#footnotes",
    "title": "학술 논문을 효율적으로 읽는 법",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHubbard, K. E., & Dunbar, S. D. (2017). Perceptions of scientific research literature and strategies for reading papers depend on academic career stage. PloS one, 12(12), e0189753.↩︎\nShout out to Chris at CoffeeCycle! Simply the best coffee in San Diego.↩︎\nKeshav, S. (2007). How to read a paper. ACM SIGCOMM Computer Communication Review, 37(3), 83–84.↩︎"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html",
    "href": "posts/md/Rosalind_algorithmicHeights.html",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "",
    "text": "Dasgupta, Papadimitriou, Vazirani 의 책 “알고리즘” 에 포함된 연습문제의 모음입니다.\nRosalind 는 프로젝트 오일러, 구글 코드 잼 에서 영감을 얻었습니다. 이 프로젝트의 이름은 DNA 이중나선을 발견하는 데 기여한 로잘린드 프랭클린 에서 따왔습니다. Rosalind 는 프로그래밍 실력을 키우고자 하는 생물학자와 분자생물학의 계산 문제를 접해본 적이 없는 프로그래머들에게 도움이 될 것입니다."
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#problem",
    "href": "posts/md/Rosalind_algorithmicHeights.html#problem",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "1.1 Problem",
    "text": "1.1 Problem\nThe Fibonacci numbers \\(0,1,1,2,3,5,8,13,21,34,…\\) are generated by the simple rule.\nGiven: A positive integer \\(n≤25\\).\nReturn: The value of Fn.\nSource: Algorithms by Dasgupta, Papadimitriou, Vazirani. McGraw-Hill. 2006."
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "1.2 Sample Dataset",
    "text": "1.2 Sample Dataset\n6"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "1.3 Sample Output",
    "text": "1.3 Sample Output\n8"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "1.4 Solution",
    "text": "1.4 Solution\ndef fibonacci(n: int) -&gt; int:\n    fib: list[int] = [0] * (n + 1)\n    fib[1] = 1\n    \n    for i in range(2, n + 1):\n        fib[i] = fib[i - 1] + fib[i - 2]\n    \n    return fib[n]\n\nn: int = 6\nprint(f\"피보나치 수열의 {n}번째 항: {fibonacci(n)}\")\n아주 큰수의 피보나치 수열을 계산하는 경우에는 다음 코드.\nimport gmpy2\nfrom gmpy2 import mpz\n\ndef fibonacci_gmpy2(n: int) -&gt; mpz:\n    a: mpz = gmpy2.mpz(0)\n    b: mpz = gmpy2.mpz(1)\n    \n    for _ in range(n):\n        a, b = b, a + b\n    \n    return a\n\n# 예제 사용\nn: int = 60000\nresult: mpz = fibonacci_gmpy2(n)\n\nprint(f\"피보나치 수열의 {n}번째 항:\")\nprint(result)\nprint(f\"자릿수: {len(str(result))}\")"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-1",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-1",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "2.1 Sample Dataset",
    "text": "2.1 Sample Dataset\n5\n6\n10 20 30 40 50\n40 10 35 15 40 20"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-1",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-1",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "2.2 Sample Output",
    "text": "2.2 Sample Output\n4 1 -1 -1 4 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-1",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-1",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "2.3 Solution",
    "text": "2.3 Solution\nfrom typing import List, Tuple\n\ndef binary_search(arr: List[int], target: int) -&gt; int:\n    low, high = 0, len(arr) - 1\n    \n    while low &lt;= high:\n        mid = (low + high) // 2\n        if arr[mid] == target:\n            return mid + 1  # Adding 1 to convert from 0-based to 1-based indexing\n        elif arr[mid] &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    \n    return -1  # Target not found\n\ndef parse_input(input_str: str) -&gt; Tuple[int, int, List[int], List[int]]:\n    lines = input_str.strip().split(\"\\n\")\n    n = int(lines[0])\n    m = int(lines[1])\n    array = list(map(int, lines[2].split()))\n    items = list(map(int, lines[3].split()))\n    \n    return n, m, array, items\n\nsample_input = \"\"\"\n5\n6\n10 20 30 40 50\n40 10 35 15 40 20\n\"\"\"\n\nn, m, array, items = parse_input(sample_input)\n\nresults: List[str] = [str(binary_search(array, item)) for item in items]\nprint(' '.join(results))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-2",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-2",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "3.1 Sample Dataset",
    "text": "3.1 Sample Dataset\n6\n6 10 4 5 1 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-2",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-2",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "3.2 Sample Output",
    "text": "3.2 Sample Output\n12"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-2",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-2",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "3.3 Solution",
    "text": "3.3 Solution\nfrom typing import List\n\ndef parse_input(input_str: str) -&gt; List[int]:\n    lines = input_str.strip().split(\"\\n\")\n    n = int(lines[0])  # Get the number of elements\n    array = list(map(int, lines[1].split()))\n    return array\n\ndef insertion_sort(array: List[int]) -&gt; int:\n    swaps = 0\n    for i in range(1, len(array)):\n        key = array[i]\n        j = i - 1\n        while j &gt;= 0 and array[j] &gt; key:\n            array[j + 1] = array[j]\n            swaps += 1\n            j -= 1\n        array[j + 1] = key\n    return swaps\n\n# Sample input\nsample_input = \"\"\"\n6\n6 10 4 5 1 2\n\"\"\"\n\narray: List[int] = parse_input(sample_input)\nswap_count: int = insertion_sort(array)\nprint(f\"{swap_count}\")"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-3",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-3",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "4.1 Sample Dataset",
    "text": "4.1 Sample Dataset\n6 7\n1 2\n2 3\n6 3\n5 6\n2 5\n2 4\n4 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-3",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-3",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "4.2 Sample Output",
    "text": "4.2 Sample Output\n2 4 2 2 2 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-3",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-3",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "4.3 Solution",
    "text": "4.3 Solution\nfrom typing import List, Tuple\n\ndef parse_input(input_str: str) -&gt; Tuple[int, List[int]]:\n    lines = input_str.strip().split(\"\\n\")\n    nodes, edges = map(int, lines[0].split())\n    edge_list: List[int] = []\n    for line in lines[1:]:\n        edge_list.extend(map(int, line.split()))\n    return nodes, edge_list\n\ndef calculate_degrees(nodes: int, edge_list: List[int]) -&gt; List[int]:\n    degrees: List[int] = [0] * nodes\n    for node in edge_list:\n        degrees[node - 1] += 1\n    return degrees\n\n# Sample input\nsample_input = \"\"\"\n6 7\n1 2\n2 3\n6 3\n5 6\n2 5\n2 4\n4 1\n\"\"\"\n\nnodes: int\nedge_list: List[int]\nnodes, edge_list = parse_input(sample_input)\ndegrees: List[int] = calculate_degrees(nodes, edge_list)\nprint(\" \".join(map(str, degrees)))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-4",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-4",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "5.1 Sample Dataset",
    "text": "5.1 Sample Dataset\n5 4\n1 2\n2 3\n4 3\n2 4"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-4",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-4",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "5.2 Sample Output",
    "text": "5.2 Sample Output\n3 5 5 5 0"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-4",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-4",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "5.3 Solution",
    "text": "5.3 Solution\nfrom typing import List, Tuple, Dict\n\ndef parse_input(input_str: str) -&gt; Tuple[int, List[List[int]]]:\n    lines = input_str.strip().split(\"\\n\")\n    nodes, edges = map(int, lines[0].split())\n    edge_list: List[List[int]] = []\n    for line in lines[1:]:\n        edge_list.append(list(map(int, line.split())))\n    return nodes, edge_list\n\nsample_input = \"\"\"\n5 4\n1 2\n2 3\n4 3\n2 4\n\"\"\"\n\nnodes: int\nedges_list: List[List[int]]\nnodes, edges_list = parse_input(sample_input)\n\n# Create an adjacency list\nnodes_neighbours: Dict[str, List[str]] = {str(i+1): [] for i in range(nodes)}\nfor edge in edges_list:\n    nodes_neighbours[str(edge[0])].append(str(edge[1]))\n    nodes_neighbours[str(edge[1])].append(str(edge[0]))\n\n# Calculate and print the sum of neighbors' degrees\nfor i in range(1, nodes + 1):\n    sum_of_neighbors_degrees: int = sum(len(nodes_neighbours[str(neighbor)]) for neighbor in nodes_neighbours[str(i)])\n    print(sum_of_neighbors_degrees, end=\" \")"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-5",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-5",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "6.1 Sample Dataset",
    "text": "6.1 Sample Dataset\n4 8\n5 5 5 5 5 5 5 5\n8 7 7 7 1 7 3 7\n7 1 6 5 10 100 1000 1\n5 1 6 7 1 1 10 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-5",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-5",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "6.2 Sample Output",
    "text": "6.2 Sample Output\n5 7 -1 -1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-5",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-5",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "6.3 Solution",
    "text": "6.3 Solution\nfrom typing import List\n\ndef string_to_int_list(string: str) -&gt; List[int]:\n    return list(map(int, string.split()))\n\ndef find_possible_majority(numbers: List[int]) -&gt; int:\n    leader_index = 0\n    leader_count = 1\n    for i in range(len(numbers)):\n        if numbers[leader_index] == numbers[i]:\n            leader_count += 1\n        else:\n            leader_count -= 1\n        if leader_count == 0:\n            leader_index = i\n            leader_count = 1\n    return numbers[leader_index]\n\ndef is_majority(numbers: List[int], candidate: int) -&gt; bool:\n    count = sum(1 for num in numbers if num == candidate)\n    return count &gt; len(numbers) / 2\n\n# Using Moore's voting algorithm\ndef find_majority(numbers: List[int]) -&gt; int:\n    candidate = find_possible_majority(numbers)\n    return candidate if is_majority(numbers, candidate) else -1\n\nsample_input = \"\"\"\n4 8\n5 5 5 5 5 5 5 5\n8 7 7 7 1 7 3 7\n7 1 6 5 10 100 1000 1\n5 1 6 7 1 1 10 1\n\"\"\"\n\n_, *number_lists = sample_input.strip().split(\"\\n\")\nnumber_lists: List[List[int]] = [string_to_int_list(line) for line in number_lists]\nprint(*[find_majority(numbers) for numbers in number_lists])"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-6",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-6",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "7.1 Sample Dataset",
    "text": "7.1 Sample Dataset\n4\n2 4 10 18\n3\n-5 11 12"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-6",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-6",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "7.2 Sample Output",
    "text": "7.2 Sample Output\n-5 2 4 10 11 12 18"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-6",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-6",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "7.3 Solution",
    "text": "7.3 Solution\nfrom typing import List\n\ndef parse_ints(x: str) -&gt; List[int]:\n    return [int(num) for num in x.split()]\n\ndef merge_sorted_lists(list1: List[int], list2: List[int]) -&gt; List[int]:\n    merged = []\n    i, j = 0, 0\n    \n    while i &lt; len(list1) and j &lt; len(list2):\n        if list1[i] &lt; list2[j]:\n            merged.append(list1[i])\n            i += 1\n        else:\n            merged.append(list2[j])\n            j += 1\n    \n    merged.extend(list1[i:])\n    merged.extend(list2[j:])\n    \n    return merged\n\nsample_input = \"\"\"\n4\n2 4 10 18\n3\n-5 11 12\n\"\"\"\n\n_, list1_str, _, list2_str = sample_input.strip().split(\"\\n\")\n\nlist1 = parse_ints(list1_str)\nlist2 = parse_ints(list2_str)\n\nresult = merge_sorted_lists(list1, list2)\nprint(*result)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-7",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-7",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "8.1 Sample Dataset",
    "text": "8.1 Sample Dataset\n4 5\n2 -3 4 10 5\n8 2 4 -2 -8\n-5 2 3 2 -4\n5 4 -5 6 8"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-7",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-7",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "8.2 Sample Output",
    "text": "8.2 Sample Output\n-1\n2 4\n-1\n1 3"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-7",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-7",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "8.3 Solution",
    "text": "8.3 Solution\nfrom typing import List, Tuple, Optional\n\ndef parse_ints(x: str) -&gt; List[int]:\n    return [int(num) for num in x.split()]\n\ndef two_sum(target: int, numbers: List[int]) -&gt; Optional[Tuple[int, int]]:\n    complement_indices = {}\n    for i, num in enumerate(numbers, start=1):\n        if num in complement_indices:\n            return complement_indices[num], i\n        complement_indices[target - num] = i\n    return None\n\ndef process_input(input_str: str) -&gt; Tuple[int, int, List[List[int]]]:\n    lines = input_str.strip().split(\"\\n\")\n    k, n = parse_ints(lines[0])\n    arrays = [parse_ints(line) for line in lines[1:]]\n    return k, n, arrays\n\nsample_input = \"\"\"\n4 5\n2 -3 4 10 5\n8 2 4 -2 -8\n-5 2 3 2 -4\n5 4 -5 6 8\n\"\"\"\n\nk, n, arrays = process_input(sample_input)\n\nfor arr in arrays:\n    result = two_sum(0, arr)\n    if result:\n        print(*result)\n    else:\n        print(-1)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-8",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-8",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "9.1 Sample Dataset",
    "text": "9.1 Sample Dataset\n4 5\n2 -3 4 10 5\n8 -6 4 -2 -8\n-5 2 3 2 -4\n2 4 -5 6 8"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-8",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-8",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "9.2 Sample Output",
    "text": "9.2 Sample Output\n-1\n1 2 4\n1 2 3\n-1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-8",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-8",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "9.3 Solution",
    "text": "9.3 Solution\nfrom typing import List, Tuple, Optional\n\ndef parse_ints(x: str) -&gt; List[int]:\n    return [int(num) for num in x.split()]\n\ndef process_input(input_str: str) -&gt; Tuple[int, int, List[List[int]]]:\n    lines = input_str.strip().split(\"\\n\")\n    k, n = parse_ints(lines[0])\n    arrays = [parse_ints(line) for line in lines[1:]]\n    return k, n, arrays\n\ndef three_sum(n: int, a: List[int]) -&gt; List[int]:\n    h = {}\n    for i in range(n):\n        for j in range(i + 1, n):\n            s = a[i] + a[j]\n            if s in h:\n                return [h[s] + 1, i + 1, j + 1]\n        h[-a[i]] = i\n    return [-1]\n\nsample_input = \"\"\"\n4 5\n2 -3 4 10 5\n8 -6 4 -2 -8\n-5 2 3 2 -4\n2 4 -5 6 8\n\"\"\"\n\nk, n, arrays = process_input(sample_input)\n\nfor arr in arrays:\n    print(*three_sum(n, arr))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-9",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-9",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "10.1 Sample Dataset",
    "text": "10.1 Sample Dataset\n6 6\n4 6\n6 5\n4 3\n3 5\n2 1\n1 4"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-9",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-9",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "10.2 Sample Output",
    "text": "10.2 Sample Output\n0 -1 2 1 3 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-9",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-9",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "10.3 Solution",
    "text": "10.3 Solution\nfrom io import StringIO\nfrom typing import List, Dict, Union, Iterator\nfrom collections import deque\n\ndef parse_ints(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(handle: Iterator[str]) -&gt; str:\n    line = next(handle).strip()\n    while not line:\n        line = next(handle).strip()\n    return line\n\ndef parse_graph(handle: Iterator[str], directed: bool = False, weighted: bool = False) -&gt; Dict[int, List[Union[int, Dict[str, int]]]]:\n    nodes, n_edges = parse_ints(read_non_empty_line(handle))\n    \n    graph = {n: [] for n in range(1, nodes + 1)}\n    \n    for _ in range(n_edges):\n        edge = parse_ints(read_non_empty_line(handle))\n        if weighted:\n            f, t, w = edge\n            graph[f].append({\"n\": t, \"w\": w})\n            if not directed:\n                graph[t].append({\"n\": f, \"w\": w})\n        else:\n            f, t = edge\n            graph[f].append(t)\n            if not directed:\n                graph[t].append(f)\n\n    return graph\n\ndef bfs(graph: Dict[int, List[int]], start: int = 1) -&gt; List[int]:\n    n = max(graph.keys())\n    distances = [-1] * (n + 1)\n    distances[start] = 0\n    queue = deque([start])\n    \n    while queue:\n        node = queue.popleft()\n        for neighbor in graph[node]:\n            if distances[neighbor] == -1:\n                queue.append(neighbor)\n                distances[neighbor] = distances[node] + 1\n    \n    return distances[1:]\n\n# Sample input\nsample_input = \"\"\"\n6 6\n4 6\n6 5\n4 3\n3 5\n2 1\n1 4\n\"\"\"\n\ngraph = parse_graph(StringIO(sample_input), directed=True)\nprint(*bfs(graph))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-10",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-10",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "11.1 Sample Dataset",
    "text": "11.1 Sample Dataset\n12 13\n1 2\n1 5\n5 9\n5 10\n9 10\n3 4\n3 7\n3 8\n4 8\n7 11\n8 11\n11 12\n8 12"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-10",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-10",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "11.2 Sample Output",
    "text": "11.2 Sample Output\n3"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-10",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-10",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "11.3 Solution",
    "text": "11.3 Solution\nfrom typing import List, Dict, Set, Union\n\ndef parse_integers(input_string: str) -&gt; List[int]:\n    return list(map(int, input_string.split()))\n    \ndef parse_graph(input_lines: List[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Dict[int, List[Union[int, Dict[str, int]]]]:\n    graph_info = input_lines[0]\n    if graph_info == \"\":\n        graph_info = input_lines[1]\n    num_nodes, num_edges = parse_integers(graph_info)\n    edge_list = input_lines[1:num_edges+1]\n    adjacency_list: Dict[int, List[Union[int, Dict[str, int]]]] = {}\n\n    for node in range(1, num_nodes + 1):\n        adjacency_list[node] = list()\n\n    for edge in edge_list:\n        if is_weighted:\n            from_node, to_node, weight = parse_integers(edge)\n            adjacency_list[from_node].append({\"node\": to_node, \"weight\": weight})\n            if not is_directed:\n                adjacency_list[to_node].append({\"node\": from_node, \"weight\": weight})\n        else:\n            from_node, to_node = parse_integers(edge)\n            adjacency_list[from_node].append(to_node)\n            if not is_directed:\n                adjacency_list[to_node].append(from_node)\n\n    return adjacency_list\n\ndef find_connected_component(start_node: int, graph: Dict[int, List[int]]) -&gt; Set[int]:\n    def depth_first_search(current_node: int, visited_nodes: Set[int]) -&gt; Set[int]:\n        visited_nodes.add(current_node)\n        for neighbor in graph[current_node]:\n            if neighbor not in visited_nodes:\n                depth_first_search(neighbor, visited_nodes)\n        return visited_nodes\n\n    return depth_first_search(start_node, set())\n\ndef find_all_components(graph: Dict[int, List[int]]) -&gt; List[Set[int]]:\n    unvisited_nodes = set(graph.keys())\n    all_components: List[Set[int]] = list()\n    while unvisited_nodes:\n        component = find_connected_component(next(iter(unvisited_nodes)), graph)\n        unvisited_nodes -= component\n        all_components.append(component)\n    return all_components\n\nsample_input = \"\"\"\n12 13\n1 2\n1 5\n5 9\n5 10\n9 10\n3 4\n3 7\n3 8\n4 8\n7 11\n8 11\n11 12\n8 12\n\"\"\"\n\ngraph: Dict[int, List[int]] = parse_graph(sample_input.strip().split(\"\\n\"))\nprint(len(find_all_components(graph)))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-11",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-11",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "12.1 Sample Dataset",
    "text": "12.1 Sample Dataset\n5\n1 3 5 7 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-11",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-11",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "12.2 Sample Output",
    "text": "12.2 Sample Output\n7 5 1 3 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-12",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-12",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "13.1 Sample Dataset",
    "text": "13.1 Sample Dataset\n10\n20 19 35 -18 17 -20 20 1 4 4"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-12",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-12",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "13.2 Sample Output",
    "text": "13.2 Sample Output\n-20 -18 1 4 4 17 19 20 20 35"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-11",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-11",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "13.3 Solution",
    "text": "13.3 Solution\nfrom typing import List\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef merge(left: List[int], right: List[int]) -&gt; List[int]:\n    merged = []\n    left_index, right_index = 0, 0\n    \n    while left_index &lt; len(left) and right_index &lt; len(right):\n        if left[left_index] &lt; right[right_index]:\n            merged.append(left[left_index])\n            left_index += 1\n        else:\n            merged.append(right[right_index])\n            right_index += 1\n            \n    # Append any remaining elements from both halves\n    merged.extend(left[left_index:])\n    merged.extend(right[right_index:])\n    \n    return merged\n\ndef merge_sort(array: List[int]) -&gt; List[int]:\n    if len(array) &gt; 1:\n        mid_index = len(array) // 2\n        left_half = merge_sort(array[:mid_index])\n        right_half = merge_sort(array[mid_index:])\n        return merge(left_half, right_half)\n    else:\n        return array\n\nsample_input = \"\"\"\n10\n20 19 35 -18 17 -20 20 1 4 4\n\"\"\"\n\n_, numbers = sample_input.strip().split(\"\\n\")\nprint(*merge_sort(parse_integers(numbers)))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-13",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-13",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "14.1 Sample Dataset",
    "text": "14.1 Sample Dataset\n9\n7 2 5 6 1 3 9 4 8"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-13",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-13",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "14.2 Sample Output",
    "text": "14.2 Sample Output\n4 2 5 6 1 3 7 9 8"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-12",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-12",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "14.3 Solution",
    "text": "14.3 Solution\nfrom typing import List\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(number) for number in line.split()]\n\ndef partition(array: List[int]) -&gt; List[int]:\n    if len(array) &lt;= 1:\n        return array\n    \n    pivot = array[0]\n    swap_index = 0\n    \n    for current_index in range(1, len(array)):\n        if array[current_index] &lt;= pivot:\n            swap_index += 1\n            array[current_index], array[swap_index] = array[swap_index], array[current_index]\n    \n    array[0], array[swap_index] = array[swap_index], array[0]\n    return array\n\nsample_input = \"\"\"\n9\n7 2 5 6 1 3 9 4 8\n\"\"\"\n\n_, input_numbers = sample_input.strip().split(\"\\n\")\nresult = partition(parse_integers(input_numbers))\nprint(*result)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-14",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-14",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "15.1 Sample Dataset",
    "text": "15.1 Sample Dataset\n9\n4 5 6 4 1 2 5 7 4"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-14",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-14",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "15.2 Sample Output",
    "text": "15.2 Sample Output\n2 1 4 4 4 5 7 6 5"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-13",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-13",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "15.3 Solution",
    "text": "15.3 Solution\nfrom typing import List, Tuple\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(number) for number in line.split()]\n\ndef three_way_partition(array: List[int], start: int = None, end: int = None) -&gt; Tuple[int, int]:\n    if start is None:\n        start = 0\n    if end is None:\n        end = len(array) - 1\n    \n    pivot = array[start]\n    low = start\n    current = start\n    high = end\n\n    while current &lt;= high:\n        if array[current] &lt; pivot:\n            array[current], array[low] = array[low], array[current]\n            current += 1\n            low += 1\n        elif array[current] &gt; pivot:\n            array[current], array[high] = array[high], array[current]\n            high -= 1\n        else:\n            current += 1\n\n    return low, high\n\nsample_input = \"\"\"\n9\n4 5 6 4 1 2 5 7 4\n\"\"\"\n\n_, input_line = sample_input.strip().split(\"\\n\")\nnumbers = parse_integers(input_line)\nthree_way_partition(numbers)\nprint(*numbers)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-15",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-15",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "16.1 Sample Dataset",
    "text": "16.1 Sample Dataset\n2\n\n4 5\n3 4\n4 2\n3 2\n3 1\n1 2\n\n4 4\n1 2\n3 4\n2 4\n4 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-15",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-15",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "16.2 Sample Output",
    "text": "16.2 Sample Output\n1 -1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-14",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-14",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "16.3 Solution",
    "text": "16.3 Solution\nfrom typing import List, Dict, Union, Iterator\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iterator: Iterator[str]) -&gt; str:\n    line = next(input_iterator).strip()\n    while not line:\n        line = next(input_iterator).strip()\n    return line\n\ndef create_graph(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Dict[int, List[Union[int, Dict[str, int]]]]:\n    num_nodes, num_edges = parse_integers(read_non_empty_line(input_iterator))\n    \n    graph = {node: [] for node in range(1, num_nodes + 1)}\n    \n    for _ in range(num_edges):\n        edge_data = parse_integers(read_non_empty_line(input_iterator))\n        if is_weighted:\n            from_node, to_node, weight = edge_data\n            graph[from_node].append({\"node\": to_node, \"weight\": weight})\n            if not is_directed:\n                graph[to_node].append({\"node\": from_node, \"weight\": weight})\n        else:\n            from_node, to_node = edge_data\n            graph[from_node].append(to_node)\n            if not is_directed:\n                graph[to_node].append(from_node)\n\n    return graph\n\ndef depth_first_search(graph: Dict[int, List[Union[int, Dict[str, int]]]], start: int, max_depth: int):\n    def dfs_recursive(current_node: int, current_depth: int, visited: set):\n        if current_depth == max_depth:\n            yield current_node\n        if current_depth &lt; max_depth:\n            for neighbor in graph[current_node]:\n                neighbor_node = neighbor if isinstance(neighbor, int) else neighbor['node']\n                if neighbor_node not in visited:\n                    yield from dfs_recursive(neighbor_node, current_depth + 1, visited | {current_node})\n\n    return dfs_recursive(start, 0, set())\n\ndef has_square_cycle(graph: Dict[int, List[Union[int, Dict[str, int]]]], cycle_length: int) -&gt; int:\n    for start_node in graph:\n        for end_node in depth_first_search(graph, start_node, cycle_length - 1):\n            if end_node in graph[start_node]:\n                return 1\n    return -1\n\ndef parse_multiple_graphs(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; List[Dict[int, List[Union[int, Dict[str, int]]]]]:\n    num_graphs = int(read_non_empty_line(input_iterator))\n    return [create_graph(input_iterator, is_directed, is_weighted) for _ in range(num_graphs)]\n\n# Sample input\nsample_input = \"\"\"\n2\n\n4 5\n3 4\n4 2\n3 2\n3 1\n1 2\n\n4 4\n1 2\n3 4\n2 4\n4 1\n\"\"\".strip().split(\"\\n\")\n\n# Process the input and print results\ngraphs = parse_multiple_graphs(iter(sample_input), is_directed=False)\nresults = [has_square_cycle(graph, 4) for graph in graphs]\nprint(*results)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-16",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-16",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "17.1 Sample Dataset",
    "text": "17.1 Sample Dataset\n2\n\n3 3\n1 2\n3 2\n3 1\n\n4 3\n1 4\n3 1\n1 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-16",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-16",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "17.2 Sample Output",
    "text": "17.2 Sample Output\n-1 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-15",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-15",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "17.3 Solution",
    "text": "17.3 Solution\nfrom typing import List, Dict, Union, Iterator\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iterator: Iterator[str]) -&gt; str:\n    line = next(input_iterator).strip()\n    while not line:\n        line = next(input_iterator).strip()\n    return line\n\ndef parse_graph(input_iterator: Iterator[str], directed: bool = False, weighted: bool = False) -&gt; Dict[int, List[Union[int, Dict[str, int]]]]:\n    num_nodes, num_edges = parse_integers(read_non_empty_line(input_iterator))\n    \n    graph = {node: [] for node in range(1, num_nodes + 1)}\n    \n    for _ in range(num_edges):\n        edge = parse_integers(read_non_empty_line(input_iterator))\n        if weighted:\n            from_node, to_node, weight = edge\n            graph[from_node].append({\"node\": to_node, \"weight\": weight})\n            if not directed:\n                graph[to_node].append({\"node\": from_node, \"weight\": weight})\n        else:\n            from_node, to_node = edge\n            graph[from_node].append(to_node)\n            if not directed:\n                graph[to_node].append(from_node)\n\n    return graph\n    \ndef is_bipartite(graph: Dict[int, List[int]]) -&gt; int:\n    color = {1: 0}\n    queue = [1]\n    while queue:\n        current_node = queue.pop(0)\n        for neighbor in graph[current_node]:\n            neighbor_color = (color[current_node] + 1) % 2\n            if neighbor not in color:\n                queue.append(neighbor)\n                color[neighbor] = neighbor_color\n            elif color[neighbor] != neighbor_color:\n                return -1\n    return 1\n\ndef process_multiple_graphs(input_iterator: Iterator[str]) -&gt; List[int]:\n    num_cases = int(read_non_empty_line(input_iterator))\n    results = []\n    for _ in range(num_cases):\n        graph = parse_graph(input_iterator)\n        results.append(is_bipartite(graph))\n    return results\n\nsample_input = \"\"\"\n2\n\n3 3\n1 2\n3 2\n3 1\n\n4 3\n1 4\n3 1\n1 2\n\"\"\".strip().split(\"\\n\")\n\n# Convert the list to an iterator\nsample_input_iterator = iter(sample_input)\n\n# Process multiple graphs\nresults = process_multiple_graphs(sample_input_iterator)\nprint(*results)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-17",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-17",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "18.1 Sample Dataset",
    "text": "18.1 Sample Dataset\n3\n\n2 1\n1 2\n\n4 4\n4 1\n1 2\n2 3\n3 1\n\n4 3\n4 3\n3 2\n2 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-17",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-17",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "18.2 Sample Output",
    "text": "18.2 Sample Output\n1 -1 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-16",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-16",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "18.3 Solution",
    "text": "18.3 Solution\nfrom typing import List, Dict, Union, Iterator\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iter: Iterator[str]) -&gt; str:\n    line = next(input_iter).strip()\n    while not line:\n        line = next(input_iter).strip()\n    return line\n\ndef parse_graph(input_iter: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Dict[int, List[Union[int, Dict[str, int]]]]:\n    node_count, edge_count = parse_integers(read_non_empty_line(input_iter))\n    \n    graph = {node: [] for node in range(1, node_count + 1)}\n    \n    for _ in range(edge_count):\n        edge = parse_integers(read_non_empty_line(input_iter))\n        if is_weighted:\n            source, target, weight = edge\n            graph[source].append({\"node\": target, \"weight\": weight})\n            if not is_directed:\n                graph[target].append({\"node\": source, \"weight\": weight})\n        else:\n            source, target = edge\n            graph[source].append(target)\n            if not is_directed:\n                graph[target].append(source)\n\n    return graph\n\ndef graph_nodes(graph):\n    nodes = set()\n    for node, neighbors in graph.items():\n        nodes.add(node)\n        nodes = nodes.union(neighbors)\n    return nodes\n\ndef remove_leaves(graph):\n    nodes = graph_nodes(graph)\n    leaves = nodes - graph.keys()\n    return {n: set(v) - leaves for n, v in graph.items() if len(set(v) - leaves)}\n\ndef is_dag(graph):\n    while graph:\n        new_graph = remove_leaves(graph)\n        if len(graph) == len(new_graph):\n            return -1\n        graph = new_graph\n    return 1\n\ndef parse_multiple_graphs(input_iter: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; List[Dict[int, List[Union[int, Dict[str, int]]]]]:\n    case_count = int(read_non_empty_line(input_iter))\n    graphs = []\n    for _ in range(case_count):\n        graphs.append(parse_graph(input_iter, is_directed, is_weighted))\n    return graphs\n\nsample_input = \"\"\"\n3\n\n2 1\n1 2\n\n4 4\n4 1\n1 2\n2 3\n3 1\n\n4 3\n4 3\n3 2\n2 1\n\"\"\".strip().split(\"\\n\")\n\n# Convert the list to an iterator\ninput_iterator = iter(sample_input)\n\n# Parse multiple graphs\ngraphs = parse_multiple_graphs(input_iterator, is_directed=True)\n\n# Process each graph\nresults = [is_dag(graph) for graph in graphs]\nprint(*results)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-18",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-18",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "19.1 Sample Dataset",
    "text": "19.1 Sample Dataset\n6 10\n3 4 4\n1 2 4\n1 3 2\n2 3 3\n6 3 2\n3 5 5\n5 4 1\n3 2 1\n2 4 2\n2 5 3"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-18",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-18",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "19.2 Sample Output",
    "text": "19.2 Sample Output\n0 3 2 5 6 -1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-17",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-17",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "19.3 Solution",
    "text": "19.3 Solution\nfrom math import inf\nfrom heapq import heappush, heappop\nfrom typing import List, Dict, Union, Iterator\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iter: Iterator[str]) -&gt; str:\n    line = next(input_iter).strip()\n    while not line:\n        line = next(input_iter).strip()\n    return line\n\ndef parse_graph(input_iter: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Dict[int, List[Union[int, Dict[str, int]]]]:\n    vertex_count, edge_count = parse_integers(read_non_empty_line(input_iter))\n    \n    adjacency_list = {vertex: [] for vertex in range(1, vertex_count + 1)}\n    \n    for _ in range(edge_count):\n        edge = parse_integers(read_non_empty_line(input_iter))\n        if is_weighted:\n            source, destination, weight = edge\n            adjacency_list[source].append({\"vertex\": destination, \"weight\": weight})\n            if not is_directed:\n                adjacency_list[destination].append({\"vertex\": source, \"weight\": weight})\n        else:\n            source, destination = edge\n            adjacency_list[source].append(destination)\n            if not is_directed:\n                adjacency_list[destination].append(source)\n\n    return adjacency_list\n\ndef dijkstra(graph, start_vertex=1):\n    distances = [inf for _ in range(len(graph) + 1)]\n    distances[start_vertex] = 0\n    priority_queue = []\n    heappush(priority_queue, (0, start_vertex))\n    visited = set()\n\n    while priority_queue:\n        current_vertex = heappop(priority_queue)[1]\n        visited.add(current_vertex)\n        for neighbor in graph[current_vertex]:\n            if isinstance(neighbor, dict):\n                next_vertex = neighbor[\"vertex\"]\n                edge_weight = neighbor[\"weight\"]\n            else:\n                next_vertex = neighbor\n                edge_weight = 1  # Assume unit weight for unweighted graphs\n            \n            if next_vertex not in visited:\n                new_distance = distances[current_vertex] + edge_weight\n                if new_distance &lt; distances[next_vertex]:\n                    distances[next_vertex] = new_distance\n                    heappush(priority_queue, (distances[next_vertex], next_vertex))\n\n    return [-1 if distance == inf else distance for distance in distances[1:]]\n\nsample_input = \"\"\"\n6 10\n3 4 4\n1 2 4\n1 3 2\n2 3 3\n6 3 2\n3 5 5\n5 4 1\n3 2 1\n2 4 2\n2 5 3\n\"\"\".strip().split(\"\\n\")\n\n# Convert the list to an iterator\ninput_iterator = iter(sample_input)\n\ngraph = parse_graph(input_iterator, is_directed=True, is_weighted=True)\nprint(*dijkstra(graph))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-19",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-19",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "20.1 Sample Dataset",
    "text": "20.1 Sample Dataset\n9\n2 6 7 1 3 5 4 8 9"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-19",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-19",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "20.2 Sample Output",
    "text": "20.2 Sample Output\n1 2 3 4 5 6 7 8 9"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-18",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-18",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "20.3 Solution",
    "text": "20.3 Solution\nfrom typing import List, Any\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef bubble_up(heap: List[int], index: int) -&gt; None:\n    if index == 0:\n        return\n    parent_index: int = (index - 1) // 2\n    current_index: int = index\n    if heap[parent_index] &gt; heap[current_index]:\n        bubble_up(heap, parent_index)\n    else:\n        heap[parent_index], heap[current_index] = heap[current_index], heap[parent_index]\n        bubble_up(heap, parent_index)\n\ndef build_heap(array: List[int]) -&gt; List[int]:\n    heap: List[int] = []\n    for i, element in enumerate(array):\n        heap.append(element)\n        bubble_up(heap, i)\n    return heap\n\ndef sift_down(heap: List[int], start_index: int, end_index: int) -&gt; None:\n    root_index: int = start_index\n    while root_index * 2 + 1 &lt;= end_index:\n        left_child_index: int = root_index * 2 + 1\n        right_child_index: int = left_child_index + 1\n        swap_index: int = root_index\n        if heap[swap_index] &lt; heap[left_child_index]:\n            swap_index = left_child_index\n        if right_child_index &lt;= end_index and heap[swap_index] &lt; heap[right_child_index]:\n            swap_index = right_child_index\n        if swap_index != root_index:\n            heap[root_index], heap[swap_index] = heap[swap_index], heap[root_index]\n            root_index = swap_index\n        else:\n            return\n\ndef heap_sort(array: List[int]) -&gt; List[int]:\n    heap: List[int] = build_heap(array)\n    last_index: int = len(heap) - 1\n    while last_index &gt; 0:\n        heap[0], heap[last_index] = heap[last_index], heap[0]\n        last_index -= 1\n        sift_down(heap, 0, last_index)\n    return heap\n\nsample_input: str = \"\"\"\n9\n2 6 7 1 3 5 4 8 9\n\"\"\"\n\n_, input_array_str = sample_input.strip().split(\"\\n\")\ninput_array: List[int] = parse_integers(input_array_str)\nsorted_array: List[int] = heap_sort(input_array)\nprint(*sorted_array)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-20",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-20",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "21.1 Sample Dataset",
    "text": "21.1 Sample Dataset\n5\n-6 1 15 8 10"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-20",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-20",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "21.2 Sample Output",
    "text": "21.2 Sample Output\n2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-19",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-19",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "21.3 Solution",
    "text": "21.3 Solution\nfrom typing import List, Tuple\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef merge_and_count_inversions(left: List[int], right: List[int]) -&gt; Tuple[List[int], int]:\n    merged: List[int] = []\n    inversion_count: int = 0\n    left_length: int = len(left)\n    left_index: int = 0\n\n    while left and right:\n        if left[0] &lt;= right[0]:\n            left_index += 1\n            merged.append(left.pop(0))\n        else:\n            inversion_count += left_length - left_index\n            merged.append(right.pop(0))\n\n    merged.extend(left)\n    merged.extend(right)\n    return merged, inversion_count\n\ndef merge_sort_and_count_inversions(arr: List[int]) -&gt; Tuple[List[int], int]:\n    if len(arr) &gt; 1:\n        mid: int = len(arr) // 2\n        left_half, left_inversions = merge_sort_and_count_inversions(arr[:mid])\n        right_half, right_inversions = merge_sort_and_count_inversions(arr[mid:])\n        merged, merge_inversions = merge_and_count_inversions(left_half, right_half)\n        total_inversions: int = left_inversions + right_inversions + merge_inversions\n        return merged, total_inversions\n    else:\n        return arr, 0\n\nsample_input: str = \"\"\"\n5\n-6 1 15 8 10\n\"\"\"\n\n_, input_array_str = sample_input.strip().split(\"\\n\")\ninput_array: List[int] = parse_integers(input_array_str)\n_, inversion_count = merge_sort_and_count_inversions(input_array)\nprint(inversion_count)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-21",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-21",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "22.1 Sample Dataset",
    "text": "22.1 Sample Dataset\n9 13\n1 2 10\n3 2 1\n3 4 1\n4 5 3\n5 6 -1\n7 6 -1\n8 7 1\n1 8 8\n7 2 -4\n2 6 2\n6 3 -2\n9 5 -10\n9 4 7"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-21",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-21",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "22.2 Sample Output",
    "text": "22.2 Sample Output\n0 5 5 6 9 7 9 8 x"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-20",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-20",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "22.3 Solution",
    "text": "22.3 Solution\nfrom math import inf, isinf\nfrom typing import List, Dict, Union, Iterator, Set, Tuple\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iterator: Iterator[str]) -&gt; str:\n    line = next(input_iterator).strip()\n    while not line:\n        line = next(input_iterator).strip()\n    return line\n\nEdgeInfo = Dict[str, int]\nGraph = Dict[int, List[Union[int, EdgeInfo]]]\n\ndef parse_graph(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Graph:\n    vertex_count, edge_count = parse_integers(read_non_empty_line(input_iterator))\n    \n    adjacency_list: Graph = {vertex: [] for vertex in range(1, vertex_count + 1)}\n    \n    for _ in range(edge_count):\n        edge_data = parse_integers(read_non_empty_line(input_iterator))\n        if is_weighted:\n            source_vertex, target_vertex, edge_weight = edge_data\n            adjacency_list[source_vertex].append({\"node\": target_vertex, \"weight\": edge_weight})\n            if not is_directed:\n                adjacency_list[target_vertex].append({\"node\": source_vertex, \"weight\": edge_weight})\n        else:\n            source_vertex, target_vertex = edge_data\n            adjacency_list[source_vertex].append(target_vertex)\n            if not is_directed:\n                adjacency_list[target_vertex].append(source_vertex)\n\n    return adjacency_list\n\ndef count_edges(graph: Graph) -&gt; int:\n    return sum(len(neighbors) for neighbors in graph.values())\n\ndef get_all_vertices(graph: Graph) -&gt; Set[int]:\n    all_vertices: Set[int] = set(graph.keys())\n    for neighbors in graph.values():\n        for neighbor in neighbors:\n            if isinstance(neighbor, dict):\n                all_vertices.add(neighbor[\"node\"])\n            else:\n                all_vertices.add(neighbor)\n    return all_vertices\n\ndef bellman_ford(graph: Graph, start_vertex: int = 1) -&gt; List[Union[int, str]]:\n    distances: Dict[int, float] = {vertex: inf for vertex in get_all_vertices(graph)}\n    distances[start_vertex] = 0\n    for _ in range(count_edges(graph) - 1):\n        for current_vertex, neighbors in graph.items():\n            for neighbor in neighbors:\n                if isinstance(neighbor, dict):\n                    target_vertex = neighbor[\"node\"]\n                    edge_weight = neighbor[\"weight\"]\n                    if distances[current_vertex] + edge_weight &lt; distances[target_vertex]:\n                        distances[target_vertex] = distances[current_vertex] + edge_weight\n    return [\"x\" if isinf(distance) else distance for distance in distances.values()]\n\nsample_input: str = \"\"\"\n9 13\n1 2 10\n3 2 1\n3 4 1\n4 5 3\n5 6 -1\n7 6 -1\n8 7 1\n1 8 8\n7 2 -4\n2 6 2\n6 3 -2\n9 5 -10\n9 4 7\n\"\"\".strip().split(\"\\n\")\n\n# Convert the list to an iterator\ninput_iterator = iter(sample_input)\n\n# Parse the graph with correct parameters\ngraph = parse_graph(input_iterator, is_directed=True, is_weighted=True)\n\n# Run Bellman-Ford algorithm\nresult = bellman_ford(graph)\nprint(*result)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-22",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-22",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "23.1 Sample Dataset",
    "text": "23.1 Sample Dataset\n2\n\n4 5\n2 4 2\n3 2 1\n1 4 3\n2 1 10\n1 3 4\n\n4 5\n3 2 1\n2 4 2\n4 1 3\n2 1 10\n1 3 4"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-22",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-22",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "23.2 Sample Output",
    "text": "23.2 Sample Output\n-1 10"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-21",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-21",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "23.3 Solution",
    "text": "23.3 Solution\nfrom typing import List, Dict, Union, Iterator, Tuple\nfrom math import inf\nfrom heapq import heappush, heappop\nfrom io import StringIO\n\nGraphNode = Union[int, Dict[str, int]]\nGraph = Dict[int, List[GraphNode]]\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iter: Iterator[str]) -&gt; str:\n    line = next(input_iter).strip()\n    while not line:\n        line = next(input_iter).strip()\n    return line\n\ndef parse_graph(input_iter: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Graph:\n    vertex_count, edge_count = parse_integers(read_non_empty_line(input_iter))\n    \n    adj_list: Graph = {vertex: [] for vertex in range(1, vertex_count + 1)}\n    \n    for _ in range(edge_count):\n        edge_data = parse_integers(read_non_empty_line(input_iter))\n        if is_weighted:\n            src, dest, weight = edge_data\n            adj_list[src].append({\"node\": dest, \"weight\": weight})\n            if not is_directed:\n                adj_list[dest].append({\"node\": src, \"weight\": weight})\n        else:\n            src, dest = edge_data\n            adj_list[src].append(dest)\n            if not is_directed:\n                adj_list[dest].append(src)\n\n    return adj_list\n\ndef dijkstra(graph: Graph, start: int = 1) -&gt; List[int]:\n    distances = [inf for _ in range(len(graph) + 1)]\n    distances[start] = 0\n    priority_queue = []\n    heappush(priority_queue, (0, start))\n    visited = set()\n\n    while priority_queue:\n        _, current_node = heappop(priority_queue)\n        if current_node in visited:\n            continue\n        visited.add(current_node)\n        for neighbor in graph[current_node]:\n            if isinstance(neighbor, dict):\n                neighbor_node, edge_weight = neighbor[\"node\"], neighbor[\"weight\"]\n            else:\n                neighbor_node, edge_weight = neighbor, 1\n            if neighbor_node not in visited:\n                new_distance = distances[current_node] + edge_weight\n                if new_distance &lt; distances[neighbor_node]:\n                    distances[neighbor_node] = new_distance\n                    heappush(priority_queue, (new_distance, neighbor_node))\n\n    return [-1 if d == inf else d for d in distances[1:]]\n\ndef extract_first_edges(input_handle: StringIO) -&gt; List[List[int]]:\n    lines = input_handle.read().splitlines()\n    graph_count = int(lines[0])\n    current_line = 1\n    first_edges = []\n    for _ in range(graph_count):\n        while not lines[current_line]:\n            current_line += 1\n        _, edge_count = map(int, lines[current_line].split())\n        first_edges.append(list(map(int, lines[current_line + 1].split())))\n        current_line += int(edge_count) + 1\n    return first_edges\n\ndef calculate_cycle_length(graph: Graph, edge: List[int]) -&gt; int:\n    distance = dijkstra(graph, start=edge[1])[edge[0] - 1]\n    return distance if distance == -1 else distance + edge[2]\n\ndef parse_multiple_graphs(input_str: str, directed: bool = False, weighted: bool = True) -&gt; List[Graph]:\n    input_iter = iter(input_str.splitlines())\n    graph_count = int(next(input_iter))\n    graphs = []\n    for _ in range(graph_count):\n        graphs.append(parse_graph(input_iter, is_directed=directed, is_weighted=weighted))\n    return graphs\n\n# Sample input\nsample_input = \"\"\"\n2\n\n4 5\n2 4 2\n3 2 1\n1 4 3\n2 1 10\n1 3 4\n\n4 5\n3 2 1\n2 4 2\n4 1 3\n2 1 10\n1 3 4\n\"\"\"\n\n# Main execution\ninput_handle = StringIO(sample_input.strip())\nfirst_edges = extract_first_edges(input_handle)\ninput_handle.seek(0)  # Reset the StringIO object to the beginning\ngraphs = parse_multiple_graphs(input_handle.read(), directed=True, weighted=True)\nresults = [calculate_cycle_length(graphs[i], first_edges[i]) for i in range(len(graphs))]\nprint(*results)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-23",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-23",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "24.1 Sample Dataset",
    "text": "24.1 Sample Dataset\n11\n2 36 5 21 8 13 11 20 5 4 1\n8"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-23",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-23",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "24.2 Sample Output",
    "text": "24.2 Sample Output\n13"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-22",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-22",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "24.3 Solution",
    "text": "24.3 Solution\nfrom typing import List, Tuple\nfrom io import StringIO\n\ndef parse_integers(line: str) -&gt; List[int]:\n    \"\"\"Parse a line of space-separated integers into a list of integers.\"\"\"\n    return [int(num) for num in line.split()]\n\ndef three_way_partition(arr: List[int], low: int = 0, high: int = None) -&gt; Tuple[int, int]:\n    \"\"\"Partition the array into three parts based on a pivot.\"\"\"\n    if high is None:\n        high = len(arr) - 1\n    \n    pivot = arr[low]\n    mid = low\n\n    while mid &lt;= high:\n        if arr[mid] &lt; pivot:\n            arr[low], arr[mid] = arr[mid], arr[low]\n            low += 1\n            mid += 1\n        elif arr[mid] &gt; pivot:\n            arr[mid], arr[high] = arr[high], arr[mid]\n            high -= 1\n        else:\n            mid += 1\n\n    return low, high\n\ndef find_kth_element(arr: List[int], k: int) -&gt; int:\n    \"\"\"Find the k-th element in the array using a three-way partitioning method.\"\"\"\n    def find_kth_recursive(arr: List[int], k: int, low: int, high: int) -&gt; int:\n        left, right = three_way_partition(arr, low, high)\n        \n        if k &lt; left:\n            return find_kth_recursive(arr, k, low, left - 1)\n        elif k &gt; right:\n            return find_kth_recursive(arr, k, right + 1, high)\n        else:\n            return arr[k]\n\n    return find_kth_recursive(arr, k, 0, len(arr) - 1)\n\ndef process_input(input_str: str) -&gt; Tuple[List[int], int]:\n    \"\"\"Process the input string and return the list of integers and the index k.\"\"\"\n    lines = input_str.strip().split('\\n')\n    arr = parse_integers(lines[1])\n    k = int(lines[2])\n    return arr, k\n\nsample_input = \"\"\"\n11\n2 36 5 21 8 13 11 20 5 4 1\n8\n\"\"\"\n\narr, k = process_input(sample_input)\nresult = find_kth_element(arr, k - 1)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-24",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-24",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "25.1 Sample Dataset",
    "text": "25.1 Sample Dataset\n10\n4 -6 7 8 -9 100 12 13 56 17\n3"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-24",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-24",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "25.2 Sample Output",
    "text": "25.2 Sample Output\n-9 -6 4"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-23",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-23",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "25.3 Solution",
    "text": "25.3 Solution\nfrom typing import List\n\ndef parse_integers(line: str) -&gt; List[int]:\n    \"\"\"Parse a line of space-separated integers into a list of integers.\"\"\"\n    return [int(num) for num in line.split()]\n\ndef heapify_up(heap: List[int], i: int) -&gt; None:\n    \"\"\"Maintain heap property by moving an element up the heap.\"\"\"\n    while i &gt; 0:\n        parent = (i - 1) // 2\n        if heap[parent] &lt; heap[i]:\n            heap[parent], heap[i] = heap[i], heap[parent]\n            i = parent\n        else:\n            break\n\ndef build_max_heap(arr: List[int]) -&gt; List[int]:\n    \"\"\"Build a max heap from an array.\"\"\"\n    heap = []\n    for x in arr:\n        heap.append(x)\n        heapify_up(heap, len(heap) - 1)\n    return heap\n\ndef sift_down(heap: List[int], start: int, end: int) -&gt; None:\n    \"\"\"Maintain heap property by moving an element down the heap.\"\"\"\n    root = start\n    while root * 2 + 1 &lt;= end:\n        left = root * 2 + 1\n        right = left + 1\n        swap = root\n        if heap[swap] &lt; heap[left]:\n            swap = left\n        if right &lt;= end and heap[swap] &lt; heap[right]:\n            swap = right\n        if swap != root:\n            heap[root], heap[swap] = heap[swap], heap[root]\n            root = swap\n        else:\n            return\n\ndef heap_sort(arr: List[int]) -&gt; List[int]:\n    \"\"\"Sort an array using heap sort algorithm.\"\"\"\n    heap = build_max_heap(arr)\n    for i in range(len(heap) - 1, 0, -1):\n        heap[0], heap[i] = heap[i], heap[0]\n        sift_down(heap, 0, i - 1)\n    return heap\n\ndef partial_sort(arr: List[int], k: int) -&gt; List[int]:\n    \"\"\"Find and sort the k smallest elements in the array.\"\"\"\n    heap = build_max_heap(arr[:k])\n    for x in arr[k:]:\n        if x &lt; heap[0]:\n            heap[0] = x\n            sift_down(heap, 0, k - 1)\n    return heap_sort(heap)\n\n# Sample input processing\nsample_input = \"\"\"\n10\n4 -6 7 8 -9 100 12 13 56 17\n3\n\"\"\"\n\n_, arr_str, k_str = sample_input.strip().split(\"\\n\")\narr = parse_integers(arr_str)\nk = int(k_str)\n\n# Perform partial sort and print result\nresult = partial_sort(arr, k)\nprint(*result)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-25",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-25",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "26.1 Sample Dataset",
    "text": "26.1 Sample Dataset\n4 5\n1 2\n3 1\n3 2\n4 3\n4 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-25",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-25",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "26.2 Sample Output",
    "text": "26.2 Sample Output\n4 3 1 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-24",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-24",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "26.3 Solution",
    "text": "26.3 Solution\nfrom typing import List, Iterator, Dict, Union, Set\nfrom collections import defaultdict\nfrom io import StringIO\n\nGraphNode = Union[int, Dict[str, int]]\nGraph = Dict[int, List[GraphNode]]\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iterator: Iterator[str]) -&gt; str:\n    line = next(input_iterator).strip()\n    while not line:\n        line = next(input_iterator).strip()\n    return line\n\ndef parse_graph(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Graph:\n    vertex_count, edge_count = parse_integers(read_non_empty_line(input_iterator))\n    \n    adjacency_list: Graph = {vertex: [] for vertex in range(1, vertex_count + 1)}\n    \n    for _ in range(edge_count):\n        edge_data = parse_integers(read_non_empty_line(input_iterator))\n        if is_weighted:\n            source, destination, weight = edge_data\n            adjacency_list[source].append({\"node\": destination, \"weight\": weight})\n            if not is_directed:\n                adjacency_list[destination].append({\"node\": source, \"weight\": weight})\n        else:\n            source, destination = edge_data\n            adjacency_list[source].append(destination)\n            if not is_directed:\n                adjacency_list[destination].append(source)\n\n    return adjacency_list\n\ndef get_all_graph_nodes(graph: Graph) -&gt; Set[int]:\n    nodes = set()\n    for node, neighbors in graph.items():\n        nodes.add(node)\n        for neighbor in neighbors:\n            if isinstance(neighbor, dict):\n                nodes.add(neighbor[\"node\"])\n            else:\n                nodes.add(neighbor)\n    return nodes\n\ndef topological_sort(graph: Graph) -&gt; List[int]:\n    def dfs_topological(vertex: int, visited: Dict[int, bool], stack: List[int]):\n        visited[vertex] = True\n        for neighbor in graph[vertex]:\n            neighbor_node = neighbor[\"node\"] if isinstance(neighbor, dict) else neighbor\n            if not visited[neighbor_node]:\n                dfs_topological(neighbor_node, visited, stack)\n        stack.append(vertex)\n\n    visited = defaultdict(bool)\n    stack = []\n    for node in get_all_graph_nodes(graph):\n        if not visited[node]:\n            dfs_topological(node, visited, stack)\n\n    return stack[::-1]\n\n# Sample input processing\nsample_input = \"\"\"\n4 5\n1 2\n3 1\n3 2\n4 3\n4 2\n\"\"\"\n\ninput_iterator = iter(StringIO(sample_input.strip()).readlines())\ngraph = parse_graph(input_iterator, is_directed=True)\nsorted_nodes = topological_sort(graph)\nprint(*sorted_nodes)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-26",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-26",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "27.1 Sample Dataset",
    "text": "27.1 Sample Dataset\n2\n\n3 3\n1 2\n2 3\n1 3\n\n4 3\n4 3\n3 2\n4 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-26",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-26",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "27.2 Sample Output",
    "text": "27.2 Sample Output\n1 1 2 3\n-1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-25",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-25",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "27.3 Solution",
    "text": "27.3 Solution\nfrom typing import List, Iterator, Dict, Set\nfrom collections import defaultdict\nfrom io import StringIO\n\nGraph = Dict[int, List[int]]\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return list(map(int, line.split()))\n\ndef read_non_empty_line(input_iterator: Iterator[str]) -&gt; str:\n    return next(line for line in input_iterator if line.strip())\n\ndef parse_graph(input_iterator: Iterator[str], is_directed: bool = False) -&gt; Graph:\n    vertex_count, edge_count = parse_integers(read_non_empty_line(input_iterator))\n    \n    adjacency_list: Graph = defaultdict(list)\n    \n    for _ in range(edge_count):\n        source, destination = parse_integers(read_non_empty_line(input_iterator))\n        adjacency_list[source].append(destination)\n        if not is_directed:\n            adjacency_list[destination].append(source)\n\n    return adjacency_list\n\ndef parse_multiple_graphs(input_iterator: Iterator[str], is_directed: bool = False) -&gt; List[Graph]:\n    num_cases = int(read_non_empty_line(input_iterator))\n    return [parse_graph(input_iterator, is_directed) for _ in range(num_cases)]\n\ndef graph_nodes(graph: Graph) -&gt; Set[int]:\n    return set(graph.keys()) | set.union(*(set(val) for val in graph.values()))\n\ndef topological_sort(graph: Graph) -&gt; List[int]:\n    def dfs(node: int, visited: Set[int], stack: List[int]):\n        visited.add(node)\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                dfs(neighbor, visited, stack)\n        stack.append(node)\n\n    visited: Set[int] = set()\n    stack: List[int] = []\n    for node in graph_nodes(graph):\n        if node not in visited:\n            dfs(node, visited, stack)\n\n    return stack[::-1]\n\ndef hdag(graph: Graph) -&gt; List[int]:\n    sorted_nodes = topological_sort(graph)\n    for a, b in zip(sorted_nodes, sorted_nodes[1:]):\n        if b not in graph[a]:\n            return [-1]\n    return [1] + sorted_nodes\n\nsample_input = \"\"\"\n2\n\n3 3\n1 2\n2 3\n1 3\n\n4 3\n4 3\n3 2\n4 1\n\"\"\"\n\ninput_iterator = iter(StringIO(sample_input.strip()).readlines())\ngraphs = parse_multiple_graphs(input_iterator, is_directed=True)\nfor graph in graphs:\n    print(*hdag(graph))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-27",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-27",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "28.1 Sample Dataset",
    "text": "28.1 Sample Dataset\n2\n\n4 5\n1 4 4\n4 2 3\n2 3 1\n3 1 6\n2 1 -7\n\n3 4\n1 2 -8\n2 3 20\n3 1 -1\n3 2 -30"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-27",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-27",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "28.2 Sample Output",
    "text": "28.2 Sample Output\n-1 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-26",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-26",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "28.3 Solution",
    "text": "28.3 Solution\nfrom io import StringIO\nfrom typing import List, Dict, Union, Iterator\nfrom collections import deque\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iterator: Iterator[str]) -&gt; str:\n    line = next(input_iterator).strip()\n    while not line:\n        line = next(input_iterator).strip()\n    return line\n\ndef parse_graph(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Dict[int, List[Union[int, Dict[str, int]]]]:\n    node_count, edge_count = parse_integers(read_non_empty_line(input_iterator))\n    \n    adjacency_list = {node: [] for node in range(1, node_count + 1)}\n    \n    for _ in range(edge_count):\n        edge_data = parse_integers(read_non_empty_line(input_iterator))\n        if is_weighted:\n            source, target, weight = edge_data\n            adjacency_list[source].append({\"node\": target, \"weight\": weight})\n            if not is_directed:\n                adjacency_list[target].append({\"node\": source, \"weight\": weight})\n        else:\n            source, target = edge_data\n            adjacency_list[source].append(target)\n            if not is_directed:\n                adjacency_list[target].append(source)\n\n    return adjacency_list\n\ndef parse_multiple_graphs(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; List[Dict[int, List[Union[int, Dict[str, int]]]]]:\n    test_case_count = int(read_non_empty_line(input_iterator))\n    return [parse_graph(input_iterator, is_directed, is_weighted) for _ in range(test_case_count)]\n\ndef count_edges(graph):\n    edge_count = 0\n    for _, neighbors in graph.items():\n        for _ in neighbors:\n            edge_count += 1\n    return edge_count\n\ndef get_all_nodes(graph):\n    source_nodes = list(graph.keys())\n    target_nodes = [neighbor[\"node\"] for neighbors in graph.values() for neighbor in neighbors]\n    return set(source_nodes) | set(target_nodes)\n\ndef detect_negative_weight_cycle(graph):\n    distances = {node: 10**20 for node in get_all_nodes(graph)}\n    distances[1] = 0\n    for _ in range(count_edges(graph) - 1):\n        for source, neighbors in graph.items():\n            for neighbor in neighbors:\n                distances[neighbor[\"node\"]] = min(distances[source] + neighbor[\"weight\"], distances[neighbor[\"node\"]])\n    for source, neighbors in graph.items():\n        for neighbor in neighbors:\n            if distances[source] + neighbor[\"weight\"] &lt; distances[neighbor[\"node\"]]:\n                return 1\n    return -1\n\nsample_input = \"\"\"\n2\n\n4 5\n1 4 4\n4 2 3\n2 3 1\n3 1 6\n2 1 -7\n\n3 4\n1 2 -8\n2 3 20\n3 1 -1\n3 2 -30\n\"\"\"\n\ninput_iterator = iter(StringIO(sample_input.strip()).readlines())\ngraphs = parse_multiple_graphs(input_iterator, is_directed=True, is_weighted=True)\nprint(*[detect_negative_weight_cycle(graph) for graph in graphs])"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-28",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-28",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "29.1 Sample Dataset",
    "text": "29.1 Sample Dataset\n7\n5 -2 4 7 8 -10 11"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-28",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-28",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "29.2 Sample Output",
    "text": "29.2 Sample Output\n-10 -2 4 5 7 8 11"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-27",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-27",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "29.3 Solution",
    "text": "29.3 Solution\nfrom typing import List\n\ndef parse_integers(input_line: str) -&gt; List[int]:\n    return [int(number) for number in input_line.split()]\n\ndef three_way_partition(array: List[int], start: int = None, end: int = None) -&gt; tuple:\n    if start is None:\n        start = 0\n    if end is None:\n        end = len(array) - 1\n    pivot = array[start]\n    current = start\n    while current &lt;= end:\n        if array[current] &lt; pivot:\n            array[current], array[start] = array[start], array[current]\n            current += 1\n            start += 1\n        elif array[current] &gt; pivot:\n            array[current], array[end] = array[end], array[current]\n            end -= 1\n        else:\n            current += 1\n    return start, end\n\ndef quick_sort(array: List[int]):\n    def quick_sort_recursive(array: List[int], start: int, end: int):\n        if start &lt; end:\n            partition_start, partition_end = three_way_partition(array, start, end)\n            if start &lt; partition_start:\n                quick_sort_recursive(array, start, partition_start - 1)\n            if end &gt; partition_end:\n                quick_sort_recursive(array, partition_end + 1, end)\n\n    quick_sort_recursive(array, 0, len(array) - 1)\n\nsample_input = \"\"\"\n7\n5 -2 4 7 8 -10 11\n\"\"\"\n\n_, input_numbers = sample_input.strip().split(\"\\n\")\nnumbers = parse_integers(input_numbers)  # Convert the input string to a list of integers\nquick_sort(numbers)\nprint(*numbers)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-29",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-29",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "30.1 Sample Dataset",
    "text": "30.1 Sample Dataset\n6 7\n4 1\n1 2\n2 4\n5 6\n3 2\n5 3\n3 5"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-29",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-29",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "30.2 Sample Output",
    "text": "30.2 Sample Output\n3"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-28",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-28",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "30.3 Solution",
    "text": "30.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Dict, Set, Union, Iterator\nfrom io import StringIO\n\nGraph = Dict[int, List[Union[int, Dict[str, int]]]]\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iterator: Iterator[str]) -&gt; str:\n    return next(line.strip() for line in input_iterator if line.strip())\n\ndef parse_graph(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Graph:\n    node_count, edge_count = parse_integers(read_non_empty_line(input_iterator))\n    \n    graph = {n: [] for n in range(1, node_count + 1)}\n    \n    for _ in range(edge_count):\n        edge = parse_integers(read_non_empty_line(input_iterator))\n        if is_weighted:\n            source, target, weight = edge\n            graph[source].append({\"n\": target, \"w\": weight})\n            if not is_directed:\n                graph[target].append({\"n\": source, \"w\": weight})\n        else:\n            source, target = edge\n            graph[source].append(target)\n            if not is_directed:\n                graph[target].append(source)\n\n    return graph\n\ndef get_graph_nodes(graph: Graph) -&gt; Set[int]:\n    return set(graph.keys())\n\ndef topological_sort(graph: Graph) -&gt; List[int]:\n    def dfs(node: int, visited: Set[int], stack: List[int]):\n        visited.add(node)\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                dfs(neighbor, visited, stack)\n        stack.append(node)\n\n    visited: Set[int] = set()\n    stack: List[int] = []\n    for node in get_graph_nodes(graph):\n        if node not in visited:\n            dfs(node, visited, stack)\n\n    return stack[::-1]\n\ndef find_component(start_node: int, graph: Graph) -&gt; Set[int]:\n    def visit(node: int, visited: Set[int]) -&gt; Set[int]:\n        visited.add(node)\n        for neighbor in set(graph[node]) - visited:\n            visit(neighbor, visited)\n        return visited\n\n    return visit(start_node, set())\n\ndef reverse_graph(graph: Graph) -&gt; Graph:\n    reversed_graph = defaultdict(list)\n    for node in graph:\n        for child in graph[node]:\n            reversed_graph[child].append(node)\n    return reversed_graph\n\ndef strongly_connected_components(graph: Graph) -&gt; Iterator[Set[int]]:\n    order = topological_sort(graph)\n    reversed_graph = reverse_graph(graph)\n    while order:\n        node = order.pop(0)\n        component = find_component(node, reversed_graph)\n        order = [x for x in order if x not in component]\n        for key in reversed_graph.keys():\n            reversed_graph[key] = [n for n in reversed_graph[key] if n not in component]\n        yield component\n\nsample_input = \"\"\"\n6 7\n4 1\n1 2\n2 4\n5 6\n3 2\n5 3\n3 5\n\"\"\"\n\ninput_iterator = iter(StringIO(sample_input.strip()).readlines())\ngraph = parse_graph(input_iterator, is_directed=True)\nprint(len(list(strongly_connected_components(graph))))"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-30",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-30",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "31.1 Sample Dataset",
    "text": "31.1 Sample Dataset\n2\n\n2 4\n1 2\n-1 2\n1 -2\n-1 -2\n\n3 4\n1 2\n2 3\n-1 -2\n-2 -3"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-30",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-30",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "31.2 Sample Output",
    "text": "31.2 Sample Output\n0\n1 1 -2 3"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-29",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-29",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "31.3 Solution",
    "text": "31.3 Solution\nimport sys\nfrom collections import defaultdict\nfrom io import StringIO\n\ndef parse_integers(line):\n    return list(map(int, line.split()))\n\nclass RecursionLimitManager:\n    def __init__(self, new_limit):\n        self.new_limit = new_limit\n\n    def __enter__(self):\n        self.old_limit = sys.getrecursionlimit()\n        sys.setrecursionlimit(self.new_limit)\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        sys.setrecursionlimit(self.old_limit)\n\ndef reverse_graph(original_graph):\n    reversed_graph = defaultdict(list)\n    for node in original_graph:\n        for neighbor in original_graph[node]:\n            reversed_graph[neighbor].append(node)\n    return reversed_graph\n    \ndef find_strongly_connected_components(graph):\n    node_order = topological_sort(graph)\n    reversed_graph = reverse_graph(graph)\n    while node_order:\n        start_node = node_order.pop(0)\n        component = find_component(start_node, reversed_graph)\n        node_order = [node for node in node_order if node not in component]\n        for key in reversed_graph.keys():\n            reversed_graph[key] = [node for node in reversed_graph[key] if node not in component]\n        yield component\n\ndef find_component_index(node, components):\n    for index, component in enumerate(components):\n        if node in component:\n            return index\n            \ndef condense_graph(original_graph, components):\n    condensed_graph = {}\n    for index, component in enumerate(components):\n        condensed_graph[index] = set(\n            [\n                find_component_index(neighbor, components)\n                for node in component\n                for neighbor in original_graph[node]\n                if neighbor not in component\n            ]\n        )\n    return condensed_graph\n\ndef topological_sort(graph):\n    def depth_first_search(node, visited_nodes, node_stack):\n        visited_nodes[node] = True\n        for neighbor in graph[node]:\n            if not visited_nodes[neighbor]:\n                depth_first_search(neighbor, visited_nodes, node_stack)\n        node_stack.append(node)\n\n    visited_nodes = defaultdict(bool)\n    node_stack = []\n    for node in get_graph_nodes(graph):\n        if not visited_nodes[node]:\n            depth_first_search(node, visited_nodes, node_stack)\n\n    return node_stack[::-1]\n\ndef parse_2sat_instances(input_handle):\n    instance_count = int(next(input_handle))\n    for _ in range(instance_count):\n        yield parse_2sat_instance(input_handle)\n\ndef parse_2sat_instance(input_handle):\n    info = next(input_handle)\n    if info == \"\\n\":\n        info = next(input_handle)\n    variable_count, clause_count = parse_integers(info)\n    clauses = [next(input_handle) for _ in range(clause_count)]\n    implication_graph = {}\n\n    for variable in range(1, variable_count + 1):\n        implication_graph[variable] = list()\n        implication_graph[-variable] = list()\n\n    for clause in clauses:\n        literal1, literal2 = parse_integers(clause)\n        implication_graph[-literal1].append(literal2)\n        implication_graph[-literal2].append(literal1)\n\n    return implication_graph\n\ndef solve_2sat(implication_graph):\n    components = list(find_strongly_connected_components(implication_graph))\n    condensed_graph = condense_graph(implication_graph, components)\n\n    for component_index in topological_sort(condensed_graph):\n        for literal in components[component_index]:\n            if -literal in components[component_index]:\n                return 0, []\n\n    assignment = []\n    for component_index in reversed(topological_sort(condensed_graph)):\n        for literal in components[component_index]:\n            if literal not in assignment and -literal not in assignment:\n                assignment.append(literal)\n\n    return 1, sorted(assignment, key=lambda x: abs(x))\n\nsample_input = \"\"\"\n2\n\n2 4\n1 2\n-1 2\n1 -2\n-1 -2\n\n3 4\n1 2\n2 3\n-1 -2\n-2 -3\n\"\"\"\n\nwith RecursionLimitManager(5000):\n    input_iterator = iter(StringIO(sample_input.strip()).readlines())\n    implication_graphs = parse_2sat_instances(input_iterator)\n    for implication_graph in implication_graphs:\n        is_satisfiable, satisfying_assignment = solve_2sat(implication_graph)\n        print(is_satisfiable, *satisfying_assignment)"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-31",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-31",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "32.1 Sample Dataset",
    "text": "32.1 Sample Dataset\n2\n\n3 2\n3 2\n2 1\n\n3 2\n3 2\n1 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-31",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-31",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "32.2 Sample Output",
    "text": "32.2 Sample Output\n3 -1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-30",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-30",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "32.3 Solution",
    "text": "32.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Dict, Union, Iterator\nfrom io import StringIO\n\nGraph = Dict[int, List[Union[int, Dict[str, int]]]]\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return [int(num) for num in line.split()]\n\ndef read_non_empty_line(input_iterator: Iterator[str]) -&gt; str:\n    return next(line.strip() for line in input_iterator if line.strip())\n\ndef parse_graph(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Graph:\n    node_count, edge_count = parse_integers(read_non_empty_line(input_iterator))\n    \n    graph = {node: [] for node in range(1, node_count + 1)}\n    \n    for _ in range(edge_count):\n        edge = parse_integers(read_non_empty_line(input_iterator))\n        if is_weighted:\n            source, target, weight = edge\n            graph[source].append({\"n\": target, \"w\": weight})\n            if not is_directed:\n                graph[target].append({\"n\": source, \"w\": weight})\n        else:\n            source, target = edge\n            graph[source].append(target)\n            if not is_directed:\n                graph[target].append(source)\n\n    return graph\n\ndef parse_multiple_graphs(input_iterator: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; List[Graph]:\n    test_case_count = int(read_non_empty_line(input_iterator))\n    return [parse_graph(input_iterator, is_directed, is_weighted) for _ in range(test_case_count)]\n\ndef bfs(graph: Graph, start_node=1) -&gt; List[int]:\n    node_count = len(graph)\n    distances = [-1 for _ in range(node_count + 1)]\n    distances[start_node] = 0\n    queue = [start_node]\n    \n    while queue:\n        current_node = queue.pop(0)\n        for neighbor in graph[current_node]:\n            if distances[neighbor] == -1:\n                queue.append(neighbor)\n                distances[neighbor] = distances[current_node] + 1\n    return distances[1:]\n\ndef find_good_start_node(graph: Graph) -&gt; int:\n    for node in graph:\n        distances = bfs(graph, node)\n        all_nodes_reachable = [distance &gt;= 0 for distance in distances]\n        if all(all_nodes_reachable):\n            return node\n    return -1\n\nsample_input = \"\"\"\n2\n\n3 2\n3 2\n2 1\n\n3 2\n3 2\n1 2\n\"\"\"\n\ninput_iterator = iter(StringIO(sample_input.strip()).readlines())\n\ngraphs = parse_multiple_graphs(input_iterator, is_directed=True)\nprint(*[find_good_start_node(g) for g in graphs])"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-32",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-32",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "33.1 Sample Dataset",
    "text": "33.1 Sample Dataset\n2\n\n3 2\n3 2\n2 1\n\n3 2\n3 2\n1 2"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-32",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-32",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "33.2 Sample Output",
    "text": "33.2 Sample Output\n1 -1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-31",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-31",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "33.3 Solution",
    "text": "33.3 Solution\nimport sys\nfrom collections import defaultdict\n\ndef parse_integers(string):\n    return list(map(int, string.split()))\n\ndef create_graph(input_stream, is_directed=False, is_weighted=False):\n    while True:\n        try:\n            graph_info = next(input_stream).strip()\n            if graph_info:\n                break\n        except StopIteration:\n            return None\n\n    try:\n        vertex_count, edge_count = parse_integers(graph_info)\n    except ValueError:\n        return None\n\n    edge_list = []\n    for _ in range(edge_count):\n        try:\n            edge = next(input_stream).strip()\n            if edge:\n                edge_list.append(edge)\n        except StopIteration:\n            break\n\n    adjacency_list = {v: [] for v in range(1, vertex_count + 1)}\n\n    for edge in edge_list:\n        if is_weighted:\n            source, target, weight = parse_integers(edge)\n            adjacency_list[source].append({\"vertex\": target, \"weight\": weight})\n            if not is_directed:\n                adjacency_list[target].append({\"vertex\": source, \"weight\": weight})\n        else:\n            source, target = parse_integers(edge)\n            adjacency_list[source].append(target)\n            if not is_directed:\n                adjacency_list[target].append(source)\n\n    return adjacency_list\n\ndef parse_multiple_graphs(input_stream, is_directed=False, is_weighted=False):\n    try:\n        graph_count = int(next(input_stream).strip())\n    except (StopIteration, ValueError):\n        return []\n\n    graph_list = []\n    for _ in range(graph_count):\n        graph = create_graph(input_stream, is_directed=is_directed, is_weighted=is_weighted)\n        if graph is not None:\n            graph_list.append(graph)\n        else:\n            break\n    return graph_list\n\ndef reverse_graph(graph):\n    reversed_graph = defaultdict(list)\n    for vertex in graph:\n        for neighbor in graph[vertex]:\n            reversed_graph[neighbor].append(vertex)\n    return reversed_graph\n\ndef get_all_vertices(graph):\n    vertex_set = set()\n    for vertex, neighbors in graph.items():\n        vertex_set.add(vertex)\n        vertex_set.update(neighbors)\n    return vertex_set\n\ndef topological_sort(graph):\n    def dfs(vertex, visited, stack):\n        visited[vertex] = True\n        for neighbor in graph[vertex]:\n            if not visited[neighbor]:\n                dfs(neighbor, visited, stack)\n        stack.append(vertex)\n\n    visited = defaultdict(bool)\n    stack = []\n    for vertex in get_all_vertices(graph):\n        if not visited[vertex]:\n            dfs(vertex, visited, stack)\n\n    return stack[::-1]\n\ndef find_connected_component(start_vertex, graph):\n    def dfs(vertex, visited):\n        visited.add(vertex)\n        for neighbor in set(graph[vertex]) - visited:\n            dfs(neighbor, visited)\n        return visited\n\n    return dfs(start_vertex, set())\n\ndef strongly_connected_components(graph):\n    vertex_order = topological_sort(graph)\n    reversed_graph = reverse_graph(graph)\n    while vertex_order:\n        start_vertex = vertex_order.pop(0)\n        component = find_connected_component(start_vertex, reversed_graph)\n        vertex_order = [v for v in vertex_order if v not in component]\n        for vertex in reversed_graph.keys():\n            reversed_graph[vertex] = [v for v in reversed_graph[vertex] if v not in component]\n        yield component\n\ndef hamiltonian_dag(graph):\n    sorted_vertices = topological_sort(graph)\n    for v1, v2 in zip(sorted_vertices, sorted_vertices[1:]):\n        if v2 not in graph[v1]:\n            return [-1]\n    return [1] + sorted_vertices\n\ndef find_component_index(vertex, components):\n    for index, component in enumerate(components):\n        if vertex in component:\n            return index\n\ndef condense_graph(graph, components):\n    condensed_graph = {}\n    for i, component in enumerate(components):\n        condensed_graph[i] = set(\n            find_component_index(neighbor, components)\n            for vertex in component\n            for neighbor in graph[vertex]\n            if neighbor not in component\n        )\n    return condensed_graph\n\ndef is_semi_connected(graph):\n    components = list(strongly_connected_components(graph))\n    condensed_graph = condense_graph(graph, components)\n    return -1 if hamiltonian_dag(condensed_graph) == [-1] else 1\n\nsample_input = \"\"\"\n2\n\n3 2\n3 2\n2 1\n\n3 2\n3 2\n1 2\n\"\"\"\n\ninput_lines = sample_input.strip().split(\"\\n\")\ninput_iterator = iter(input_lines)\ngraphs = parse_multiple_graphs(input_iterator, is_directed=True)\nprint(*[is_semi_connected(g) for g in graphs])"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-33",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-dataset-33",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "34.1 Sample Dataset",
    "text": "34.1 Sample Dataset\n5 6\n2 3 4\n4 3 -2\n1 4 1\n1 5 -3\n2 4 -2\n5 4 1"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#sample-output-33",
    "href": "posts/md/Rosalind_algorithmicHeights.html#sample-output-33",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "34.2 Sample Output",
    "text": "34.2 Sample Output\n0 x -4 -2 -3"
  },
  {
    "objectID": "posts/md/Rosalind_algorithmicHeights.html#solution-32",
    "href": "posts/md/Rosalind_algorithmicHeights.html#solution-32",
    "title": "Rosalind Algorithmic Heights 문제풀이",
    "section": "34.3 Solution",
    "text": "34.3 Solution\nfrom math import inf\nfrom typing import List, Dict, Set, Iterator, Union, Optional\nfrom collections import defaultdict\n\ndef parse_integers(line: str) -&gt; List[int]:\n    return list(map(int, line.split()))\n\ndef create_graph(input_stream: Iterator[str], is_directed: bool = False, is_weighted: bool = False) -&gt; Dict[int, List[Union[int, Dict[str, int]]]]:\n    graph_info = next(input_stream)\n    if graph_info == \"\\n\":\n        graph_info = next(input_stream)\n    \n    vertex_count, edge_count = parse_integers(graph_info)\n    edge_list = [next(input_stream) for _ in range(edge_count)]\n    adjacency_list: Dict[int, List[Union[int, Dict[str, int]]]] = {v: [] for v in range(1, vertex_count + 1)}\n\n    for edge in edge_list:\n        if is_weighted:\n            source, target, weight = parse_integers(edge)\n            adjacency_list[source].append({\"vertex\": target, \"weight\": weight})\n            if not is_directed:\n                adjacency_list[target].append({\"vertex\": source, \"weight\": weight})\n        else:\n            source, target = parse_integers(edge)\n            adjacency_list[source].append(target)\n            if not is_directed:\n                adjacency_list[target].append(source)\n\n    return adjacency_list\n\ndef get_all_vertices(graph: Dict[int, List[Union[int, Dict[str, int]]]]) -&gt; Set[int]:\n    vertex_set: Set[int] = set()\n    for vertex, neighbors in graph.items():\n        vertex_set.add(vertex)\n        vertex_set.update([n if isinstance(n, int) else n[\"vertex\"] for n in neighbors])\n    return vertex_set\n\ndef topological_sort(graph: Dict[int, List[int]]) -&gt; List[int]:\n    def depth_first_search(vertex: int, visited: Dict[int, bool], stack: List[int]) -&gt; None:\n        visited[vertex] = True\n        for neighbor in graph[vertex]:\n            if not visited[neighbor]:\n                depth_first_search(neighbor, visited, stack)\n        stack.append(vertex)\n\n    visited: Dict[int, bool] = defaultdict(bool)\n    stack: List[int] = []\n    for vertex in get_all_vertices(graph):\n        if not visited[vertex]:\n            depth_first_search(vertex, visited, stack)\n\n    return stack[::-1]\n\ndef simplify_graph(graph: Dict[int, List[Dict[str, int]]]) -&gt; Dict[int, List[int]]:\n    return {k: [x[\"vertex\"] for x in v] for k, v in graph.items()}\n\ndef shortest_distances_acyclic_graph(graph: Dict[int, List[Dict[str, int]]]) -&gt; List[Union[str, int]]:\n    vertex_count = len(graph)\n    distances = [inf for _ in range(vertex_count + 1)]\n    distances[1] = 0\n    topological_order = topological_sort(simplify_graph(graph))\n    \n    for current_vertex in topological_order:\n        seen_vertices = set()\n        for edge in reversed(graph[current_vertex]):\n            neighbor = edge[\"vertex\"]\n            if neighbor not in seen_vertices:\n                seen_vertices.add(neighbor)\n                if distances[current_vertex] + edge[\"weight\"] &lt; distances[neighbor]:\n                    distances[neighbor] = distances[current_vertex] + edge[\"weight\"]\n    \n    return [\"x\" if d == inf else d for d in distances[1:]]\n\nsample_input = \"\"\"\n5 6\n2 3 4\n4 3 -2\n1 4 1\n1 5 -3\n2 4 -2\n5 4 1\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\ninput_iterator: Iterator[str] = iter(input_lines)\nresult = shortest_distances_acyclic_graph(create_graph(input_iterator, is_directed=True, is_weighted=True))\nprint(*result)"
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html",
    "href": "posts/md/Intsall_DockQ.html",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "",
    "text": "DockQ 는 단백질, DNA 및 저분자 화합물 도킹 모델에 대한 품질을 측정하는 도구입니다. DockQ 점수는 0 에서 1 까지의 범위로 측정되며 품질은 아래와 같이 정의됩니다."
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#pixi-를-사용하는-방법",
    "href": "posts/md/Intsall_DockQ.html#pixi-를-사용하는-방법",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "1.1 pixi 를 사용하는 방법",
    "text": "1.1 pixi 를 사용하는 방법\ngit clone https://github.com/bjornwallner/DockQ/\ncd DockQ\npixi init .\npixi add \"python=3.9.1\"\npixi add \"numpy=1.23.4\"\npixi shell"
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#uv-를-사용하는-방법",
    "href": "posts/md/Intsall_DockQ.html#uv-를-사용하는-방법",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "1.2 uv 를 사용하는 방법",
    "text": "1.2 uv 를 사용하는 방법\ngit clone https://github.com/bjornwallner/DockQ/\ncd DockQ\nuv venv venv\nsource venv/bin/activate\n(venv) uv pip install numpy==1.23.4\n(venv) uv pip install ."
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#명령어-예시",
    "href": "posts/md/Intsall_DockQ.html#명령어-예시",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "2.1 명령어 예시",
    "text": "2.1 명령어 예시\n하나 이상의 인터페이스가 있는 모델/원형 단백질 복합체에서 DockQ 를 실행하면 각 인터페이스에 대한 결과를 얻을 수 있습니다. 결과는 모든 인터페이스에서 평균을 최대화하도록 계산됩니다:\npixi run DockQ examples/1A2K_r_l_b.model.pdb examples/1A2K_r_l_b.pdb\n플래그 --short 를 사용하면 더 간결한 출력을 볼 수 있습니다."
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#단백질---단백질-결합-스코어링",
    "href": "posts/md/Intsall_DockQ.html#단백질---단백질-결합-스코어링",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "2.2 단백질 - 단백질 결합 스코어링",
    "text": "2.2 단백질 - 단백질 결합 스코어링\n기본적으로 DockQ 는 원본 인터페이스와 모델에 있는 인터페이스 간에 최적의 매핑을 찾으려고 시도합니다.\n가장 간단한 경우는 호모다이머가 모델링된 경우입니다. 그런 다음 원본의 인터페이스 AB(원래 체인 A 와 B 사이) 를 모델 AB 인터페이스와 비교할 수 있지만 체인의 순서를 변경하면 일반적으로 결과가 변경 되므로 BA 와도 비교할 수 있습니다. DockQ homodimer_model.pdb homodimer_native.pdb 명령어를 입력하면 소프트웨어가 가장 높은 DockQ 점수를 가진 매핑 (AB -&gt; AB 또는 AB -&gt; BA) 값을 출력합니다.\n사용자가 특정 매핑을 적용하려면 --mapping 플래그를 사용할 수 있습니다. 이는 예를 들어 모델/원형에 많은 수의 상동 (homologous) 사슬이 포함되어 있는 경우 계산 속도를 높일 수 있어 유용합니다.\n\n2.2.1 완전 매핑\n사용자는 원본 체인과 모델 체인 간의 전체 매핑을 정의합니다: --mapping MODELCHAINS:NATIVECHAINS. 예를 들어, 앞의 경우 두 가지 가능한 매핑이 있을 수 있습니다:\n\n--mapping AB:AB(원본 체인 A 는 모델 체인 A 에, 원본 체인 B 는 모델 B 에 해당)\n--mapping AB:BA(원본 체인 A 는 모델 체인 B 에, 원본 체인 B 는 모델 A 에 해당)\n\n콜론 : 앞의 쌍은 모델에서 체인 순서를 정의하고, 뒤의 쌍은 원본에서 순서를 정의합니다.\n\n\n2.2.2 부분 매핑\n사용자가 매핑의 일부를 수정하고 나머지는 DockQ 가 최적화하도록 하려는 경우 와일드카드를 사용할 수 있습니다. 예를 들어, 테트라머에 모델에 ABCD 체인이 있고 원본에는 WXYZ 체인이 있는 경우 아래 명령어를 사용합니다.\n--mapping A*:W*\n여기서 와일드카드 * 는 DockQ 가 A -&gt; W 를 고정된 상태로 유지하면서 BCD 와 XYZ 사이의 매핑을 최적화해야 함을 나타냅니다. 두개의 체인을 고정하는 경우는 아래 명령어를 사용합니다.\n--mapping AD*:WY*\n\n\n2.2.3 원본 인터페이스의 하위 집합으로 검색 제한하기\n사용자가 원본 인터페이스 중 하나 이상의 특정 인터페이스에 관심이 있고 나머지는 무시해야 하는 경우 다음 명령어를 사용합니다.\n--mapping *:WX\n그러면 DockQ 가 모델에서 네이티브의 WX 인터페이스와 가장 잘 일치하는 인터페이스를 찾습니다. 여러 원본 인터페이스를 사용하는 경우 명령어는 아래와 같습니다.\n--mapping *:WXY"
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#단백질---저분자-화합물-결합-스코어링",
    "href": "posts/md/Intsall_DockQ.html#단백질---저분자-화합물-결합-스코어링",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "2.3 단백질 - 저분자 화합물 결합 스코어링",
    "text": "2.3 단백질 - 저분자 화합물 결합 스코어링\n단백질과 동일한 방식으로 PDB 또는 mmCIF 파일의 저분자를 스코어링하고 매핑을 최적화할 수 있습니다. --small_molecules 플래그를 추가하기만 하면 됩니다:\n# 헤모글로빈 사슬(네이티브의 사슬 A와 B)과 HEM 및 PO4 그룹(사슬 E, F, G)의 도킹을 비교합니다.\n$ DockQ examples/1HHO_hem.cif examples/2HHB_hem.cif --small_molecule --mapping :ABEFG --short\n\n\n\n\n\n\nNote\n\n\n\n저분자에 대해서는 LRMSD 값만 출력됩니다.\n\n\n참고로 저분자 화합물은 PDB/mmCIF 파일에 포함되어야 합니다. 또한 별도의 체인 식별자가 있어야 합니다 (mmCIF 파일에서는 label_asym_id 필드가 사용됩니다)."
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#단백질---dnarna-결합-스코어링",
    "href": "posts/md/Intsall_DockQ.html#단백질---dnarna-결합-스코어링",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "2.4 단백질 - DNA/RNA 결합 스코어링",
    "text": "2.4 단백질 - DNA/RNA 결합 스코어링\nDNA 와 관련된 인터페이스는 단백질 인터페이스와 마찬가지로 똑같이 점수가 매겨집니다. 단백질-DNA 또는 DNA-DNA 인터페이스에 대해서는 단백질-단백질 인터페이스와 동일한 방식으로 DockQ 점수가 계산됩니다. 이중 나선 사슬의 경우 두 개의 DockQ 점수가 출력됩니다."
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#파이썬-모듈로-dockq-사용하기",
    "href": "posts/md/Intsall_DockQ.html#파이썬-모듈로-dockq-사용하기",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "3.1 파이썬 모듈로 DockQ 사용하기",
    "text": "3.1 파이썬 모듈로 DockQ 사용하기\nDockQ 를 설치한 후에는 파이썬 코드에서 모듈로 사용할 수도 있습니다.\nfrom DockQ.DockQ import load_PDB, run_on_all_native_interfaces\n\nmodel = load_PDB(\"examples/1A2K_r_l_b.model.pdb\")\nnative = load_PDB(\"examples/1A2K_r_l_b.pdb\")\n\n# model:native 체인에 대한 인터페이스를 딕셔너리 형태로 매핑\nchain_map = {\"A\":\"A\", \"B\":\"B\"}\n# 결과는 딕셔너리 형태로 반환.\nrun_on_all_native_interfaces(model, native, chain_map=chain_map)"
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#단일-수용체-또는-리간드에-여러-사슬-병합하기",
    "href": "posts/md/Intsall_DockQ.html#단일-수용체-또는-리간드에-여러-사슬-병합하기",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "3.2 단일 수용체 또는 리간드에 여러 사슬 병합하기",
    "text": "3.2 단일 수용체 또는 리간드에 여러 사슬 병합하기\n여러 개의 사슬 (예를 들면 항체의 중쇄와 경쇄 사슬) 을 단일 수용체 또는 리간드로 병합하려면 이슈 #33 을 참고하세요."
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#인용하기",
    "href": "posts/md/Intsall_DockQ.html#인용하기",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "4.1 인용하기",
    "text": "4.1 인용하기\nDockQ 를 사용하는 경우 아래 프리프린트를 인용하세요.https://doi.org/10.1101/2024.05.28.596225"
  },
  {
    "objectID": "posts/md/Intsall_DockQ.html#pixi-의존성-파일",
    "href": "posts/md/Intsall_DockQ.html#pixi-의존성-파일",
    "title": "DockQ로 도킹 모델 품질 측정하기",
    "section": "4.2 pixi 의존성 파일",
    "text": "4.2 pixi 의존성 파일\n[tool.pixi.project]\nchannels = [\"conda-forge\"]\nplatforms = [\"linux-64\"]\n\n[tool.pixi.pypi-dependencies]\ndockq = { path = \".\", editable = true }\n\n[tool.pixi.dependencies]\nnumpy = \"1.23.4.*\"\npython = \"3.9.1.*\"\nbiopython = \"&gt;=1.84,&lt;2\"\njupyterlab = \"&gt;=4.2.4,&lt;5\""
  },
  {
    "objectID": "posts/md/How_ML4math.html",
    "href": "posts/md/How_ML4math.html",
    "title": "머신러닝을 위한 수학",
    "section": "",
    "text": "책 “아카이시 마사노리, 딥러닝을 위한 수학, 위키북스, 2020”을 읽고 저자가 정리한 딥러닝을 위한 수학의 최단기 코스 목차를 참고해 내용을 하였습니다. 더 자세한 설명과 파이썬 코드를 알고 싶으시다면 책을 읽어보시는 걸 추천드립니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#알아야하는-필수-개념",
    "href": "posts/md/How_ML4math.html#알아야하는-필수-개념",
    "title": "머신러닝을 위한 수학",
    "section": "1.1 알아야하는 필수 개념",
    "text": "1.1 알아야하는 필수 개념\n\n손실함수\n행렬과 행렬 연산\n경사하강법\n시그모이드 함수\n소프트맥스 함수\n가능도함수와 최대가능도 추정\n오차역전파"
  },
  {
    "objectID": "posts/md/How_ML4math.html#함수",
    "href": "posts/md/How_ML4math.html#함수",
    "title": "머신러닝을 위한 수학",
    "section": "2.1 함수",
    "text": "2.1 함수\n관련 개념: 합성함수와 역함수, 극한과 미분, 다변수 함수\n함수는 정의역의 각 원소에 공역의 원소를 오직 하나씩 대응시키는 관계입니다.4 함수는 종종 ’ 보이지 않는 마술상자 ’ 에 비유됩니다. 입력값을 넣으면 특정한 출력값이 나오는 구조이기 때문입니다. 이 비유는 단순해 보이지만 함수의 본질적인 개념을 잘 표현합니다\n\n입력값 (정의역) 에 대해 단 하나의 출력값 (공역) 이 존재합니다.\n모든 입력값은 반드시 하나의 출력값을 가져야 합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#합성함수와-역함수",
    "href": "posts/md/How_ML4math.html#합성함수와-역함수",
    "title": "머신러닝을 위한 수학",
    "section": "2.2 합성함수와 역함수",
    "text": "2.2 합성함수와 역함수\n관련 개념: 로그함수\n합성함수와 역함수는 함수의 성질을 이해하고 복잡한 관계를 분석하는 데 중요한 도구입니다. 특히 합성함수는 여러 단계의 변환을 하나의 함수로 표현할 수 있게 해주며, 역함수는 함수의 가역성을 나타내는 중요한 개념입니다.\n합성함수: 합성함수는 두 개 이상의 함수를 연속적으로 적용하여 만든 새로운 함수입니다. 함수 \\(f\\) 와 \\(g\\) 가 있을 때 \\(g∘f(x) = g(f(x))\\) 로 정의되는 함수를 \\(f\\) 와 \\(g\\) 의 합성함수라고 합니다. 합성함수는 여러 함수를 거쳐 한 집합에서 다른 집합으로 직접 연결하는 ” 다리 ” 역할을 합니다.5\n\n교환법칙이 성립하지 않습니다. \\(g∘f ≠ f∘g\\)\n결합법칙은 성립합니다. \\((h∘g)∘f = h∘(g∘f)\\)\n\n역함수: 역함수는 주어진 함수의 입력과 출력을 서로 바꾼 함수입니다. 함수 \\(f\\) 에 대해 \\(y = f(x)\\) 이면 \\(x = f^(-1)(y)\\) 가 성립하는 함수 \\(f^(-1)\\) 를 \\(f\\) 의 역함수라고 합니다. 역함수는 원래 함수의 ” 되돌리기 ” 연산으로 볼 수 있습니다.\n\n모든 함수가 역함수를 가지는 것은 아닙니다. 일대일대응 함수만이 역함수를 가집니다.6\n\\(f(f^(-1)(x)) = f^(-1)(f(x)) = x\\) 가 성립합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#극한과-미분",
    "href": "posts/md/How_ML4math.html#극한과-미분",
    "title": "머신러닝을 위한 수학",
    "section": "2.3 극한과 미분",
    "text": "2.3 극한과 미분\n관련개념: 극대와 극소, 다항식의 미분, 곱의 미분, 적분, 합성함수의 미분, 로그함수의 미분, 편미분, 전미분\n극한은 어떤 변수가 특정 값에 한없이 가까워질 때 함수값이 수렴하는 값을 의미합니다.\n\n엡실론 - 델타 정의: 함수 \\(f(x)\\) 의 \\(x\\) 가 \\(a\\) 로 접근할 때의 극한 \\(L\\) 은 다음과 같이 정의됩니다: 모든 \\(ε &gt; 0\\) 에 대해, \\(δ &gt; 0\\) 이 존재하여 \\(0 &lt; |x - a| &lt; δ\\) 일 때 \\(|f(x) - L| &lt; ε\\) 이 성립합니다.\n직관적으로 \\(x\\) 가 \\(a\\) 에 충분히 가까워지면 \\(f(x)\\) 는 \\(L\\) 에 원하는 만큼 가까워질 수 있다는 의미입니다.\n엡실론 - 델타 정의는 해석학의 기초가 되며 이를 통해 함수의 성질을 엄밀하게 분석할 수 있습니다.\n\n미분은 함수의 순간변화율을 나타내는 개념입니다.\n\n극한을 이용한 정의: 함수 \\(f(x)\\) 의 \\(x = a\\) 에서의 미분은 다음과 같이 정의됩니다: \\(f'(a) = lim_{h→0} [f(a+h) - f(a)] / h\\)\n이는 함수의 그래프 상의 한 점에서의 접선의 기울기를 의미하며 \\(f'(a)\\) 를 함수 \\(f(a)\\) 의 도함수라 합니다.\n\n극한과 미분의 관계\n\n미분은 극한의 개념을 기반으로 정의됩니다. 미분 계수는 특정한 형태의 극한값입니다.\n함수가 미분 가능하려면 해당 점에서 극한이 존재해야 합니다. 즉, 좌극한과 우극한이 같아야 합니다.\n극한의 존재가 미분 가능성을 보장하지는 않습니다. 예를 들어, \\(|x|\\) 는 \\(x = 0\\) 에서 극한은 존재하지만 미분은 불가능합니다.\n미분 가능한 함수는 반드시 연속함수이지만 그 역은 성립하지 않습니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#극대와-극소",
    "href": "posts/md/How_ML4math.html#극대와-극소",
    "title": "머신러닝을 위한 수학",
    "section": "2.4 극대와 극소",
    "text": "2.4 극대와 극소\n관련개념: 경사하강법, 가능도함수와 최대가능도 추정\n극대와 극소는 함수의 그래프에서 봉우리와 골짜기를 나타내며, 이러한 점들을 찾는 것은 함수의 최적화 및 분석에 있어 매우 중요합니다. 극대와 극소를 찾기 위해서는 도함수를 사용하여 함수의 기울기가 \\(0\\) 이 되는 지점을 찾고, 이 지점들이 극대인지 극소인지 추가적인 분석을 통해 확인합니다. 이 개념들은 최적화 문제와 함수의 그래프 분석에서 중요한 역할을 합니다.\n극대: 함수 \\(y = f(x)\\) 가 \\(x = a\\) 에서 극대 (maximum) 를 가진다고 할 때, 이는 \\(f(a)\\) 가 \\(a\\) 의 근처에 있는 모든 \\(x\\) 에 대해 \\(f(x) \\leq f(a)\\) 를 만족하는 경우를 의미합니다.\n\n극대점에서는 함수의 기울기가 \\(0\\) 이 되거나, 변화가 없는 상태가 됩니다. 즉, 도함수 \\(f'(x) = 0\\) 이거나 정의되지 않는 점일 수 있습니다.\n\n극소: 함수 \\(y = f(x)\\) 가 \\(x = b\\) 에서 극소 (minimum) 를 가진다고 할 때 이는 \\(f(b)\\) 가 \\(b\\) 의 근처에 있는 모든 \\(x\\) 에 대해 \\(f(x) \\geq f(b)\\) 를 만족하는 경우를 의미합니다.\n\n극소점에서도 마찬가지로 함수의 기울기가 \\(0\\) 이 되거나 변화가 없는 상태가 됩니다. 즉, 도함수 \\(f'(x) = 0\\) 이거나 정의되지 않는 점일 수 있습니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#다항식의-미분",
    "href": "posts/md/How_ML4math.html#다항식의-미분",
    "title": "머신러닝을 위한 수학",
    "section": "2.5 다항식의 미분",
    "text": "2.5 다항식의 미분\n관련개념: 시그모이드 함수\n다항식의 미분은 함수의 순간 변화율을 나타내며, 곡선의 기울기를 구하거나 최적화 문제를 해결하는 데 사용됩니다. 다항식의 미분은 각 항을 개별적으로 미분한 후 그 결과를 더하는 방식으로 이루어집니다. 다항식 미분의 주요 특징은 다음과 같습니다:\n\n상수항의 미분: 상수 c 에 대해 \\((c)' = 0\\)\n거듭제곱 함수의 미분: \\(x^n\\) 의 미분은 \\(nx^(n-1)\\) 입니다. 즉, \\((x^n)' = nx^(n-1)\\).\n선형성: \\((f(x) + g(x))' = f'(x) + g'(x)\\), \\((cf(x))' = c * f'(x)\\). 여기서 \\(c\\) 는 상수.\n\n다항식의 미분 정의: 다항식 \\(f(x) = a_n x^n + a_(n-1) x^(n-1) + ... + a_1 x + a_0\\) 의 미분은 다음과 같이 정의됩니다:\n\n\\(f'(x) = n * a_n x^(n-1) + (n-1) * a_(n-1) x^(n-2) + ... + 2 * a_2 x + a_1\\) 이 정의를 사용하면 모든 다항식의 미분을 쉽게 계산할 수 있습니다.\n\n예를 들면 아래와 같습니다: - \\((3x^2 + 2x - 1)'= 6x + 2\\) - \\((x^3 - 5x + 7)' = 3x^2 - 5\\)"
  },
  {
    "objectID": "posts/md/How_ML4math.html#곱의-미분",
    "href": "posts/md/How_ML4math.html#곱의-미분",
    "title": "머신러닝을 위한 수학",
    "section": "2.6 곱의 미분",
    "text": "2.6 곱의 미분\n관련개념: 몫의 미분\n곱의 미분법칙 (product rule) 은 두 함수의 곱을 미분할 때 사용하는 규칙으로 복잡한 함수의 미분을 단순화하고 해결하는 데 활용됩니다.\n곱의 미분은 두 함수 \\(u(x)\\) 와 \\(v(x)\\) 의 곱 \\(f(x) = u(x)v(x)\\) 에 대한 미분은 다음과 같이 정의됩니다: \\(f'(x) = u'(x)v(x) + u(x)v'(x)\\). 여기서 \\(f'(x)\\) 는 \\(f(x)\\) 의 도함수이며 \\(u'(x)\\) 는 \\(u(x)\\) 의 도함수이고 \\(v'(x)\\) 는 \\(v(x)\\) 의 도함수입니다.\n곱의 미분 특징:\n\n이 법칙은 두 함수의 곱을 미분할 때 각 함수를 개별적으로 미분한 후 조합하는 방법을 제공합니다.\n첫 번째 함수의 미분에 두 번째 함수를 곱하고, 두 번째 함수의 미분에 첫 번째 함수를 곱한 후 이 둘을 더합니다.\n이 법칙은 두 개 이상의 함수의 곱에도 확장 적용할 수 있습니다.\n곱의 미분법칙은 합의 미분법칙과 함께 복잡한 함수의 미분을 가능하게 합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#합성함수의-미분",
    "href": "posts/md/How_ML4math.html#합성함수의-미분",
    "title": "머신러닝을 위한 수학",
    "section": "2.7 합성함수의 미분",
    "text": "2.7 합성함수의 미분\n관련 개념: 몫의 미분, 지수함수의 미분, 전미분과 합성함수\n합성함수의 미분, 즉 연쇄법칙 (chain rule) 은 신경망과 같은 복잡한 구조의 미분을 계산할 때 핵심적인 역할을 합니다.\n합성함수의 미분 정의: 두 함수 \\(f(x)\\) 와 \\(g(x)\\) 의 합성함수 \\(h(x) = f(g(x))\\) 에 대한 미분은 다음과 같이 정의됩니다: \\(h'(x) = f'(g(x)) * g'(x)\\) . 여기서 \\(h'(x)\\) 는 합성함수 \\(h(x)\\) 의 도함수, \\(f'(g(x))\\) 는 \\(f\\) 의 도함수를 \\(g(x)\\) 에 대해 계산한 것, \\(g'(x)\\) 는 \\(g(x)\\) 의 도함수입니다.\n연쇄법칙의 특징\n\n이 법칙은 ” 바깥 함수의 미분 ” 곱하기 ” 안쪽 함수의 미분 ” 으로 이해할 수 있습니다.\n연쇄법칙은 여러 함수가 중첩된 복잡한 함수의 미분을 가능하게 합니다.\n이 법칙은 다변수 함수의 편미분에도 확장 적용될 수 있습니다.\n연쇄법칙은 역함수의 미분을 구할 때도 유용합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#몫의-미분",
    "href": "posts/md/How_ML4math.html#몫의-미분",
    "title": "머신러닝을 위한 수학",
    "section": "2.8 몫의 미분",
    "text": "2.8 몫의 미분\n관련 개념: 소프트맥스 함수\n몫의 미분법칙 (quotient rule) 은 두 함수의 몫을 미분할 때 사용하는 규칙입니다. 두 함수 \\(u(x)\\)$와 \\(v(x)\\)$ 의 몫 \\(f(x) = \\frac{u(x)}{v(x)}\\) 에 대한 미분은 다음과 같이 정의됩니다:\n\\[\nf'(x) = \\frac{u'(x)v(x) - u(x)v'(x)}{[v(x)]^2}\n\\]\n여기서\n\n\\(f'(x)\\) 는 \\(f(x)\\) 의 도함수입니다.\n\\(u'(x)\\) 는 \\(u(x)\\) 의 도함수입니다.\n\\(v'(x)\\) 는 \\(v(x)\\) 의 도함수입니다.\n\n몫의 미분법칙은 복잡한 함수의 미분을 단순화하고, 다양한 수학적 문제를 해결하는 데 중요한 도구입니다. 이 법칙의 특징은 다음과 같습니다:\n\n분자와 분모의 역할: 분자의 미분에 분모를 곱하고, 분모의 미분에 분자를 곱한 후, 이 두 값을 뺍니다.\n분모의 제곱: 결과적으로 분모의 제곱이 분모로 들어갑니다.\n적용 조건: 분모 \\(v(x)\\) 는 0 이 아니어야 합니다. 이는 함수가 정의되지 않는 점을 피하기 위함입니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#적분",
    "href": "posts/md/How_ML4math.html#적분",
    "title": "머신러닝을 위한 수학",
    "section": "2.9 적분",
    "text": "2.9 적분\n관련 개념: 확률밀도함수와 확률분포함수\n적분은 고대 그리스의 아르키메데스가 처음 아이디어를 제시했으며, 이후 앙리 르베그와 같은 수학자들에 의해 이론적으로 완성되었습니다. 적분은 물리학, 공학, 경제학 등 다양한 분야에서 널리 사용되며, 예를 들어 곡면으로 이루어진 수영장을 채우기 위해 필요한 물의 양을 계산하는 데 활용될 수 있습니다.7\n적분은 크게 두 가지로 나뉩니다: 부정적분과 정적분입니다. 부정적분은 함수의 원시함수를 찾는 과정이며, 정적분은 특정 구간에서의 함수의 값을 계산하는 과정입니다.8 적분의 주요 특징과 의미는 다음과 같습니다:\n\n정의: 적분은 함수의 그래프 아래 면적을 계산하는 방법입니다.\n종류:\n\n부정적분: 함수의 원시함수를 찾는 과정\n정적분: 특정 구간에서의 함수의 값을 계산하는 과정 9\n\n정적분의 개념: 주어진 함수의 구간을 아주 작은 직사각형들로 나누어 그 넓이의 합을 구하는 방식입니다.\n리만 적분: 가장 일반적으로 사용되는 적분 형식 중 하나로, 함수를 잘게 나누어 근사값을 구하는 방식입니다.\n미적분학의 기본 정리: 미분과 적분이 서로 역연산 관계에 있음을 보여주는 중요한 정리입니다.10\n응용: 적분은 물리학, 공학, 경제학 등 다양한 분야에서 사용되며, 면적, 부피, 일, 에너지 등 다양한 물리량을 계산하는 데 활용됩니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#벡터",
    "href": "posts/md/How_ML4math.html#벡터",
    "title": "머신러닝을 위한 수학",
    "section": "3.1 벡터",
    "text": "3.1 벡터\n관련개념: 덧셈, 뺄셈, 스칼라 곱셈, 편미분\n벡터는 수학과 물리학에서 크기와 방향을 동시에 가지는 양을 나타내는 개념입니다. 벡터는 일반적으로 화살표로 표현되며, 시작점과 끝점을 통해 방향과 크기를 나타냅니다. 주요 특징은 다음과 같습니다:\n\n크기와 방향: 벡터는 크기 (길이) 와 방향을 가지며, 이 두 가지 요소로 정의됩니다. 크기는 벡터의 길이를 나타내고, 방향은 벡터가 가리키는 방향을 나타냅니다.\n자유벡터: 벡터는 시점 (시작점) 이 자유로울 수 있으며, 같은 크기와 방향을 가지면 동일한 벡터로 간주됩니다. 이러한 벡터를 자유벡터라고 합니다.\n벡터의 연산: 벡터는 덧셈, 뺄셈, 스칼라 곱 등의 연산이 가능합니다. 이러한 연산을 통해 벡터의 크기와 방향을 조작할 수 있습니다.11\n벡터 공간: 벡터는 벡터 공간의 원소로 간주되며, 벡터 공간은 벡터의 선형 결합으로 구성됩니다. 벡터 공간은 수학적 구조를 정의하는 데 중요한 역할을 합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#덧셈-뺄셈-스칼라-곱셈",
    "href": "posts/md/How_ML4math.html#덧셈-뺄셈-스칼라-곱셈",
    "title": "머신러닝을 위한 수학",
    "section": "3.2 덧셈, 뺄셈, 스칼라 곱셈",
    "text": "3.2 덧셈, 뺄셈, 스칼라 곱셈\n관련개념: 길이와 거리\n벡터의 덧셈과 뺄셈은 벡터의 결합과 분리를 나타내며, 스칼라 곱셈은 벡터의 크기를 조절하는 데 사용됩니다.12 각 연산의 정의는 다음과 같습니다:\n벡터의 덧셈: 두 벡터 \\(\\mathbf{a}\\) 와 \\(\\mathbf{b}\\) 의 덧셈은 대응하는 성분끼리 더하는 것입니다. - 수식: \\(\\mathbf{a} + \\mathbf{b} = (a_1 + b_1, a_2 + b_2, \\ldots, a_n + b_n)\\) - 특징: 벡터 덧셈은 교환법칙과 결합법칙이 성립합니다.\n벡터의 뺄셈: 두 벡터 \\(\\mathbf{a}\\) 와 \\(\\mathbf{b}\\) 의 뺄셈은 대응하는 성분끼리 빼는 것입니다. - 수식: \\(\\mathbf{a} - \\mathbf{b} = (a_1 - b_1, a_2 - b_2, \\ldots, a_n - b_n)\\) - 특징: 벡터 뺄셈은 벡터 덧셈과 유사하게 처리되며, 벡터의 방향을 반대로 하는 것과 같습니다.\n스칼라 곱셈: 벡터 \\(\\mathbf{a}\\) 에 스칼라 \\(c\\) 를 곱하는 것은 벡터의 각 성분에 \\(c\\) 를 곱하는 것입니다.\n\n수식: \\(c\\mathbf{a} = (ca_1, ca_2, \\ldots, ca_n)\\)\n특징: 스칼라 곱셈은 벡터의 크기를 변화시키며, 방향은 변하지 않습니다. 스칼라가 음수일 경우 벡터의 방향이 반대가 됩니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#길이와-거리",
    "href": "posts/md/How_ML4math.html#길이와-거리",
    "title": "머신러닝을 위한 수학",
    "section": "3.3 길이와 거리",
    "text": "3.3 길이와 거리\n관련개념: 내적, 코사인 유사도\n벡터의 길이: 벡터의 길이 (또는 크기) 는 벡터가 나타내는 화살표의 길이를 의미하며, 이는 벡터의 각 성분의 제곱의 합의 제곱근으로 계산됩니다.\n\n수식: 벡터 \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) 의 길이는 다음과 같이 정의됩니다: \\[\n\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}\n\\]\n특징: 벡터의 길이는 항상 0 이상의 값을 가지며, 벡터가 원점에서 얼마나 떨어져 있는지를 나타냅니다.\n\n벡터 간의 거리: 두 벡터 사이의 거리는 두 벡터를 연결하는 선분의 길이를 의미합니다. - 수식: 벡터 \\(\\mathbf{u} = (u_1, u_2, \\ldots, u_n)\\) 와 \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) 사이의 거리는 다음과 같이 정의됩니다: \\[\n  \\text{거리}(\\mathbf{u}, \\mathbf{v}) = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + \\ldots + (u_n - v_n)^2}\n  \\] - 특징: 이 거리는 유클리드 거리로, 두 점 사이의 직선 거리를 측정하며, 항상 0 이상의 값을 가집니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#삼각함수",
    "href": "posts/md/How_ML4math.html#삼각함수",
    "title": "머신러닝을 위한 수학",
    "section": "3.4 삼각함수",
    "text": "3.4 삼각함수\n관련개념: 내적\n삼각함수는 삼각형의 각과 변의 관계를 나타내는 함수로, 주로 직각삼각형에서 정의됩니다.13 삼각함수의 주요 정의는 다음과 같습니다:\n\n사인 (Sine, \\(\\sin\\)): 직각삼각형에서 한 각의 사인은 그 각의 대변의 길이를 빗변의 길이로 나눈 값입니다.\n코사인 (Cosine, \\(\\cos\\)): 직각삼각형에서 한 각의 코사인은 그 각의 인접변의 길이를 빗변의 길이로 나눈 값입니다.\n탄젠트 (Tangent, \\(\\tan\\)): 직각삼각형에서 한 각의 탄젠트는 그 각의 대변의 길이를 인접변의 길이로 나눈 값입니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#내적",
    "href": "posts/md/How_ML4math.html#내적",
    "title": "머신러닝을 위한 수학",
    "section": "3.5 내적",
    "text": "3.5 내적\n관련개념: 코사인 유사도, 행렬과 행렬 연산, 경사하강법\n내적은 두 벡터의 각 성분을 곱한 후 더하는 연산으로, 실수체에서는 이중선형 (bilinear) 함수로 정의됩니다. 이는 두 벡터의 방향이 얼마나 일치하는지를 나타냅니다.14 내적의 정의는 다음과 같습니다:\n\n수식: 두 벡터 \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_n)\\) 와 \\(\\mathbf{b} = (b_1, b_2, \\ldots, b_n)\\) 의 내적은 다음과 같이 계산됩니다: \\[\n\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\ldots + a_nb_n\n\\]\n성질:\n\n교환법칙: \\(\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{b} \\cdot \\mathbf{a}\\)\n분배법칙: \\(\\mathbf{a} \\cdot (\\mathbf{b} + \\mathbf{c}) = \\mathbf{a} \\cdot \\mathbf{b} + \\mathbf{a} \\cdot \\mathbf{c}\\)\n스칼라곱과의 호환성: \\(c(\\mathbf{a} \\cdot \\mathbf{b}) = (c\\mathbf{a}) \\cdot \\mathbf{b} = \\mathbf{a} \\cdot (c\\mathbf{b})\\), 여기서 \\(c\\) 는 스칼라입니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#코사인-유사도",
    "href": "posts/md/How_ML4math.html#코사인-유사도",
    "title": "머신러닝을 위한 수학",
    "section": "3.6 코사인 유사도",
    "text": "3.6 코사인 유사도\n코사인 유사도 (Cosine Similarity) 는 두 벡터 간의 유사도를 측정하는 방법으로, 두 벡터 사이의 코사인 각도를 이용하여 유사도를 계산합니다. 코사인 유사도는 벡터의 방향에 기반하여 유사성을 측정하기 때문에, 크기에 영향을 받지 않고 패턴이나 방향의 유사성을 평가하는 데 적합합니다. 따라서 자연어 처리 (NLP) 에서 문서 간의 유사도를 측정하거나 추천 시스템에서 아이템 간의 유사도를 계산하는 데 널리 사용됩니다. 코사인 유사도는 다음과 같은 특징을 가지고 있습니다:\n\n정의: 코사인 유사도는 두 벡터가 이루는 각의 코사인을 계산하여 두 벡터의 방향이 얼마나 유사한지를 나타냅니다. 수식으로는 다음과 같이 표현됩니다: \\[\n\\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n\\] 여기서 \\(\\mathbf{A} \\cdot \\mathbf{B}\\) 는 두 벡터의 내적이고, \\(\\|\\mathbf{A}\\|\\) 와 \\(\\|\\mathbf{B}\\|\\) 는 각각 벡터의 크기 (길이) 입니다.\n특징:\n\n코사인 유사도는 -1 에서 1 사이의 값을 가지며, 1 에 가까울수록 두 벡터의 방향이 유사하다는 것을 의미합니다.\n벡터의 크기보다는 방향에 초점을 맞추기 때문에, 문서의 길이가 다르거나 크기가 다른 벡터를 비교할 때 유용합니다 15"
  },
  {
    "objectID": "posts/md/How_ML4math.html#행렬과-행렬-연산",
    "href": "posts/md/How_ML4math.html#행렬과-행렬-연산",
    "title": "머신러닝을 위한 수학",
    "section": "3.7 행렬과 행렬 연산",
    "text": "3.7 행렬과 행렬 연산\n행렬은 수 또는 다항식 등을 직사각형 모양으로 배열한 것입니다. 행렬은 수학과 과학의 다양한 분야에서 사용되며, 특히 물리학, 컴퓨터 그래픽스, 확률론, 통계학 등에서 중요한 역할을 합니다. 행렬에 대한 주요 연산은 다음과 같습니다:\n\n덧셈: 같은 크기의 두 행렬에 대해서만 정의되며, 대응하는 원소끼리 더합니다.\n스칼라배: 행렬의 각 원소에 스칼라를 곱하는 연산입니다.\n곱셈: 첫 번째 행렬의 열의 수와 두 번째 행렬의 행의 수가 같을 때 정의됩니다. 결과 행렬의 각 원소는 첫 번째 행렬의 행과 두 번째 행렬의 열의 원소를 곱한 후 합한 값입니다.\n전치 행렬: 행과 열을 교환한 행렬입니다. 즉, 원래 행렬의 \\(i\\)- 행, \\(j\\)- 열의 원소가 전치 행렬에서는 \\(j\\)- 행, \\(i\\)- 열의 원소가 됩니다.16\n역행렬: 정방행렬 (정사각 행렬) 에 대해서만 정의되며, 행렬 곱셈의 역원 역할을 합니다. 모든 정방행렬이 역행렬을 가지는 것은 아닙니다.\n행렬식: 정방행렬에 대해 정의되는 값으로, 행렬의 특성을 나타냅니다. 기하학적으로는 행렬이 나타내는 변환의 부피 스케일링을 의미합니다.17"
  },
  {
    "objectID": "posts/md/How_ML4math.html#다변수-함수",
    "href": "posts/md/How_ML4math.html#다변수-함수",
    "title": "머신러닝을 위한 수학",
    "section": "4.1 다변수 함수",
    "text": "4.1 다변수 함수\n관련개념: 함수, 편미분\n다변수 함수는 두 개 이상의 독립변수를 가지는 함수를 의미합니다. 단변수 함수의 개념을 고차원으로 확장한 것으로 더 복잡한 현상을 수학적으로 표현할 때 사용합니다. 주요 특징과 정의는 다음과 같습니다:\n\n정의: 다변수 함수는 여러 개의 입력 변수를 받아 하나의 출력값을 반환하는 함수입니다. 일반적으로 다음과 같이 표현됩니다: \\(f(x_1, x_2, ..., x_n) = y\\) 여기서 \\(x_1, x_2, ..., x_n\\) 은 독립변수이고, \\(y\\) 는 종속변수입니다.\n정의역: 다변수 함수의 정의역은 n 차원 공간의 부분집합입니다. 예를 들어, 2 변수 함수의 경우 정의역은 2 차원 평면의 일부가 됩니다.\n치역: 함수의 출력값들의 집합으로 일반적으로 실수 집합의 부분집합입니다.\n그래프: 다변수 함수의 그래프는 \\(n+1\\) 차원 공간에 존재합니다. 예를 들어, 2 변수 함수의 그래프는 3 차원 공간에 표현됩니다.\n연속성과 미분가능성: 단변수 함수와 마찬가지로 다변수 함수도 연속성과 미분가능성을 가질 수 있습니다. 다만, 이를 판단하는 기준이 더 복잡해집니다.\n편미분: 다변수 함수에서는 각 변수에 대한 편미분을 고려해야 합니다. 이는 다른 변수들을 상수로 취급하고 한 변수에 대해서만 미분하는 것을 의미합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#편미분",
    "href": "posts/md/How_ML4math.html#편미분",
    "title": "머신러닝을 위한 수학",
    "section": "4.2 편미분",
    "text": "4.2 편미분\n관련개념: 극한과 미분, 벡터, 전미분, 전미분과 함성함수\n편미분은 다변수 함수에서 특정 변수에 대해 미분하는 것을 의미합니다. 편미분의 주요 특징과 정의는 다음과 같습니다:\n\n정의: 다변수 함수 \\(f(x, y, z, ...)\\) 에서 한 변수에 대해서만 미분하고 나머지 변수들은 상수로 취급하여 미분하는 것입니다.\n표기법: 편미분은 일반적으로 \\(∂\\) 기호를 사용하여 표현합니다. 예를 들어, \\(f(x,y)\\) 의 \\(x\\) 에 대한 편미분은 \\(∂f/∂x\\) 로 표기합니다.\n계산 방법: 편미분을 계산할 때는 미분하려는 변수 이외의 모든 변수를 상수로 취급하고 일반적인 미분 규칙을 적용합니다.\n기하학적 의미: 편미분은 다변수 함수의 그래프에서 특정 방향으로의 기울기를 나타냅니다.\n전미분과의 관계: 모든 변수에 대한 편미분을 종합하여 전미분을 구할 수 있습니다.\n\n편미분은 복잡한 다변수 함수의 동작을 이해하고 분석하는 데 중요한 도구로, 현대 과학과 공학의 여러 분야에서 핵심적인 역할을 합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#전미분",
    "href": "posts/md/How_ML4math.html#전미분",
    "title": "머신러닝을 위한 수학",
    "section": "4.3 전미분",
    "text": "4.3 전미분\n관련개념: 편미분, 극한과 미분, 전미분과 합성함수, 경사하강법\n전미분은 다변수 함수의 변화량을 특정 지점에서 근사화하는 방법으로, 각 변수의 변화에 따른 함수 값의 총 변화를 나타냅니다. 전미분은 다변수 함수의 변화를 이해하고 분석하는 데 중요한 도구로, 특히 복잡한 시스템의 근사해를 구하는 데 유용합니다.18 전미분은 다음과 같이 정의할 수 있습니다:\n\n정의: 다변수 함수 \\(f(x_1, x_2, \\ldots, x_n)\\) 의 전미분은 각 변수의 작은 변화에 대한 함수의 총 변화량을 근사화하는 선형 표현입니다. 이는 함수의 변화량을 각 변수의 변화량에 비례하여 나타냅니다.\n수식: 함수 \\(f(x, y)\\) 의 전미분은 다음과 같이 표현됩니다: \\[\ndf = \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy\n\\] 여기서 \\(\\frac{\\partial f}{\\partial x}\\) 와 \\(\\frac{\\partial f}{\\partial y}\\) 는 각각 \\(x\\) 와 \\(y\\) 에 대한 편미분이며, \\(dx\\) 와 \\(dy\\) 는 \\(x\\) 와 \\(y\\) 의 작은 변화량입니다.\n기하학적 의미: 전미분은 함수의 그래프에서 접평면의 방정식을 구성하며, 이는 함수의 변화율을 각 변수 방향으로 나타내는 벡터의 합으로 이해할 수 있습니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#전미분과-합성함수",
    "href": "posts/md/How_ML4math.html#전미분과-합성함수",
    "title": "머신러닝을 위한 수학",
    "section": "4.4 전미분과 합성함수",
    "text": "4.4 전미분과 합성함수\n관련개념: 편미분, 전미분, 합성함수의 미분\n전미분과 합성함수는 모두 복잡한 함수의 변화를 분석하는 데 중요한 도구입니다. 전미분은 다변수 함수의 전체적인 변화를 이해하는 데 사용되며, 합성함수는 여러 단계의 변환을 거치는 함수를 다룰 때 유용합니다.\n전미분: 전미분은 다변수 함수의 모든 변수에 대한 변화를 동시에 고려한 미분입니다.\n\n다변수 함수 \\(f(x, y, ...)\\) 의 전미분은 각 변수의 편미분을 모두 더한 형태로 표현됩니다.\n예를 들어, 2 변수 함수 \\(f(x, y)\\) 의 전미분은 다음과 같이 정의됩니다: \\[df = (∂f/∂x)dx + (∂f/∂y)dy\\]\n여기서 \\(∂f/∂x\\) 와 \\(∂f/∂y\\) 는 각각 \\(x\\) 와 \\(y\\) 에 대한 편미분이며, \\(dx\\) 와 \\(dy\\) 는 \\(x\\) 와 \\(y\\) 의 미소 변화량을 나타냅니다.\n\n합성함수: 합성함수는 두 개 이상의 함수를 연속적으로 적용하여 만든 새로운 함수입니다.\n\n함수 \\(f\\) 와 \\(g\\) 가 있을 때, \\((g ∘ f)(x) = g(f(x))\\) 로 정의되는 함수를 \\(f\\) 와 \\(g\\) 의 합성함수라고 합니다.\n합성함수는 한 함수의 출력을 다른 함수의 입력으로 사용하는 것을 의미합니다.\n합성함수의 미분에는 연쇄법칙이 적용됩니다. 즉, \\((g ∘ f)'(x) = g'(f(x)) · f'(x)\\) 입니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#손실함수",
    "href": "posts/md/How_ML4math.html#손실함수",
    "title": "머신러닝을 위한 수학",
    "section": "4.5 손실함수",
    "text": "4.5 손실함수\n관련개념: 미분과 적분, 편미분, 가능도함수와 최대가능도 추정, 경사하강법\n손실함수는 모델이 예측한 값과 실제 정답 사이의 차이를 수치화하는 함수입니다. 이는 모델이 얼마나 잘못 예측하고 있는지를 측정하는 도구로, 학습 과정에서 모델의 성능을 개선하는 데 사용됩니다.\n\n4.5.1 역할\n\n성능 평가: 모델의 예측 정확도를 평가합니다.\n학습 방향 제시: 손실값을 최소화하는 방향으로 모델을 조정합니다.\n최적화 기준: 모델 파라미터를 조정하는 기준이 됩니다.\n\n\n\n4.5.2 주요 손실함수 종류\n회귀 문제에서의 손실함수:\n\nMSE (Mean Squared Error): 가장 기본적인 손실함수, 예측값과 실제값의 차이를 제곱하여 평균을 냄, 이상치에 민감할 수 있음.\nMAE (Mean Absolute Error): 예측값과 실제값의 절대 차이의 평균, MSE 에 비해 이상치에 덜 민감함.\nRMSE (Root Mean Squared Error): MSE 에 제곱근을 취한 값\n\n분류 문제에서의 손실함수:\n\n엔트로피 개념을 활용한 손실함수를 주로 사용합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#경사하강법",
    "href": "posts/md/How_ML4math.html#경사하강법",
    "title": "머신러닝을 위한 수학",
    "section": "4.6 경사하강법",
    "text": "4.6 경사하강법\n관련개념: 극대와 극소, 내적\n경사하강법 (Gradient Descent) 은 주어진 함수의 최소값을 찾기 위한 최적화 알고리즘입니다. 이 방법은 함수의 기울기 (gradient) 를 이용하여 함수의 값을 점진적으로 줄여나가는 방식으로 작동합니다. 경사하강법은 함수의 최적화 문제를 해결하는 데 중요한 도구로, 특히 기계 학습에서 모델의 학습과 최적화에 사용됩니다 .경사하강법의 주요 특징은 다음과 같습니다:\n\n목적: 경사하강법은 함수의 최소값을 찾는 것을 목표로 합니다. 이는 손실 함수 (loss function) 를 최소화하는 데 사용되며, 특히 기계 학습에서 모델의 최적화에 널리 활용됩니다.\n작동 원리:\n\n함수의 현재 위치에서의 기울기를 계산합니다.\n기울기의 반대 방향으로 일정한 크기 (step size 또는 learning rate) 만큼 이동합니다.\n이 과정을 반복하여 함수의 값을 점진적으로 줄입니다.\n\n수식:\n\n변수 \\(\\theta\\) 에 대해 경사하강법의 업데이트 규칙은 다음과 같이 표현됩니다: \\[\n\\theta := \\theta - \\alpha \\nabla f(\\theta)\n\\] 여기서 \\(\\alpha\\) 는 학습률 (learning rate), \\(\\nabla f(\\theta)\\) 는 함수 \\(f\\) 의 \\(\\theta\\) 에서의 기울기입니다.\n\n변형: 경사하강법에는 여러 변형이 존재합니다. 예를 들어, 확률적 경사하강법 (Stochastic Gradient Descent, SGD) 은 데이터의 전체 집합 대신 무작위로 선택된 일부 데이터를 사용하여 기울기를 계산합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#지수함수",
    "href": "posts/md/How_ML4math.html#지수함수",
    "title": "머신러닝을 위한 수학",
    "section": "5.1 지수함수",
    "text": "5.1 지수함수\n관련개념: 로그함수, 시그모이드 함수, 소프트맥스 함수, 확률밀도함수와 확률분포함수\n지수함수는 일반적으로 \\(f(x) = a^x\\) 형태로 표현되며, 여기서 \\(a\\) 는 양의 상수 (기저, base) 이고, \\(x\\) 는 지수입니다. 가장 흔히 사용되는 기저는 자연상수 \\(e\\) 로, 이 경우의 지수함수는 \\(f(x) = e^x\\) 입니다.\n\n특징:\n\n연속성과 미분 가능성: 지수함수는 모든 실수 \\(x\\) 에 대해 연속적이며 미분 가능합니다.\n증가 또는 감소: 기저 \\(a\\) 가 1 보다 크면 함수는 증가하고, 0 보다 크고 1 보다 작으면 함수는 감소합니다.\n고유한 미분 특성: 특히 \\(f(x) = e^x\\) 의 경우, 미분해도 자기 자신이 되는 고유한 성질을 가집니다. 즉, \\(\\frac{d}{dx} e^x = e^x\\)."
  },
  {
    "objectID": "posts/md/How_ML4math.html#로그함수",
    "href": "posts/md/How_ML4math.html#로그함수",
    "title": "머신러닝을 위한 수학",
    "section": "5.2 로그함수",
    "text": "5.2 로그함수\n관련개념: 합성함수와 역함수, 로그함수와 미분, 가능도함수와 최대가능도 추정\n로그함수는 지수함수의 역함수로 정의되며, 다음과 같은 특징을 가집니다:\n\n정의: 로그함수는 일반적으로 \\(y = \\log_a x\\) 형태로 표현됩니다. 여기서 \\(a\\) 는 로그의 밑 (base) 이며, \\(x\\) 는 진수입니다. 이는 \\(a^y = x\\) 와 동일한 의미를 가집니다.\n주요 성질:\n\n로그함수는 양수에 대해서만 정의됩니다. 즉, \\(x &gt; 0\\) 일 때만 정의됩니다.\n\\(\\log_a 1 = 0\\) (모든 양의 실수 \\(a\\) 에 대해)\n\\(\\log_a a = 1\\)\n로그함수는 단조 증가 함수입니다.\n\n\n\n\n\n\n\n\nNote\n\n\n\n 단조 증가 함수란 x 값이 증가할 때 y 값도 항상 증가하거나 같은 값을 유지하는 함수를 말합니다. 이 성질 때문에 로그함수는 넓은 범위의 값을 좁은 범위로 압축하는 데 유용하게 사용됩니다. 예를 들어, 지진의 규모나 소리의 강도를 표현하는 데 사용됩니다.\n\n\n\n특별한 경우:\n\n자연로그: \\(\\ln x = \\log_e x\\) (밑이 자연상수 \\(e\\) 인 경우)\n상용로그: \\(\\log x = \\log_{10} x\\) (밑이 10 인 경우)\n\n로그의 성질:\n\n곱의 로그: \\(\\log_a (xy) = \\log_a x + \\log_a y\\)\n몫의 로그: \\(\\log_a (x/y) = \\log_a x - \\log_a y\\)\n거듭제곱의 로그: \\(log_a (x^n) = n \\log_a x\\)"
  },
  {
    "objectID": "posts/md/How_ML4math.html#로그함수의-미분",
    "href": "posts/md/How_ML4math.html#로그함수의-미분",
    "title": "머신러닝을 위한 수학",
    "section": "5.3 로그함수의 미분",
    "text": "5.3 로그함수의 미분\n관련개념: 극한과 미분, 지수함수의 미분, 가능도함수와 최대가능도 추정\n로그함수의 미분은 다음과 같이 정의됩니다:\n\n자연로그 함수 \\(ln(x)\\) 의 미분: \\[ \\frac{d}{dx} \\ln(x) = \\frac{1}{x}, \\quad x &gt; 0 \\]\n일반적인 로그함수 \\(log_a(x)\\) 의 미분: \\[ \\frac{d}{dx} \\log_a(x) = \\frac{1}{x \\ln(a)}, \\quad x &gt; 0, a &gt; 0, a \\neq 1 \\]\n\n로그함수의 주요 특징은 다음과 같습니다:\n\n로그함수의 미분은 항상 함수 자체의 역수에 비례합니다.\n자연로그의 경우, 미분 결과가 단순히 \\(1/x\\) 로 나타납니다. 이는 자연로그가 미분 관점에서 가장 단순한 형태를 가진다는 것을 보여줍니다.\n일반 로그함수의 경우, 밑 (base) 에 따라 추가적인 상수 \\(1/ln(a)\\) 가 곱해집니다.\n로그함수의 미분은 \\(x &gt; 0\\) 인 양수 영역에서만 정의됩니다. 이는 로그함수 자체가 양수 영역에서만 정의되기 때문입니다.\n로그함수의 미분은 \\(x\\) 가 증가함에 따라 감소하는 형태를 보입니다. 이는 로그함수의 증가 속도가 \\(x\\) 가 커질수록 느려짐을 의미합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#지수함수의-미분",
    "href": "posts/md/How_ML4math.html#지수함수의-미분",
    "title": "머신러닝을 위한 수학",
    "section": "5.4 지수함수의 미분",
    "text": "5.4 지수함수의 미분\n관련개념: 합성함수의 미분, 시그모이드 함수\n지수함수의 미분은 다음과 같이 정의됩니다:\n\n자연지수 함수 \\(e^x\\) 의 미분: \\[ \\frac{d}{dx} e^x = e^x \\] 이는 자연지수 함수의 가장 중요한 특징 중 하나로, 미분해도 자기 자신이 됩니다.\n일반적인 지수함수 a^x 의 미분 (\\(a &gt; 0, a ≠ 1\\)): \\[ \\frac{d}{dx} a^x = a^x \\ln(a) \\]\n\n이 정의의 주요 특징은 다음과 같습니다:\n\n지수함수의 미분은 항상 자기 자신에 비례합니다. 이는 지수함수의 성장률이 함수값에 비례한다는 것을 의미합니다.\n자연지수 함수 \\(e^x\\) 의 경우, 미분 결과가 단순히 자기 자신이 됩니다. 이는 \\(e^x\\) 가 미분 관점에서 가장 단순한 형태를 가진다는 것을 보여줍니다.\n일반 지수함수의 경우, 밑 (base) 에 따라 추가적인 상수 \\(ln(a)\\) 가 곱해집니다.\n지수함수의 미분은 모든 실수에 대해 정의되며, 항상 양수입니다 (\\(a &gt; 0\\) 인 경우).\n지수함수의 미분은 \\(x\\) 가 증가함에 따라 지수적으로 증가하는 형태를 보입니다. 이는 지수함수의 성장 속도가 \\(x\\) 가 커질수록 빨라짐을 의미합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#시그모이드-함수",
    "href": "posts/md/How_ML4math.html#시그모이드-함수",
    "title": "머신러닝을 위한 수학",
    "section": "5.5 시그모이드 함수",
    "text": "5.5 시그모이드 함수\n관련개념: 다항식의 미분, 지수함수의 미분, 합성함수의 미분, 로그함수\n시그모이드 함수 (Sigmoid function) 는 S 자 모양의 곡선을 그리는 함수로, 주로 기계학습과 신경망에서 활성화 함수로 사용됩니다. 시그모이드 함수의 정의와 주요 특징은 다음과 같습니다:\n\n정의: 시그모이드 함수는 일반적으로 다음과 같이 정의됩니다: \\[ f(x) = \\frac{1}{1 + e^{-x}} \\] 여기서 \\(e\\) 는 자연상수 (약 2.71828) 입니다.\n특징:\n\n함수의 출력 범위는 0 에서 1 사이입니다.\n\\(x\\) 가 음의 무한대로 갈 때 함수값은 0 에 가까워지고, 양의 무한대로 갈 때 1 에 가까워집니다.\n\\(x = 0\\) 일 때 함수값은 0.5 입니다.\n함수는 연속적이며 모든 점에서 미분 가능합니다.\n\n미분: 시그모이드 함수의 미분은 다음과 같습니다: \\[ f'(x) = f(x)(1 - f(x)) \\] 이 특성은 신경망의 역전파 알고리즘에서 중요하게 사용됩니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#소프트맥스-함수",
    "href": "posts/md/How_ML4math.html#소프트맥스-함수",
    "title": "머신러닝을 위한 수학",
    "section": "5.6 소프트맥스 함수",
    "text": "5.6 소프트맥스 함수\n관련개념: 합성함수의 미분, 시그모이드함수, 몫의 미분, 편미분\n소프트맥스 함수는 다중 클래스 분류 문제에서 주로 사용되는 함수로, 주어진 입력 벡터를 확률 분포로 변환하는 역할을 합니다. 이 함수는 다음과 같은 특징과 정의를 가집니다:\n\n정의: 소프트맥스 함수는 \\(k\\) 차원의 입력 벡터 \\(z\\) 를 받아 각 클래스에 대한 확률을 계산합니다. 각 클래스 \\(i\\) 에 대한 확률 \\(p_i\\) 는 다음과 같이 정의됩니다:\n\\[ p_i = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} \\]\n여기서 \\(e\\) 는 자연상수이고, \\(z_i\\) 는 입력 벡터의 \\(i\\) 번째 요소입니다.\n특징:\n\n확률 분포: 소프트맥스 함수의 출력은 확률 분포를 이루며, 모든 출력 값의 합은 1 이 됩니다.\n분류 작업: 각 요소가 특정 클래스에 속할 확률을 나타내므로, 다중 클래스 분류 작업에 적합합니다.19\n\n응용:\n\n신경망의 출력층: 신경망에서 소프트맥스 함수는 출력층에 사용되어 각 클래스에 대한 확률을 계산합니다.\n역전파 가능: 미분 가능하기 때문에 신경망의 학습 과정에서 역전파 알고리즘을 통해 최적화할 수 있습니다.20"
  },
  {
    "objectID": "posts/md/How_ML4math.html#확률변수와-확률-분포",
    "href": "posts/md/How_ML4math.html#확률변수와-확률-분포",
    "title": "머신러닝을 위한 수학",
    "section": "6.1 확률변수와 확률 분포",
    "text": "6.1 확률변수와 확률 분포\n확률변수는 확률적인 결과에 따라 값이 바뀌는 변수를 의미합니다. 이는 확률실험의 가능한 결과에 수치적 값을 할당하는 함수로 이해할 수 있습니다. 확률변수는 다음과 같은 유형으로 나뉩니다:\n\n이산확률변수: 가능한 값이 유한하거나 셀 수 있는 경우. 예를 들어, 주사위를 던졌을 때 나오는 숫자.\n연속확률변수: 가능한 값이 연속적인 경우. 예를 들어, 특정 구간 내에서의 온도 측정값.\n\n확률변수는 측정 가능한 함수로, 확률공간에서 정의되며, 일반적으로 실수값을 가집니다.21\n확률 분포는 확률변수가 취할 수 있는 값과 그 값이 발생할 확률을 나타내는 함수입니다. 확률 분포는 확률변수의 유형에 따라 다음과 같이 구분됩니다:\n\n이산확률분포: 이산확률변수의 확률 분포로, 각 가능한 값에 대해 확률을 할당합니다. 예를 들어, 이항분포, 포아송분포 등이 있습니다.\n연속확률분포: 연속확률변수의 확률 분포로, 확률 밀도 함수를 통해 정의됩니다. 예를 들어, 정규분포, 지수분포 등이 있습니다.\n\n확률 분포는 확률변수의 특성을 설명하고, 데이터 분석 및 예측 모델링에 필수적인 역할을 합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#확률밀도함수와-확률분포함수",
    "href": "posts/md/How_ML4math.html#확률밀도함수와-확률분포함수",
    "title": "머신러닝을 위한 수학",
    "section": "6.2 확률밀도함수와 확률분포함수",
    "text": "6.2 확률밀도함수와 확률분포함수\n관련개념: 극한과 미분, 적분, 지수함수\n확률밀도함수 (Probability Density Function, PDF): 확률밀도함수는 연속 확률변수의 확률 분포를 나타내는 함수입니다. 특정 구간 내에서 확률변수가 취할 수 있는 값의 확률을 나타내며, 특정 값에서의 확률은 정의되지 않습니다.\n\n확률밀도함수의 적분은 확률을 나타내며, 전체 영역에 대한 적분은 1 이 됩니다.\n특정 구간 \\([a, b]\\) 에서 확률변수가 존재할 확률은 확률밀도함수를 해당 구간에서 적분한 값으로 계산됩니다.\n특정 값에서의 확률은 0 이지만, 구간을 통해 확률을 계산합니다.22\n\n확률분포함수 (Probability Distribution Function, CDF): 확률분포함수는 확률변수가 특정 값 이하일 확률을 나타내는 함수입니다. 이산 확률변수와 연속 확률변수 모두에 대해 정의됩니다.\n\n확률분포함수는 단조 증가 함수이며, 0 에서 1 사이의 값을 가집니다.\n연속 확률변수의 경우, 확률밀도함수의 적분으로 표현할 수 있습니다.\n이산 확률변수의 경우, 각 값의 확률을 누적하여 나타냅니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#가능도함수와-최대가능도-추정",
    "href": "posts/md/How_ML4math.html#가능도함수와-최대가능도-추정",
    "title": "머신러닝을 위한 수학",
    "section": "6.3 가능도함수와 최대가능도 추정",
    "text": "6.3 가능도함수와 최대가능도 추정\n관련개념: 극대와 극소, 로그함수, 로그함수의 미분, 확률밀도함수와 확률분포함수\n가능도 함수 (Likelihood Function): 가능도 함수는 주어진 데이터가 특정한 확률 분포를 따를 확률을 나타내는 함수입니다. 이는 주로 모수 추정에서 사용되며, 특정 모수 값이 주어졌을 때 관측된 데이터가 발생할 가능성을 계산합니다.\n\n표기: 일반적으로 가능도 함수는 \\(L(\\theta | x)\\) 로 표현되며, 여기서 \\(\\theta\\) 는 모수 (parameter), \\(x\\) 는 관측된 데이터입니다.\n특징: 가능도 함수는 확률 분포의 모수에 대한 함수로, 데이터가 주어졌을 때 모수가 어떤 값을 가질 가능성이 높은지를 평가합니다.\n\n최대가능도 추정 (Maximum Likelihood Estimation, MLE): 최대가능도 추정은 가능도 함수를 최대화하는 모수 값을 찾는 방법입니다. 즉, 주어진 데이터에 대해 가장 가능성이 높은 모수 값을 추정하는 방법입니다.\n\n방법: MLE 는 가능도 함수의 최대값을 찾는 최적화 문제로, 보통 미분을 통해 가능도 함수의 극대점을 찾습니다.\n응용: MLE 는 다양한 통계 모델에서 모수 추정에 널리 사용되며, 특히 회귀 분석, 분류 문제, 베이지안 통계 등에서 중요한 역할을 합니다."
  },
  {
    "objectID": "posts/md/How_ML4math.html#footnotes",
    "href": "posts/md/How_ML4math.html#footnotes",
    "title": "머신러닝을 위한 수학",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://thebook.io/007019/↩︎\nhttps://darkpgmr.tistory.com/45↩︎\nhttps://darkpgmr.tistory.com/45↩︎\nhttps://namu.wiki/w/%ED%95%A8%EC%88%98↩︎\nhttps://holymath.tistory.com/entry/%ED%95%A9%EC%84%B1%ED%95%A8%EC%88%98%EC%9D%98-%EA%B8%B0%EB%B3%B8%EC%84%B1%EC%A7%88↩︎\nhttps://namu.wiki/w/%ED%95%A8%EC%88%98↩︎\nhttps://ko.wikipedia.org/wiki/%EC%A0%81%EB%B6%84↩︎\nhttps://namu.wiki/w/%EC%A0%81%EB%B6%84↩︎\nhttps://unolab.tistory.com/entry/%EC%A0%81%EB%B6%84%EC%9D%98-%EC%A0%95%EC%9D%98-%EC%A0%95%EC%A0%81%EB%B6%84%EC%9D%98-%EC%A0%95%EC%9D%98↩︎\nhttps://www.youtube.com/watch?v=LEr4RdMxpLY↩︎\nhttps://www.youtube.com/watch?v=LsQoiETuBBc↩︎\nhttps://angeloyeo.github.io/2020/09/07/basic_vector_operation.html↩︎\nhttps://blog.naver.com/pss2072/220798287435↩︎\nhttps://namu.wiki/w/%EB%82%B4%EC%A0%81↩︎\nhttps://wikidocs.net/24603↩︎\nhttp://bigdata.dongguk.ac.kr/lectures/disc_math/_book/matrix.html↩︎\nhttps://minusi.tistory.com/entry/%ED%96%89%EB%A0%AC-%EB%8C%80%EC%88%98↩︎\nhttps://namu.wiki/w/%EB%AF%B8%EB%B6%84%ED%98%95%EC%8B%9D↩︎\nhttps://syj9700.tistory.com/38↩︎\nhttps://hyunhp.tistory.com/696↩︎\nhttps://www.datadata.link/qa66/↩︎\nhttps://www.youtube.com/watch?v=S1WIW662LJQ↩︎"
  },
  {
    "objectID": "posts/md/How_sabotage.html",
    "href": "posts/md/How_sabotage.html",
    "title": "사보타지 매뉴얼",
    "section": "",
    "text": "2008 년에 CIA 에서 공개한 Simple Sabotage Field Manual 의 번역본입니다. 문서가 1944 년도에 작성된 것이다보니 현재와는 맞지 않는 내용이 많습니다. 그래도 읽다보면 재치 있는 부분도 있고 읽어볼 가치가 있는 팁들인 것 같습니다. 특히 직장에 관련된 내용은 그때나 지금이나 별반 차이가 없는 것 처럼 보입니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#개인적-동기",
    "href": "posts/md/How_sabotage.html#개인적-동기",
    "title": "사보타지 매뉴얼",
    "section": "4.1 개인적 동기",
    "text": "4.1 개인적 동기\n\n일반 시민에게는 사보타지를 수행할 직접적이고 개인적인 동기가 없을 가능성이 높다. 그러니 적의 퇴각이나 통치 정부 조직의 붕괴 등을 통해 간접적인 개인적 이득을 기대하도록 해야 한다. 예를 들면 사보타지 행위는 X 위원장과 그의 대리인 Y 와 Z 가 쫓겨나는 날, 특히 불쾌한 법령과 제한이 폐지되는 날, 식량이 도착하는 날 등 해당 분야에 대한 이득을 가능한 한 구체적으로 명시한다. 개인의 자유, 언론의 자유 등에 대한 추상적인 표현은 전 세계 대부분의 지역에서 설득력을 얻지 못할 것이며 많은 지역의 시민들은 이해조차 할 수 없을 것이다.\n개별적인 행동은 그 효과가 제한적이기 때문에 사보타지 행위자가 자신이 조직의 일원이라는 느낌을 갖지 못하면 용기를 잃게 될 수 있다. 이런 감정은 특정 사보타지가 성공했다는 소식을 듣거나 읽음으로써 간접적으로 얻을 수도 있다. 비록 그 사보타지가 자신의 주변에서는 적용되지 않을지라도 다른 사람의 성공은 유사한 행동을 시도할 용기를 줄 수 있기 때문이다. 이는 사보타지의 효과를 칭송하는 메시지를 방송함으로써 직접적으로 전달될 수도 있다. 따라서 성공적인 사보타지는 참여하는 인구 비율에 대한 추정치가 방송으로 널리 퍼지고 지속적으로 행위자가 늘어나는 것이다.\n\n나 (b) 보다 더 중요한 것은 시민 사보타지 행위자가 책임감을 느끼고 다른 사람에게 사보타지를 교육하기 시작하는 상황을 만드는 것이다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#파괴-장려",
    "href": "posts/md/How_sabotage.html#파괴-장려",
    "title": "사보타지 매뉴얼",
    "section": "4.2 파괴 장려",
    "text": "4.2 파괴 장려\n상황이 적절한 경우 방해 행위자에게 그가 적에 대한 정당방위 또는 다른 파괴 행위에 대한 보복으로 행동하고 있음을 강조해야 한다. 사보타지에 대한 제안을 제시할 때 유머를 적절히 섞으면 공포의 긴장을 완화할 수 있다.\n\n사보타지 행위자는 생각을 바꿔 이전에는 도구를 날카롭게 유지했다면 이제는 무뎌지게 내버려 두어야 한다. 이전에는 윤활유를 바른 표면에 이제 샌딩을 해야 하며 평소 부지런했다면 이제 게으르고 부주의하게 행동해야 한다. 자신과 일상 생활의 사물에 대해 거꾸로 생각하도록 격려하면 행위자는 가까운 환경에서 많은 사보타지 기회를 보게 될 것이다.다시 말해 무엇이든 방해할 수 있다는 마음가짐을 갖도록 격려해야 한다.\n물리적 파괴에 가담할 수 있는 잠재적 시민 사보타지 행위자들 중에는 두 가지 극단적인 유형이 있다. 첫번째는 기술적으로 훈련받지 않고 고용된 사람이다. 이런 사람들은 자신이 파괴할 수 있는 대상과 파괴해야 하는 대상에 대한 구체적인 정보가 필요하다.\n다른 극단적인 유형에는 선반 작업자나 자동차 정비공과 같은 기술자들이 있다. 이 사람들은 구체적인 정보 없이도 시설에 적합한 사보타지 방법을 고안해낼 수 있다.\n사보타지에 관한 제안과 정보를 전파하기 위해 다양한 매체가 사용될 수 있다. 즉각적인 상황에 따라 사용할 수 있는 매체로는 방송국이나 라디오 방송 또는 전단지가 있으며 특정 지역 또는 직업군을 대상으로 할 수 있다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#안전-조치",
    "href": "posts/md/How_sabotage.html#안전-조치",
    "title": "사보타지 매뉴얼",
    "section": "4.3 안전 조치",
    "text": "4.3 안전 조치\n\n사보타지 행위자의 활동량은 그가 가진 기회뿐만 아니라 그가 느끼는 위험의 정도에 따라 결정된다. 사보타지 행위자가 너무 많이 체포되면 나쁜 소식은 빠르게 전파되어 사보타지 행위가 억제된다.\n사보타지 행위자가 탐지 및 보복을 당하지 않도록 무기, 시간, 표적 선택에 대한 준비하는 것은 어렵지 않다. 다음과 같은 예시가 있다:\n\n\n\n무해한 것으로 보이는 도구를 사용하라. 칼이나 톱은 다용도 도구이므로 쉽게 휴대할 수 있다. 성냥, 자갈, 머리카락, 소금, 못 및 기타 수십 가지의 파괴 도구들은 의심을 받지 않고 휴대하거나 거주지에 보관할 수 있다. 또한 특정 업종에서 일하는 하는 경우 렌치, 망치등을 쉽게 휴대하고 보관할 수 있습을 것이다.\n\n\n책임을 회피 할 수 있는 행위를 시도하라. 예를 들어 공장의 중앙 화재 상자에 있는 배선을 끊는 행위는 실수로 그럴 수 있기 때문에 책임을 회피 할 수 있다. 또는 다른 사람들의 눈을 피해 군용 차량이나 트럭을 대상으로 저지르는 사보타지 행위도 있다.\n\n\n전날 잠을 설쳐서 너무 졸려 전기 회로에 렌치를 떨어뜨렸다 같은 그럴듯한 핑계가 있다면 비난받을 수 있는 행동을 하는 것도 두려워하지 마라. 항상 사과를 충분히 하고, 어리석음, 무지를 가장해 그러한 사보타지를 ‘빠져나갈’ 수 있는 경우가 많다.\n\n\n사보타지를 저지른 후에 무슨 일이 일어나는지 확인하고 싶은 유혹을 뿌리쳐야 한다. 괜히 주위를 배회하면 의심을 불러온다. 물론 바로 퇴근하는 것이 의심스러운 상황도 있을 수 있으니 업무방해 행위를 저지른 경우라면 얼마간 직장에 남아 있어야 한다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#평상시",
    "href": "posts/md/How_sabotage.html#평상시",
    "title": "사보타지 매뉴얼",
    "section": "5.1 평상시",
    "text": "5.1 평상시\n\n단순한 사보타지는 악의적인 장난 그 이상이며, 항상 적의 물자와 인력에 해를 끼칠 수 있는 행위로 구성되어야 합니다.\n사보타지는 일상적인 장비를 독창적으로 사용해야 합니다. 주변을 다른 시각으로 바라보면 온갖 종류의 무기가 나타날 것입니다.\n사보타지는 자신의 능력이나 도구의 용량을 넘어서는 목표물을 공격해서는 안 됩니다. 예를 들어 경험이 없는 사람은 폭발물을 사용하려고 시도해서는 안 되며 성냥이나 기타 익숙한 도구를 사용해야 합니다.\n사보타지는 적이 사용 중이거나 적이 조기에 사용할 예정인 것으로 알려진 물체와 재료만 손상하려고 시도해야 합니다. 중공업의 거의 모든 제품 그리고 연료와 윤활유는 적이 사용할 것이라고 가정할 수 있으나 특별한 지식이 없는 사람이 식량 작물이나 식료품의 파괴 행위를 시도하는 것은 바람직하지 않습니다.\n시민 사보타지 행위자는 군용 물자에 접근할 수 있는 경우가 드물지만 이러한 물자를 우선적으로 파괴하는 것을 고려해야 합니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#전시전",
    "href": "posts/md/How_sabotage.html#전시전",
    "title": "사보타지 매뉴얼",
    "section": "5.2 전시전",
    "text": "5.2 전시전\n군사적 의미에서 평온한 시기에는 적에게 자재와 장비의 흐름을 줄이기 위해 산업 생산 공정의 사보타지에 집중합니다. 예를 들면 군용 트럭의 고무 타이어를 찢는 것은 가치 있는 행위보다 생산 공장의 타이어를 망치는 것은 훨씬 더 가치 있는 행위입니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#전시중",
    "href": "posts/md/How_sabotage.html#전시중",
    "title": "사보타지 매뉴얼",
    "section": "5.3 전시중",
    "text": "5.3 전시중\n\n전투 작전 지역이거나 곧 전투 작전 지역이 될 예정인 지역에 대한 가장 중대한 사보타지는 그 효과가 직접적이고 즉각적인 것입니다. 비록 그 영향이 상대적으로 경미하고 국지적이더라도 이런 유형의 사보타지는 그 영향이 광범위하지만 간접적이고 지연되는 활동보다 선호되어야 합니다.\n\n\n\n사보타지는 모든 종류의 교통 시설을 공격하도록 장려해야 합니다. 이러한 시설에는 도로, 철도, 자동차, 트럭, 오토바이, 자전거, 기차, 트램 등이 포함됩니다.\n\n\n당국이 지시나 사기 진작 자료를 전송하는 데 사용할 수 있는 모든 통신 시설은 사보타지의 대상이 되어야 합니다. 여기에는 전화, 전신 및 전력 시스템, 라디오, 신문, 현수막이 포함됩니다.\n\n\n그 자체로 가치가 있거나 교통 및 통신의 효율적인 기능에 필요한 중요 물자도 사보타지의 표적이 되어야 합니다. 여기에는 석유, 휘발유, 타이어, 음식, 물 등이 포함될 수 있습니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#건물",
    "href": "posts/md/How_sabotage.html#건물",
    "title": "사보타지 매뉴얼",
    "section": "6.1 건물",
    "text": "6.1 건물\n창고, 병영, 사무실, 호텔 및 공장 건물은 사보타지의 탁월한 목표입니다. 이들은 특히 화재 피해에 극히 취약하며 관리인, 청소부 및 일반 방문객과 같은 훈련 받지 않은 사람도 시도가 가능합니다. 또, 피해를 입었을 시에는 적에게 상대적으로 큰 피해를 주게 됩니다.\n\n6.1.1 화재\n화재는 가연성 물질이 쌓여있는 곳 어디에서든지 시작될 수 있습니다. 창고가 명백히 좋은 목표이겠으나 화재 사보타지가 거기에만 국한될 필요는 없을 것입니다.\n\n가능하면 장소를 떠난 상태에서 불이 시작하도록 준비하세요. 예를 들면 양초와 종이를 함께 사용하여 태우려는 가연성 물질에 최대한 가깝게 놓습니다: 종이에서 3~4 센티미터 너비의 띠를 찢어 양초 바닥에 두세 번 감습니다. 종이를 더 꼬아서 느슨한 끈으로 만들어 양초 바닥에 감습니다. 양초 불꽃이 감싸고 있는 띠에 도달하면 양초에 불이 붙고 주변 종이에 차례로 불이 붙을 겁니다.\n이런 종류의 방화는 인화성 물질에 적합합니다. 인화성이 없는 물질에 불을 붙이려고 시도하지 마세요. 만약 인화성이 없는 물질에 불을 붙이려면 양초와 휘발유에 적신 종이를 단단히 말거나 꼬아서 사용하세요.\n간이 도화선을 만들려면 끈의 한쪽 끝을 기름에 담급니다. 기름기가 묻은 끈과 깨끗한 끈이 만나는 부분에 화약을 넉넉히 묻혀서 문지릅니다. 그런 다음 깨끗한 끈의 끝부분에 불을 붙입니다. 기름과 화약이 닿을 때까지 불꽃 없이 천천히 타다가 (담배가 타는 것과 거의 같은 방식) 갑자기 타오를 것입니다. 그러면 그리스로 처리된 끈이 불꽃과 함께 연소됩니다. 그리스와 화약 대신 성냥을 사용해도 같은 효과를 얻을 수 있습니다. 끈이 눌리거나 매듭이 생기지 않도록 주의하면서 성냥 머리 위로 끈을 통과시키세요. 이 역시 갑작스러운 불꽃을 일으킬 수 있습니다. 이 유형의 도화선은 끈이 정해진 속도로 연소한다는 장점이 있습니다. 선택한 끈의 길이와 두께에 따라 불의 시간을 정할 수 있습니다.\n위에서 제안한 도화선을 사용하여 근무 시간 이후에 사무실에서 불을 피우세요. 기록 및 기타 유형의 문서를 파괴하는 것은 적에게 심각한 손해를 끼칠 수 있습니다.\n폐기물을 보관하는 지하실에 기름기가 많은 폐기물을 쌓아두는 경우 때때로 자연 발화하기도 하지만 담배나 성냥으로 쉽게 불이 붙을 수 있습니다. 당신이 야간 근무를 하는 청소부라면 화재를 가장 먼저 신고할 수도 있지만 너무 빨리 신고하지는 말아야합니다.\n깨끗한 공장은 화재에 취약하지 않지만 더러운 공장은 화재에 취약합니다. 작업자는 쓰레기를 함부로 버리고 청소부는 청소에 똑바로 하지 않음으로 더럽게 할 수 있습니다. 만약 먼지와 쓰레기가 충분히 쌓이면 내화 건물이 인화성이 될 수 있습니다.\n야간에 비어 있는 방에서 조명용 가스를 사용하는 경우 창문을 단단히 닫고 가스를 켜고 촛불을 켜둔 채로 두세요. 시간이 지나면 가스가 폭발하여 화재가 발생할 수도 있고 그렇지 않을 수도 있습니다.\n\n\n\n6.1.2 물 및 기타\n\n스프링쿨러 시스템을 작동시켜 창고 속의 물품들을 못쓰게 만듭니다. 스프링클러 헤드를 망치로 세게 두드리거나 그 아래에 성냥을 대면 됩니다.\n화장실에 휴지를 비치하지 말고 휴지, 머리카락, 기타 장애물을 단단히 말아서 화장실에 넣습니다. 스펀지에 두꺼운 전분 또는 설탕 용액을 적신뒤 꽉 짜서 끈으로 감싸고 말립니다. 완전히 마르면 끈을 제거합니다. 이제 스펀지는 단단하고 단단한 공 모양이 될 것입니다. 그것을 하수구에 넣으면 스펀지가 서서히 정상 크기로 팽창하여 하수 시스템을 막습니다.\n낮에 공공건물의 전구 아래에 동전을 넣어두면 밤에 조명이 켜지면 퓨즈가 끊어집니다. 퓨즈 뒤에 동전을 넣거나 무거운 전선을 넣으면 퓨즈 자체가 무력화될 수 있습니다. 그러면 합선으로 인해 화재가 발생하거나 변압기가 손상되거나 넓은 지역으로 전기 공급이 중단될 수 있습니다.\n경비원이 없는 공공건물의 모든 출입문 자물쇠에 종이, 나무 조각, 머리핀 등 들어갈 수 있는 모든 것을 끼워 넣으세요."
  },
  {
    "objectID": "posts/md/How_sabotage.html#공업-생산-제조",
    "href": "posts/md/How_sabotage.html#공업-생산-제조",
    "title": "사보타지 매뉴얼",
    "section": "6.2 공업 생산: 제조",
    "text": "6.2 공업 생산: 제조\n\n6.2.1 도구\n\n절삭 도구들이 무뎌지게 합니다. 그러면 효율 떨어져 생산성이 떨어지면 다른 자재와 부품에 손상을 줄 수 있습니다.\n사용하지 않는 톱은 약간 뒤틀리게 보관하면 사용하게 될 때 잘 부서집니다.\n줄날을 사용할 때 아주 빠르게 움직이면 강한 압력을 줄 수 있습니다.\n줄날은 바이스나 공작물에 두드려서 청소하세요. 이렇게 하면 쉽게 부러집니다.\n비트와 드릴은 옆쪽에서 강한 압력을 받으면 부러지기 쉽습니다.\n프레스 펀치에 정해진 것보다 많은 재료 (예: 블랭크 1 개가 아닌 2 개) 를 넣으면 펀치가 고장날 수 있습니다.\n공압 드릴, 리벳터 등과 같은 동력 구동 공구는 더러워지면 효율이 떨어집니다. 윤활과 전기 접점에 먼지가 쌓이게 두고 이물질을 넣어 오염시킵니다.\n\n\n\n6.2.2 기름 및 윤활유\n오일 및 윤활 시스템은 쉽게 파괴될 수 있을 뿐만 아니라 움직이는 부품이 있는 모든 기계에서 중요합니다. 오일 및 윤활 시스템을 방해하면 산업 공정의 주요 지점에서 생산 속도가 느려지거나 작업이 완전히 중단될 수 있습니다.\n\n금속 먼지나 고운 모래, 유리 가루, 연마제 및 이와 유사한 단단하고 거친 물질을 윤활 시스템에 직접 넣습니다. 이러한 물질은 매끄러운 표면을 긁어 피스톤, 실린더 벽, 샤프트 및 베어링을 망가뜨릴 수 있습니다. 모터가 과열되어 멈추게 되면 광범위한 수리가 필요합니다.\n필터 시스템을 열어 연필이나 기타 날카로운 물체로 필터 망을 찌른 다음 다시 덮으면 기계가 마모될 수 있습니다. 또는 신속하게 처리할 수 있다면 필터를 제거하세요.\n윤활 시스템이나 필터에 직접 접근할 수 없는 경우, 보관 중에 오일을 희석하여 오일의 효과를 줄일 수 있습니다. 이 경우 거의 모든 액체가 오일을 묽게 만들 수 있습니다. 소량의 황산, 광택제, 물 또는 아마씨유가 특히 효과적입니다.\n중유가 필요한 곳에 묽은 오일을 사용하면 기계가 고장 나거나 움직이는 축이 가열되어 멈출 수 있습니다.\n막히는 물질을 윤활 시스템에 넣거나 떠다니는 경우 보관 중인 오일에 넣습니다. 머리카락, 노끈 조각, 죽은 곤충 및 기타 여러 가지 일반적인 물체를 꼬아서 빗으면 공급 라인과 필터를 통한 오일의 흐름을 막거나 방해하는 데 효과적입니다.\n일부 상황에서는 윤활 시스템에서 마개를 제거하거나 오일이 저장된 드럼과 캔에 구멍을 뚫어 오일의 효과를 방해하는 대신 완전히 파괴할 수 있습니다.\n\n\n\n6.2.3 냉각 시스템\n\n수냉식 냉각 시스템에 쌀이나 밀과 같은 딱딱한 곡물을 몇 꼬집 넣으면 엔진이나 모터가 상당히 손상되어 상당히 짧은 시간 내에 작동을 멈출 수 있습니다. 그들은 부풀어 오르고 물의 순환을 막아 방해물을 제거하기 위해 냉각 시스템을 분해해야합니다. 톱밥이나 머리카락도 수냉 시스템을 막는 데 사용될 수 있습니다.\n과열된 모터의 냉각 시스템에 매우 차가운 물을 빠르게 주입하면 엔진 하우징에 수축과 상당한 변형이 발생합니다. 이러한 처리를 몇 번 반복하면 균열과 심각한 손상이 발생할 수 있습니다.\n흡기 또는 배기 밸브에 먼지와 폐기물이 막히면 공기 냉각 시스템의 효율을 떨어뜨릴 수 있습니다. 벨트로 작동하는 팬을 시스템에 사용하는 경우 벨트의 절반 이상을 들쭉날쭉하게 자르면 벨트가 미끄러져 결국 부하를 받아 분리되고 모터가 과열될 수 있습니다.\n\n\n\n6.2.4 가솔린 및 기름 연료\n연료 탱크와 연료 공급 엔진은 일반적으로 접근이 용이하고 쉽게 열 수 있습니다. 따라서 간단한 사보타지 활동에 매우 취약한 표적이 될 수 있습니다.\n\n가솔린 엔진의 연료 탱크에 톱밥이나 쌀이나 밀과 같은 딱딱한 곡물을 몇 꼬집 넣습니다. 입자가 공급 라인을 막아서 엔진이 멈추게 됩니다. 문제의 원인을 찾으려면 시간이 좀 걸릴 수 있습니다. 구하기는 어렵지만 오래된 고무줄이나 연필 지우개에서 찾을 수 있는 천연 고무 부스러기도 효과적입니다.\n설탕을 모을 수 있다면 휘발유 엔진의 연료 탱크에 넣으세요. 휘발유와 함께 타면서 끈적끈적한 덩어리로 변해 엔진을 완전히 망가뜨리고 대대적인 청소와 수리가 필요하게 됩니다. 꿀과 당밀은 설탕만큼 좋습니다. 휘발유 10 갤런당 약 75~100 그램을 사용하세요.\n휘발유에 다른 불순물이 유입되면 엔진이 빠르게 마모되고 결국 고장을 일으킬 수 있습니다. 부석, 모래, 분쇄 유리 및 금속 먼지의 미세 입자는 휘발유 탱크에 쉽게 유입될 수 있습니다. 입자가 매우 미세해야 카뷰레터를 통과할 수 있습니다.\n물, 소변, 와인 또는 기타 비교적 많은 양의 간단한 액체는 실린더에서 연소가 일어나지 않고 엔진이 움직이지 않을 정도로 휘발유를 희석시킬 수 있습니다. 휘발유 1 파인트에서 20 갤런이면 충분합니다. 소금물을 사용하면 부식과 영구적인 모터 손상의 원인이 됩니다.\n디젤 엔진의 경우 인화점이 낮은 오일을 연료 탱크에 넣으면 엔진이 작동하지 않습니다. 잘못된 종류의 오일을 넣었을 때 이미 탱크에 적절한 오일이 있는 경우 엔진이 흔들리며 헛도는 현상만 발생합니다.\n\n가솔린 및 오일 엔진의 연료 라인은 배기관을 자주 통과합니다. 기계가 정지해 있을 때 연료 라인에 작은 구멍을 뚫고 왁스로 구멍을 막으면 됩니다. 엔진이 작동하고 배기관이 뜨거워지면 왁스가 녹아 연료가 배기관으로 떨어지고 불꽃이 시작됩니다.\n휘발유가 보관된 방에 접근할 수 있는 경우, 촛불을 켜둔 채로 방에 들어가면 밀폐된 방에 축적된 유증기가 시간이 지나면 폭발할 수 있다는 점을 기억하세요. 그러나 휘발유 통에서 실내 공기 중으로 상당한 양의 증발이 일어나야 합니다. 통의 윗부분을 제거해도 충분한 양의 휘발유가 공기 중에 노출되지 않으면 칼또는 송곳으로 통에 구멍을 뚫습니다. 이렇게 하면 증발 속도가 크게 빨라집니다. 촛불에 불을 붙이기 전에 창문을 닫고 방을 최대한 밀폐해야 합니다. 옆 방의 창문이 활짝 열려 있으면 휘발유뿐만 아니라 주변의 모든 것을 파괴하는 큰 불이 날 수 있습니다.\n\n\n\n6.2.5 전기 모터\n전기 모터는 전문가가 아니라면 망가트리기 쉽지 않고 비숙련자는 부상의 위험이 있습니다.\n\n모든 유형의 전기 모터에서 가변 저항기를 높은 저항 지점으로 설정하세요. 과열되어 화재가 발생할 수 있습니다.\n과부하 릴레이를 모터의 용량을 초과하는 매우 높은 값으로 조정하세요. 그런 다음 모터가 과열되어 고장날 정도로 과부하를 가하세요.\n먼지, 흙, 수분은 전자 장비의 적이라는 것을 기억하세요. 전기 모터의 전선이 종단과 연결되는 부분과 절연되어 있는 부분에 먼지와 흙을 흘리면 전류가 비효율적으로 흐르거나, 쇼트가 발생할 것입니다.\n전선의 피복을 손상시키고, 연결 부위의 너트를 헐겁게 하고, 전선의 땜과 연결 부위를 엉망으로 처리하여 전류를 낭비하고 전기 모터의 출력을 저하시키세요.\n고정자가 손상되면 직류 모터의 출력이 감소하거나 단락이 발생할 수 있습니다: 고정자 고정 링을 풀거나 제거합니다. 고정자에 탄소, 흑연 또는 금속 먼지를 뿌립니다. 고정자 접촉부에 그리스나 오일을 약간 발라줍니다. 고정자 막대가 서로 가까이 있는 경우 금속 가루로 막대 사이의 간격을 메우거나 끌로 가장자리를 톱질하여 인접한 막대의 톱니가 만나거나 거의 만나 전류가 한 막대에서 다른 막대로 흐를 수 있도록 합니다.\n회전하는 브러시를 마모시킬 수 있는 곳에 우표 크기의 절반 정도의 고운 입자의 사포를 놓습니다. 그러면 모터가 화재로 인해 파괴됩니다.\n슬립 링에 탄소, 흑연 또는 금속 가루를 뿌려 전류가 누설되거나 단락이 발생할 수 있습니다. 모터가 유휴 상태일 때는 끌로 슬립링에 흠집을 내세요.\n전기자 표면에 그리스가 섞인 먼지가 묻으면 접촉이 제대로 이루어지지 않아 모터가 멈추거나 효율이 떨어집니다.\n전기 모터를 과열시키려면 모래와 진한 그리스를 섞어 고정자와 회전자 사이에 바르거나 얇은 금속 조각을 그 사이에 끼워 넣으십시오. 전류가 효율적으로 생성되지 않도록 기름, 타르 또는 페인트를 넣으십시오.\n3 상 전류를 사용하는 모터의 경우, 기계가 정지한 상태에서 인입선 중 하나에 칼이나 줄로 깊게 상처를 내거나 퓨즈 3 개 중 하나를 끊어진 퓨즈로 교체합니다. 첫 번째 경우에는 모터가 잠시 작동한 후 멈추고 두 번째 경우에는 시동이 걸리지 않습니다.\n\n\n\n6.2.6 변압기\n\n오일 충전식 변압기는 오일 탱크에 물이나 소금을 부으면 고장이 날 수 있습니다.\n공랭식 변압기의 경우, 주변에 잔해를 쌓아 통풍을 막아주세요.\n모든 유형의 변압기의 경우 외부 부싱 및 기타 노출된 전기 부품에 탄소, 흑연 또는 금속 먼지를 뿌려 주세요.\n\n\n\n6.2.7 터빈\n대부분의 터빈은 무겁고 견고하게 제작되어 있으며 망가트리기 어렵습니다.\n\n수력 터빈을 점검하거나 수리한 후에는 커버가 날아가서 발전소에 물이 넘치지 않도록 커버를 단단히 고정하세요. 증기 터빈의 덮개가 느슨하면 누수가 발생하고 속도가 느려집니다.\n수력 터빈에서는 펜 스톡 헤드의 스크리닝 바로 너머에 큰 고철 조각을 삽입하여 물이 손상된 재료를 플랜트 장비로 운반 할 수 있도록합니다.\n수리를 위해 터빈의 증기 라인을 열면 고철 조각을 넣어 증기가 다시 올라올 때 터빈 기계에 분사되도록 합니다.\n터빈의 연료 공급 라인에 구멍을 뚫어 뜨거운 증기 파이프에 떨어진 기름으로 불이 나도록 하세요.\n\n\n\n6.2.8 보일러\n\n가능한 모든 방법으로 증기 보일러의 효율을 줄이세요. 물을 너무 많이 넣어서 시동이 느리게 걸리게 하거나 불을 낮게 유지하여 비효율적으로 만듭니다. 증기 보일러가 마르지 않은 상태에서 불을 올리면 금이 가고 망가질 수 있습니다. 특히 좋은 방법은 석회석이나 석회가 포함된 물을 보일러에 계속 넣으면 바닥과 측면에 석회가 쌓이는 것입니다. 이 침전물은 열에 대해 매우 우수한 단열 효과를 제공하며 충분히 쌓이면 보일러는 완전히 쓸모없어집니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#생산.-금속",
    "href": "posts/md/How_sabotage.html#생산.-금속",
    "title": "사보타지 매뉴얼",
    "section": "6.3 생산. 금속",
    "text": "6.3 생산. 금속\n\n6.3.1 철과 강철\n\n용광로는 수리를 위해 자주 가동을 중단해야 하는 상태로 유지합니다. 용광로 내부 라이닝용 내화벽돌을 만들 때는 타르의 비율을 늘려서 빨리 마모되고 지속적으로 라이닝을 다시 하도록 합니다.\n주조용 코어에 기포가 가득 차 불완전한 주조 결과가 나오도록 코어를 만듭니다.\n금형의 코어가 제대로 지지되지 않아 코어의 위치가 잘못되어 코어가 흘러내리거나 주물이 손상되게 합니다.\n철이나 강철을 열처리할 때 과도한 열을 가하여 철근과 강괴의 품질을 낮추세요.\n\n\n\n6.3.2 다른 금속\n\n가능한 제안 사항이 없습니다.\n\n\n\n6.3.3 생산: 채광 및 광물 추출\n\n\n6.3.4 탄광\n\n오일 램프를 꺼버리세요, 오일램프를 다시 켜려면 눅눅하지 않은 곳을 찾아야 하기 때문에 오랜 시간이 필요합니다.\n공압식 픽을 만들때 픽을 제대로 굳히지 않으면 금방 무뎌집니다.\n공압식 픽은 쉽게 고장날 수 있습니다. 오일 레버에 소량의 물을 부으면 픽이 작동을 멈춥니다. 석탄 먼지와 부적절한 윤활도 고장을 일으킬 수 있습니다.\n석탄을 운반하는 버킷 컨베이어를 당기는 체인을 약화시키세요. 픽이나 삽으로 때리면 체인이 정상적인 힘에도 끊어질 수 있습니다.\n레일과 전환 지점에 장애물을 설치해 광산 차량이 탈선하도록 만듭니다.\n석탄에 돌이나 다른 쓸모없는 물건을 섞어 보내세요.\n\n\n\n6.3.5 생산: 농업\n\n\n6.3.6 기계류\n\n공업 생산: 제조 &gt; 가솔린 및 기름 연료, 전기 모터, 작물과 가축 항목 참고.\n\n\n\n6.3.7 작물과 가축\n농작물과 가축은 식량 공급이 많거나 적이 식량을 요구하는 것으로 알려진 곳에서만 파괴해야 합니다.\n\n가축에게 농작물을 먹이십시오. 농작물을 너무 일찍 또는 늦게 수확하세요. 저장된 곡물, 과일, 채소를 물에 담가 썩게 합니다. 과일과 채소를 햇볕에 방치하여 상하게 합니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#교통-수단-철도",
    "href": "posts/md/How_sabotage.html#교통-수단-철도",
    "title": "사보타지 매뉴얼",
    "section": "6.4 교통 수단: 철도",
    "text": "6.4 교통 수단: 철도\n\n6.4.1 승객\n\n열차 여행이 되도록 불편하도록 만드세요. 티켓을 잘못 발행하고 여정의 일부를 빼먹도록 하세요. 열차의 한 자리에 두 개의 티켓을 발급하여 다툼이 발생하게 만들고 열차가 떠날 때까지 시간을 지연시키세요. 열차의 출발 및 도착을 알리는 알림을 고의로 잘못되게 만드세요.\n목적지를 향해 가고 있는 열차에서 승무원들은 음식을 특별히 맛없게 만들고 한밤중에 티켓을 확인하는등 손님을 최대한 귀찮게 만들어야 합니다.\n짐이 잘못 놓여지거나 다른 역에 내려지게 하세요.\n열차 운전자는 열차를 고의로 지연시키거나 예정에 없던 정차를 하게 만들어야 합니다.\n\n\n\n6.4.2 스위치, 신호 및 라우팅\n\n신호기와 스위치가 포함된 배전반의 전선이 잘못된 단자에 연결되지 않도록 교환합니다.\n신호 암이 작동하지 않도록 푸시 막대를 풀고, 신호등을 고장내고, 적색 및 녹색 신호등의 컬러 렌즈를 교환합니다.\n선로에서 스위치 지점이 움직이지 않도록 펼쳐서 고정하거나 스위치 지점 사이에 돌이나 흙더미를 놓습니다.\n스위치 포인트의 전기 연결부와 인근 바닥에 소금을 충분히 뿌립니다. 비가 오면 스위치가 단락될 수 있습니다.\n차량이 잘못된 열차에 투입되도록 수리가 필요한 차량의 표지를 떼어내고 차량과 차량 사이의 커플링은 가능한 느슨하게 유지합니다.\n\n\n\n6.4.3 선로\n\n곡선에서 외부 레일 섹션에 연결되는 연결 플레이트에서 볼트를 빼고 연결 조인트의 양쪽에서 자갈, 콘크리트 또는 흙을 몇 피트 정도 퍼냅니다.\n조인트에서 연결 플레이트를 분리하고 양쪽의 침목 못을 풀면 레일이 서로 벌려집니다.\n\n\n\n6.4.4 기름과 윤활유\n\n공업 생산: 제조 &gt; 가솔린과 기름 연료 참고\n윤활 파이프를 집게로 꽉 쥐거나 망치로 찌그러뜨려 오일의 흐름이 막히게 합니다.\n\n\n\n6.4.5 냉각 시스템\n\n공업 생산: 제조 &gt; 냉각 시스템 참고.\n\n\n\n6.4.6 가솔린 및 기름 연료\n\n공업 생산: 제조 &gt; 가솔린 및 기름연료 참고.\n\n\n\n6.4.7 전기 모터\n\n공업 생산: 제조 &gt; 냉각 시스템 및 가솔린 및 기름 연료 참고.\n\n\n\n6.4.8 보일러\n\n공업 생산: 제조 &gt; 보일러 참고\n검사 후 엔진의 보일러에 중유 또는 타르를 넣거나 비누 0.5 킬로그램을 물에 넣으십시오.\n\n\n\n6.4.9 브레이크 및 기타\n\n일부러 고속으로 주행하고 커브길과 내리막길에서는 브레이크를 과도하게 사용해 엔진에 과부하를 줍니다.\n에어 브레이크 밸브 또는 급수관에 구멍을 뚫습니다.\n여객 열차의 마지막 칸이나 화물의 앞칸에서는 저널 박스의 솜을 제거하고 기름 묻은 헝겊으로 교체합니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#교통-수단-자동차",
    "href": "posts/md/How_sabotage.html#교통-수단-자동차",
    "title": "사보타지 매뉴얼",
    "section": "6.5 교통 수단: 자동차",
    "text": "6.5 교통 수단: 자동차\n\n6.5.1 도로\n도로에 가하는 사보타지는 시간이 오래 걸리기 때문에 작전 당일에 하는 것은 비효율적입니다.\n\n교차로와 갈림길의 표지판을 변경합니다. 적이 잘못된 길로 가면 실수를 발견하기까지 수 킬로미터가 걸릴 수 있습니다.\n길을 물어보면 잘못된 길을 알려줍니다.\n통행량이 많은 도로를 손상시키면 지나가는 차량과 자연이 나머지를 처리해 줍니다. 비포장 도로는 삽으로 파낼 수도 있습니다. 작은 개울이 흘러 도로를 잠식하게 하면 도로를 손상시키는 데 몇 분 밖에 걸리지 않을 것입니다.\n깨진 유리, 못, 날카로운 돌을 도로에 뿌려 타이어를 펑크 낼 수 있습니다.\n\n\n\n6.5.2 승객\n\n버스기사는 적이 내리길 원하는 정류장을 지나칠 수 있습니다. 택시기사는 적의 목적지로 가는 가장 먼 경로를 선택하여 적의 시간과 돈을 낭비하게 할 수 있습니다.\n\n\n\n6.5.3 기름 및 윤활유\n\n공업 생산: 제조 &gt; 냉각 시스템 참고\n윤활유 펌프를 끊어버리면 80km 를 운행하기 전에 베어링이 망가질 것입니다.\n\n\n\n6.5.4 레디에이터\n\n공업 생산: 제조 &gt; 냉각 시스템 참고\n\n\n\n6.5.5 연료\n\n공업 생산: 제조 &gt; 가솔린 및 기름 연료 참고\n\n\n\n6.5.6 배터리 및 점화\n\n점화 잠금장치에 나무 조각을 끼우거나 배전반 뒤의 연결부를 풀고 점화 플러그에 먼지를 넣어서 손상 시킬 수 있습니다.\n주차된 차량의 조명을 켜서 배터리가 방전되도록 합니다.\n기계공은 여러 가지 방법으로 교모하게 배터리를 망칠 수 있습니다: 베터리 셀에서 밸브 캡을 떼어내고 드라이버를 비스듬히 밀어 넣어 셀의 판을 깨뜨리면 밖에서는 손상이 보이지 않습니다. 베터리 셀에 철이나 구리를 넣으면 셀의 수명이 크게 단축됩니다. 특히 각 베터리 셀에 식초를 조금 넣으면 배터리 수명이 크게 줄어듭니다.\n\n\n\n6.5.7 기어\n\n변속기 및 기타 기어에서 윤활유를 제거하거나 가벼운 윤활유를 넣으세요.\n트럭, 트랙터 및 기타 무거운 기어가 장착된 기계의 경우 기어 케이스가 불안정하게 고정되도록 볼트 구멍의 절반에만 볼트를 끼우세요. 사용 중 기어가 심하게 흔들려 곧 수리가 필요할 수 있습니다.\n\n\n\n6.5.8 타이어\n\n방치된 차량의 타이어에 구멍을 내거나 펑크를 내세요. 성냥갑이나 기타 작은 상자 안에 못을 넣고 정지된 자동차의 뒷바퀴 앞에 수직으로 세워두면 자동차가 출발할 때 못이 타이어를 깔끔하게 뚫을 수 있습니다.\n타이어 수리점은 타이어를 손상시키기 쉽습니다: 펑크 수리 시 유리, 가성소다 등을 안에 흘리면 튜브에 금방 구멍이 나거나 부식될 수 있습니다. 또는 펑크 난 타이어를 수리할 때 펑크를 일으킨 물체를 그대로 두면 됩니다.\n수리 후 타이어를 조립할 때는 타이어 림과 휠 림 사이에 이물질이 끼게 만드세요.\n타이어에 공기를 넣을 때는 정상 공기압보다 낮게 유지하세요. 타이어가 더 많이 마모될 수 있습니다. 양쪽 바퀴에 타이어를 채울 때는 안쪽 타이어에 바깥쪽 타이어보다 훨씬 높은 압력으로 공기를 넣어야 하며, 이렇게 하면 두 타이어 모두 더 빨리 마모됩니다. 바퀴의 정렬이 잘못되면 타이어도 빨리 마모되므로 바퀴를 조정하기 위해 들어올 때 정렬이 맞지 않은 상태로 둡니다.\n타이어 재고가 많이 있는 경우 휘발유 등을 타이어에 흘려서 타이어를 경화되게 할 수 있습니다. 다만 합성 고무는 이러한 화학 물질에 덜 민감합니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#교통-수단-물",
    "href": "posts/md/How_sabotage.html#교통-수단-물",
    "title": "사보타지 매뉴얼",
    "section": "6.6 교통 수단: 물",
    "text": "6.6 교통 수단: 물\n\n6.6.1 항해\n\n바지선 및 선박 항해사는 이동하는 수로의 상태에 대한 잘못된 소문을 퍼뜨립니다. 다른 바지선 및 보트 선장에게 시간이 더 걸리거나 운하를 우회해야 하는 수로를 따라가라고 알립니다.\n바지선 및 선박의 항해사는 다리 근처에서는 시간을 낭비하고 대기해야 하는 다른 선박의 시간을 낭비합니다. 또한 선박이 ‘실수로’ 좌초되는 것도 매우 효율적인 시간 낭비가 됩니다.\n도개교에 근무하는 직원은 실수로 다리를 올렸다가 내렸다 하며 다리 위나 아래 수로의 교통을 지연시킬 수 있습니다.\n화물선의 나침반에 자성을 망가트립니다. 나침반의 자성을 제거하거나 나침반 근처에 큰 철근이나 철을 숨겨서 나침반이 잘못 작동되도록 합니다.\n\n\n\n6.6.2 화물\n\n화물을 적재하거나 하역할 때 부주의하게 취급하면 화물이 손상될 수 있습니다. 가장 약하고 가벼운 상자와 상자가 화물칸 바닥에 있고 가장 무거운 상자가 그 위에 오도록 화물을 적치합니다. 비와 갑판 세척으로 인해 화물이 손상되도록 커버와 타포린은 허술하게 씌웁니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#통신",
    "href": "posts/md/How_sabotage.html#통신",
    "title": "사보타지 매뉴얼",
    "section": "6.7 통신",
    "text": "6.7 통신\n\n6.7.1 전화\n\n사무실, 호텔, 교환실에서 적의 전화 연결을 지연시키거나, 잘못된 번호를 알려주거나, 실수로 끊거나, 회선을 다시 사용할 수 없도록 계속 연결을 시도합니다.\n적 본부에 하루에 한 번 이상 전화를 걸어 공식 업무, 특히 군사 업무를 방해하고, 전화를 받으면 잘못된 번호를 받았다고 말합니다. 군대나 경찰서에 전화하여 익명으로 다음과 같은 허위 신고를 합니다. 화재, 공습, 폭탄에 대해 익명으로 허위 신고합니다.\n적이 사용하는 사무실과 건물에서는 전화기의 수화기를 망가트립니다. 전기 기술자와 전화 수리공은 연결 상태가 좋지 않고 절연이 손상되어 혼신 및 기타 종류의 전기 간섭을 만들어 통화가 어렵거나 불가능하게 만들 수 있습니다.\n자동 교환기의 배터리는 못, 또는 동전으로 손상시킬 수 있습니다. 이런 식으로 배터리의 절반을 처리할 수 있다면 교환기의 작동이 중단됩니다.\n\n\n\n6.7.2 전신\n\n전보 전달을 고의로 지연시킵니다.\n적지로의 전보를 왜곡하여 다른 전보를 보내거나 장거리 통화를 해야만 하는 상황으로 만듭니다. 예를 들어, “최소”를 “최대”로 변경하여 전보를 받는 사람이 “최소” 인지 “최대” 인지 알 수 없도록 하는 등 단어의 한 글자를 변경하여 이를 수행할 수 있는 경우도 있습니다.\n\n\n\n6.7.3 전화선\n\n전화 및 전신 통신선을 자르고 피복을 망가뜨려 간섭을 발생시키세요.\n\n\n\n6.7.4 우편\n\n우체국 직원은 적의 우편을 잘못된 행낭에 넣는 등의 행위를 통해 우편이 하루 이상 지연되도록 할 수 있습니다.\n\n\n\n6.7.5 영화\n\n영사기 관리자는 초점을 흐리거나 필름을 빠르게 또는 종종 멈추게 함으로써 뉴스와 적의 선전 영화를 망칠 수 있습니다.\n관객들은 박수를 쳐 연사의 말을 묻히게 하거나, 큰 소리로 잡담을 함으로써 적의 선전 영화를 망칠 수 있습니다.\n나방 20 ~ 30 마리를 종이백에 넣어 감으로써 적 선전 영화의 상영을 중단시킬 수 있습니다. 종이백을 가지고 들어가 영화관의 빈 바닥에 내려놓은 뒤 열어두면 나방들이 튀어나와 영사기의 빛으로 몰려들어 펄럭거리는 그림자로 화면을 뒤덮을 것입니다.\n\n\n\n6.7.6 라디오\n\n방송국 기술자들은 목소리 송출을 과다 변조함으로써 손쉽게 적의 선전 방송이나 지시사항이 마치 물속에서 말하는 것처럼 들리게 할 수 있습니다.\n자신의 아파트 건물에서는 적이 모든 사람이 듣기를 원할 때 라디오 수신을 방해할 수 있습니다. 전등 코드 끝에서 전등 플러그를 뽑고, 코드에서 전선을 꺼내 2구 플러그의 두 단자 또는 4구 플러그의 세 단자에 묶습니다. 그런 다음 전선을 가지고 다니면서 벽이나 바닥에 있는 콘센트에 최대한 많이 꽂아보세요. 플러그를 새 회로에 꽂을 때마다 퓨즈가 끊어지고 새 퓨즈로 교체할 때까지 모든 무전기가 고장 상태가 됩니다.\n어떤 전자 장비이든 절연체를 손상시킴으로써 이웃집, 거대 발전기, 네온사인, 형광등, 엑스레이 장치 및 송전선 등에 무선 간섭을 일으킬 수 있습니다. 만약 작업자가 적 비행장 근처 고압선의 절연체를 손상시킬 수 있다면 지상에서 비행기로의 무선 통신이 장시간동안 어려워질 것입니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#전력",
    "href": "posts/md/How_sabotage.html#전력",
    "title": "사보타지 매뉴얼",
    "section": "6.8 전력",
    "text": "6.8 전력\n\n6.8.1 터빈, 전기 모터, 변압기\n\n공업 생산: 제조 &gt; 전기모터, 변압기, 터빈 참고\n\n\n\n6.8.2 송전 선로\n\n전선 작업자는 절연체를 느슨하게 하고 더러워지게 하여 고의로 누전을 일으킬 수 있습니다. 두 개의 평행한 송전선 사이에 매우 무거운 끈을 여러 번 앞뒤로 묶어 매번 전선 주위를 여러 바퀴 감는 것도 매우 쉬울 것입니다. 미리 끈을 소금으로 심하게 절인뒤 감습니다. 그러면 비가 오면 줄이 도체가 되어 단락이 발생할 수 있습니다."
  },
  {
    "objectID": "posts/md/How_sabotage.html#조직-전반적인-사보타지",
    "href": "posts/md/How_sabotage.html#조직-전반적인-사보타지",
    "title": "사보타지 매뉴얼",
    "section": "6.9 조직 전반적인 사보타지",
    "text": "6.9 조직 전반적인 사보타지\n\n6.9.1 회의\n\n모든 일은 절차를 통해 하자고 주장합니다. 신속한 결정을 하기 위한 절차는 절대 허용하지 않습니다.\n회의 시간에는 연설을 하세요. 가능한 자주 그리고 굉장히 길게 일화와 개인 경험담에 대한 이야기를 하므로 요점을 찾을 수 없도록 하세요. 특히 적절한 “애국적” 코멘트를 추가하는 것을 중요합니다.\n항상 ” 추가적인 고려 ” 를 위해 위원회를 만들어 내용을 검토하세요. 위원회는 5 인 이상으로 가능한 크게 만들어야 합니다.\n무의미한 이슈를 가능한 자주 끌어 들입니다.\n보도문, 의사록, 결의안에 대한 표현을 계속 검토하고 수정하세요.\n지난 회의에서 결정된 사항을 되돌아보고 그 결정의 타당성에 대해 다시 의문을 제기하세요.\n“조심” 하자고 주장하고 다른 회의 참석자들에게도 “합리적” 으로 생각하며 서두르지 않아야 나중에 당황하지 않는다고 말하세요.\n어떤 결정이든 그 타당성에 대해 걱정하세요. 그 결정이 조직 내에서 숙고된 것인지 아니면 다른 상위 조직의 정책과 상충되지는 않는지 계속 의문을 제기하세요.\n\n\n\n6.9.2 관리자 및 감독자\n\n항상 서면으로 된 주문서를 요구하세요.\n주문서의 내용을 의도적으로 오해합니다. 그러한 주문서에 대해 끝 없이 질문을 할 수 있고 긴 서신을 서로 주고 받을 수 있습니다. 그리고 아주 애매한 용어를 사용합니다.\n주문서 전달을 지연시킵니다. 주문서 일부가 완성되었어도 전체가 준비될 때까지는 전달하지 마세요.\n주문을 이행하는 중 조금의 지연도 전체 중단으로 이어질 수 있도록 현재 재고를 거의 다 써버릴 때까지 새로운 작업 재료를 주문하지 마세요.\n확보하기 어려운 자재를 주문하세요. 저급한 자재는 저급한 산출물로 이어진다고 따지세요.\n작업 할당 시에는 항상 중요하지 않은 작업부터 나눠주세요. 중요한 작업은 실력 없는 노동자에게 할당 하세요.\n상대적으로 덜 중요한 제품에 완벽한 작업이 필요하다고 고집 부리세요. 눈에 보이는 결함이 있는 것은 다시 보완하도록 돌려보내고 맨 눈으로는 볼 수 없는 결함을 가진 부품은 통과시키세요.\n부품과 자재가 공장 내의 잘못된 곳으로 보내지도록 잘못된 경로로 보내세요.\n새로운 노동자를 훈련시킬 때는 불완전하거나 잘못된 지시를 하세요.\n사기를 낮춰 생산량을 감소시키기 위해 실력 없는 노동자를 부적절하게 승진시키세요. 실력 있는 노동자는 차별하고 작업물에 대해 불평하세요.\n더 중요한 일이 있을 때는 새로운 회의를 개최하세요.\n그럴 듯한 방법으로 서류 작업을 늘리세요.\n지침 발행, 수표 발행 등과 관련된 절차와 허가를 늘리고 한 명이면 됐을 모든 일을 세 명이 승인해야 하게 만드세요.\n모든 규정을 빼놓지 않고 다 적용하세요.\n\n\n\n6.9.3 회사원\n\n주문을 받아 적을 때 자재의 수량을 잘 못적으세요. 그리고 비슷한 이름들은 서로 헷갈리게 적고 잘못된 주소를 사용하세요.\n정부 기관과의 의사소통이 길어지게 하세요.\n중요 문서를 잘못되게 정리하세요.\n사본을 만들 때는 하나를 덜 만들어서 추후에 복사 작업이 추가로 필요하게 만드세요.\n상관에게 중요한 전화가 오면 바쁘거나 통화 중이라고 말하세요.\n다음 번 수거 때까지 우편물을 보내지 말고 보관하고 있으세요.\n내부 정보처럼 위장된 흉흉한 소문을 퍼뜨리세요.\n\n\n\n6.9.4 종업원\n\n천천히 일하세요. 작업할때 필요한 동작의 수를 늘리세요. 예를 들면 무거운 망치 대신 가벼운 망치를 쓰거나, 큰 렌치가 필요할 때 작은 렌치를 사용하거나, 충분한 힘이 필요할 때 힘을 거의 쓰지 않는다거나 등등이 있습니다.\n일할 때 가능한 많은 중단이 발생하도록 하세요. 선반이나 펀치머신 위에서 작업 중 자재를 바꿔야할 때, 쓸 데 없이 많은 시간을 들이세요. 절단 작업 또는 치수 작업 중이라면, 필요한 것보다 두 배 더 자주 수치를 확인하세요. 화장실을 갈 경우 쓸 데 없이 더 길게 다녀오세요. 항상 도구를 잃어버리고 다니도록 하세요.\n실제로는 이해하더라도 외국어로 된 지시사항은 못알아 듣는 척하세요.\n지시사항이 이해하기 어려운 척 하여 한 번 이상 더 반복하도록 물어보세요. 또는 특별히 걱정이 많은 척 하여 쓸데 없는 질문으로 관리자를 훼방 놓으세요.\n작업을 엉망으로 한 뒤 나쁜 도구, 기계, 또는 장비 탓을 하세요. 그리고 일을 제대로 할 수 없다고 불평하세요.\n신입 노동자에게 기술과 경험을 전수하지 마세요.\n가능한 모든 방법으로 관리를 어렵게 하세요. 문서의 양식을 이해하기 어렵게 작성하여 실수를 하거나 필수 정보를 빼놓아서 다시 작성하게 만드세요.\n가능하다면 직원의 고충을 관리자에게 보고하는 조직에 가입하거나 만드세요. 보고 절차가 관리자를 불편하게 하는지, 각 보고에 많은 수의 직원이 참여하는지 고충에 대해 최소 하나 이상의 회의를 수반하는지 등등을 확인하세요.\n주문 받은 자재를 잘못 납품하세요.\n정상 부품들을 불량 부품들과 함께 섞어 놓으세요."
  },
  {
    "objectID": "posts/md/How_sabotage.html#조직의-사기를-낮추고-혼란을-일으키는-행동들",
    "href": "posts/md/How_sabotage.html#조직의-사기를-낮추고-혼란을-일으키는-행동들",
    "title": "사보타지 매뉴얼",
    "section": "6.10 조직의 사기를 낮추고 혼란을 일으키는 행동들",
    "text": "6.10 조직의 사기를 낮추고 혼란을 일으키는 행동들\n\n질문에 대해 불필요하게 복잡하고 모호하게 답변하기\n근거 없는 위협이나 위험을 당국에 신고하기\n의도적으로 무지한 척 행동하기\n법적 문제를 일으키지 않는 선에서 타인을 불편하게 만들기\n공공 서비스나 규정을 고의로 잘못 해석하기\n사소한 일에도 과도하게 불만을 표출하기\n특정 국적이나 배경을 가진 사람들을 공개적으로 차별하기\n특정인의 등장 시 대화를 갑자기 중단하기\n상황에 맞지 않게 과도한 감정 표현하기\n공식 매체나 문화 행사를 의도적으로 외면하기\n공동체 활동에 비협조적인 태도 보이기"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html",
    "href": "posts/md/Rosalind_textbookTrack.html",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "",
    "text": "Phillip Compeau 와 Pavel Pevzner 가 쓴 책 “능동적 접근 방식의 생물정보학 알고리즘” 에서 제공되는 연습 문제 모음입니다.\nRosalind 는 프로젝트 오일러, 구글 코드 잼 에서 영감을 얻었습니다. 이 프로젝트의 이름은 DNA 이중나선을 발견하는 데 기여한 로잘린드 프랭클린 에서 따왔습니다. Rosalind 는 프로그래밍 실력을 키우고자 하는 생물학자와 분자생물학의 계산 문제를 접해본 적이 없는 프로그래머들에게 도움이 될 것입니다."
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#compute-the-number-of-times-a-pattern-appears-in-a-text",
    "href": "posts/md/Rosalind_textbookTrack.html#compute-the-number-of-times-a-pattern-appears-in-a-text",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "0.1 Compute the Number of Times a Pattern Appears in a Text",
    "text": "0.1 Compute the Number of Times a Pattern Appears in a Text\nThis is the first problem in a collection of”code challenges”to accompany Bioinformatics Algorithms: An Active-Learning Approach by Phillip Compeau & Pavel Pevzner.\nA k-mer is a string of length k. We define Count(Text, Pattern) as the number of times that a k-mer Pattern appears as a substring of Text.\nFor example, We note that \\(Count(CGATATATCCATAGCGATATATCCATAG,ATAATA)\\) is equal to 3 (not 2) since we should account for overlapping occurrences of Pattern in Text.\nGiven: {DNA strings}} Text and Pattern.\nReturn: Count(Text, Pattern)."
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "0.2 Sample Dataset",
    "text": "0.2 Sample Dataset\nGCGCG\nGCG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "0.3 Sample Output",
    "text": "0.3 Sample Output\n2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution",
    "href": "posts/md/Rosalind_textbookTrack.html#solution",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "0.4 Solution",
    "text": "0.4 Solution\nfrom typing import Generator\n\ndef generate_substrings(text: str, size: int) -&gt; Generator[str, None, None]:\n    \"\"\"Generate all substrings of a given size from the text.\"\"\"\n    for i in range(len(text) - size + 1):\n        yield text[i:i + size]\n\ndef count_pattern_occurrences(text: str, pattern: str) -&gt; int:\n    \"\"\"Count how many times the pattern occurs in the text.\"\"\"\n    return sum(pattern == substring for substring in generate_substrings(text, len(pattern)))\n\n# Sample input\nsample_input = \"\"\"\nGCGCG\nGCG\n\"\"\"\n\n# Split input into text and pattern\ntext, pattern = sample_input.strip().split(\"\\n\")\n\n# Print the count of pattern occurrences in text\nprint(count_pattern_occurrences(text, pattern))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-1",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-1",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "1.1 Sample Dataset",
    "text": "1.1 Sample Dataset\nACGTTGCATGTCGCATGATGCATGAGAGCT\n4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-1",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-1",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "1.2 Sample Output",
    "text": "1.2 Sample Output\nCATG GCAT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-1",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-1",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "1.3 Solution",
    "text": "1.3 Solution\nfrom typing import List, Dict, Tuple\nfrom collections import defaultdict\n\ndef generate_substrings(text: str, size: int) -&gt; List[str]:\n    \"\"\"Generate all substrings of a given size from the text.\"\"\"\n    return [text[i:i + size] for i in range(len(text) - size + 1)]\n\ndef count_kmers(text: str, k: int) -&gt; Dict[str, int]:\n    \"\"\"Count occurrences of each k-mer in the text.\"\"\"\n    kmer_counts = defaultdict(int)\n    for kmer in generate_substrings(text, k):\n        kmer_counts[kmer] += 1\n    return kmer_counts\n\ndef most_frequent_kmers(kmer_counts: Dict[str, int]) -&gt; List[str]:\n    \"\"\"Find the most frequent k-mers.\"\"\"\n    max_count = max(kmer_counts.values())\n    return [kmer for kmer, count in kmer_counts.items() if count == max_count]\n\n# Sample input\nsample_input = \"\"\"\nACGTTGCATGTCGCATGATGCATGAGAGCT\n4\n\"\"\"\n\n# Split input into text and pattern size\ntext, k = sample_input.strip().split(\"\\n\")\nk = int(k)\n\n# Find and print the most frequent k-mers\nmost_frequent = most_frequent_kmers(count_kmers(text, k))\nprint(*most_frequent)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-2",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-2",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "2.1 Sample Dataset",
    "text": "2.1 Sample Dataset\nAAAACCCGGT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-2",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-2",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "2.2 Sample Output",
    "text": "2.2 Sample Output\nACCGGGTTTT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-2",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-2",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "2.3 Solution",
    "text": "2.3 Solution\ndef reverse_complement(seq: str) -&gt; str:\n    \"\"\"Return the reverse complement of a DNA sequence.\"\"\"\n    return seq[::-1].translate(str.maketrans(\"ACGT\", \"TGCA\"))\n\n# Sample input\nsample_input = \"\"\"\nAAAACCCGGT\n\"\"\"\n\n# Process the input and print the reverse complement\nsequence = sample_input.strip().split()[0]\nprint(reverse_complement(sequence))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-3",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-3",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "3.1 Sample Dataset",
    "text": "3.1 Sample Dataset\nATAT\nGATATATGCATATACTT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-3",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-3",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "3.2 Sample Output",
    "text": "3.2 Sample Output\n1 3 9"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-3",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-3",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "3.3 Solution",
    "text": "3.3 Solution\nfrom typing import List, Generator\n\ndef generate_substrings(text: str, size: int) -&gt; List[str]:\n    \"\"\"Generate all substrings of a given size from the text.\"\"\"\n    return [text[i:i + size] for i in range(len(text) - size + 1)]\n\ndef find_pattern_indices(text: str, pattern: str) -&gt; Generator[int, None, None]:\n    \"\"\"Yield starting indices where the pattern is found in the text.\"\"\"\n    for i, substring in enumerate(generate_substrings(text, len(pattern))):\n        if substring == pattern:\n            yield i\n\n# Sample input\nsample_input = \"\"\"\nATAT\nGATATATGCATATACTT\n\"\"\"\n\n# Split input into pattern and text\npattern, text = sample_input.strip().split(\"\\n\")\n\n# Print indices where the pattern is found\nprint(*find_pattern_indices(text, pattern))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-4",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-4",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "4.1 Sample Dataset",
    "text": "4.1 Sample Dataset\nCGGACTCGACAGATGTGAAGAAATGTGAAGACTGAGTGAAGAGAAGAGGAAACACGACACGACATTGCGACATAATGTACGAATGTAATGTGCCTATGGC\n5 75 4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-4",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-4",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "4.2 Sample Output",
    "text": "4.2 Sample Output\nCGACA GAAGA AATGT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-4",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-4",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "4.3 Solution",
    "text": "4.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Dict\n\ndef generate_substrings(text: str, size: int) -&gt; List[str]:\n    \"\"\"Generate all substrings of a given size from the text.\"\"\"\n    return [text[i:i + size] for i in range(len(text) - size + 1)]\n\ndef find_kmers(text: str, k: int) -&gt; Dict[str, List[int]]:\n    \"\"\"Find positions of k-length kmers within the text.\"\"\"\n    kmer_positions = defaultdict(list)\n    for i, substring in enumerate(generate_substrings(text, k)):\n        kmer_positions[substring].append(i)\n    return kmer_positions\n\ndef has_clump(positions: List[int], L: int, t: int, k: int) -&gt; bool:\n    \"\"\"Check if a given array of kmers at positions forms a clump of t within L.\"\"\"\n    for i in range(len(positions) - t + 1):\n        if (positions[i + t - 1] + k - positions[i]) &lt;= L:\n            return True\n    return False\n\n# Sample input\nsample_input = \"\"\"\nCGGACTCGACAGATGTGAAGAAATGTGAAGACTGAGTGAAGAGAAGAGGAAACACGACACGACATTGCGACATAATGTACGAATGTAATGTGCCTATGGC\n5 75 4\n\"\"\"\n\n# Split input into sequence and parameters\nseq, params = sample_input.strip().split(\"\\n\")\nk, L, t = map(int, params.split())\n\n# Find kmers and print those forming clumps\nkmers = find_kmers(seq, k)\nclumps = [kmer for kmer in kmers if has_clump(kmers[kmer], L, t, k)]\nprint(*clumps)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-5",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-5",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "5.1 Sample Dataset",
    "text": "5.1 Sample Dataset\nCCTATCGGTGGATTAGCATGTCCCTGTACGTTTCGCCGCGAACTAGTTCACACGGCTTGATGGCAAATGGTTTTTCCGGCGACCGTAATCGTCCACCGAG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-5",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-5",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "5.2 Sample Output",
    "text": "5.2 Sample Output\n53 97"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-5",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-5",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "5.3 Solution",
    "text": "5.3 Solution\nfrom typing import Generator\n\ndef find_minima(seq: str) -&gt; Generator[int, None, None]:\n    \"\"\"Find positions with the minimum skew in a DNA sequence.\"\"\"\n    skew = [0]\n    delta = {\"G\": 1, \"C\": -1, \"A\": 0, \"T\": 0}\n    \n    for i, nucleotide in enumerate(seq):\n        skew.append(skew[i] + delta[nucleotide])\n    \n    min_skew = min(skew)\n    return (i for i, value in enumerate(skew) if value == min_skew)\n\n# Sample input\nsample_input = \"\"\"\nCCTATCGGTGGATTAGCATGTCCCTGTACGTTTCGCCGCGAACTAGTTCACACGGCTTGATGGCAAATGGTTTTTCCGGCGACCGTAATCGTCCACCGAG\n\"\"\"\n\n# Process the input and print the positions with minimum skew\nsequence = sample_input.strip()\nprint(*find_minima(sequence))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-6",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-6",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "6.1 Sample Dataset",
    "text": "6.1 Sample Dataset\nGGGCCGTTGGT\nGGACCGTTGAC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-6",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-6",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "6.2 Sample Output",
    "text": "6.2 Sample Output\n3"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-6",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-6",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "6.3 Solution",
    "text": "6.3 Solution\nfrom itertools import zip_longest\nfrom typing import Tuple\n\ndef calculate_hamming_distance(sequence1: str, sequence2: str) -&gt; int:\n    return sum(base1 != base2 for base1, base2 in zip_longest(sequence1, sequence2, fillvalue=None))\n\ndef parse_dna_sequences(input_string: str) -&gt; Tuple[str, str]:\n    return tuple(input_string.strip().split(\"\\n\"))\n\n# Sample input\nSample_input = \"\"\"\nGGGCCGTTGGT\nGGACCGTTGAC\n\"\"\"\n\ndna_sequence1, dna_sequence2 = parse_dna_sequences(Sample_input)\nhamming_distance = calculate_hamming_distance(dna_sequence1, dna_sequence2)\nprint(hamming_distance)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-7",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-7",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "7.1 Sample Dataset",
    "text": "7.1 Sample Dataset\nATTCTGGA\nCGCCCGAATCCAGAACGCATTCCCATATTTCGGGACCACTGGCCTCCACGGTACGGACGTCAATCAAATGCCTAGCGGCTTGTGGTTTCTCCTACGCTCC\n3"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-7",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-7",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "7.2 Sample Output",
    "text": "7.2 Sample Output\n6 7 26 27 78"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-7",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-7",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "7.3 Solution",
    "text": "7.3 Solution\nfrom typing import Iterator, List\n\ndef generate_substrings(dna_sequence: str, substring_length: int) -&gt; Iterator[str]:\n    return (dna_sequence[i:i + substring_length] for i in range(len(dna_sequence) - substring_length + 1))\n\ndef calculate_hamming_distance(sequence1: str, sequence2: str) -&gt; int:\n    return sum(base1 != base2 for base1, base2 in zip(sequence1, sequence2))\n\ndef find_approximate_matches(pattern: str, genome: str, max_mismatch: int) -&gt; Iterator[int]:\n    pattern_length = len(pattern)\n    return (position for position, substring in enumerate(generate_substrings(genome, pattern_length))\n            if calculate_hamming_distance(substring, pattern) &lt;= max_mismatch)\n\ndef parse_input(input_data: str) -&gt; tuple[str, str, int]:\n    pattern, genome, max_mismatch_str = input_data.strip().split(\"\\n\")\n    return pattern, genome, int(max_mismatch_str)\n\nsample_input = \"\"\"\nATTCTGGA\nCGCCCGAATCCAGAACGCATTCCCATATTTCGGGACCACTGGCCTCCACGGTACGGACGTCAATCAAATGCCTAGCGGCTTGTGGTTTCTCCTACGCTCC\n3\n\"\"\"\n\npattern, genome, max_mismatch = parse_input(sample_input)\nmatch_positions = list(find_approximate_matches(pattern, genome, max_mismatch))\nprint(*match_positions)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-8",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-8",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "8.1 Sample Dataset",
    "text": "8.1 Sample Dataset\nACGTTGCATGTCGCATGATGCATGAGAGCT\n4 1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-8",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-8",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "8.2 Sample Output",
    "text": "8.2 Sample Output\nATGC ATGT GATG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-8",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-8",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "8.3 Solution",
    "text": "8.3 Solution\nfrom collections import defaultdict\nfrom itertools import product\nfrom typing import Dict, List, Iterator, Tuple\n\ndef calculate_hamming_distance(sequence1: str, sequence2: str) -&gt; int:\n    return sum(base1 != base2 for base1, base2 in zip(sequence1, sequence2))\n\ndef generate_substrings(dna_sequence: str, substring_length: int) -&gt; Iterator[str]:\n    return (dna_sequence[i:i + substring_length] for i in range(len(dna_sequence) - substring_length + 1))\n\ndef count_kmers(dna_sequence: str, kmer_length: int) -&gt; Dict[str, int]:\n    kmer_counts = defaultdict(int)\n    for kmer in generate_substrings(dna_sequence, kmer_length):\n        kmer_counts[kmer] += 1\n    return kmer_counts\n\ndef find_most_frequent(kmer_counts: Dict[str, int]) -&gt; List[str]:\n    max_count = max(kmer_counts.values())\n    return [kmer for kmer, count in kmer_counts.items() if count == max_count]\n\ndef generate_all_kmers(kmer_length: int) -&gt; Iterator[str]:\n    return (\"\".join(bases) for bases in product(\"ACGT\", repeat=kmer_length))\n\ndef count_approximate_kmers(observed_kmers: Dict[str, int], max_mismatches: int, kmer_length: int) -&gt; Iterator[Tuple[str, int]]:\n    for potential_kmer in generate_all_kmers(kmer_length):\n        count = sum(observed_kmers[observed_kmer] \n                    for observed_kmer in observed_kmers \n                    if calculate_hamming_distance(potential_kmer, observed_kmer) &lt;= max_mismatches)\n        if count &gt; 0:\n            yield (potential_kmer, count)\n\ndef parse_input(input_data: str) -&gt; Tuple[str, int, int]:\n    dna_sequence, params = input_data.strip().split(\"\\n\")\n    kmer_length, max_mismatches = map(int, params.split())\n    return dna_sequence, kmer_length, max_mismatches\n\nsample_input = \"\"\"\nACGTTGCATGTCGCATGATGCATGAGAGCT\n4 1\n\"\"\"\n\ndna_sequence, kmer_length, max_mismatches = parse_input(sample_input)\nobserved_kmers = count_kmers(dna_sequence, kmer_length)\napproximate_kmer_counts = dict(count_approximate_kmers(observed_kmers, max_mismatches, kmer_length))\nmost_frequent_kmers = find_most_frequent(approximate_kmer_counts)\nprint(*most_frequent_kmers)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-9",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-9",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "9.1 Sample Dataset",
    "text": "9.1 Sample Dataset\nACGTTGCATGTCGCATGATGCATGAGAGCT\n4 1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-9",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-9",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "9.2 Sample Output",
    "text": "9.2 Sample Output\nATGT ACAT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-9",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-9",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "9.3 Solution",
    "text": "9.3 Solution\nfrom collections import defaultdict\nfrom itertools import product\nfrom typing import Dict, List, Iterator, Tuple\n\ndef reverse_complement(dna: str) -&gt; str:\n    return dna[::-1].translate(str.maketrans(\"ACGT\", \"TGCA\"))\n\ndef hamming_distance(seq1: str, seq2: str) -&gt; int:\n    return sum(base1 != base2 for base1, base2 in zip(seq1, seq2))\n\ndef generate_substrings(dna: str, length: int) -&gt; Iterator[str]:\n    return (dna[i:i + length] for i in range(len(dna) - length + 1))\n\ndef count_kmers(dna: str, kmer_length: int) -&gt; Dict[str, int]:\n    kmer_counts = defaultdict(int)\n    for kmer in generate_substrings(dna, kmer_length):\n        kmer_counts[kmer] += 1\n    return kmer_counts\n\ndef find_most_frequent(kmer_counts: Dict[str, int]) -&gt; List[str]:\n    max_count = max(kmer_counts.values())\n    return [kmer for kmer, count in kmer_counts.items() if count == max_count]\n\ndef generate_all_kmers(kmer_length: int) -&gt; Iterator[str]:\n    return (\"\".join(bases) for bases in product(\"ACGT\", repeat=kmer_length))\n\ndef count_approximate_kmers(kmer_counts: Dict[str, int], max_mismatches: int, kmer_length: int) -&gt; Iterator[Tuple[str, int]]:\n    for potential_kmer in generate_all_kmers(kmer_length):\n        count = sum(kmer_counts[observed_kmer] for observed_kmer in kmer_counts \n                    if hamming_distance(potential_kmer, observed_kmer) &lt;= max_mismatches)\n        count += sum(kmer_counts[observed_kmer] for observed_kmer in kmer_counts \n                     if hamming_distance(reverse_complement(potential_kmer), observed_kmer) &lt;= max_mismatches)\n        if count &gt; 0:\n            yield (potential_kmer, count)\n\ndef parse_input(input_data: str) -&gt; Tuple[str, int, int]:\n    dna_sequence, params = input_data.strip().split(\"\\n\")\n    kmer_length, max_mismatches = map(int, params.split())\n    return dna_sequence, kmer_length, max_mismatches\n\nsample_input = \"\"\"\nACGTTGCATGTCGCATGATGCATGAGAGCT\n4 1\n\"\"\"\ndna_sequence, kmer_length, max_mismatches = parse_input(sample_input)\nkmer_counts = count_kmers(dna_sequence, kmer_length)\napproximate_kmer_counts = dict(count_approximate_kmers(kmer_counts, max_mismatches, kmer_length))\nprint(*find_most_frequent(approximate_kmer_counts))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-10",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-10",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "10.1 Sample Dataset",
    "text": "10.1 Sample Dataset\nACGCGGCTCTGAAA\n2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-10",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-10",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "10.2 Sample Output",
    "text": "10.2 Sample Output\n2 1 0 0 0 0 2 2 1 2 1 0 0 1 1 0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-10",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-10",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "10.3 Solution",
    "text": "10.3 Solution\nfrom typing import Iterator, List, Dict, Tuple\nfrom itertools import product\n\ndef generate_substrings(text: str, size: int) -&gt; Iterator[str]:\n    return (text[i : i + size] for i in range(len(text) - size + 1))\n\ndef count_pattern_occurrences(text: str, pattern: str) -&gt; int:\n    return sum(pattern == substring for substring in generate_substrings(text, len(pattern)))\n\ndef calculate_hamming_distance(s1: str, s2: str) -&gt; int:\n    return sum(c1 != c2 for c1, c2 in zip(s1, s2))\n\ndef generate_kmers(k: int) -&gt; Iterator[str]:\n    return (\"\".join(bases) for bases in product(\"ACGT\", repeat=k))\n\ndef count_approximate_kmers(kmer_counts: Dict[str, int], max_mismatches: int, kmer_length: int) -&gt; Iterator[Tuple[str, int]]:\n    for potential_kmer in generate_kmers(kmer_length):\n        count = sum(kmer_counts[observed_kmer] for observed_kmer in kmer_counts \n                    if calculate_hamming_distance(potential_kmer, observed_kmer) &lt;= max_mismatches)\n        if count &gt; 0:\n            yield (potential_kmer, count)\n\ndef calculate_kmer_frequencies(sequence: str, kmer_length: int) -&gt; List[int]:\n    return [count_pattern_occurrences(sequence, kmer) for kmer in generate_kmers(kmer_length)]\n\ndef parse_input(input_data: str) -&gt; Tuple[str, int]:\n    sequence, kmer_length = input_data.strip().split(\"\\n\")\n    return sequence, int(kmer_length)\n\nsample_input = \"\"\"\nACGCGGCTCTGAAA\n2\n\"\"\"\nsequence, kmer_length = parse_input(sample_input)\nkmer_frequencies = calculate_kmer_frequencies(sequence, kmer_length)\nprint(*kmer_frequencies"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-11",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-11",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "11.1 Sample Dataset",
    "text": "11.1 Sample Dataset\nAGT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-11",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-11",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "11.2 Sample Output",
    "text": "11.2 Sample Output\n11"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-11",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-11",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "11.3 Solution",
    "text": "11.3 Solution\nfrom typing import Dict, Tuple\n\ndef create_nucleotide_to_number_map() -&gt; Dict[str, int]:\n    return {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n\ndef convert_nucleotide_to_number(nucleotide: str, nucleotide_map: Dict[str, int]) -&gt; int:\n    return nucleotide_map[nucleotide]\n\ndef convert_dna_pattern_to_number(dna_pattern: str, nucleotide_map: Dict[str, int]) -&gt; int:\n    if not dna_pattern:\n        return 0\n    return 4 * convert_dna_pattern_to_number(dna_pattern[:-1], nucleotide_map) + convert_nucleotide_to_number(dna_pattern[-1], nucleotide_map)\n\ndef parse_input(input_data: str) -&gt; str:\n    return input_data.strip()\n\nsample_input = \"\"\"\nAGT\n\"\"\"\n\ndna_pattern = parse_input(sample_input)\nnucleotide_map = create_nucleotide_to_number_map()\nresult = convert_dna_pattern_to_number(dna_pattern, nucleotide_map)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-12",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-12",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "12.1 Sample Dataset",
    "text": "12.1 Sample Dataset\n45\n4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-12",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-12",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "12.2 Sample Output",
    "text": "12.2 Sample Output\nAGTC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-12",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-12",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "12.3 Solution",
    "text": "12.3 Solution\nfrom typing import Tuple\n\ndef number_to_nucleotide(index: int) -&gt; str:\n    nucleotides = [\"A\", \"C\", \"G\", \"T\"]\n    return nucleotides[index]\n\ndef number_to_dna_pattern(index: int, length: int) -&gt; str:\n    if length == 1:\n        return number_to_nucleotide(index)\n    quotient, remainder = divmod(index, 4)\n    return number_to_dna_pattern(quotient, length - 1) + number_to_nucleotide(remainder)\n\ndef parse_input(input_data: str) -&gt; Tuple[int, int]:\n    index_str, length_str = input_data.strip().split(\"\\n\")\n    return int(index_str), int(length_str)\n\nsample_input = \"\"\"\n45\n4\n\"\"\"\nindex, length = parse_input(sample_input)\ndna_pattern = number_to_dna_pattern(index, length)\nprint(dna_pattern)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-13",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-13",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "13.1 Sample Dataset",
    "text": "13.1 Sample Dataset\nACG\n1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-13",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-13",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "13.2 Sample Output",
    "text": "13.2 Sample Output\nCCG\nTCG\nGCG\nAAG\nATG\nAGG\nACA\nACC\nACT\nACG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-13",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-13",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "13.3 Solution",
    "text": "13.3 Solution\nfrom typing import Set, List, Tuple, Iterator\n\ndef calculate_hamming_distance(seq1: str, seq2: str) -&gt; int:\n    return sum(base1 != base2 for base1, base2 in zip(seq1, seq2))\n\ndef generate_immediate_neighbors(sequence: str) -&gt; Iterator[str]:\n    nucleotides = [\"A\", \"T\", \"G\", \"C\"]\n    for i, current_base in enumerate(sequence):\n        for new_base in nucleotides:\n            if new_base != current_base:\n                yield sequence[:i] + new_base + sequence[i + 1:]\n\ndef generate_neighbors(sequence: str, max_distance: int) -&gt; Set[str]:\n    nucleotides = [\"A\", \"T\", \"G\", \"C\"]\n    if max_distance == 0:\n        return {sequence}\n    if len(sequence) == 1:\n        return set(nucleotides)\n    \n    neighbors = set()\n    suffix_neighbors = generate_neighbors(sequence[1:], max_distance)\n    for suffix in suffix_neighbors:\n        if calculate_hamming_distance(sequence[1:], suffix) &lt; max_distance:\n            neighbors.update(base + suffix for base in nucleotides)\n        else:\n            neighbors.add(sequence[0] + suffix)\n    return neighbors\n\ndef parse_input(input_data: str) -&gt; Tuple[str, int]:\n    sequence, distance = input_data.strip().split(\"\\n\")\n    return sequence, int(distance)\n\nsample_input = \"\"\"\nACG\n1\n\"\"\"\n\nsequence, max_distance = parse_input(sample_input)\nneighbor_sequences = generate_neighbors(sequence, max_distance)\nprint(*sorted(neighbor_sequences), sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-14",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-14",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "14.1 Sample Dataset",
    "text": "14.1 Sample Dataset\n3 1\nATTTGGC\nTGCCTTA\nCGGTATC\nGAAAATT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-14",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-14",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "14.2 Sample Output",
    "text": "14.2 Sample Output\nATA ATT GTT TTT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-14",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-14",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "14.3 Solution",
    "text": "14.3 Solution\nfrom typing import List, Set, Iterator\n\ndef calculate_hamming_distance(sequence1: str, sequence2: str) -&gt; int:\n    return sum(c1 != c2 for c1, c2 in zip(sequence1, sequence2))\n\ndef generate_neighbors(sequence: str, max_distance: int) -&gt; Set[str]:\n    nucleotides = [\"A\", \"T\", \"G\", \"C\"]\n    if max_distance == 0:\n        return {sequence}\n    if len(sequence) == 1:\n        return set(nucleotides)\n    \n    neighbor_set = set()\n    for neighbor in generate_neighbors(sequence[1:], max_distance):\n        if calculate_hamming_distance(sequence[1:], neighbor) &lt; max_distance:\n            for nucleotide in nucleotides:\n                neighbor_set.add(nucleotide + neighbor)\n        else:\n            neighbor_set.add(sequence[0] + neighbor)\n    return neighbor_set\n\ndef generate_substrings(text: str, substring_length: int) -&gt; Iterator[str]:\n    for i in range(len(text) - substring_length + 1):\n        yield text[i : i + substring_length]\n\ndef get_all_kmers(dna_sequences: List[str], kmer_length: int) -&gt; Set[str]:\n    return set(kmer for sequence in dna_sequences for kmer in generate_substrings(sequence, kmer_length))\n\ndef contains_approximate_match(pattern: str, text: str, max_distance: int) -&gt; bool:\n    return any(calculate_hamming_distance(substring, pattern) &lt;= max_distance \n               for substring in generate_substrings(text, len(pattern)))\n\ndef enumerate_motifs(dna_sequences: List[str], kmer_length: int, max_distance: int) -&gt; Set[str]:\n    motif_patterns = set()\n    for kmer in get_all_kmers(dna_sequences, kmer_length):\n        for neighbor_kmer in generate_neighbors(kmer, max_distance):\n            if all(contains_approximate_match(neighbor_kmer, sequence, max_distance) for sequence in dna_sequences):\n                motif_patterns.add(neighbor_kmer)\n    return motif_patterns\n\n# Sample input\nsample_input = \"\"\"\n3 1\nATTTGGC\nTGCCTTA\nCGGTATC\nGAAAATT\n\"\"\"\n\ninput_params, *dna_sequences = sample_input.strip().split(\"\\n\")\nkmer_length, max_distance = map(int, input_params.split())\nprint(*sorted(enumerate_motifs(dna_sequences, kmer_length, max_distance)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-15",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-15",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "15.1 Sample Dataset",
    "text": "15.1 Sample Dataset\n3\nAAATTGACGCAT\nGACGACCACGTT\nCGTCAGCGCCTG\nGCTGAGCACCGG\nAGTACGGGACAG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-15",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-15",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "15.2 Sample Output",
    "text": "15.2 Sample Output\nACG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-15",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-15",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "15.3 Solution",
    "text": "15.3 Solution\nfrom typing import Iterator, List\nfrom itertools import product\nimport math\n\ndef generate_substrings(text: str, substring_length: int) -&gt; Iterator[str]:\n    for i in range(len(text) - substring_length + 1):\n        yield text[i : i + substring_length]\n\ndef generate_kmers(kmer_length: int) -&gt; Iterator[str]:\n    return (\"\".join(nucleotides) for nucleotides in product(\"ACGT\", repeat=kmer_length))\n\ndef calculate_hamming_distance(sequence1: str, sequence2: str) -&gt; int:\n    return sum(nucleotide1 != nucleotide2 for nucleotide1, nucleotide2 in zip(sequence1, sequence2))\n\ndef find_minimum_distance(pattern: str, text: str) -&gt; int:\n    return min(calculate_hamming_distance(substring, pattern) for substring in generate_substrings(text, len(pattern)))\n\ndef calculate_total_distance(pattern: str, dna_sequences: List[str]) -&gt; int:\n    return sum(find_minimum_distance(pattern, sequence) for sequence in dna_sequences)\n\ndef find_median_string(dna_sequences: List[str], kmer_length: int) -&gt; str:\n    min_distance = math.inf\n    median_kmer = \"\"\n    \n    for kmer in generate_kmers(kmer_length):\n        current_distance = calculate_total_distance(kmer, dna_sequences)\n        if current_distance &lt; min_distance:\n            min_distance = current_distance\n            median_kmer = kmer\n    \n    return median_kmer\n\n# Sample input\nsample_input = \"\"\"\n3\nAAATTGACGCAT\nGACGACCACGTT\nCGTCAGCGCCTG\nGCTGAGCACCGG\nAGTACGGGACAG\n\"\"\"\n\nkmer_length, *dna_sequences = sample_input.strip().split(\"\\n\")\nresult = find_median_string(dna_sequences, int(kmer_length))\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-16",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-16",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "16.1 Sample Dataset",
    "text": "16.1 Sample Dataset\nACCTGTTTATTGCCTAAGTTCCGAACAAACCCAATATAGCCCGAGGGCCT\n5\n0.2 0.2 0.3 0.2 0.3\n0.4 0.3 0.1 0.5 0.1\n0.3 0.3 0.5 0.2 0.4\n0.1 0.2 0.1 0.1 0.2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-16",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-16",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "16.2 Sample Output",
    "text": "16.2 Sample Output\nCCGAG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-16",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-16",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "16.3 Solution",
    "text": "16.3 Solution\nfrom typing import Iterator, List\nimport math\n\ndef generate_substrings(text: str, substring_length: int) -&gt; Iterator[str]:\n    for i in range(len(text) - substring_length + 1):\n        yield text[i : i + substring_length]\n\ndef find_profile_most_probable_kmer(sequence: str, kmer_length: int, profile_matrix: List[List[float]]) -&gt; str:\n    nucleotide_index = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n    max_probability = -1\n    most_probable_kmer = \"\"\n\n    for kmer in generate_substrings(sequence, kmer_length):\n        kmer_probability = math.prod(profile_matrix[nucleotide_index[kmer[j]]][j] for j in range(kmer_length))\n        if kmer_probability &gt; max_probability:\n            max_probability = kmer_probability\n            most_probable_kmer = kmer\n\n    return most_probable_kmer\n\n# Sample input\nsample_input = \"\"\"\nACCTGTTTATTGCCTAAGTTCCGAACAAACCCAATATAGCCCGAGGGCCT\n5\n0.2 0.2 0.3 0.2 0.3\n0.4 0.3 0.1 0.5 0.1\n0.3 0.3 0.5 0.2 0.4\n0.1 0.2 0.1 0.1 0.2\n\"\"\"\n\ndna_sequence, kmer_length, *profile_rows = sample_input.strip().split(\"\\n\")\nprofile_matrix = [list(map(float, row.split())) for row in profile_rows]\nresult = find_profile_most_probable_kmer(dna_sequence, int(kmer_length), profile_matrix)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-17",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-17",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "17.1 Sample Dataset",
    "text": "17.1 Sample Dataset\n3 5\nGGCGTTCAGGCA\nAAGAATCAGTCA\nCAAGGAGTTCGC\nCACGTCAATCAC\nCAATAATATTCG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-17",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-17",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "17.2 Sample Output",
    "text": "17.2 Sample Output\nCAG\nCAG\nCAA\nCAA\nCAA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-17",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-17",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "17.3 Solution",
    "text": "17.3 Solution\nfrom typing import Iterator, List, Dict\nfrom collections import Counter\nimport math\n\ndef generate_kmers(sequence: str, kmer_length: int) -&gt; Iterator[str]:\n    for i in range(len(sequence) - kmer_length + 1):\n        yield sequence[i : i + kmer_length]\n\ndef find_most_probable_kmer(sequence: str, kmer_length: int, profile: List[List[float]]) -&gt; str:\n    nucleotide_to_index: Dict[str, int] = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n    max_probability: float = -1\n    most_probable_kmer: str = \"\"\n\n    for kmer in generate_kmers(sequence, kmer_length):\n        kmer_probability: float = math.prod(profile[nucleotide_to_index[kmer[j]]][j] for j in range(kmer_length))\n        if kmer_probability &gt; max_probability:\n            max_probability = kmer_probability\n            most_probable_kmer = kmer\n\n    return most_probable_kmer\n\ndef create_profile(sequences: List[str], pseudocount: int = 0) -&gt; List[List[float]]:\n    nucleotides: List[str] = [\"A\", \"C\", \"G\", \"T\"]\n    profile: List[List[float]] = [[] for _ in nucleotides]\n    for i, nucleotide in enumerate(nucleotides):\n        profile[i] = [\n            (sum(seq[j] == nucleotide for seq in sequences) + pseudocount) / len(sequences)\n            for j in range(len(sequences[0]))\n        ]\n    return profile\n\ndef calculate_score(motifs: List[str]) -&gt; int:\n    score: int = 0\n    for i in range(len(motifs[0])):\n        column: List[str] = [motif[i] for motif in motifs]\n        most_common: str = Counter(column).most_common()[0][0]\n        score += sum(nucleotide != most_common for nucleotide in column)\n    return score\n\ndef greedy_motif_search(dna_sequences: List[str], kmer_length: int, pseudocount: int = 0) -&gt; List[str]:\n    best_motifs: List[str] = [seq[:kmer_length] for seq in dna_sequences]\n    for kmer in generate_kmers(dna_sequences[0], kmer_length):\n        current_motifs: List[str] = [kmer]\n        for i in range(1, len(dna_sequences)):\n            current_profile: List[List[float]] = create_profile(current_motifs, pseudocount=pseudocount)\n            current_motifs.append(find_most_probable_kmer(dna_sequences[i], kmer_length, current_profile))\n        if calculate_score(current_motifs) &lt; calculate_score(best_motifs):\n            best_motifs = current_motifs\n    return best_motifs\n\n# Sample input\nsample_input: str = \"\"\"\n3 5\nGGCGTTCAGGCA\nAAGAATCAGTCA\nCAAGGAGTTCGC\nCACGTCAATCAC\nCAATAATATTCG\n\"\"\"\n\nints, *dna = sample_input.strip().split(\"\\n\")\nk, t = map(int, ints.split())\nresult: List[str] = greedy_motif_search(dna, k)\nprint(*result, sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-18",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-18",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "18.1 Sample Dataset",
    "text": "18.1 Sample Dataset\n3 5\nGGCGTTCAGGCA\nAAGAATCAGTCA\nCAAGGAGTTCGC\nCACGTCAATCAC\nCAATAATATTCG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-18",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-18",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "18.2 Sample Output",
    "text": "18.2 Sample Output\nTTC\nATC\nTTC\nATC\nTTC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-18",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-18",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "18.3 Solution",
    "text": "18.3 Solution\nfrom typing import Iterator, List, Dict\nfrom collections import Counter\nimport math\n\ndef generate_kmers(sequence: str, kmer_length: int) -&gt; Iterator[str]:\n    for i in range(len(sequence) - kmer_length + 1):\n        yield sequence[i : i + kmer_length]\n\ndef find_most_probable_kmer(sequence: str, kmer_length: int, profile: List[List[float]]) -&gt; str:\n    nucleotide_to_index: Dict[str, int] = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n    return max(\n        generate_kmers(sequence, kmer_length),\n        key=lambda kmer: math.prod(profile[nucleotide_to_index[nucleotide]][position] \n                                   for position, nucleotide in enumerate(kmer))\n    )\n\ndef create_profile(motifs: List[str], pseudocount: int = 0) -&gt; List[List[float]]:\n    nucleotides: List[str] = [\"A\", \"C\", \"G\", \"T\"]\n    motif_count: int = len(motifs)\n    motif_length: int = len(motifs[0])\n    \n    return [\n        [(sum(motif[position] == nucleotide for motif in motifs) + pseudocount) / motif_count \n         for position in range(motif_length)]\n        for nucleotide in nucleotides\n    ]\n\ndef calculate_score(motifs: List[str]) -&gt; int:\n    return sum(\n        sum(nucleotide != Counter(column).most_common(1)[0][0] for nucleotide in column)\n        for column in zip(*motifs)\n    )\n\ndef greedy_motif_search(dna_sequences: List[str], kmer_length: int, pseudocount: int = 0) -&gt; List[str]:\n    best_motifs: List[str] = [sequence[:kmer_length] for sequence in dna_sequences]\n    \n    for kmer in generate_kmers(dna_sequences[0], kmer_length):\n        current_motifs: List[str] = [kmer]\n        for sequence in dna_sequences[1:]:\n            profile: List[List[float]] = create_profile(current_motifs, pseudocount)\n            current_motifs.append(find_most_probable_kmer(sequence, kmer_length, profile))\n        \n        if calculate_score(current_motifs) &lt; calculate_score(best_motifs):\n            best_motifs = current_motifs\n    \n    return best_motifs\n\n# Sample input\nsample_input: str = \"\"\"\n3 5\nGGCGTTCAGGCA\nAAGAATCAGTCA\nCAAGGAGTTCGC\nCACGTCAATCAC\nCAATAATATTCG\n\"\"\"\n\nk_value, _, *dna_sequences = sample_input.strip().split()\nk_value = int(k_value)\nresult: List[str] = greedy_motif_search(dna_sequences, k_value, pseudocount=1)\nprint(*result, sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-19",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-19",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "19.1 Sample Dataset",
    "text": "19.1 Sample Dataset\n8 5\nCGCCCCTCTCGGGGGTGTTCAGTAAACGGCCA\nGGGCGAGGTATGTGTAAGTGCCAAGGTGCCAG\nTAGTACCGAGACCGAAAGAAGTATACAGGCGT\nTAGATCAAGTTTCAGGTGCACGTCGGTGAACC\nAATCCACCAGCTCCACGTGCAATGTTGGCCTA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-19",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-19",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "19.2 Sample Output",
    "text": "19.2 Sample Output\nAACGGCCA\nAAGTGCCA\nTAGTACCG\nAAGTTTCA\nACGTGCAA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-19",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-19",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "19.3 Solution",
    "text": "19.3 Solution\nfrom typing import List, Tuple, Callable\nfrom collections import Counter\nfrom random import randint\nimport math\n\ndef generate_kmers(sequence: str, kmer_length: int) -&gt; List[str]:\n    return [sequence[i:i+kmer_length] for i in range(len(sequence) - kmer_length + 1)]\n\ndef find_most_probable_kmer(sequence: str, kmer_length: int, profile: List[List[float]]) -&gt; str:\n    nucleotide_to_index = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n    return max(\n        generate_kmers(sequence, kmer_length),\n        key=lambda kmer: math.prod(profile[nucleotide_to_index[nucleotide]][j] for j, nucleotide in enumerate(kmer))\n    )\n\ndef create_profile(motifs: List[str], pseudocount: int = 0) -&gt; List[List[float]]:\n    nucleotides = [\"A\", \"C\", \"G\", \"T\"]\n    profile = []\n    for nucleotide in nucleotides:\n        profile.append([\n            (sum(seq[j] == nucleotide for seq in motifs) + pseudocount) / (len(motifs) + 4 * pseudocount)\n            for j in range(len(motifs[0]))\n        ])\n    return profile\n\ndef calculate_score(motifs: List[str]) -&gt; int:\n    return sum(\n        sum(nucleotide != Counter(column).most_common(1)[0][0] for nucleotide in column)\n        for column in zip(*motifs)\n    )\n\ndef generate_random_kmer(sequence: str, kmer_length: int) -&gt; str:\n    start = randint(0, len(sequence) - kmer_length)\n    return sequence[start : start + kmer_length]\n\ndef find_motifs(profile: List[List[float]], dna_sequences: List[str]) -&gt; List[str]:\n    kmer_length = len(profile[0])\n    return [find_most_probable_kmer(seq, kmer_length, profile) for seq in dna_sequences]\n\ndef randomized_motif_search(dna_sequences: List[str], kmer_length: int) -&gt; Tuple[int, List[str]]:\n    motifs = [generate_random_kmer(seq, kmer_length) for seq in dna_sequences]\n    best_score = math.inf\n    \n    while True:\n        profile = create_profile(motifs, pseudocount=1)\n        motifs = find_motifs(profile, dna_sequences)\n        current_score = calculate_score(motifs)\n        \n        if current_score &gt;= best_score:\n            return best_score, motifs\n        \n        best_score = current_score\n\ndef find_best_motifs(search_function: Callable, iterations: int, *args) -&gt; List[str]:\n    best_score, best_motifs = search_function(*args)\n    \n    for _ in range(iterations - 1):\n        score, motifs = search_function(*args)\n        if score &lt; best_score:\n            best_score, best_motifs = score, motifs\n    \n    return best_motifs\n\n# Sample input\nsample_input = \"\"\"\n8 5\nCGCCCCTCTCGGGGGTGTTCAGTAAACGGCCA\nGGGCGAGGTATGTGTAAGTGCCAAGGTGCCAG\nTAGTACCGAGACCGAAAGAAGTATACAGGCGT\nTAGATCAAGTTTCAGGTGCACGTCGGTGAACC\nAATCCACCAGCTCCACGTGCAATGTTGGCCTA\n\"\"\"\n\nkmer_length, _, *dna_sequences = sample_input.strip().split()\nkmer_length = int(kmer_length)\n\nresult = find_best_motifs(randomized_motif_search, 1000, dna_sequences, kmer_length)\nprint(*result, sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-20",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-20",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "20.1 Sample Dataset",
    "text": "20.1 Sample Dataset\n8 5 100\nCGCCCCTCTCGGGGGTGTTCAGTAAACGGCCA\nGGGCGAGGTATGTGTAAGTGCCAAGGTGCCAG\nTAGTACCGAGACCGAAAGAAGTATACAGGCGT\nTAGATCAAGTTTCAGGTGCACGTCGGTGAACC\nAATCCACCAGCTCCACGTGCAATGTTGGCCTA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-20",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-20",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "20.2 Sample Output",
    "text": "20.2 Sample Output\nTCTCGGGG\nCCAAGGTG\nTACAGGCG\nTTCAGGTG\nTCCACGTG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-20",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-20",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "20.3 Solution",
    "text": "20.3 Solution\nfrom typing import List, Iterator, Tuple, Callable\nfrom collections import Counter\nimport math\nimport random\n\ndef generate_kmers(sequence: str, kmer_length: int) -&gt; Iterator[str]:\n    for i in range(len(sequence) - kmer_length + 1):\n        yield sequence[i : i + kmer_length]\n\ndef create_profile(motifs: List[str], pseudocount: int = 0) -&gt; List[List[float]]:\n    nucleotides: List[str] = [\"A\", \"C\", \"G\", \"T\"]\n    motif_count: int = len(motifs)\n    motif_length: int = len(motifs[0])\n    \n    return [\n        [(sum(motif[position] == nucleotide for motif in motifs) + pseudocount) / (motif_count + 4*pseudocount)\n         for position in range(motif_length)]\n        for nucleotide in nucleotides\n    ]\n\ndef calculate_score(motifs: List[str]) -&gt; int:\n    return sum(\n        sum(nucleotide != Counter(column).most_common(1)[0][0] for nucleotide in column)\n        for column in zip(*motifs)\n    )\n\ndef generate_random_kmer(sequence: str, kmer_length: int) -&gt; str:\n    start_index = random.randint(0, len(sequence) - kmer_length)\n    return sequence[start_index : start_index + kmer_length]\n\ndef find_best_motifs(search_function: Callable, iterations: int, *args) -&gt; List[str]:\n    best_score, best_motifs = search_function(*args)\n    for _ in range(iterations - 1):\n        score, motifs = search_function(*args)\n        if score &lt; best_score:\n            best_score, best_motifs = score, motifs\n    return best_motifs\n\ndef calculate_kmer_probabilities(sequence: str, kmer_length: int, profile: List[List[float]]) -&gt; List[float]:\n    nucleotide_to_index: Dict[str, int] = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n    return [\n        math.prod(profile[nucleotide_to_index[kmer[j]]][j] for j in range(kmer_length))\n        for kmer in generate_kmers(sequence, kmer_length)\n    ]\n\ndef select_random_kmer(sequence: str, kmer_length: int, profile: List[List[float]]) -&gt; str:\n    probabilities = calculate_kmer_probabilities(sequence, kmer_length, profile)\n    start_index = random.choices(range(len(probabilities)), probabilities)[0]\n    return sequence[start_index : start_index + kmer_length]\n\ndef gibbs_sampler(dna_sequences: List[str], kmer_length: int, num_iterations: int) -&gt; Tuple[int, List[str]]:\n    motifs = [generate_random_kmer(seq, kmer_length) for seq in dna_sequences]\n    best_motifs = motifs.copy()\n    for _ in range(num_iterations):\n        i = random.randint(0, len(dna_sequences) - 1)\n        profile = create_profile(motifs[:i] + motifs[i + 1 :], pseudocount=1)\n        motifs[i] = select_random_kmer(dna_sequences[i], kmer_length, profile)\n        if calculate_score(motifs) &lt; calculate_score(best_motifs):\n            best_motifs = motifs.copy()\n    return calculate_score(best_motifs), best_motifs\n\n# Sample input\nsample_input = \"\"\"\n8 5 100\nTCTCGGGG\nCCAAGGTG\nTACAGGCG\nTTCAGGTG\nTCCACGTG\n\"\"\"\n\nkmer_length, num_sequences, num_iterations, *dna_sequences = sample_input.strip().split()\nkmer_length = int(kmer_length)\nnum_iterations = int(num_iterations)\nresult = find_best_motifs(gibbs_sampler, 20, dna_sequences, kmer_length, num_iterations)\nprint(*result, sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-21",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-21",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "21.1 Sample Dataset",
    "text": "21.1 Sample Dataset\nAAA\nTTACCTTAAC GATATCTGTC ACGGCGTTCG CCCTAAAGAG CGTCAGAGGT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-21",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-21",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "21.2 Sample Output",
    "text": "21.2 Sample Output\n5"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-21",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-21",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "21.3 Solution",
    "text": "21.3 Solution\nfrom typing import List, Iterator\nimport math\n\ndef generate_kmers(sequence: str, kmer_length: int) -&gt; Iterator[str]:\n    for i in range(len(sequence) - kmer_length + 1):\n        yield sequence[i : i + kmer_length]\n\ndef calculate_hamming_distance(seq1: str, seq2: str) -&gt; int:\n    return sum(base1 != base2 for base1, base2 in zip(seq1, seq2))\n\ndef find_minimum_distance(pattern: str, text: str) -&gt; int:\n    return min(calculate_hamming_distance(kmer, pattern) for kmer in generate_kmers(text, len(pattern)))\n\ndef calculate_pattern_distance_to_strings(pattern: str, dna_strings: List[str]) -&gt; int:\n    return sum(find_minimum_distance(pattern, dna_string) for dna_string in dna_strings)\n\n# Sample input\nsample_input = \"\"\"\nAAA\nTTACCTTAAC GATATCTGTC ACGGCGTTCG CCCTAAAGAG CGTCAGAGGT\n\"\"\"\n\npattern, dna_strings_raw = sample_input.strip().split(\"\\n\")\ndna_strings = dna_strings_raw.split()\n\nresult = calculate_pattern_distance_to_strings(pattern, dna_strings)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-22",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-22",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "22.1 Sample Dataset",
    "text": "22.1 Sample Dataset\n5\nCAATCCAAC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-22",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-22",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "22.2 Sample Output",
    "text": "22.2 Sample Output\nCAATC\nAATCC\nATCCA\nTCCAA\nCCAAC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-22",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-22",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "22.3 Solution",
    "text": "22.3 Solution\nfrom typing import Iterator\n\ndef generate_kmers(sequence: str, kmer_length: int) -&gt; Iterator[str]:\n    for i in range(len(sequence) - kmer_length + 1):\n        yield sequence[i:i + kmer_length]\n\nsample_input: str = \"\"\"\n5\nCAATCCAAC\n\"\"\"\n\ninput_lines: list[str] = sample_input.strip().split(\"\\n\")\nkmer_length: int = int(input_lines[0])\ndna_sequence: str = input_lines[1]\n\nfor kmer in generate_kmers(dna_sequence, kmer_length):\n    print(kmer)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-23",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-23",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "23.1 Sample Dataset",
    "text": "23.1 Sample Dataset\nACCGA\nCCGAA\nCGAAG\nGAAGC\nAAGCT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-23",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-23",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "23.2 Sample Output",
    "text": "23.2 Sample Output\nACCGAAGCT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-23",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-23",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "23.3 Solution",
    "text": "23.3 Solution\nfrom typing import List\n\ndef reconstruct_dna_sequence(kmers: List[str]) -&gt; str:\n    reconstructed_sequence: str = kmers[0]\n    for i in range(1, len(kmers)):\n        reconstructed_sequence += kmers[i][-1]\n    return reconstructed_sequence\n\nsample_input: str = \"\"\"\nACCGA\nCCGAA\nCGAAG\nGAAGC\nAAGCT\n\"\"\"\n\nkmer_list: List[str] = sample_input.strip().split(\"\\n\")\nprint(reconstruct_dna_sequence(kmer_list))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-24",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-24",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "24.1 Sample Dataset",
    "text": "24.1 Sample Dataset\nATGCG\nGCATG\nCATGC\nAGGCA\nGGCAT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-24",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-24",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "24.2 Sample Output",
    "text": "24.2 Sample Output\nGCATG -&gt; CATGC\nCATGC -&gt; ATGCG\nAGGCA -&gt; GGCAT\nGGCAT -&gt; GCATG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-24",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-24",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "24.3 Solution",
    "text": "24.3 Solution\nfrom typing import List\n\ndef overlap_graph(patterns: List[str]) -&gt; List[tuple[str, str]]:\n    adj_list = []\n    for i in range(len(patterns)):\n        for j in range(len(patterns)):\n            if i != j and patterns[i][1:] == patterns[j][:-1]:\n                adj_list.append((patterns[i], patterns[j]))\n    return adj_list\n\nsample_input = \"\"\"\nATGCG\nGCATG\nCATGC\nAGGCA\nGGCAT\n\"\"\"\n\nPatterns: List[str] = sample_input.strip().split(\"\\n\")\n\nadj_list = overlap_graph(Patterns)\nfor edge in adj_list:\n    print(f\"{edge[0]} -&gt; {edge[1]}\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-25",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-25",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "25.1 Sample Dataset",
    "text": "25.1 Sample Dataset\n4\nAAGATTCTCTAC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-25",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-25",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "25.2 Sample Output",
    "text": "25.2 Sample Output\nAAG -&gt; AGA\nAGA -&gt; GAT\nATT -&gt; TTC\nCTA -&gt; TAC\nCTC -&gt; TCT\nGAT -&gt; ATT\nTCT -&gt; CTA,CTC\nTTC -&gt; TCT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-25",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-25",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "25.3 Solution",
    "text": "25.3 Solution\nfrom collections import OrderedDict\nfrom typing import List, Set, OrderedDict as OrderedDictType\n\ndef construct_de_bruijn_graph(sequence: str, kmer_length: int) -&gt; OrderedDictType[str, Set[str]]:\n    adjacency_list: OrderedDictType[str, Set[str]] = OrderedDict()\n    \n    for i in range(len(sequence) - kmer_length + 2):\n        adjacency_list[sequence[i:i + kmer_length - 1]] = set()\n\n    for i in range(len(sequence) - kmer_length + 1):\n        prefix = sequence[i:i + kmer_length - 1]\n        suffix = sequence[i + 1:i + kmer_length]\n        adjacency_list[prefix].add(suffix)\n\n    return adjacency_list\n\nsample_input: str = \"\"\"\n4\nAAGATTCTCTAC\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nkmer_length: int = int(input_lines[0])\ndna_sequence: str = input_lines[1]\n\nadjacency_list = construct_de_bruijn_graph(dna_sequence, kmer_length)\nfor node, neighbors in adjacency_list.items():\n    if neighbors:\n        print(f\"{node} -&gt; {','.join(neighbors)}\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-26",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-26",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "26.1 Sample Dataset",
    "text": "26.1 Sample Dataset\nGAGG\nCAGG\nGGGG\nGGGA\nCAGG\nAGGG\nGGAG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-26",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-26",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "26.2 Sample Output",
    "text": "26.2 Sample Output\nGAG -&gt; AGG\nCAG -&gt; AGG,AGG\nGGG -&gt; GGG,GGA\nAGG -&gt; GGG\nGGA -&gt; GAG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-26",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-26",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "26.3 Solution",
    "text": "26.3 Solution\nfrom typing import List, Dict\n\ndef construct_de_bruijn_graph(kmers: List[str]) -&gt; Dict[str, List[str]]:\n    adjacency_list: Dict[str, List[str]] = {}\n    for kmer in kmers:\n        prefix = kmer[:-1]\n        suffix = kmer[1:]\n        if prefix not in adjacency_list:\n            adjacency_list[prefix] = [suffix]\n        else:\n            adjacency_list[prefix].append(suffix)\n    return adjacency_list\n\nsample_input: str = \"\"\"\nGAGG\nCAGG\nGGGG\nGGGA\nCAGG\nAGGG\nGGAG\n\"\"\"\n\nkmer_list: List[str] = sample_input.strip().split(\"\\n\")\n\nadjacency_list = construct_de_bruijn_graph(kmer_list)\nfor node, neighbors in adjacency_list.items():\n    print(f\"{node} -&gt; {','.join(neighbors)}\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-27",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-27",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "27.1 Sample Dataset",
    "text": "27.1 Sample Dataset\n0 -&gt; 3\n1 -&gt; 0\n2 -&gt; 1,6\n3 -&gt; 2\n4 -&gt; 2\n5 -&gt; 4\n6 -&gt; 5,8\n7 -&gt; 9\n8 -&gt; 7\n9 -&gt; 6"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-27",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-27",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "27.2 Sample Output",
    "text": "27.2 Sample Output\n6-&gt;8-&gt;7-&gt;9-&gt;6-&gt;5-&gt;4-&gt;2-&gt;1-&gt;0-&gt;3-&gt;2-&gt;6"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-27",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-27",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "27.3 Solution",
    "text": "27.3 Solution\nfrom re import split\nfrom random import choice\nfrom typing import Dict, List, Tuple\n\ndef parse_adjacency_list(adjacency_list_text: List[str]) -&gt; Dict[str, List[str]]:\n    adjacency_dict: Dict[str, List[str]] = {}\n    for element in adjacency_list_text:\n        node, neighbors = split(' -&gt; ', element)\n        adjacency_dict[node] = neighbors.split(',')\n    return adjacency_dict\n\ndef remove_edge(graph: Dict[str, List[str]], source: str, target: str) -&gt; Dict[str, List[str]]:\n    graph[source].remove(target)\n    if not graph[source]:\n        del graph[source]\n    return graph\n\ndef find_eulerian_cycle(graph: Dict[str, List[str]]) -&gt; List[str]:\n    # Form a cycle by randomly walking in the graph\n    start_node, edges = choice(list(graph.items()))\n    next_node = choice(edges)\n    graph = remove_edge(graph, start_node, next_node)\n\n    cycle: List[str] = [start_node, next_node]\n    current_node = next_node\n    while current_node != start_node:\n        edges = graph[current_node]\n        next_node = choice(edges)\n        graph = remove_edge(graph, current_node, next_node)\n        current_node = next_node\n        cycle.append(current_node)\n\n    while graph:\n        potential_starts: List[Tuple[int, str]] = [(idx, node) for idx, node in enumerate(cycle) if node in graph]\n        idx, new_start = choice(potential_starts)\n\n        # Form new_cycle by traversing cycle (starting at new_start) and then randomly walking\n        new_cycle = cycle[idx:] + cycle[1:idx + 1]\n\n        next_node = choice(graph[new_start])\n        graph = remove_edge(graph, new_start, next_node)\n        current_node = next_node\n        new_cycle.append(current_node)\n        while current_node != new_start:\n            edges = graph[current_node]\n            next_node = choice(edges)\n            graph = remove_edge(graph, current_node, next_node)\n            current_node = next_node\n            new_cycle.append(current_node)\n        cycle = new_cycle\n    return cycle\n\nsample_input: str = \"\"\"\n0 -&gt; 3\n1 -&gt; 0\n2 -&gt; 1,6\n3 -&gt; 2\n4 -&gt; 2\n5 -&gt; 4\n6 -&gt; 5,8\n7 -&gt; 9\n8 -&gt; 7\n9 -&gt; 6\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nadjacency_list = parse_adjacency_list(input_lines)\n\nprint(\"-&gt;\".join(find_eulerian_cycle(adjacency_list)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-28",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-28",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "28.1 Sample Dataset",
    "text": "28.1 Sample Dataset\n0 -&gt; 2\n1 -&gt; 3\n2 -&gt; 1\n3 -&gt; 0,4\n6 -&gt; 3,7\n7 -&gt; 8\n8 -&gt; 9\n9 -&gt; 6"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-28",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-28",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "28.2 Sample Output",
    "text": "28.2 Sample Output\n6-&gt;7-&gt;8-&gt;9-&gt;6-&gt;3-&gt;0-&gt;2-&gt;1-&gt;3-&gt;4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-28",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-28",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "28.3 Solution",
    "text": "28.3 Solution\nfrom re import split\nfrom random import choice\nfrom typing import Dict, List, Tuple\n\ndef parse_adjacency_list(adjacency_text: List[str]) -&gt; Dict[str, List[str]]:\n    adjacency_dict: Dict[str, List[str]] = {}\n    for line in adjacency_text:\n        node, neighbors = split(' -&gt; ', line)\n        adjacency_dict[node] = neighbors.split(',')\n    return adjacency_dict\n\ndef remove_edge(graph: Dict[str, List[str]], source: str, target: str) -&gt; Dict[str, List[str]]:\n    graph[source].remove(target)\n    if not graph[source]:\n        del graph[source]\n    return graph\n\ndef find_eulerian_cycle(graph: Dict[str, List[str]]) -&gt; List[str]:\n    start_node, edges = choice(list(graph.items()))\n    next_node = choice(edges)\n    graph = remove_edge(graph, start_node, next_node)\n\n    cycle: List[str] = [start_node, next_node]\n    current_node = next_node\n    while current_node != start_node:\n        edges = graph[current_node]\n        next_node = choice(edges)\n        graph = remove_edge(graph, current_node, next_node)\n        current_node = next_node\n        cycle.append(current_node)\n\n    while graph:\n        potential_starts: List[Tuple[int, str]] = [(idx, node) for idx, node in enumerate(cycle) if node in graph]\n        idx, new_start = choice(potential_starts)\n\n        new_cycle = cycle[idx:] + cycle[1:idx + 1]\n\n        next_node = choice(graph[new_start])\n        graph = remove_edge(graph, new_start, next_node)\n        current_node = next_node\n        new_cycle.append(current_node)\n        while current_node != new_start:\n            edges = graph[current_node]\n            next_node = choice(edges)\n            graph = remove_edge(graph, current_node, next_node)\n            current_node = next_node\n            new_cycle.append(current_node)\n        cycle = new_cycle\n    return cycle\n\ndef find_eulerian_path(graph: Dict[str, List[str]]) -&gt; List[str]:\n    degree_differences: Dict[str, int] = {}\n    for source, targets in graph.items():\n        degree_differences[source] = degree_differences.get(source, 0) + len(targets)\n        for target in targets:\n            degree_differences[target] = degree_differences.get(target, 0) - 1\n\n    start_node = [node for node, diff in degree_differences.items() if diff == -1][0]\n    end_node = [node for node, diff in degree_differences.items() if diff == 1][0]\n    \n    if start_node in graph:\n        graph[start_node].append(end_node)\n    else:\n        graph[start_node] = [end_node]\n\n    cycle = find_eulerian_cycle(graph)\n    for idx, node in enumerate(cycle):\n        if node == start_node and cycle[(idx + 1) % len(cycle)] == end_node:\n            return cycle[idx + 1:] + cycle[1:idx + 1]\n\n    return cycle  # This should never happen if the input is valid\n\nsample_input: str = \"\"\"\n0 -&gt; 2\n1 -&gt; 3\n2 -&gt; 1\n3 -&gt; 0,4\n6 -&gt; 3,7\n7 -&gt; 8\n8 -&gt; 9\n9 -&gt; 6\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nadjacency_list = parse_adjacency_list(input_lines)\n\nprint(\"-&gt;\".join(find_eulerian_path(adjacency_list)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-29",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-29",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "29.1 Sample Dataset",
    "text": "29.1 Sample Dataset\n4\nCTTA\nACCA\nTACC\nGGCT\nGCTT\nTTAC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-29",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-29",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "29.2 Sample Output",
    "text": "29.2 Sample Output\nGGCTTACCA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-29",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-29",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "29.3 Solution",
    "text": "29.3 Solution\nfrom typing import List, Dict, Tuple\nfrom random import choice\n\ndef construct_de_bruijn_graph(kmers: List[str]) -&gt; Dict[str, List[str]]:\n    adjacency_dict: Dict[str, List[str]] = {}\n    for kmer in kmers:\n        prefix = kmer[:-1]\n        suffix = kmer[1:]\n        if prefix not in adjacency_dict:\n            adjacency_dict[prefix] = [suffix]\n        else:\n            adjacency_dict[prefix].append(suffix)\n    return adjacency_dict\n\ndef remove_edge(graph: Dict[str, List[str]], source: str, target: str) -&gt; Dict[str, List[str]]:\n    graph[source].remove(target)\n    if not graph[source]:\n        del graph[source]\n    return graph\n\ndef find_eulerian_cycle(graph: Dict[str, List[str]]) -&gt; List[str]:\n    start_node, edges = choice(list(graph.items()))\n    next_node = choice(edges)\n    graph = remove_edge(graph, start_node, next_node)\n\n    cycle: List[str] = [start_node, next_node]\n    current_node = next_node\n    while current_node != start_node:\n        edges = graph[current_node]\n        next_node = choice(edges)\n        graph = remove_edge(graph, current_node, next_node)\n        current_node = next_node\n        cycle.append(current_node)\n\n    while graph:\n        potential_starts: List[Tuple[int, str]] = [(idx, node) for idx, node in enumerate(cycle) if node in graph]\n        idx, new_start = choice(potential_starts)\n\n        new_cycle = cycle[idx:] + cycle[1:idx + 1]\n\n        next_node = choice(graph[new_start])\n        graph = remove_edge(graph, new_start, next_node)\n        current_node = next_node\n        new_cycle.append(current_node)\n        while current_node != new_start:\n            edges = graph[current_node]\n            next_node = choice(edges)\n            graph = remove_edge(graph, current_node, next_node)\n            current_node = next_node\n            new_cycle.append(current_node)\n        cycle = new_cycle\n    return cycle\n\ndef find_eulerian_path(graph: Dict[str, List[str]]) -&gt; List[str]:\n    degree_differences: Dict[str, int] = {}\n    for source, targets in graph.items():\n        degree_differences[source] = degree_differences.get(source, 0) + len(targets)\n        for target in targets:\n            degree_differences[target] = degree_differences.get(target, 0) - 1\n\n    start_node = [node for node, diff in degree_differences.items() if diff == -1][0]\n    end_node = [node for node, diff in degree_differences.items() if diff == 1][0]\n    \n    if start_node in graph:\n        graph[start_node].append(end_node)\n    else:\n        graph[start_node] = [end_node]\n\n    cycle = find_eulerian_cycle(graph)\n    for idx, node in enumerate(cycle):\n        if node == start_node and cycle[(idx + 1) % len(cycle)] == end_node:\n            return cycle[idx + 1:] + cycle[1:idx + 1]\n\n    return cycle  # This should never happen if the input is valid\n\ndef reconstruct_string(kmers: List[str]) -&gt; str:\n    adjacency_list = construct_de_bruijn_graph(kmers)\n    path = find_eulerian_path(adjacency_list)\n    reconstructed_string = path[0][:-1]\n    for node in path:\n        reconstructed_string += node[-1]\n    return reconstructed_string\n\nsample_input: str = \"\"\"\n4\nCTTA\nACCA\nTACC\nGGCT\nGCTT\nTTAC\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nk: int = int(input_lines[0])\nkmers: List[str] = input_lines[1:]\n\nprint(reconstruct_string(kmers))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-30",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-30",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "30.1 Sample Dataset",
    "text": "30.1 Sample Dataset\n4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-30",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-30",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "30.2 Sample Output",
    "text": "30.2 Sample Output\n1111010010110000"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-30",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-30",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "30.3 Solution",
    "text": "30.3 Solution\nfrom typing import List, Dict, Tuple\nfrom random import choice\n\ndef construct_de_bruijn_graph(kmers: List[str]) -&gt; Dict[str, List[str]]:\n    adjacency_dict: Dict[str, List[str]] = {}\n    for kmer in kmers:\n        prefix = kmer[:-1]\n        suffix = kmer[1:]\n        if prefix not in adjacency_dict:\n            adjacency_dict[prefix] = [suffix]\n        else:\n            adjacency_dict[prefix].append(suffix)\n    return adjacency_dict\n\ndef remove_edge(graph: Dict[str, List[str]], source: str, target: str) -&gt; Dict[str, List[str]]:\n    graph[source].remove(target)\n    if not graph[source]:\n        del graph[source]\n    return graph\n\ndef find_eulerian_cycle(graph: Dict[str, List[str]]) -&gt; List[str]:\n    start_node, edges = choice(list(graph.items()))\n    next_node = choice(edges)\n    graph = remove_edge(graph, start_node, next_node)\n\n    cycle: List[str] = [start_node, next_node]\n    current_node = next_node\n    while current_node != start_node:\n        edges = graph[current_node]\n        next_node = choice(edges)\n        graph = remove_edge(graph, current_node, next_node)\n        current_node = next_node\n        cycle.append(current_node)\n\n    while graph:\n        potential_starts: List[Tuple[int, str]] = [(idx, node) for idx, node in enumerate(cycle) if node in graph]\n        idx, new_start = choice(potential_starts)\n\n        new_cycle = cycle[idx:] + cycle[1:idx + 1]\n\n        next_node = choice(graph[new_start])\n        graph = remove_edge(graph, new_start, next_node)\n        current_node = next_node\n        new_cycle.append(current_node)\n        while current_node != new_start:\n            edges = graph[current_node]\n            next_node = choice(edges)\n            graph = remove_edge(graph, current_node, next_node)\n            current_node = next_node\n            new_cycle.append(current_node)\n        cycle = new_cycle\n    return cycle\n\ndef generate_k_universal_circular_string(k: int) -&gt; str:\n    kmers: List[str] = []\n    for i in range(2 ** k):\n        kmer = format(i, f'0{k}b')\n        kmers.append(kmer)\n\n    adjacency_list = construct_de_bruijn_graph(kmers)\n    cycle = find_eulerian_cycle(adjacency_list)\n\n    cycle = cycle[:len(cycle) - k + 1]\n    circular_string = cycle[0][:-1]\n    for node in cycle:\n        circular_string += node[-1]\n    return circular_string\n\nsample_input: str = \"\"\"\n4\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nk: int = int(input_lines[0])\n\nprint(generate_k_universal_circular_string(k))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-31",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-31",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "31.1 Sample Dataset",
    "text": "31.1 Sample Dataset\n4 2\nGAGA|TTGA\nTCGT|GATG\nCGTG|ATGT\nTGGT|TGAG\nGTGA|TGTT\nGTGG|GTGA\nTGAG|GTTG\nGGTC|GAGA\nGTCG|AGAT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-31",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-31",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "31.2 Sample Output",
    "text": "31.2 Sample Output\nGTGGTCGTGAGATGTTGA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-31",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-31",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "31.3 Solution",
    "text": "31.3 Solution\nimport sys\nfrom collections import defaultdict\nfrom typing import List, Tuple, Dict\nfrom random import choice\n\ndef remove_edge(graph: Dict[str, List[str]], source: str, target: str) -&gt; Dict[str, List[str]]:\n    graph[source].remove(target)\n    if not graph[source]:\n        del graph[source]\n    return graph\n\ndef find_eulerian_cycle(graph: Dict[str, List[str]]) -&gt; List[str]:\n    start_node, edges = choice(list(graph.items()))\n    next_node = choice(edges)\n    graph = remove_edge(graph, start_node, next_node)\n\n    cycle: List[str] = [start_node, next_node]\n    current_node = next_node\n    while current_node != start_node:\n        edges = graph[current_node]\n        next_node = choice(edges)\n        graph = remove_edge(graph, current_node, next_node)\n        current_node = next_node\n        cycle.append(current_node)\n\n    while graph:\n        potential_starts: List[Tuple[int, str]] = [(idx, node) for idx, node in enumerate(cycle) if node in graph]\n        idx, new_start = choice(potential_starts)\n\n        new_cycle = cycle[idx:] + cycle[1:idx + 1]\n\n        next_node = choice(graph[new_start])\n        graph = remove_edge(graph, new_start, next_node)\n        current_node = next_node\n        new_cycle.append(current_node)\n        while current_node != new_start:\n            edges = graph[current_node]\n            next_node = choice(edges)\n            graph = remove_edge(graph, current_node, next_node)\n            current_node = next_node\n            new_cycle.append(current_node)\n        cycle = new_cycle\n    return cycle\n\ndef find_eulerian_path(graph: Dict[str, List[str]]) -&gt; List[str]:\n    degree_differences: Dict[str, int] = {}\n    for source, targets in graph.items():\n        degree_differences[source] = degree_differences.get(source, 0) + len(targets)\n        for target in targets:\n            degree_differences[target] = degree_differences.get(target, 0) - 1\n\n    start_node = [node for node, diff in degree_differences.items() if diff == -1][0]\n    end_node = [node for node, diff in degree_differences.items() if diff == 1][0]\n    \n    if start_node in graph:\n        graph[start_node].append(end_node)\n    else:\n        graph[start_node] = [end_node]\n\n    cycle = find_eulerian_cycle(graph)\n    for idx, node in enumerate(cycle):\n        if node == start_node and cycle[(idx + 1) % len(cycle)] == end_node:\n            return cycle[idx + 1:] + cycle[1:idx + 1]\n\n    return cycle  # This should never happen if the input is valid\n\ndef construct_de_bruijn_graph_paired_reads(paired_reads: List[Tuple[str, str]]) -&gt; Dict[Tuple[str, str], List[Tuple[str, str]]]:\n    graph = defaultdict(list)\n    for pair in paired_reads:\n        graph[(pair[0][:-1], pair[1][:-1])].append((pair[0][1:], pair[1][1:]))\n    return graph\n\ndef string_spelled_by_gapped_patterns(gapped_patterns: List[Tuple[str, str]], k: int, d: int) -&gt; str:\n    prefix_string = ''.join(pattern[0][0] for pattern in gapped_patterns[:-1]) + gapped_patterns[-1][0]\n    suffix_string = ''.join(pattern[1][0] for pattern in gapped_patterns[:-1]) + gapped_patterns[-1][1]\n    \n    for i in range(k + d + 1, len(prefix_string)):\n        if prefix_string[i] != suffix_string[i - k - d - 1]:\n            return \"-1\"\n    return prefix_string + suffix_string[len(suffix_string) - k - d - 1:]\n\ndef reconstruct_string_from_read_pairs(k: int, d: int, paired_reads: List[Tuple[str, str]]) -&gt; str:\n    graph = construct_de_bruijn_graph_paired_reads(paired_reads)\n    path = find_eulerian_path(graph)\n    return string_spelled_by_gapped_patterns(path, k - 1, d)\n\nsample_input: str = \"\"\"\n4 2\nGAGA|TTGA\nTCGT|GATG\nCGTG|ATGT\nTGGT|TGAG\nGTGA|TGTT\nGTGG|GTGA\nTGAG|GTTG\nGGTC|GAGA\nGTCG|AGAT\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nk, d = map(int, input_lines[0].split())\npaired_reads: List[Tuple[str, str]] = [tuple(line.split(\"|\")) for line in input_lines[1:]]\n\nprint(reconstruct_string_from_read_pairs(k, d, paired_reads))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-32",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-32",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "32.1 Sample Dataset",
    "text": "32.1 Sample Dataset\nATG\nATG\nTGT\nTGG\nCAT\nGGA\nGAT\nAGA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-32",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-32",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "32.2 Sample Output",
    "text": "32.2 Sample Output\nAGA ATG ATG CAT GAT TGGA TGT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-32",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-32",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "32.3 Solution",
    "text": "32.3 Solution\nfrom typing import List, Dict, Tuple\nfrom collections import defaultdict\n\ndef construct_de_bruijn_graph(kmers: List[str]) -&gt; Dict[str, List[str]]:\n    adjacency_dict: Dict[str, List[str]] = defaultdict(list)\n    for kmer in kmers:\n        prefix, suffix = kmer[:-1], kmer[1:]\n        adjacency_dict[prefix].append(suffix)\n    return adjacency_dict\n\ndef remove_edge(graph: Dict[str, List[str]], source: str, target: str) -&gt; Dict[str, List[str]]:\n    graph[source].remove(target)\n    if not graph[source]:\n        del graph[source]\n    return graph\n\ndef find_maximal_non_branching_paths(graph: Dict[str, List[str]]) -&gt; List[List[str]]:\n    paths: List[List[str]] = []\n    in_out_degrees: Dict[str, List[int]] = {}\n\n    # Calculate in and out degrees\n    for source, targets in graph.items():\n        if source not in in_out_degrees:\n            in_out_degrees[source] = [0, len(targets)]\n        else:\n            in_out_degrees[source][1] += len(targets)\n\n        for target in targets:\n            if target not in in_out_degrees:\n                in_out_degrees[target] = [1, 0]\n            else:\n                in_out_degrees[target][0] += 1\n\n    # Find all non-branching paths\n    for node in list(in_out_degrees):\n        if in_out_degrees[node] != [1, 1]:\n            if in_out_degrees[node][1] &gt; 0:\n                while node in graph:\n                    next_node = graph[node][0]\n                    non_branching_path = [node, next_node]\n                    graph = remove_edge(graph, node, next_node)\n                    while in_out_degrees[next_node] == [1, 1]:\n                        following_node = graph[next_node][0]\n                        non_branching_path.append(following_node)\n                        graph = remove_edge(graph, next_node, following_node)\n                        next_node = following_node\n                    paths.append(non_branching_path)\n\n    # Find isolated cycles\n    while graph:\n        start_node = next(iter(graph))\n        current_node = graph[start_node][0]\n        graph = remove_edge(graph, start_node, current_node)\n        cycle = [start_node, current_node]\n        while current_node != start_node:\n            next_node = graph[current_node][0]\n            cycle.append(next_node)\n            graph = remove_edge(graph, current_node, next_node)\n            current_node = next_node\n        paths.append(cycle)\n\n    return paths\n\ndef generate_contigs(kmers: List[str]) -&gt; List[str]:\n    graph = construct_de_bruijn_graph(kmers)\n    paths = find_maximal_non_branching_paths(graph)\n    contigs: List[str] = []\n    for path in paths:\n        contig = path[0]\n        for node in path[1:]:\n            contig += node[-1]\n        contigs.append(contig)\n    return contigs\n\nsample_input: str = \"\"\"\nATG\nATG\nTGT\nTGG\nCAT\nGGA\nGAT\nAGA\n\"\"\"\n\nkmers: List[str] = sample_input.strip().split(\"\\n\")\ncontigs = generate_contigs(kmers)\ncontigs.sort()\nprint(\" \".join(contigs))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-33",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-33",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "33.1 Sample Dataset",
    "text": "33.1 Sample Dataset\n4 2\nGACC|GCGC\nACCG|CGCC\nCCGA|GCCG\nCGAG|CCGG\nGAGC|CGGA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-33",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-33",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "33.2 Sample Output",
    "text": "33.2 Sample Output\nGACCGAGCGCCGGA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-33",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-33",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "33.3 Solution",
    "text": "33.3 Solution\nfrom typing import List, Tuple\n\ndef reconstruct_string_from_gapped_patterns(gapped_patterns: List[Tuple[str, str]], k: int, d: int) -&gt; str:\n    prefix_string = ''\n    suffix_string = ''\n    for i, (prefix, suffix) in enumerate(gapped_patterns):\n        if i != len(gapped_patterns) - 1:\n            prefix_string += prefix[0]\n            suffix_string += suffix[0]\n        else:\n            prefix_string += prefix\n            suffix_string += suffix\n    \n    for i in range(k + d + 1, len(prefix_string)):\n        if prefix_string[i] != suffix_string[i - k - d - 1]:\n            return \"-1\"\n    \n    return prefix_string + suffix_string[len(suffix_string) - k - d - 1:]\n\nsample_input: str = \"\"\"\n4 2\nGACC|GCGC\nACCG|CGCC\nCCGA|GCCG\nCGAG|CCGG\nGAGC|CGGA\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nk, d = map(int, input_lines[0].split())\ngapped_patterns: List[Tuple[str, str]] = [tuple(line.split(\"|\")) for line in input_lines[1:]]\n\nprint(reconstruct_string_from_gapped_patterns(gapped_patterns, k - 1, d))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-34",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-34",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "34.1 Sample Dataset",
    "text": "34.1 Sample Dataset\n1 -&gt; 2\n2 -&gt; 3\n3 -&gt; 4,5\n6 -&gt; 7\n7 -&gt; 6"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-34",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-34",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "34.2 Sample Output",
    "text": "34.2 Sample Output\n1 -&gt; 2 -&gt; 3\n3 -&gt; 4\n3 -&gt; 5\n6 -&gt; 7 -&gt; 6"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-34",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-34",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "34.3 Solution",
    "text": "34.3 Solution\nfrom typing import List, Dict, Tuple\nfrom re import split\n\ndef parse_adjacency_list(adjacency_text: List[str]) -&gt; Dict[str, List[str]]:\n    adjacency_dict: Dict[str, List[str]] = {}\n    for line in adjacency_text:\n        source, targets = split(' -&gt; ', line)\n        adjacency_dict[source] = targets.split(',')\n    return adjacency_dict\n\ndef remove_edge(graph: Dict[str, List[str]], source: str, target: str) -&gt; Dict[str, List[str]]:\n    graph[source].remove(target)\n    if not graph[source]:\n        del graph[source]\n    return graph\n\ndef find_maximal_non_branching_paths(graph: Dict[str, List[str]]) -&gt; List[List[str]]:\n    paths: List[List[str]] = []\n    in_out_degrees: Dict[str, List[int]] = {}\n\n    # Calculate in and out degrees\n    for node, neighbors in graph.items():\n        if node not in in_out_degrees:\n            in_out_degrees[node] = [0, len(neighbors)]\n        else:\n            in_out_degrees[node][1] += len(neighbors)\n\n        for neighbor in neighbors:\n            if neighbor not in in_out_degrees:\n                in_out_degrees[neighbor] = [1, 0]\n            else:\n                in_out_degrees[neighbor][0] += 1\n\n    # Find all non-branching paths\n    for node in list(in_out_degrees):\n        if in_out_degrees[node] != [1, 1]:\n            if in_out_degrees[node][1] &gt; 0:\n                while node in graph:\n                    next_node = graph[node][0]\n                    non_branching_path = [node, next_node]\n                    graph = remove_edge(graph, node, next_node)\n                    while in_out_degrees[next_node] == [1, 1]:\n                        following_node = graph[next_node][0]\n                        non_branching_path.append(following_node)\n                        graph = remove_edge(graph, next_node, following_node)\n                        next_node = following_node\n                    paths.append(non_branching_path)\n\n    # Find isolated cycles\n    while graph:\n        start_node = next(iter(graph))\n        current_node = graph[start_node][0]\n        graph = remove_edge(graph, start_node, current_node)\n        cycle = [start_node, current_node]\n        while current_node != start_node:\n            next_node = graph[current_node][0]\n            cycle.append(next_node)\n            graph = remove_edge(graph, current_node, next_node)\n            current_node = next_node\n        paths.append(cycle)\n\n    return paths\n\nsample_input: str = \"\"\"\n1 -&gt; 2\n2 -&gt; 3\n3 -&gt; 4,5\n6 -&gt; 7\n7 -&gt; 6\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nadjacency_list = parse_adjacency_list(input_lines)\n\nresult = find_maximal_non_branching_paths(adjacency_list)\nfor path in result:\n    print(\" -&gt; \".join(path))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-35",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-35",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "35.1 Sample Dataset",
    "text": "35.1 Sample Dataset\nAUGGCCAUGGCGCCCAGAACUGAGAUCAAUAGUACCCGUAUUAACGGGUGA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-35",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-35",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "35.2 Sample Output",
    "text": "35.2 Sample Output\nMAMAPRTEINSTRING"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-35",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-35",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "35.3 Solution",
    "text": "35.3 Solution\nfrom typing import Dict, List\n\ndef translate_rna_to_protein(rna_sequence: str) -&gt; str:\n    # RNA 코돈을 아미노산으로 변환하는 딕셔너리\n    codon_table: Dict[str, str] = {\n        'UUU': 'F', 'UUC': 'F', 'UUA': 'L', 'UUG': 'L',\n        'UCU': 'S', 'UCC': 'S', 'UCA': 'S', 'UCG': 'S',\n        'UAU': 'Y', 'UAC': 'Y', 'UAA': '*', 'UAG': '*',\n        'UGU': 'C', 'UGC': 'C', 'UGA': '*', 'UGG': 'W',\n        'CUU': 'L', 'CUC': 'L', 'CUA': 'L', 'CUG': 'L',\n        'CCU': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n        'CAU': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q',\n        'CGU': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R',\n        'AUU': 'I', 'AUC': 'I', 'AUA': 'I', 'AUG': 'M',\n        'ACU': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n        'AAU': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K',\n        'AGU': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R',\n        'GUU': 'V', 'GUC': 'V', 'GUA': 'V', 'GUG': 'V',\n        'GCU': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n        'GAU': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E',\n        'GGU': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G'\n    }\n    \n    protein_sequence: str = \"\"\n    \n    # RNA 서열을 3개의 뉴클레오티드(코돈)씩 나누어 처리\n    for i in range(0, len(rna_sequence), 3):\n        codon: str = rna_sequence[i:i+3]\n        \n        # 코돈이 3개의 뉴클레오티드로 완전하지 않으면 중단\n        if len(codon) != 3:\n            break\n        \n        # 코돈을 아미노산으로 변환\n        amino_acid: str = codon_table.get(codon, '')\n        \n        # 종결 코돈('*')을 만나면 번역 중단\n        if amino_acid == '*':\n            break\n        \n        protein_sequence += amino_acid\n    \n    return protein_sequence\n\n# 입력 RNA 서열\nsample_input: str = \"\"\"\nAUGGCCAUGGCGCCCAGAACUGAGAUCAAUAGUACCCGUAUUAACGGGUGA\n\"\"\"\n\nrna_sequence: str = ''.join(sample_input.strip().split())\n\n# RNA를 단백질로 번역\nprotein: str = translate_rna_to_protein(rna_sequence)\n\nprint(protein)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-36",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-36",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "36.1 Sample Dataset",
    "text": "36.1 Sample Dataset\nATGGCCATGGCCCCCAGAACTGAGATCAATAGTACCCGTATTAACGGGTGA\nMA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-36",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-36",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "36.2 Sample Output",
    "text": "36.2 Sample Output\nATGGCC\nGGCCAT\nATGGCC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-36",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-36",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "36.3 Solution",
    "text": "36.3 Solution\nfrom typing import Dict, List\n\ndef translate_rna_to_protein(rna_sequence: str) -&gt; str:\n    codon_to_amino_acid: Dict[str, str] = {\n        'UUU': 'F', 'UUC': 'F', 'UUA': 'L', 'UUG': 'L',\n        'UCU': 'S', 'UCC': 'S', 'UCA': 'S', 'UCG': 'S',\n        'UAU': 'Y', 'UAC': 'Y', 'UAA': '*', 'UAG': '*',\n        'UGU': 'C', 'UGC': 'C', 'UGA': '*', 'UGG': 'W',\n        'CUU': 'L', 'CUC': 'L', 'CUA': 'L', 'CUG': 'L',\n        'CCU': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n        'CAU': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q',\n        'CGU': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R',\n        'AUU': 'I', 'AUC': 'I', 'AUA': 'I', 'AUG': 'M',\n        'ACU': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n        'AAU': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K',\n        'AGU': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R',\n        'GUU': 'V', 'GUC': 'V', 'GUA': 'V', 'GUG': 'V',\n        'GCU': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n        'GAU': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E',\n        'GGU': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G'\n    }\n    \n    protein_sequence: List[str] = []\n    \n    for i in range(0, len(rna_sequence), 3):\n        codon: str = rna_sequence[i:i+3]\n        \n        if len(codon) != 3:\n            break\n        \n        amino_acid: str = codon_to_amino_acid.get(codon, '')\n        \n        if amino_acid == '*':\n            break\n        \n        protein_sequence.append(amino_acid)\n    \n    return ''.join(protein_sequence)\n\ndef reverse_complement(dna_sequence: str) -&gt; str:\n    return dna_sequence[::-1].translate(str.maketrans(\"ACGT\", \"TGCA\"))\n\ndef dna_to_rna(dna_sequence: str) -&gt; str:\n    return dna_sequence.replace(\"T\", \"U\")\n\ndef find_peptide_encoding_substrings(dna_sequence: str, peptide: str) -&gt; List[str]:\n    substring_length: int = len(peptide) * 3\n    encoding_substrings: List[str] = []\n\n    for i in range(len(dna_sequence) - substring_length + 1):\n        dna_substring: str = dna_sequence[i:i + substring_length]\n        reverse_complement_substring: str = reverse_complement(dna_substring)\n\n        rna_substring: str = dna_to_rna(dna_substring)\n        reverse_complement_rna: str = dna_to_rna(reverse_complement_substring)\n\n        if (translate_rna_to_protein(rna_substring) == peptide or \n            translate_rna_to_protein(reverse_complement_rna) == peptide):\n            encoding_substrings.append(dna_substring)\n\n    return encoding_substrings\n\nsample_input: str = \"\"\"\nATGGCCATGGCCCCCAGAACTGAGATCAATAGTACCCGTATTAACGGGTGA\nMA\n\"\"\"\n\ndna_sequence, peptide = sample_input.strip().split('\\n')\nresult: List[str] = find_peptide_encoding_substrings(dna_sequence, peptide)\nprint(\"\\n\".join(result))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-37",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-37",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "37.1 Sample Dataset",
    "text": "37.1 Sample Dataset\nLEQN"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-37",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-37",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "37.2 Sample Output",
    "text": "37.2 Sample Output\n0 113 114 128 129 227 242 242 257 355 356 370 371 484"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-37",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-37",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "37.3 Solution",
    "text": "37.3 Solution\nfrom typing import Dict, List\n\nAMINO_ACID_MASSES: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\n\ndef calculate_cyclospectrum(peptide: str) -&gt; List[int]:\n    total_mass: int = sum(AMINO_ACID_MASSES[amino_acid] for amino_acid in peptide)\n    spectrum: List[int] = [0, total_mass]\n    circular_peptide: str = peptide + peptide\n    \n    for subpeptide_length in range(1, len(peptide)):\n        for start_index in range(len(peptide)):\n            subpeptide: str = circular_peptide[start_index:start_index + subpeptide_length]\n            subpeptide_mass: int = sum(AMINO_ACID_MASSES[amino_acid] for amino_acid in subpeptide)\n            spectrum.append(subpeptide_mass)\n    \n    return sorted(spectrum)\n\nsample_input: str = \"\"\"\nLEQN\n\"\"\"\n\ninput_peptide: str = sample_input.strip()\n\nresult: List[int] = calculate_cyclospectrum(input_peptide)\nprint(\" \".join(map(str, result)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-38",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-38",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "38.1 Sample Dataset",
    "text": "38.1 Sample Dataset\n1024"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-38",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-38",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "38.2 Sample Output",
    "text": "38.2 Sample Output\n14712706211"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-38",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-38",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "38.3 Solution",
    "text": "38.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Dict\n\nAMINO_ACID_MASSES: List[int] = [57, 71, 87, 97, 99, 101, 103, 113, 114, 115, 128, 129, 131, 137, 147, 156, 163, 186]\nLIGHTEST_AMINO_ACID: int = min(AMINO_ACID_MASSES)\n\ndef count_possible_peptides(target_mass: int) -&gt; int:\n    peptide_count: Dict[int, int] = defaultdict(int)\n    \n    for current_mass in range(LIGHTEST_AMINO_ACID, target_mass + 1):\n        peptide_count[current_mass] = sum(1 for amino_acid_mass in AMINO_ACID_MASSES if amino_acid_mass == current_mass)\n        \n        for amino_acid_mass in AMINO_ACID_MASSES:\n            if current_mass &gt;= amino_acid_mass:\n                peptide_count[current_mass] += peptide_count[current_mass - amino_acid_mass]\n\n    return peptide_count[target_mass]\n\nsample_input: str = \"\"\"\n1024\n\"\"\"\ntarget_peptide_mass: int = int(sample_input.strip())\nresult: int = count_possible_peptides(target_peptide_mass)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-39",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-39",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "39.1 Sample Dataset",
    "text": "39.1 Sample Dataset\n0 113 128 186 241 299 314 427"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-39",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-39",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "39.2 Sample Output",
    "text": "39.2 Sample Output\n113-128-186 113-186-128 186-128-113 128-186-113 186-113-128 128-113-186"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-39",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-39",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "39.3 Solution",
    "text": "39.3 Solution\nfrom typing import List, Set\n\nAMINO_ACID_MASSES: List[int] = [57, 71, 87, 97, 99, 101, 103, 113, 114, 115, 128, 129, 131, 137, 147, 156, 163, 186]\n\ndef calculate_cyclospectrum(peptide: List[int]) -&gt; List[int]:\n    spectrum: List[int] = [0, sum(peptide)]\n    extended_peptide: List[int] = peptide + peptide\n    for k in range(1, len(peptide)):\n        for i in range(len(peptide)):\n            subpeptide: List[int] = extended_peptide[i:i + k]\n            spectrum.append(sum(subpeptide))\n    spectrum.sort()\n    return spectrum\n\ndef calculate_linear_spectrum(peptide: List[int]) -&gt; List[int]:\n    prefix_mass: List[int] = [0]\n    for mass in peptide:\n        prefix_mass.append(prefix_mass[-1] + mass)\n    linear_spectrum: List[int] = [0]\n    for i in range(len(peptide)):\n        for j in range(i + 1, len(peptide) + 1):\n            linear_spectrum.append(prefix_mass[j] - prefix_mass[i])\n    linear_spectrum.sort()\n    return linear_spectrum\n\ndef expand_peptides(peptides: List[List[int]]) -&gt; List[List[int]]:\n    expanded_peptides: List[List[int]] = []\n    for peptide in peptides:\n        for mass in AMINO_ACID_MASSES:\n            expanded_peptides.append(peptide + [mass])\n    return expanded_peptides\n\ndef is_consistent(peptide: List[int], spectrum: List[int]) -&gt; bool:\n    if sum(peptide) &gt; spectrum[-1] - AMINO_ACID_MASSES[0]:\n        return False\n    peptide_spectrum: List[int] = calculate_linear_spectrum(peptide)\n    return all(mass in spectrum for mass in peptide_spectrum)\n\ndef cyclopeptide_sequencing(spectrum: List[int]) -&gt; Set[str]:\n    candidate_peptides: List[List[int]] = [[]]\n    result: Set[str] = set()\n    \n    while candidate_peptides:\n        candidate_peptides = expand_peptides(candidate_peptides)\n        for peptide in candidate_peptides[:]:\n            if sum(peptide) == spectrum[-1]:\n                if calculate_cyclospectrum(peptide) == spectrum:\n                    result.add(\"-\".join(map(str, peptide)))\n                candidate_peptides.remove(peptide)\n            elif not is_consistent(peptide, spectrum):\n                candidate_peptides.remove(peptide)\n    \n    return result\n\nsample_input: str = \"\"\"\n0 113 128 186 241 299 314 427\n\"\"\"\n\ninput_spectrum: List[int] = [int(x) for x in sample_input.strip().split()]\n\nresult: Set[str] = cyclopeptide_sequencing(input_spectrum)\nprint(\" \".join(result))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-40",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-40",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "40.1 Sample Dataset",
    "text": "40.1 Sample Dataset\nNQEL\n0 99 113 114 128 227 257 299 355 356 370 371 484"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-40",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-40",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "40.2 Sample Output",
    "text": "40.2 Sample Output\n11"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-40",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-40",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "40.3 Solution",
    "text": "40.3 Solution\nfrom typing import Dict, List\n\nAMINO_ACID_MASSES: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\n\ndef calculate_cyclospectrum(peptide: str) -&gt; List[int]:\n    total_mass: int = sum(AMINO_ACID_MASSES[aa] for aa in peptide)\n    spectrum: List[int] = [0, total_mass]\n    extended_peptide: str = peptide + peptide\n    \n    for length in range(1, len(peptide)):\n        for start in range(len(peptide)):\n            subpeptide: str = extended_peptide[start:start + length]\n            subpeptide_mass: int = sum(AMINO_ACID_MASSES[aa] for aa in subpeptide)\n            spectrum.append(subpeptide_mass)\n    \n    spectrum.sort()\n    return spectrum\n\ndef calculate_score(peptide: str, experimental_spectrum: List[int]) -&gt; int:\n    theoretical_spectrum: List[int] = calculate_cyclospectrum(peptide)\n    score: int = 0\n    unique_masses: set = set(theoretical_spectrum + experimental_spectrum)\n    \n    for mass in unique_masses:\n        score += min(theoretical_spectrum.count(mass), experimental_spectrum.count(mass))\n    \n    return score\n\nsample_input: str = \"\"\"\nNQEL\n0 99 113 114 128 227 257 299 355 356 370 371 484\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\ninput_peptide: str = input_lines[0]\ninput_spectrum: List[int] = [int(x) for x in input_lines[1].split()]\n\nresult: int = calculate_score(input_peptide, input_spectrum)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-41",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-41",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "41.1 Sample Dataset",
    "text": "41.1 Sample Dataset\n10\n0 71 113 129 147 200 218 260 313 331 347 389 460"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-41",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-41",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "41.2 Sample Output",
    "text": "41.2 Sample Output\n113-147-71-129"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-41",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-41",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "41.3 Solution",
    "text": "41.3 Solution\nfrom typing import List, Set, Dict\n\nAMINO_ACID_MASSES: List[int] = [57, 71, 87, 97, 99, 101, 103, 113, 114, 115, 128, 129, 131, 137, 147, 156, 163, 186]\n\ndef expand_peptides(peptides: List[List[int]]) -&gt; List[List[int]]:\n    expanded_peptides: List[List[int]] = []\n    for peptide in peptides:\n        for mass in AMINO_ACID_MASSES:\n            expanded_peptides.append(peptide + [mass])\n    return expanded_peptides\n\ndef calculate_cyclospectrum(peptide: List[int]) -&gt; List[int]:\n    total_mass: int = sum(peptide)\n    spectrum: List[int] = [0, total_mass]\n    extended_peptide: List[int] = peptide + peptide\n    \n    for length in range(1, len(peptide)):\n        for start in range(len(peptide)):\n            subpeptide: List[int] = extended_peptide[start:start + length]\n            subpeptide_mass: int = sum(subpeptide)\n            spectrum.append(subpeptide_mass)\n    \n    spectrum.sort()\n    return spectrum\n\ndef is_consistent(peptide: List[int], spectrum: List[int]) -&gt; bool:\n    peptide_spectrum: List[int] = calculate_cyclospectrum(peptide)\n    return all(peptide_spectrum.count(mass) &lt;= spectrum.count(mass) for mass in set(peptide_spectrum))\n\ndef cyclopeptide_sequencing(spectrum: List[int]) -&gt; Set[str]:\n    result: Set[str] = set()\n    candidate_peptides: List[List[int]] = [[]]\n    while candidate_peptides:\n        candidate_peptides = expand_peptides(candidate_peptides)\n        for peptide in candidate_peptides[:]:\n            if sum(peptide) == spectrum[-1]:\n                if calculate_cyclospectrum(peptide) == spectrum:\n                    result.add(\"-\".join(map(str, peptide)))\n                candidate_peptides.remove(peptide)\n            elif not is_consistent(peptide, spectrum):\n                candidate_peptides.remove(peptide)\n    return result\n\ndef calculate_score(peptide: List[int], spectrum: List[int]) -&gt; int:\n    peptide_spectrum: List[int] = calculate_cyclospectrum(peptide)\n    score: int = 0\n    unique_masses: Set[int] = set(peptide_spectrum + spectrum)\n    for mass in unique_masses:\n        score += min(peptide_spectrum.count(mass), spectrum.count(mass))\n    return score\n\ndef trim_leaderboard(leaderboard: List[List[int]], spectrum: List[int], n: int) -&gt; List[List[int]]:\n    if len(leaderboard) &lt;= n:\n        return leaderboard\n\n    scores: Dict[int, int] = {}\n    for i, peptide in enumerate(leaderboard):\n        scores[i] = calculate_score(peptide, spectrum)\n\n    sorted_scores: List[int] = sorted(scores.values(), reverse=True)\n    threshold: int = sorted_scores[n - 1] if n &lt;= len(sorted_scores) else sorted_scores[-1]\n\n    return [leaderboard[idx] for idx, score in scores.items() if score &gt;= threshold]\n\ndef leaderboard_cyclopeptide_sequencing(spectrum: List[int], n: int) -&gt; List[int]:\n    leaderboard: List[List[int]] = [[]]\n    leader_peptide: List[int] = []\n\n    while leaderboard:\n        leaderboard = expand_peptides(leaderboard)\n        for peptide in leaderboard[:]:\n            if sum(peptide) == spectrum[-1]:\n                if calculate_score(peptide, spectrum) &gt; calculate_score(leader_peptide, spectrum):\n                    leader_peptide = peptide\n            elif sum(peptide) &gt; spectrum[-1]:\n                leaderboard.remove(peptide)\n        leaderboard = trim_leaderboard(leaderboard, spectrum, n)\n    return leader_peptide\n\nsample_input: str = \"\"\"\n10\n0 71 113 129 147 200 218 260 313 331 347 389 460\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nn: int = int(input_lines[0])\nspectrum: List[int] = [int(x) for x in input_lines[1].split()]\n\nresult: List[int] = leaderboard_cyclopeptide_sequencing(spectrum, n)\nprint(\"-\".join(map(str, result)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-42",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-42",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "42.1 Sample Dataset",
    "text": "42.1 Sample Dataset\n0 137 186 323"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-42",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-42",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "42.2 Sample Output",
    "text": "42.2 Sample Output\n137 137 186 186 323 49"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-42",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-42",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "42.3 Solution",
    "text": "42.3 Solution\nfrom typing import List, Dict\n\ndef calculate_spectrum_convolution(spectrum: List[int]) -&gt; List[int]:\n    spectrum.sort()\n    convolution_list: List[int] = []\n    \n    for i in range(len(spectrum) - 1):\n        for j in range(i + 1, len(spectrum)):\n            mass_difference: int = spectrum[j] - spectrum[i]\n            if mass_difference != 0:\n                convolution_list.append(mass_difference)\n\n    frequency_dict: Dict[int, int] = {}\n    for mass in set(convolution_list):\n        frequency_dict[mass] = convolution_list.count(mass)\n\n    sorted_masses: List[int] = sorted(frequency_dict, key=frequency_dict.get, reverse=True)\n    \n    result: List[int] = []\n    for mass in sorted_masses:\n        result.extend([mass] * frequency_dict[mass])\n    \n    return result\n\nsample_input: str = \"\"\"\n0 137 186 323\n\"\"\"\n\ninput_spectrum: List[int] = [int(x) for x in sample_input.strip().split()]\n\nconvolution_result: List[int] = calculate_spectrum_convolution(input_spectrum)\nprint(\" \".join(map(str, convolution_result)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-43",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-43",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "43.1 Sample Dataset",
    "text": "43.1 Sample Dataset\n20\n60\n57 57 71 99 129 137 170 186 194 208 228 265 285 299 307 323 356 364 394 422 493"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-43",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-43",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "43.2 Sample Output",
    "text": "43.2 Sample Output\n99-71-137-57-72-57"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-43",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-43",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "43.3 Solution",
    "text": "43.3 Solution\nfrom typing import List, Dict, Tuple\n\ndef calculate_spectrum_convolution(spectrum: List[int]) -&gt; List[int]:\n    spectrum.sort()\n    convolution_list: List[int] = []\n    for i in range(len(spectrum) - 1):\n        for j in range(i, len(spectrum)):\n            if spectrum[j] - spectrum[i] != 0:\n                convolution_list.append(spectrum[j] - spectrum[i])\n\n    frequency_dict: Dict[int, int] = {}\n    for mass in set(convolution_list):\n        frequency_dict[mass] = convolution_list.count(mass)\n\n    sorted_masses: List[int] = [k for k, _ in sorted(frequency_dict.items(), key=lambda item: item[1], reverse=True)]\n    result: List[int] = []\n    for mass in sorted_masses:\n        result += [mass] * frequency_dict[mass]\n    return result\n\ndef trim_leaderboard(leaderboard: List[List[int]], spectrum: List[int], n: int) -&gt; List[List[int]]:\n    if len(leaderboard) &lt;= n:\n        return leaderboard\n\n    scores: Dict[int, int] = {}\n    for i, peptide in enumerate(leaderboard):\n        scores[i] = calculate_score(peptide, spectrum)\n\n    sorted_scores: List[int] = sorted(scores.values(), reverse=True)\n    threshold: int = sorted_scores[n - 1]\n\n    return [leaderboard[idx] for idx, score in scores.items() if score &gt;= threshold]\n\ndef calculate_cyclospectrum(peptide: List[int]) -&gt; List[int]:\n    spectrum: List[int] = [0, sum(peptide)]\n    extended_peptide: List[int] = peptide + peptide\n    for k in range(1, len(peptide)):\n        for i in range(len(peptide)):\n            subpeptide: List[int] = extended_peptide[i:i + k]\n            spectrum.append(sum(subpeptide))\n    spectrum.sort()\n    return spectrum\n\ndef calculate_score(peptide: List[int], spectrum: List[int]) -&gt; int:\n    peptide_spectrum: List[int] = calculate_cyclospectrum(peptide)\n    score: int = 0\n    unique_masses: set = set(peptide_spectrum + spectrum)\n    for mass in unique_masses:\n        score += min(peptide_spectrum.count(mass), spectrum.count(mass))\n    return score\n\ndef find_top_masses(spectrum: List[int], m: int) -&gt; List[int]:\n    convolution: List[int] = calculate_spectrum_convolution(spectrum)\n    filtered_convolution: List[int] = [x for x in convolution if 57 &lt;= x &lt;= 200]\n\n    frequency_dict: Dict[int, int] = {}\n    for mass in set(filtered_convolution):\n        frequency_dict[mass] = filtered_convolution.count(mass)\n\n    sorted_elements: List[Tuple[int, int]] = sorted(frequency_dict.items(), key=lambda kv: kv[1], reverse=True)\n    top_masses: List[int] = [mass for mass, freq in sorted_elements if freq &gt;= sorted_elements[m - 1][1]]\n    top_masses.sort()\n    return top_masses\n\ndef expand_peptides(peptides: List[List[int]], masses: List[int]) -&gt; List[List[int]]:\n    expanded_peptides: List[List[int]] = []\n    for peptide in peptides:\n        for mass in masses:\n            expanded_peptides.append(peptide + [mass])\n    return expanded_peptides\n\ndef convolution_cyclopeptide_sequencing(spectrum: List[int], m: int, n: int) -&gt; List[int]:\n    masses: List[int] = find_top_masses(spectrum, m)\n    leaderboard: List[List[int]] = [[]]\n    leader_peptide: List[int] = []\n\n    while leaderboard:\n        leaderboard = expand_peptides(leaderboard, masses)\n        for peptide in leaderboard[:]:\n            if sum(peptide) == spectrum[-1]:\n                if calculate_score(peptide, spectrum) &gt; calculate_score(leader_peptide, spectrum):\n                    leader_peptide = peptide\n            elif sum(peptide) &gt; spectrum[-1]:\n                leaderboard.remove(peptide)\n        leaderboard = trim_leaderboard(leaderboard, spectrum, n)\n    return leader_peptide\n\nsample_input: str = \"\"\"\n20\n60\n57 57 71 99 129 137 170 186 194 208 228 265 285 299 307 323 356 364 394 422 493\n\"\"\"\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nm: int = int(input_lines[0])\nn: int = int(input_lines[1])\nspectrum: List[int] = [int(x) for x in input_lines[2].split()]\n\nresult: List[int] = convolution_cyclopeptide_sequencing(spectrum, m, n)\nprint(\"-\".join(map(str, result)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-44",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-44",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "44.1 Sample Dataset",
    "text": "44.1 Sample Dataset\nNQEL"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-44",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-44",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "44.2 Sample Output",
    "text": "44.2 Sample Output\n0 113 114 128 129 242 242 257 370 371 484"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-44",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-44",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "44.3 Solution",
    "text": "44.3 Solution\nfrom typing import List, Dict\n\nAMINO_ACID_MASSES: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137,\n    'K': 128, 'M': 131, 'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156,\n    'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\n\ndef calculate_linear_spectrum(peptide: str) -&gt; List[int]:\n    prefix_masses: List[int] = [0]\n    \n    for amino_acid in peptide:\n        current_mass = prefix_masses[-1] + AMINO_ACID_MASSES[amino_acid]\n        prefix_masses.append(current_mass)\n    \n    linear_spectrum: List[int] = [0]\n    \n    for i in range(len(peptide)):\n        for j in range(i + 1, len(peptide) + 1):\n            subpeptide_mass = prefix_masses[j] - prefix_masses[i]\n            linear_spectrum.append(subpeptide_mass)\n    \n    return sorted(linear_spectrum)\n\n# Sample input\nsample_peptide: str = \"NQEL\"\n\n# Calculate and print the linear spectrum\nresult: List[int] = calculate_linear_spectrum(sample_peptide)\nprint(\" \".join(map(str, result)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-45",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-45",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "45.1 Sample Dataset",
    "text": "45.1 Sample Dataset\nNQEL\n0 99 113 114 128 227 257 299 355 356 370 371 484"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-45",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-45",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "45.2 Sample Output",
    "text": "45.2 Sample Output\n8"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-45",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-45",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "45.3 Solution",
    "text": "45.3 Solution\nfrom typing import List, Dict\n\nAMINO_ACID_MASSES: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137,\n    'K': 128, 'M': 131, 'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156,\n    'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\n\ndef calculate_linear_spectrum(peptide: str) -&gt; List[int]:\n    prefix_masses: List[int] = [0]\n    \n    for amino_acid in peptide:\n        current_mass = prefix_masses[-1] + AMINO_ACID_MASSES[amino_acid]\n        prefix_masses.append(current_mass)\n    \n    linear_spectrum: List[int] = [0]\n    \n    for i in range(len(peptide)):\n        for j in range(i + 1, len(peptide) + 1):\n            subpeptide_mass = prefix_masses[j] - prefix_masses[i]\n            linear_spectrum.append(subpeptide_mass)\n    \n    return sorted(linear_spectrum)\n\ndef calculate_linear_score(peptide: str, experimental_spectrum: List[int]) -&gt; int:\n    theoretical_spectrum: List[int] = calculate_linear_spectrum(peptide)\n    score: int = 0\n    unique_masses: set = set(theoretical_spectrum + experimental_spectrum)\n    \n    for mass in unique_masses:\n        score += min(theoretical_spectrum.count(mass), experimental_spectrum.count(mass))\n    \n    return score\n\n# Sample input\nsample_input: str = \"\"\"\nNQEL\n0 99 113 114 128 227 257 299 355 356 370 371 484\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\ninput_peptide: str = input_lines[0]\ninput_spectrum: List[int] = [int(x) for x in input_lines[1].split()]\n\n# Calculate and print the linear score\nresult: int = calculate_linear_score(input_peptide, input_spectrum)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-46",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-46",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "46.1 Sample Dataset",
    "text": "46.1 Sample Dataset\nLAST ALST TLLT TQAS\n0 71 87 101 113 158 184 188 259 271 372\n2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-46",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-46",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "46.2 Sample Output",
    "text": "46.2 Sample Output\nLAST ALST"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-46",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-46",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "46.3 Solution",
    "text": "46.3 Solution\nfrom typing import List, Dict\n\nAMINO_ACID_MASSES: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137,\n    'K': 128, 'M': 131, 'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156,\n    'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\n\ndef calculate_linear_spectrum(peptide: str) -&gt; List[int]:\n    prefix_masses: List[int] = [0]\n    \n    for amino_acid in peptide:\n        current_mass = prefix_masses[-1] + AMINO_ACID_MASSES[amino_acid]\n        prefix_masses.append(current_mass)\n    \n    linear_spectrum: List[int] = [0]\n    \n    for i in range(len(peptide)):\n        for j in range(i + 1, len(peptide) + 1):\n            subpeptide_mass = prefix_masses[j] - prefix_masses[i]\n            linear_spectrum.append(subpeptide_mass)\n    \n    return sorted(linear_spectrum)\n\ndef calculate_linear_score(peptide: str, experimental_spectrum: List[int]) -&gt; int:\n    theoretical_spectrum: List[int] = calculate_linear_spectrum(peptide)\n    score: int = 0\n    unique_masses: set = set(theoretical_spectrum + experimental_spectrum)\n    \n    for mass in unique_masses:\n        score += min(theoretical_spectrum.count(mass), experimental_spectrum.count(mass))\n    \n    return score\n\ndef trim_leaderboard(leaderboard: List[str], spectrum: List[int], n: int) -&gt; List[str]:\n    if len(leaderboard) &lt;= n:\n        return leaderboard\n\n    peptide_scores: Dict[int, int] = {}\n    for i, peptide in enumerate(leaderboard):\n        peptide_scores[i] = calculate_linear_score(peptide, spectrum)\n\n    sorted_scores: List[int] = sorted(peptide_scores.values(), reverse=True)\n    score_threshold: int = sorted_scores[n - 1]\n\n    return [leaderboard[idx] for idx, score in peptide_scores.items() if score &gt;= score_threshold]\n\n# Sample input\nsample_input: str = \"\"\"\nLAST ALST TLLT TQAS\n0 71 87 101 113 158 184 188 259 271 372\n2\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\ninput_leaderboard: List[str] = input_lines[0].split()\ninput_spectrum: List[int] = [int(x) for x in input_lines[1].split()]\ninput_n: int = int(input_lines[2])\n\nresult: List[str] = trim_leaderboard(input_leaderboard, input_spectrum, input_n)\nprint(\" \".join(result))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-47",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-47",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "47.1 Sample Dataset",
    "text": "47.1 Sample Dataset\n-10 -8 -7 -6 -5 -4 -3 -3 -2 -2 0 0 0 0 0 2 2 3 3 4 5 6 7 8 10"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-47",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-47",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "47.2 Sample Output",
    "text": "47.2 Sample Output\n0 2 4 7 10"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-47",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-47",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "47.3 Solution",
    "text": "47.3 Solution\nfrom typing import List, Set, Optional\n\ndef calculate_absolute_differences(set_a: Set[int], set_b: Set[int]) -&gt; List[int]:\n    return [abs(a - b) for a in set_a for b in set_b]\n\ndef is_multiset_subset(subset: List[int], superset: List[int]) -&gt; bool:\n    return all(subset.count(elem) &lt;= superset.count(elem) for elem in set(subset))\n\ndef multiset_difference(set_a: List[int], set_b: List[int]) -&gt; List[int]:\n    difference: List[int] = []\n    unique_elements: Set[int] = set(set_a)\n    for elem in unique_elements:\n        count_difference = set_a.count(elem) - set_b.count(elem)\n        if count_difference &gt; 0:\n            difference.extend([elem] * count_difference)\n    return sorted(difference)\n\ndef place_elements(distances: List[int]) -&gt; Optional[Set[int]]:\n    if not distances:\n        return placed_elements\n\n    current_distance: int = distances[-1]\n    \n    # Try placing on the left\n    left_differences: List[int] = calculate_absolute_differences({current_distance}, placed_elements)\n    if is_multiset_subset(left_differences, distances):\n        placed_elements.add(current_distance)\n        remaining_distances_left: List[int] = multiset_difference(distances, left_differences)\n        left_result: Optional[Set[int]] = place_elements(remaining_distances_left)\n        if left_result:\n            return left_result\n        placed_elements.remove(current_distance)\n\n    # Try placing on the right\n    right_differences: List[int] = calculate_absolute_differences({total_width - current_distance}, placed_elements)\n    if is_multiset_subset(right_differences, distances):\n        placed_elements.add(total_width - current_distance)\n        remaining_distances_right: List[int] = multiset_difference(distances, right_differences)\n        right_result: Optional[Set[int]] = place_elements(remaining_distances_right)\n        if right_result:\n            return right_result\n        placed_elements.remove(total_width - current_distance)\n\n    return None\n\n# Sample input\nsample_input: str = \"\"\"\n-10 -8 -7 -6 -5 -4 -3 -3 -2 -2 0 0 0 0 0 2 2 3 3 4 5 6 7 8 10\n\"\"\"\n\ninput_distances: List[int] = [int(x) for x in sample_input.strip().split()]\npositive_distances: List[int] = [x for x in input_distances if x &gt; 0]\n\ntotal_width: int = positive_distances.pop(-1)\nplaced_elements: Set[int] = {0, total_width}\n\nresult: Optional[Set[int]] = place_elements(positive_distances)\nprint(\" \".join(map(str, sorted(result))))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-48",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-48",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "48.1 Sample Dataset",
    "text": "48.1 Sample Dataset\n40\n1,5,10,20,25,50"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-48",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-48",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "48.2 Sample Output",
    "text": "48.2 Sample Output\n2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-48",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-48",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "48.3 Solution",
    "text": "48.3 Solution\nfrom typing import List\n\ndef min_coins_for_change(target_amount: int, available_coins: List[int]) -&gt; int:\n    min_coins_needed = [0]\n    for current_amount in range(1, target_amount + 1):\n        min_coins_needed.append(target_amount + 1)\n        for coin in available_coins:\n            if current_amount &gt;= coin:\n                coins_for_current = min_coins_needed[current_amount - coin] + 1\n                if coins_for_current &lt; min_coins_needed[current_amount]:\n                    min_coins_needed[current_amount] = coins_for_current\n    return min_coins_needed[target_amount]\n\nsample_input = \"\"\"\n40\n1,5,10,20,25,50\n\"\"\"\n\ninput_lines = sample_input.strip().split(\"\\n\")\ntarget_amount = int(input_lines[0])\navailable_coins = [int(x) for x in input_lines[1].split(\",\")]\n\nprint(min_coins_for_change(target_amount, available_coins))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-49",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-49",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "49.1 Sample Dataset",
    "text": "49.1 Sample Dataset\n4 4\n1 0 2 4 3\n4 6 5 2 1\n4 4 5 2 1\n5 6 8 5 3\n-\n3 2 4 0\n3 2 4 2\n0 7 3 3\n3 3 0 2\n1 3 2 2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-49",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-49",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "49.2 Sample Output",
    "text": "49.2 Sample Output\n34"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-49",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-49",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "49.3 Solution",
    "text": "49.3 Solution\nfrom typing import List, Tuple\n\ndef parse_manhattan_tourist_input(input_text: str) -&gt; Tuple[int, int, List[List[int]], List[List[int]]]:\n    lines = input_text.strip().split('\\n')\n    rows, cols = map(int, lines[0].split())\n    \n    down_weights = [[0] * (cols + 1) for _ in range(rows)]\n    for i in range(rows):\n        line = list(map(int, lines[i + 1].split()))\n        for j in range(cols + 1):\n            down_weights[i][j] = line[j]\n\n    separator_index = rows + 1\n    right_weights = [[0] * cols for _ in range(rows + 1)]\n    for i in range(rows + 1):\n        line = list(map(int, lines[i + separator_index + 1].split()))\n        for j in range(cols):\n            right_weights[i][j] = line[j]\n\n    return rows, cols, down_weights, right_weights\n\ndef calculate_longest_manhattan_path(rows: int, cols: int, down_weights: List[List[int]], right_weights: List[List[int]]) -&gt; int:\n    path_scores = [[0] * (cols + 1) for _ in range(rows + 1)]\n    \n    for i in range(1, rows + 1):\n        path_scores[i][0] = path_scores[i - 1][0] + down_weights[i - 1][0]\n\n    for j in range(1, cols + 1):\n        path_scores[0][j] = path_scores[0][j - 1] + right_weights[0][j - 1]\n\n    for i in range(1, rows + 1):\n        for j in range(1, cols + 1):\n            path_scores[i][j] = max(path_scores[i - 1][j] + down_weights[i - 1][j],\n                                    path_scores[i][j - 1] + right_weights[i][j - 1])\n    \n    return path_scores[rows][cols]\n\nsample_input = \"\"\"\n4 4\n1 0 2 4 3\n4 6 5 2 1\n4 4 5 2 1\n5 6 8 5 3\n-\n3 2 4 0\n3 2 4 2\n0 7 3 3\n3 3 0 2\n1 3 2 2\n\"\"\"\n\nrows, cols, down_weights, right_weights = parse_manhattan_tourist_input(sample_input)\nlongest_path_score = calculate_longest_manhattan_path(rows, cols, down_weights, right_weights)\nprint(f\"{longest_path_score}\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-50",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-50",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "50.1 Sample Dataset",
    "text": "50.1 Sample Dataset\nAACCTTGG\nACACTGTGA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-50",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-50",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "50.2 Sample Output",
    "text": "50.2 Sample Output\nAACTGG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-50",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-50",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "50.3 Solution",
    "text": "50.3 Solution\nfrom typing import List, Tuple\n\ndef longest_common_subsequence(sequence1: str, sequence2: str) -&gt; str:\n    padded_seq1 = '-' + sequence1\n    padded_seq2 = '-' + sequence2\n\n    score_matrix: List[List[int]] = [[0 for _ in range(len(padded_seq2))] for _ in range(len(padded_seq1))]\n    backtrack_matrix: List[List[str]] = [['' for _ in range(len(padded_seq2))] for _ in range(len(padded_seq1))]\n\n    for i in range(1, len(padded_seq1)):\n        for j in range(1, len(padded_seq2)):\n            match_score = score_matrix[i - 1][j - 1] + (1 if padded_seq1[i] == padded_seq2[j] else 0)\n            score_matrix[i][j] = max(score_matrix[i - 1][j], score_matrix[i][j - 1], match_score)\n\n            if score_matrix[i][j] == score_matrix[i - 1][j]:\n                backtrack_matrix[i][j] = \"up\"\n            elif score_matrix[i][j] == score_matrix[i][j - 1]:\n                backtrack_matrix[i][j] = \"left\"\n            else:\n                backtrack_matrix[i][j] = \"diag\"\n\n    lcs = \"\"\n    i, j = len(padded_seq1) - 1, len(padded_seq2) - 1\n    while i &gt; 0 and j &gt; 0:\n        if backtrack_matrix[i][j] == \"diag\":\n            lcs = padded_seq1[i] + lcs\n            i -= 1\n            j -= 1\n        elif backtrack_matrix[i][j] == \"left\":\n            j -= 1\n        else:\n            i -= 1\n\n    return lcs\n\nsample_input = \"\"\"\nAACCTTGG\nACACTGTGA\n\"\"\"\n\ninput_lines = sample_input.strip().split(\"\\n\")\nsequence1 = input_lines[0]\nsequence2 = input_lines[1]\n\nprint(longest_common_subsequence(sequence1, sequence2))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-51",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-51",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "51.1 Sample Dataset",
    "text": "51.1 Sample Dataset\n0\n4\n0-&gt;1:7\n0-&gt;2:4\n2-&gt;3:2\n1-&gt;4:1\n3-&gt;4:3"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-51",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-51",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "51.2 Sample Output",
    "text": "51.2 Sample Output\n9\n0-&gt;2-&gt;3-&gt;4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-51",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-51",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "51.3 Solution",
    "text": "51.3 Solution\nfrom typing import List, Tuple, Dict, Optional\n\nclass Node:\n    def __init__(self, label: str):\n        self.label: str = label\n        self.parent_nodes: List[Tuple['Node', int]] = []\n        self.target_nodes: List[Tuple['Node', int]] = []\n        self.visited: bool = False\n\nclass DAG:\n    def __init__(self):\n        self.nodes_dict: Dict[str, Node] = {}\n        self.distances: Dict[str, float] = {}\n        self.backtrack: Dict[str, Optional[str]] = {}\n\n    def add_node(self, label: str) -&gt; Node:\n        if label in self.nodes_dict:\n            return self.nodes_dict[label]\n\n        new_node = Node(label)\n        self.nodes_dict[label] = new_node\n        return new_node\n\n    def construct_dag(self, adjacency_list: List[str]) -&gt; None:\n        for line in adjacency_list:\n            source_label, temp = line.split(\"-&gt;\")\n            target_label, weight_str = temp.split(\":\")\n            weight = int(weight_str)\n\n            source_node = self.add_node(source_label)\n            target_node = self.add_node(target_label)\n\n            source_node.target_nodes.append((target_node, weight))\n            target_node.parent_nodes.append((source_node, weight))\n\n    def topological_sort_util(self, current_node: Node, sorted_labels: List[str]) -&gt; None:\n        current_node.visited = True\n        for neighbor, _ in current_node.target_nodes:\n            if not neighbor.visited:\n                self.topological_sort_util(neighbor, sorted_labels)\n        sorted_labels.insert(0, current_node.label)\n\n    def topological_sort(self) -&gt; List[str]:\n        sorted_labels: List[str] = []\n        for node in self.nodes_dict.values():\n            if not node.visited:\n                self.topological_sort_util(node, sorted_labels)\n        return sorted_labels\n\n    def longest_path(self, source: str, sink: str) -&gt; Tuple[float, List[str]]:\n        for label in self.nodes_dict:\n            self.distances[label] = float(\"-inf\")\n\n        self.distances[source] = 0\n        self.backtrack[source] = None\n\n        topological_order = self.topological_sort()\n        for label in topological_order:\n            current_node = self.nodes_dict[label]\n            for target_node, weight in current_node.target_nodes:\n                if self.distances[target_node.label] &lt; self.distances[label] + weight:\n                    self.distances[target_node.label] = self.distances[label] + weight\n                    self.backtrack[target_node.label] = label\n\n        path: List[str] = [sink]\n        current_label = self.backtrack[sink]\n        while current_label != source:\n            path = [current_label] + path\n            current_label = self.backtrack[current_label]\n        path = [source] + path\n        return self.distances[sink], path\n\n# Sample input\nsample_input: str = \"\"\"\n0\n4\n0-&gt;1:7\n0-&gt;2:4\n2-&gt;3:2\n1-&gt;4:1\n3-&gt;4:3\n\"\"\"\n\ninput_lines = sample_input.strip().split(\"\\n\")\nsource_label: str = input_lines[0]\nsink_label: str = input_lines[1]\nadjacency_list: List[str] = input_lines[2:]\n\ngraph = DAG()\ngraph.construct_dag(adjacency_list)\nlongest_distance, longest_path = graph.longest_path(source_label, sink_label)\nprint(longest_distance)\nprint(\"-&gt;\".join(longest_path))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-52",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-52",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "52.1 Sample Dataset",
    "text": "52.1 Sample Dataset\nPLEASANTLY\nMEANLY"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-52",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-52",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "52.2 Sample Output",
    "text": "52.2 Sample Output\n8\nPLEASANTLY\n-MEA--N-LY"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-52",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-52",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "52.3 Solution",
    "text": "52.3 Solution\nfrom typing import Dict, Tuple, List\n\nBLOSUM62: Dict[Tuple[str, str], int] = {\n    ('W', 'F'): 1, ('L', 'R'): -2, ('S', 'P'): -1, ('V', 'T'): 0,\n    ('Q', 'Q'): 5, ('N', 'A'): -2, ('Z', 'Y'): -2, ('W', 'R'): -3,\n    ('Q', 'A'): -1, ('S', 'D'): 0, ('H', 'H'): 8, ('S', 'H'): -1,\n    ('H', 'D'): -1, ('L', 'N'): -3, ('W', 'A'): -3, ('Y', 'M'): -1,\n    ('G', 'R'): -2, ('Y', 'I'): -1, ('Y', 'E'): -2, ('B', 'Y'): -3,\n    ('Y', 'A'): -2, ('V', 'D'): -3, ('B', 'S'): 0, ('Y', 'Y'): 7,\n    ('G', 'N'): 0, ('E', 'C'): -4, ('Y', 'Q'): -1, ('Z', 'Z'): 4,\n    ('V', 'A'): 0, ('C', 'C'): 9, ('M', 'R'): -1, ('V', 'E'): -2,\n    ('T', 'N'): 0, ('P', 'P'): 7, ('V', 'I'): 3, ('V', 'S'): -2,\n    ('Z', 'P'): -1, ('V', 'M'): 1, ('T', 'F'): -2, ('V', 'Q'): -2,\n    ('K', 'K'): 5, ('P', 'D'): -1, ('I', 'H'): -3, ('I', 'D'): -3,\n    ('T', 'R'): -1, ('P', 'L'): -3, ('K', 'G'): -2, ('M', 'N'): -2,\n    ('P', 'H'): -2, ('F', 'Q'): -3, ('Z', 'G'): -2, ('X', 'L'): -1,\n    ('T', 'M'): -1, ('Z', 'C'): -3, ('X', 'H'): -1, ('D', 'R'): -2,\n    ('B', 'W'): -4, ('X', 'D'): -1, ('Z', 'K'): 1, ('F', 'A'): -2,\n    ('Z', 'W'): -3, ('F', 'E'): -3, ('D', 'N'): 1, ('B', 'K'): 0,\n    ('X', 'X'): -1, ('F', 'I'): 0, ('B', 'G'): -1, ('X', 'T'): 0,\n    ('F', 'M'): 0, ('B', 'C'): -3, ('Z', 'I'): -3, ('Z', 'V'): -2,\n    ('S', 'S'): 4, ('L', 'Q'): -2, ('W', 'E'): -3, ('Q', 'R'): 1,\n    ('N', 'N'): 6, ('W', 'M'): -1, ('Q', 'C'): -3, ('W', 'I'): -3,\n    ('S', 'C'): -1, ('L', 'A'): -1, ('S', 'G'): 0, ('L', 'E'): -3,\n    ('W', 'Q'): -2, ('H', 'G'): -2, ('S', 'K'): 0, ('Q', 'N'): 0,\n    ('N', 'R'): 0, ('H', 'C'): -3, ('Y', 'N'): -2, ('G', 'Q'): -2,\n    ('Y', 'F'): 3, ('C', 'A'): 0, ('V', 'L'): 1, ('G', 'E'): -2,\n    ('G', 'A'): 0, ('K', 'R'): 2, ('E', 'D'): 2, ('Y', 'R'): -2,\n    ('M', 'Q'): 0, ('T', 'I'): -1, ('C', 'D'): -3, ('V', 'F'): -1,\n    ('T', 'A'): 0, ('T', 'P'): -1, ('B', 'P'): -2, ('T', 'E'): -1,\n    ('V', 'N'): -3, ('P', 'G'): -2, ('M', 'A'): -1, ('K', 'H'): -1,\n    ('V', 'R'): -3, ('P', 'C'): -3, ('M', 'E'): -2, ('K', 'L'): -2,\n    ('V', 'V'): 4, ('M', 'I'): 1, ('T', 'Q'): -1, ('I', 'G'): -4,\n    ('P', 'K'): -1, ('M', 'M'): 5, ('K', 'D'): -1, ('I', 'C'): -1,\n    ('Z', 'D'): 1, ('F', 'R'): -3, ('X', 'K'): -1, ('Q', 'D'): 0,\n    ('X', 'G'): -1, ('Z', 'L'): -3, ('X', 'C'): -2, ('Z', 'H'): 0,\n    ('B', 'L'): -4, ('B', 'H'): 0, ('F', 'F'): 6, ('X', 'W'): -2,\n    ('B', 'D'): 4, ('D', 'A'): -2, ('S', 'L'): -2, ('X', 'S'): 0,\n    ('F', 'N'): -3, ('S', 'R'): -1, ('W', 'D'): -4, ('V', 'Y'): -1,\n    ('W', 'L'): -2, ('H', 'R'): 0, ('W', 'H'): -2, ('H', 'N'): 1,\n    ('W', 'T'): -2, ('T', 'T'): 5, ('S', 'F'): -2, ('W', 'P'): -4,\n    ('L', 'D'): -4, ('B', 'I'): -3, ('L', 'H'): -3, ('S', 'N'): 1,\n    ('B', 'T'): -1, ('L', 'L'): 4, ('Y', 'K'): -2, ('E', 'Q'): 2,\n    ('Y', 'G'): -3, ('Z', 'S'): 0, ('Y', 'C'): -2, ('G', 'D'): -1,\n    ('B', 'V'): -3, ('E', 'A'): -1, ('Y', 'W'): 2, ('E', 'E'): 5,\n    ('Y', 'S'): -2, ('C', 'N'): -3, ('V', 'C'): -1, ('T', 'H'): -2,\n    ('P', 'R'): -2, ('V', 'G'): -3, ('T', 'L'): -1, ('V', 'K'): -2,\n    ('K', 'Q'): 1, ('R', 'A'): -1, ('I', 'R'): -3, ('T', 'D'): -1,\n    ('P', 'F'): -4, ('I', 'N'): -3, ('K', 'I'): -3, ('M', 'D'): -3,\n    ('V', 'W'): -3, ('W', 'W'): 11, ('M', 'H'): -2, ('P', 'N'): -2,\n    ('K', 'A'): -1, ('M', 'L'): 2, ('K', 'E'): 1, ('Z', 'E'): 4,\n    ('X', 'N'): -1, ('Z', 'A'): -1, ('Z', 'M'): -1, ('X', 'F'): -1,\n    ('K', 'C'): -3, ('B', 'Q'): 0, ('X', 'B'): -1, ('B', 'M'): -3,\n    ('F', 'C'): -2, ('Z', 'Q'): 3, ('X', 'Z'): -1, ('F', 'G'): -3,\n    ('B', 'E'): 1, ('X', 'V'): -1, ('F', 'K'): -3, ('B', 'A'): -2,\n    ('X', 'R'): -1, ('D', 'D'): 6, ('W', 'G'): -2, ('Z', 'F'): -3,\n    ('S', 'Q'): 0, ('W', 'C'): -2, ('W', 'K'): -3, ('H', 'Q'): 0,\n    ('L', 'C'): -1, ('W', 'N'): -4, ('S', 'A'): 1, ('L', 'G'): -4,\n    ('W', 'S'): -3, ('S', 'E'): 0, ('H', 'E'): 0, ('S', 'I'): -2,\n    ('H', 'A'): -2, ('S', 'M'): -1, ('Y', 'L'): -1, ('Y', 'H'): 2,\n    ('Y', 'D'): -3, ('E', 'R'): 0, ('X', 'P'): -2, ('G', 'G'): 6,\n    ('G', 'C'): -3, ('E', 'N'): 0, ('Y', 'T'): -2, ('Y', 'P'): -3,\n    ('T', 'K'): -1, ('A', 'A'): 4, ('P', 'Q'): -1, ('T', 'C'): -1,\n    ('V', 'H'): -3, ('T', 'G'): -2, ('I', 'Q'): -3, ('Z', 'T'): -1,\n    ('C', 'R'): -3, ('V', 'P'): -2, ('P', 'E'): -1, ('M', 'C'): -1,\n    ('K', 'N'): 0, ('I', 'I'): 4, ('P', 'A'): -1, ('M', 'G'): -3,\n    ('T', 'S'): 1, ('I', 'E'): -3, ('P', 'M'): -2, ('M', 'K'): -1,\n    ('I', 'A'): -1, ('P', 'I'): -3, ('R', 'R'): 5, ('X', 'M'): -1,\n    ('L', 'I'): 2, ('X', 'I'): -1, ('Z', 'B'): 1, ('X', 'E'): -1,\n    ('Z', 'N'): 0, ('X', 'A'): 0, ('B', 'R'): -1, ('B', 'N'): 3,\n    ('F', 'D'): -3, ('X', 'Y'): -1, ('Z', 'R'): 0, ('F', 'H'): -1,\n    ('B', 'F'): -3, ('F', 'L'): 0, ('X', 'Q'): -1, ('B', 'B'): 4\n}\n\ndef global_alignment(sequence1: str, sequence2: str, indel_penalty: int = 5) -&gt; Tuple[int, str, str]:\n    padded_seq1: str = \"-\" + sequence1\n    padded_seq2: str = \"-\" + sequence2\n\n    score_matrix: List[List[int]] = [[0 for _ in range(len(padded_seq2))] for _ in range(len(padded_seq1))]\n    backtrack_matrix: List[List[str]] = [['' for _ in range(len(padded_seq2))] for _ in range(len(padded_seq1))]\n\n    for j in range(len(padded_seq2)):\n        score_matrix[0][j] = -indel_penalty * j\n        backtrack_matrix[0][j] = \"left\"\n\n    for i in range(len(padded_seq1)):\n        score_matrix[i][0] = -indel_penalty * i\n        backtrack_matrix[i][0] = \"up\"\n\n    for i in range(1, len(padded_seq1)):\n        for j in range(1, len(padded_seq2)):\n            key: Tuple[str, str] = (padded_seq1[i], padded_seq2[j]) if (padded_seq1[i], padded_seq2[j]) in BLOSUM62 else (padded_seq2[j], padded_seq1[i])\n            diagonal_score: int = score_matrix[i - 1][j - 1] + BLOSUM62[key]\n            up_score: int = score_matrix[i - 1][j] - indel_penalty\n            left_score: int = score_matrix[i][j - 1] - indel_penalty\n            score_matrix[i][j] = max(diagonal_score, up_score, left_score)\n            \n            if score_matrix[i][j] == diagonal_score:\n                backtrack_matrix[i][j] = \"diagonal\"\n            elif score_matrix[i][j] == up_score:\n                backtrack_matrix[i][j] = \"up\"\n            else:\n                backtrack_matrix[i][j] = \"left\"\n\n    i: int = len(padded_seq1) - 1\n    j: int = len(padded_seq2) - 1\n    aligned_seq1: str = \"\"\n    aligned_seq2: str = \"\"\n    \n    while i != 0 or j != 0:\n        direction: str = backtrack_matrix[i][j]\n        if direction == \"diagonal\":\n            aligned_seq1 = padded_seq1[i] + aligned_seq1\n            aligned_seq2 = padded_seq2[j] + aligned_seq2\n            i -= 1\n            j -= 1\n        elif direction == \"up\":\n            aligned_seq1 = padded_seq1[i] + aligned_seq1\n            aligned_seq2 = \"-\" + aligned_seq2\n            i -= 1\n        else:\n            aligned_seq1 = \"-\" + aligned_seq1\n            aligned_seq2 = padded_seq2[j] + aligned_seq2\n            j -= 1\n\n    return score_matrix[len(padded_seq1) - 1][len(padded_seq2) - 1], aligned_seq1, aligned_seq2\n\n# Sample input\nsample_input: str = \"\"\"\nPLEASANTLY\nMEANLY\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nsequence1: str = input_lines[0]\nsequence2: str = input_lines[1]\nalignment_score: int\naligned_sequence1: str\naligned_sequence2: str\nalignment_score, aligned_sequence1, aligned_sequence2 = global_alignment(sequence1, sequence2)\nprint(alignment_score)\nprint(aligned_sequence1)\nprint(aligned_sequence2)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-53",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-53",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "53.1 Sample Dataset",
    "text": "53.1 Sample Dataset\nMEANLY\nPENALTY"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-53",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-53",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "53.2 Sample Output",
    "text": "53.2 Sample Output\n15\nEANL-Y\nENALTY"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-53",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-53",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "53.3 Solution",
    "text": "53.3 Solution\nfrom typing import Dict, List, Tuple, Optional\n\nPAM250: Dict[str, Dict[str, int]] = {\n        'A': {'A': 2, 'C': -2, 'D': 0, 'E': 0, 'F': -3, 'G': 1, 'H': -1, 'I': -1, 'K': -1, 'L': -2, 'M': -1, 'N': 0,\n                'P': 1, 'Q': 0, 'R': -2, 'S': 1, 'T': 1, 'V': 0, 'W': -6, 'Y': -3},\n          'C': {'A': -2, 'C': 12, 'D': -5, 'E': -5, 'F': -4, 'G': -3, 'H': -3, 'I': -2, 'K': -5, 'L': -6, 'M': -5,\n                'N': -4, 'P': -3, 'Q': -5, 'R': -4, 'S': 0, 'T': -2, 'V': -2, 'W': -8, 'Y': 0},\n          'D': {'A': 0, 'C': -5, 'D': 4, 'E': 3, 'F': -6, 'G': 1, 'H': 1, 'I': -2, 'K': 0, 'L': -4, 'M': -3, 'N': 2,\n                'P': -1, 'Q': 2, 'R': -1, 'S': 0, 'T': 0, 'V': -2, 'W': -7, 'Y': -4},\n          'E': {'A': 0, 'C': -5, 'D': 3, 'E': 4, 'F': -5, 'G': 0, 'H': 1, 'I': -2, 'K': 0, 'L': -3, 'M': -2, 'N': 1,\n                'P': -1, 'Q': 2, 'R': -1, 'S': 0, 'T': 0, 'V': -2, 'W': -7, 'Y': -4},\n          'F': {'A': -3, 'C': -4, 'D': -6, 'E': -5, 'F': 9, 'G': -5, 'H': -2, 'I': 1, 'K': -5, 'L': 2, 'M': 0, 'N': -3,\n                'P': -5, 'Q': -5, 'R': -4, 'S': -3, 'T': -3, 'V': -1, 'W': 0, 'Y': 7},\n          'G': {'A': 1, 'C': -3, 'D': 1, 'E': 0, 'F': -5, 'G': 5, 'H': -2, 'I': -3, 'K': -2, 'L': -4, 'M': -3, 'N': 0,\n                'P': 0, 'Q': -1, 'R': -3, 'S': 1, 'T': 0, 'V': -1, 'W': -7, 'Y': -5},\n          'H': {'A': -1, 'C': -3, 'D': 1, 'E': 1, 'F': -2, 'G': -2, 'H': 6, 'I': -2, 'K': 0, 'L': -2, 'M': -2, 'N': 2,\n                'P': 0, 'Q': 3, 'R': 2, 'S': -1, 'T': -1, 'V': -2, 'W': -3, 'Y': 0},\n          'I': {'A': -1, 'C': -2, 'D': -2, 'E': -2, 'F': 1, 'G': -3, 'H': -2, 'I': 5, 'K': -2, 'L': 2, 'M': 2, 'N': -2,\n                'P': -2, 'Q': -2, 'R': -2, 'S': -1, 'T': 0, 'V': 4, 'W': -5, 'Y': -1},\n          'K': {'A': -1, 'C': -5, 'D': 0, 'E': 0, 'F': -5, 'G': -2, 'H': 0, 'I': -2, 'K': 5, 'L': -3, 'M': 0, 'N': 1,\n                'P': -1, 'Q': 1, 'R': 3, 'S': 0, 'T': 0, 'V': -2, 'W': -3, 'Y': -4},\n          'L': {'A': -2, 'C': -6, 'D': -4, 'E': -3, 'F': 2, 'G': -4, 'H': -2, 'I': 2, 'K': -3, 'L': 6, 'M': 4, 'N': -3,\n                'P': -3, 'Q': -2, 'R': -3, 'S': -3, 'T': -2, 'V': 2, 'W': -2, 'Y': -1},\n          'M': {'A': -1, 'C': -5, 'D': -3, 'E': -2, 'F': 0, 'G': -3, 'H': -2, 'I': 2, 'K': 0, 'L': 4, 'M': 6, 'N': -2,\n                'P': -2, 'Q': -1, 'R': 0, 'S': -2, 'T': -1, 'V': 2, 'W': -4, 'Y': -2},\n          'N': {'A': 0, 'C': -4, 'D': 2, 'E': 1, 'F': -3, 'G': 0, 'H': 2, 'I': -2, 'K': 1, 'L': -3, 'M': -2, 'N': 2,\n                'P': 0, 'Q': 1, 'R': 0, 'S': 1, 'T': 0, 'V': -2, 'W': -4, 'Y': -2},\n          'P': {'A': 1, 'C': -3, 'D': -1, 'E': -1, 'F': -5, 'G': 0, 'H': 0, 'I': -2, 'K': -1, 'L': -3, 'M': -2, 'N': 0,\n                'P': 6, 'Q': 0, 'R': 0, 'S': 1, 'T': 0, 'V': -1, 'W': -6, 'Y': -5},\n          'Q': {'A': 0, 'C': -5, 'D': 2, 'E': 2, 'F': -5, 'G': -1, 'H': 3, 'I': -2, 'K': 1, 'L': -2, 'M': -1, 'N': 1,\n                'P': 0, 'Q': 4, 'R': 1, 'S': -1, 'T': -1, 'V': -2, 'W': -5, 'Y': -4},\n          'R': {'A': -2, 'C': -4, 'D': -1, 'E': -1, 'F': -4, 'G': -3, 'H': 2, 'I': -2, 'K': 3, 'L': -3, 'M': 0, 'N': 0,\n                'P': 0, 'Q': 1, 'R': 6, 'S': 0, 'T': -1, 'V': -2, 'W': 2, 'Y': -4},\n          'S': {'A': 1, 'C': 0, 'D': 0, 'E': 0, 'F': -3, 'G': 1, 'H': -1, 'I': -1, 'K': 0, 'L': -3, 'M': -2, 'N': 1,\n                'P': 1, 'Q': -1, 'R': 0, 'S': 2, 'T': 1, 'V': -1, 'W': -2, 'Y': -3},\n          'T': {'A': 1, 'C': -2, 'D': 0, 'E': 0, 'F': -3, 'G': 0, 'H': -1, 'I': 0, 'K': 0, 'L': -2, 'M': -1, 'N': 0,\n                'P': 0, 'Q': -1, 'R': -1, 'S': 1, 'T': 3, 'V': 0, 'W': -5, 'Y': -3},\n          'V': {'A': 0, 'C': -2, 'D': -2, 'E': -2, 'F': -1, 'G': -1, 'H': -2, 'I': 4, 'K': -2, 'L': 2, 'M': 2, 'N': -2,\n                'P': -1, 'Q': -2, 'R': -2, 'S': -1, 'T': 0, 'V': 4, 'W': -6, 'Y': -2},\n          'W': {'A': -6, 'C': -8, 'D': -7, 'E': -7, 'F': 0, 'G': -7, 'H': -3, 'I': -5, 'K': -3, 'L': -2, 'M': -4,\n                'N': -4, 'P': -6, 'Q': -5, 'R': 2, 'S': -2, 'T': -5, 'V': -6, 'W': 17, 'Y': 0},\n          'Y': {'A': -3, 'C': 0, 'D': -4, 'E': -4, 'F': 7, 'G': -5, 'H': 0, 'I': -1, 'K': -4, 'L': -1, 'M': -2, 'N': -2,\n                'P': -5, 'Q': -4, 'R': -4, 'S': -3, 'T': -3, 'V': -2, 'W': 0, 'Y': 10}}\n\ndef local_alignment(sequence1: str, sequence2: str, indel_penalty: int = 5) -&gt; Tuple[int, str, str]:\n    padded_seq1: str = \"-\" + sequence1\n    padded_seq2: str = \"-\" + sequence2\n\n    score_matrix: List[List[int]] = [[0 for _ in range(len(padded_seq2))] for _ in range(len(padded_seq1))]\n    backtrack_matrix: List[List[Optional[str]]] = [[None for _ in range(len(padded_seq2))] for _ in range(len(padded_seq1))]\n\n    for i in range(1, len(padded_seq1)):\n        for j in range(1, len(padded_seq2)):\n            key1: str = padded_seq1[i] if padded_seq1[i] in PAM250 else padded_seq2[j]\n            key2: str = padded_seq2[j] if padded_seq1[i] in PAM250 else padded_seq1[i]\n\n            diagonal_score: int = score_matrix[i - 1][j - 1] + PAM250[key1][key2]\n            up_score: int = score_matrix[i - 1][j] - indel_penalty\n            left_score: int = score_matrix[i][j - 1] - indel_penalty\n            score_matrix[i][j] = max(diagonal_score, up_score, left_score, 0)\n\n            if score_matrix[i][j] == diagonal_score:\n                backtrack_matrix[i][j] = \"diagonal\"\n            elif score_matrix[i][j] == up_score:\n                backtrack_matrix[i][j] = \"up\"\n            elif score_matrix[i][j] == left_score:\n                backtrack_matrix[i][j] = \"left\"\n\n    max_score: int = -1\n    max_i: int = 0\n    max_j: int = 0\n    for i in range(len(padded_seq1)):\n        for j in range(len(padded_seq2)):\n            if score_matrix[i][j] &gt; max_score:\n                max_score = score_matrix[i][j]\n                max_i, max_j = i, j\n\n    i: int = max_i\n    j: int = max_j\n    aligned_seq1: str = \"\"\n    aligned_seq2: str = \"\"\n    while backtrack_matrix[i][j] is not None:\n        direction: str = backtrack_matrix[i][j]\n        if direction == \"diagonal\":\n            aligned_seq1 = padded_seq1[i] + aligned_seq1\n            aligned_seq2 = padded_seq2[j] + aligned_seq2\n            i -= 1\n            j -= 1\n        elif direction == \"up\":\n            aligned_seq1 = padded_seq1[i] + aligned_seq1\n            aligned_seq2 = \"-\" + aligned_seq2\n            i -= 1\n        else:\n            aligned_seq1 = \"-\" + aligned_seq1\n            aligned_seq2 = padded_seq2[j] + aligned_seq2\n            j -= 1\n\n    return max_score, aligned_seq1, aligned_seq2\n\n# Sample input\nsample_input: str = \"\"\"\nMEANLY\nPENALTY\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nsequence1: str = input_lines[0]\nsequence2: str = input_lines[1]\n\nalignment_score: int\naligned_sequence1: str\naligned_sequence2: str\nalignment_score, aligned_sequence1, aligned_sequence2 = local_alignment(sequence1, sequence2)\nprint(alignment_score)\nprint(aligned_sequence1)\nprint(aligned_sequence2)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-54",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-54",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "54.1 Sample Dataset",
    "text": "54.1 Sample Dataset\nPLEASANTLY\nMEANLY"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-54",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-54",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "54.2 Sample Output",
    "text": "54.2 Sample Output\n5"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-54",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-54",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "54.3 Solution",
    "text": "54.3 Solution\ndef calculate_edit_distance(source: str, target: str) -&gt; int:\n    distance_matrix = {}\n    \n    for target_index in range(len(target) + 1):\n        distance_matrix[target_index, 0] = target_index\n    \n    for source_index in range(len(source) + 1):\n        distance_matrix[0, source_index] = source_index\n\n    for target_index in range(len(target)):\n        for source_index in range(len(source)):\n            if source[source_index] == target[target_index]:\n                distance_matrix[target_index + 1, source_index + 1] = distance_matrix[target_index, source_index]\n            else:\n                distance_matrix[target_index + 1, source_index + 1] = min([\n                    distance_matrix[target_index + 1, source_index],\n                    distance_matrix[target_index, source_index],\n                    distance_matrix[target_index, source_index + 1]\n                ]) + 1\n\n    return distance_matrix[len(target), len(source)]\n\nsample_input: str = \"\"\"\nPLEASANTLY\nMEANLY\n\"\"\"\n\nsource, target = sample_input.strip().split(\"\\n\")\nprint(calculate_edit_distance(source, target))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-55",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-55",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "55.1 Sample Dataset",
    "text": "55.1 Sample Dataset\nGTAGGCTTAAGGTTA\nTAGATA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-55",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-55",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "55.2 Sample Output",
    "text": "55.2 Sample Output\n2\nTAGGCTTA\nTAGA--TA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-55",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-55",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "55.3 Solution",
    "text": "55.3 Solution\nfrom typing import Tuple, Dict\n\ndef calculate_fitting_alignment(sequence1: str, sequence2: str) -&gt; Tuple[int, str, str]:\n    score_matrix: Dict[Tuple[int, int], int] = {}\n    path_matrix: Dict[Tuple[int, int], str] = {}\n    \n    for seq2_index in range(len(sequence2) + 1):\n        score_matrix[seq2_index, 0] = -seq2_index\n        path_matrix[seq2_index, 0] = \"↑\"\n    for seq1_index in range(len(sequence1) + 1):\n        score_matrix[0, seq1_index] = 0\n        path_matrix[0, seq1_index] = \"←\"\n\n    score_matrix[0, 0] = 0\n    for seq2_index in range(len(sequence2)):\n        for seq1_index in range(len(sequence1)):\n            current_position = (seq2_index + 1, seq1_index + 1)\n            match_score = 1 if sequence1[seq1_index] == sequence2[seq2_index] else -1\n            options = [\n                score_matrix[seq2_index, seq1_index] + match_score,\n                score_matrix[seq2_index, seq1_index + 1] - 1,\n                score_matrix[seq2_index + 1, seq1_index] - 1,\n            ]\n            score_matrix[current_position] = max(options)\n            path_matrix[current_position] = [\"↖\", \"↑\", \"←\"][options.index(max(options))]\n\n    final_scores = [score_matrix[len(sequence2), i] for i in range(len(sequence1) + 1)]\n    max_score = max(final_scores)\n    seq1_end_index = final_scores.index(max_score)\n    seq2_end_index = len(sequence2)\n\n    aligned_seq1, aligned_seq2 = \"\", \"\"\n    while seq1_end_index &gt; 0 and seq2_end_index &gt; 0:\n        if path_matrix[seq2_end_index, seq1_end_index] == \"↖\":\n            aligned_seq1 += sequence1[seq1_end_index - 1]\n            aligned_seq2 += sequence2[seq2_end_index - 1]\n            seq2_end_index, seq1_end_index = seq2_end_index - 1, seq1_end_index - 1\n        elif path_matrix[seq2_end_index, seq1_end_index] == \"←\":\n            aligned_seq1 += sequence1[seq1_end_index - 1]\n            aligned_seq2 += \"-\"\n            seq1_end_index = seq1_end_index - 1\n        elif path_matrix[seq2_end_index, seq1_end_index] == \"↑\":\n            aligned_seq1 += \"-\"\n            aligned_seq2 += sequence2[seq2_end_index - 1]\n            seq2_end_index = seq2_end_index - 1\n\n    return max_score, aligned_seq1[::-1], aligned_seq2[::-1]\n\nsample_input: str = \"\"\"\nGTAGGCTTAAGGTTA\nTAGATA\n\"\"\"\n\nsequence1, sequence2 = sample_input.strip().split(\"\\n\")\nprint(*calculate_fitting_alignment(sequence1, sequence2), sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-56",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-56",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "56.1 Sample Dataset",
    "text": "56.1 Sample Dataset\nPAWHEAE\nHEAGAWGHEE"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-56",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-56",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "56.2 Sample Output",
    "text": "56.2 Sample Output\n1\nHEAE\nHEAG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-56",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-56",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "56.3 Solution",
    "text": "56.3 Solution\nfrom typing import Tuple, Dict\n\ndef calculate_overlap_alignment(sequence1: str, sequence2: str, mismatch_penalty: int = -2) -&gt; Tuple[int, str, str]:\n    score_matrix: Dict[Tuple[int, int], int] = {}\n    path_matrix: Dict[Tuple[int, int], str] = {}\n    \n    for seq2_index in range(len(sequence2) + 1):\n        score_matrix[seq2_index, 0] = seq2_index * mismatch_penalty\n        path_matrix[seq2_index, 0] = \"↑\"\n    for seq1_index in range(len(sequence1) + 1):\n        score_matrix[0, seq1_index] = 0\n        path_matrix[0, seq1_index] = \"←\"\n\n    score_matrix[0, 0] = 0\n    for seq2_index in range(len(sequence2)):\n        for seq1_index in range(len(sequence1)):\n            current_position = (seq2_index + 1, seq1_index + 1)\n            match_score = 1 if sequence1[seq1_index] == sequence2[seq2_index] else mismatch_penalty\n            options = [\n                score_matrix[seq2_index, seq1_index] + match_score,\n                score_matrix[seq2_index, seq1_index + 1] + mismatch_penalty,\n                score_matrix[seq2_index + 1, seq1_index] + mismatch_penalty,\n            ]\n            score_matrix[current_position] = max(options)\n            path_matrix[current_position] = [\"↖\", \"↑\", \"←\"][options.index(max(options))]\n\n    final_scores = [score_matrix[seq2_index, len(sequence1)] for seq2_index in range(len(sequence2) + 1)]\n    max_score = max(final_scores)\n    seq2_end_index = final_scores.index(max_score)\n    seq1_end_index = len(sequence1)\n\n    aligned_seq1, aligned_seq2 = \"\", \"\"\n    while seq1_end_index &gt; 0 and seq2_end_index &gt; 0:\n        if path_matrix[seq2_end_index, seq1_end_index] == \"↖\":\n            aligned_seq1 += sequence1[seq1_end_index - 1]\n            aligned_seq2 += sequence2[seq2_end_index - 1]\n            seq2_end_index, seq1_end_index = seq2_end_index - 1, seq1_end_index - 1\n        elif path_matrix[seq2_end_index, seq1_end_index] == \"←\":\n            aligned_seq1 += sequence1[seq1_end_index - 1]\n            aligned_seq2 += \"-\"\n            seq1_end_index = seq1_end_index - 1\n        elif path_matrix[seq2_end_index, seq1_end_index] == \"↑\":\n            aligned_seq1 += \"-\"\n            aligned_seq2 += sequence2[seq2_end_index - 1]\n            seq2_end_index = seq2_end_index - 1\n\n    return max_score, aligned_seq1[::-1], aligned_seq2[::-1]\n\nsample_input: str = \"\"\"\nPAWHEAE\nHEAGAWGHEE\n\"\"\"\n\nsequence1, sequence2 = sample_input.strip().split(\"\\n\")\nprint(*calculate_overlap_alignment(sequence1, sequence2), sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-57",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-57",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "57.1 Sample Dataset",
    "text": "57.1 Sample Dataset\nPRTEINS\nPRTWPSEIN"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-57",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-57",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "57.2 Sample Output",
    "text": "57.2 Sample Output\n8\nPRT---EINS\nPRTWPSEIN-"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-57",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-57",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "57.3 Solution",
    "text": "57.3 Solution\nfrom typing import Dict, Tuple, List, Optional\n\nBLOSUM62: Dict[Tuple[str, str], int] = {\n    ('W', 'F'): 1, ('L', 'R'): -2, ('S', 'P'): -1, ('V', 'T'): 0,\n    ('Q', 'Q'): 5, ('N', 'A'): -2, ('Z', 'Y'): -2, ('W', 'R'): -3,\n    ('Q', 'A'): -1, ('S', 'D'): 0, ('H', 'H'): 8, ('S', 'H'): -1,\n    ('H', 'D'): -1, ('L', 'N'): -3, ('W', 'A'): -3, ('Y', 'M'): -1,\n    ('G', 'R'): -2, ('Y', 'I'): -1, ('Y', 'E'): -2, ('B', 'Y'): -3,\n    ('Y', 'A'): -2, ('V', 'D'): -3, ('B', 'S'): 0, ('Y', 'Y'): 7,\n    ('G', 'N'): 0, ('E', 'C'): -4, ('Y', 'Q'): -1, ('Z', 'Z'): 4,\n    ('V', 'A'): 0, ('C', 'C'): 9, ('M', 'R'): -1, ('V', 'E'): -2,\n    ('T', 'N'): 0, ('P', 'P'): 7, ('V', 'I'): 3, ('V', 'S'): -2,\n    ('Z', 'P'): -1, ('V', 'M'): 1, ('T', 'F'): -2, ('V', 'Q'): -2,\n    ('K', 'K'): 5, ('P', 'D'): -1, ('I', 'H'): -3, ('I', 'D'): -3,\n    ('T', 'R'): -1, ('P', 'L'): -3, ('K', 'G'): -2, ('M', 'N'): -2,\n    ('P', 'H'): -2, ('F', 'Q'): -3, ('Z', 'G'): -2, ('X', 'L'): -1,\n    ('T', 'M'): -1, ('Z', 'C'): -3, ('X', 'H'): -1, ('D', 'R'): -2,\n    ('B', 'W'): -4, ('X', 'D'): -1, ('Z', 'K'): 1, ('F', 'A'): -2,\n    ('Z', 'W'): -3, ('F', 'E'): -3, ('D', 'N'): 1, ('B', 'K'): 0,\n    ('X', 'X'): -1, ('F', 'I'): 0, ('B', 'G'): -1, ('X', 'T'): 0,\n    ('F', 'M'): 0, ('B', 'C'): -3, ('Z', 'I'): -3, ('Z', 'V'): -2,\n    ('S', 'S'): 4, ('L', 'Q'): -2, ('W', 'E'): -3, ('Q', 'R'): 1,\n    ('N', 'N'): 6, ('W', 'M'): -1, ('Q', 'C'): -3, ('W', 'I'): -3,\n    ('S', 'C'): -1, ('L', 'A'): -1, ('S', 'G'): 0, ('L', 'E'): -3,\n    ('W', 'Q'): -2, ('H', 'G'): -2, ('S', 'K'): 0, ('Q', 'N'): 0,\n    ('N', 'R'): 0, ('H', 'C'): -3, ('Y', 'N'): -2, ('G', 'Q'): -2,\n    ('Y', 'F'): 3, ('C', 'A'): 0, ('V', 'L'): 1, ('G', 'E'): -2,\n    ('G', 'A'): 0, ('K', 'R'): 2, ('E', 'D'): 2, ('Y', 'R'): -2,\n    ('M', 'Q'): 0, ('T', 'I'): -1, ('C', 'D'): -3, ('V', 'F'): -1,\n    ('T', 'A'): 0, ('T', 'P'): -1, ('B', 'P'): -2, ('T', 'E'): -1,\n    ('V', 'N'): -3, ('P', 'G'): -2, ('M', 'A'): -1, ('K', 'H'): -1,\n    ('V', 'R'): -3, ('P', 'C'): -3, ('M', 'E'): -2, ('K', 'L'): -2,\n    ('V', 'V'): 4, ('M', 'I'): 1, ('T', 'Q'): -1, ('I', 'G'): -4,\n    ('P', 'K'): -1, ('M', 'M'): 5, ('K', 'D'): -1, ('I', 'C'): -1,\n    ('Z', 'D'): 1, ('F', 'R'): -3, ('X', 'K'): -1, ('Q', 'D'): 0,\n    ('X', 'G'): -1, ('Z', 'L'): -3, ('X', 'C'): -2, ('Z', 'H'): 0,\n    ('B', 'L'): -4, ('B', 'H'): 0, ('F', 'F'): 6, ('X', 'W'): -2,\n    ('B', 'D'): 4, ('D', 'A'): -2, ('S', 'L'): -2, ('X', 'S'): 0,\n    ('F', 'N'): -3, ('S', 'R'): -1, ('W', 'D'): -4, ('V', 'Y'): -1,\n    ('W', 'L'): -2, ('H', 'R'): 0, ('W', 'H'): -2, ('H', 'N'): 1,\n    ('W', 'T'): -2, ('T', 'T'): 5, ('S', 'F'): -2, ('W', 'P'): -4,\n    ('L', 'D'): -4, ('B', 'I'): -3, ('L', 'H'): -3, ('S', 'N'): 1,\n    ('B', 'T'): -1, ('L', 'L'): 4, ('Y', 'K'): -2, ('E', 'Q'): 2,\n    ('Y', 'G'): -3, ('Z', 'S'): 0, ('Y', 'C'): -2, ('G', 'D'): -1,\n    ('B', 'V'): -3, ('E', 'A'): -1, ('Y', 'W'): 2, ('E', 'E'): 5,\n    ('Y', 'S'): -2, ('C', 'N'): -3, ('V', 'C'): -1, ('T', 'H'): -2,\n    ('P', 'R'): -2, ('V', 'G'): -3, ('T', 'L'): -1, ('V', 'K'): -2,\n    ('K', 'Q'): 1, ('R', 'A'): -1, ('I', 'R'): -3, ('T', 'D'): -1,\n    ('P', 'F'): -4, ('I', 'N'): -3, ('K', 'I'): -3, ('M', 'D'): -3,\n    ('V', 'W'): -3, ('W', 'W'): 11, ('M', 'H'): -2, ('P', 'N'): -2,\n    ('K', 'A'): -1, ('M', 'L'): 2, ('K', 'E'): 1, ('Z', 'E'): 4,\n    ('X', 'N'): -1, ('Z', 'A'): -1, ('Z', 'M'): -1, ('X', 'F'): -1,\n    ('K', 'C'): -3, ('B', 'Q'): 0, ('X', 'B'): -1, ('B', 'M'): -3,\n    ('F', 'C'): -2, ('Z', 'Q'): 3, ('X', 'Z'): -1, ('F', 'G'): -3,\n    ('B', 'E'): 1, ('X', 'V'): -1, ('F', 'K'): -3, ('B', 'A'): -2,\n    ('X', 'R'): -1, ('D', 'D'): 6, ('W', 'G'): -2, ('Z', 'F'): -3,\n    ('S', 'Q'): 0, ('W', 'C'): -2, ('W', 'K'): -3, ('H', 'Q'): 0,\n    ('L', 'C'): -1, ('W', 'N'): -4, ('S', 'A'): 1, ('L', 'G'): -4,\n    ('W', 'S'): -3, ('S', 'E'): 0, ('H', 'E'): 0, ('S', 'I'): -2,\n    ('H', 'A'): -2, ('S', 'M'): -1, ('Y', 'L'): -1, ('Y', 'H'): 2,\n    ('Y', 'D'): -3, ('E', 'R'): 0, ('X', 'P'): -2, ('G', 'G'): 6,\n    ('G', 'C'): -3, ('E', 'N'): 0, ('Y', 'T'): -2, ('Y', 'P'): -3,\n    ('T', 'K'): -1, ('A', 'A'): 4, ('P', 'Q'): -1, ('T', 'C'): -1,\n    ('V', 'H'): -3, ('T', 'G'): -2, ('I', 'Q'): -3, ('Z', 'T'): -1,\n    ('C', 'R'): -3, ('V', 'P'): -2, ('P', 'E'): -1, ('M', 'C'): -1,\n    ('K', 'N'): 0, ('I', 'I'): 4, ('P', 'A'): -1, ('M', 'G'): -3,\n    ('T', 'S'): 1, ('I', 'E'): -3, ('P', 'M'): -2, ('M', 'K'): -1,\n    ('I', 'A'): -1, ('P', 'I'): -3, ('R', 'R'): 5, ('X', 'M'): -1,\n    ('L', 'I'): 2, ('X', 'I'): -1, ('Z', 'B'): 1, ('X', 'E'): -1,\n    ('Z', 'N'): 0, ('X', 'A'): 0, ('B', 'R'): -1, ('B', 'N'): 3,\n    ('F', 'D'): -3, ('X', 'Y'): -1, ('Z', 'R'): 0, ('F', 'H'): -1,\n    ('B', 'F'): -3, ('F', 'L'): 0, ('X', 'Q'): -1, ('B', 'B'): 4\n}\n\ndef insert_gap(sequence: str, position: int) -&gt; str:\n    \"\"\"Insert a gap ('-') into the sequence at the specified position.\"\"\"\n    return sequence[:position] + \"-\" + sequence[position:]\n\ndef global_alignment_affine(seq1: str, seq2: str, gap_open_penalty: int = -11, gap_extend_penalty: int = -1) -&gt; Tuple[int, str, str]:\n    \"\"\"\n    Perform global sequence alignment with affine gap penalty.\n    \n    Args:\n    seq1 (str): First sequence to align\n    seq2 (str): Second sequence to align\n    gap_open_penalty (int): Penalty for opening a gap\n    gap_extend_penalty (int): Penalty for extending a gap\n    \n    Returns:\n    Tuple[int, str, str]: Alignment score and aligned sequences\n    \"\"\"\n    scoring_matrix: Dict[Tuple[str, str], int] = BLOSUM62\n    match_score: Dict[Tuple[int, int], int] = {}\n    gap_seq1_score: Dict[Tuple[int, int], int] = {}\n    gap_seq2_score: Dict[Tuple[int, int], int] = {}\n    prev_match: Dict[Tuple[int, int], int] = {}\n    prev_gap_seq1: Dict[Tuple[int, int], int] = {}\n    prev_gap_seq2: Dict[Tuple[int, int], int] = {}\n\n    # Initialize matrices\n    gap_seq1_score[0, 0] = match_score[0, 0] = gap_seq2_score[0, 0] = 0\n    for i in range(1, len(seq1) + 1):\n        gap_seq1_score[i, 0] = gap_open_penalty + (i - 1) * gap_extend_penalty\n        match_score[i, 0] = gap_open_penalty + (i - 1) * gap_extend_penalty\n        gap_seq2_score[i, 0] = gap_open_penalty * 10  # Large penalty to avoid this case\n    for j in range(1, len(seq2) + 1):\n        gap_seq2_score[0, j] = gap_open_penalty + (j - 1) * gap_extend_penalty\n        match_score[0, j] = gap_open_penalty + (j - 1) * gap_extend_penalty\n        gap_seq1_score[0, j] = gap_open_penalty * 10  # Large penalty to avoid this case\n\n    # Fill matrices\n    for i in range(1, len(seq1) + 1):\n        for j in range(1, len(seq2) + 1):\n            # Calculate scores for gap in seq1\n            gap_seq1_options: List[int] = [\n                gap_seq1_score[i - 1, j] + gap_extend_penalty,\n                match_score[i - 1, j] + gap_open_penalty\n            ]\n            gap_seq1_score[i, j] = max(gap_seq1_options)\n            prev_gap_seq1[i, j] = gap_seq1_options.index(gap_seq1_score[i, j])\n\n            # Calculate scores for gap in seq2\n            gap_seq2_options: List[int] = [\n                gap_seq2_score[i, j - 1] + gap_extend_penalty,\n                match_score[i, j - 1] + gap_open_penalty\n            ]\n            gap_seq2_score[i, j] = max(gap_seq2_options)\n            prev_gap_seq2[i, j] = gap_seq2_options.index(gap_seq2_score[i, j])\n\n            # Calculate match/mismatch score\n            blosum_score: int = scoring_matrix.get((seq1[i-1], seq2[j-1]), scoring_matrix.get((seq2[j-1], seq1[i-1]), 0))\n            match_options: List[int] = [\n                gap_seq1_score[i, j],\n                match_score[i - 1, j - 1] + blosum_score,\n                gap_seq2_score[i, j]\n            ]\n            match_score[i, j] = max(match_options)\n            prev_match[i, j] = match_options.index(match_score[i, j])\n\n    # Traceback\n    i, j = len(seq1), len(seq2)\n    aligned_seq1, aligned_seq2 = seq1, seq2\n\n    scores: List[int] = [gap_seq1_score[i, j], match_score[i, j], gap_seq2_score[i, j]]\n    max_score: int = max(scores)\n    current_matrix: int = scores.index(max_score)\n\n    while i * j != 0:\n        if current_matrix == 0:  # In gap_seq1_score matrix\n            if prev_gap_seq1[i, j] == 1:\n                current_matrix = 1\n            i -= 1\n            aligned_seq2 = insert_gap(aligned_seq2, j)\n        elif current_matrix == 1:  # In match_score matrix\n            if prev_match[i, j] == 1:\n                i -= 1\n                j -= 1\n            else:\n                current_matrix = prev_match[i, j]\n        else:  # In gap_seq2_score matrix\n            if prev_gap_seq2[i, j] == 1:\n                current_matrix = 1\n            j -= 1\n            aligned_seq1 = insert_gap(aligned_seq1, i)\n\n    # Handle remaining overhangs\n    while i &gt; 0:\n        aligned_seq2 = insert_gap(aligned_seq2, 0)\n        i -= 1\n    while j &gt; 0:\n        aligned_seq1 = insert_gap(aligned_seq1, 0)\n        j -= 1\n\n    return max_score, aligned_seq1, aligned_seq2\n\n# Sample usage\nsample_input: str = \"\"\"\nPRTEINS\nPRTWPSEIN\n\"\"\"\n\nseq1, seq2 = sample_input.strip().split(\"\\n\")\nalignment_score, aligned_seq1, aligned_seq2 = global_alignment_affine(seq1, seq2)\nprint(alignment_score, aligned_seq1, aligned_seq2, sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-58",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-58",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "58.1 Sample Dataset",
    "text": "58.1 Sample Dataset\nPLEASANTLY\nMEASNLY"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-58",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-58",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "58.2 Sample Output",
    "text": "58.2 Sample Output\n(4, 3) (5, 4)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-58",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-58",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "58.3 Solution",
    "text": "58.3 Solution\nfrom math import floor\nfrom typing import Dict, Tuple, List\n\nBLOSUM62: Dict[Tuple[str, str], int] = {\n    ('W', 'F'): 1, ('L', 'R'): -2, ('S', 'P'): -1, ('V', 'T'): 0,\n    ('Q', 'Q'): 5, ('N', 'A'): -2, ('Z', 'Y'): -2, ('W', 'R'): -3,\n    ('Q', 'A'): -1, ('S', 'D'): 0, ('H', 'H'): 8, ('S', 'H'): -1,\n    ('H', 'D'): -1, ('L', 'N'): -3, ('W', 'A'): -3, ('Y', 'M'): -1,\n    ('G', 'R'): -2, ('Y', 'I'): -1, ('Y', 'E'): -2, ('B', 'Y'): -3,\n    ('Y', 'A'): -2, ('V', 'D'): -3, ('B', 'S'): 0, ('Y', 'Y'): 7,\n    ('G', 'N'): 0, ('E', 'C'): -4, ('Y', 'Q'): -1, ('Z', 'Z'): 4,\n    ('V', 'A'): 0, ('C', 'C'): 9, ('M', 'R'): -1, ('V', 'E'): -2,\n    ('T', 'N'): 0, ('P', 'P'): 7, ('V', 'I'): 3, ('V', 'S'): -2,\n    ('Z', 'P'): -1, ('V', 'M'): 1, ('T', 'F'): -2, ('V', 'Q'): -2,\n    ('K', 'K'): 5, ('P', 'D'): -1, ('I', 'H'): -3, ('I', 'D'): -3,\n    ('T', 'R'): -1, ('P', 'L'): -3, ('K', 'G'): -2, ('M', 'N'): -2,\n    ('P', 'H'): -2, ('F', 'Q'): -3, ('Z', 'G'): -2, ('X', 'L'): -1,\n    ('T', 'M'): -1, ('Z', 'C'): -3, ('X', 'H'): -1, ('D', 'R'): -2,\n    ('B', 'W'): -4, ('X', 'D'): -1, ('Z', 'K'): 1, ('F', 'A'): -2,\n    ('Z', 'W'): -3, ('F', 'E'): -3, ('D', 'N'): 1, ('B', 'K'): 0,\n    ('X', 'X'): -1, ('F', 'I'): 0, ('B', 'G'): -1, ('X', 'T'): 0,\n    ('F', 'M'): 0, ('B', 'C'): -3, ('Z', 'I'): -3, ('Z', 'V'): -2,\n    ('S', 'S'): 4, ('L', 'Q'): -2, ('W', 'E'): -3, ('Q', 'R'): 1,\n    ('N', 'N'): 6, ('W', 'M'): -1, ('Q', 'C'): -3, ('W', 'I'): -3,\n    ('S', 'C'): -1, ('L', 'A'): -1, ('S', 'G'): 0, ('L', 'E'): -3,\n    ('W', 'Q'): -2, ('H', 'G'): -2, ('S', 'K'): 0, ('Q', 'N'): 0,\n    ('N', 'R'): 0, ('H', 'C'): -3, ('Y', 'N'): -2, ('G', 'Q'): -2,\n    ('Y', 'F'): 3, ('C', 'A'): 0, ('V', 'L'): 1, ('G', 'E'): -2,\n    ('G', 'A'): 0, ('K', 'R'): 2, ('E', 'D'): 2, ('Y', 'R'): -2,\n    ('M', 'Q'): 0, ('T', 'I'): -1, ('C', 'D'): -3, ('V', 'F'): -1,\n    ('T', 'A'): 0, ('T', 'P'): -1, ('B', 'P'): -2, ('T', 'E'): -1,\n    ('V', 'N'): -3, ('P', 'G'): -2, ('M', 'A'): -1, ('K', 'H'): -1,\n    ('V', 'R'): -3, ('P', 'C'): -3, ('M', 'E'): -2, ('K', 'L'): -2,\n    ('V', 'V'): 4, ('M', 'I'): 1, ('T', 'Q'): -1, ('I', 'G'): -4,\n    ('P', 'K'): -1, ('M', 'M'): 5, ('K', 'D'): -1, ('I', 'C'): -1,\n    ('Z', 'D'): 1, ('F', 'R'): -3, ('X', 'K'): -1, ('Q', 'D'): 0,\n    ('X', 'G'): -1, ('Z', 'L'): -3, ('X', 'C'): -2, ('Z', 'H'): 0,\n    ('B', 'L'): -4, ('B', 'H'): 0, ('F', 'F'): 6, ('X', 'W'): -2,\n    ('B', 'D'): 4, ('D', 'A'): -2, ('S', 'L'): -2, ('X', 'S'): 0,\n    ('F', 'N'): -3, ('S', 'R'): -1, ('W', 'D'): -4, ('V', 'Y'): -1,\n    ('W', 'L'): -2, ('H', 'R'): 0, ('W', 'H'): -2, ('H', 'N'): 1,\n    ('W', 'T'): -2, ('T', 'T'): 5, ('S', 'F'): -2, ('W', 'P'): -4,\n    ('L', 'D'): -4, ('B', 'I'): -3, ('L', 'H'): -3, ('S', 'N'): 1,\n    ('B', 'T'): -1, ('L', 'L'): 4, ('Y', 'K'): -2, ('E', 'Q'): 2,\n    ('Y', 'G'): -3, ('Z', 'S'): 0, ('Y', 'C'): -2, ('G', 'D'): -1,\n    ('B', 'V'): -3, ('E', 'A'): -1, ('Y', 'W'): 2, ('E', 'E'): 5,\n    ('Y', 'S'): -2, ('C', 'N'): -3, ('V', 'C'): -1, ('T', 'H'): -2,\n    ('P', 'R'): -2, ('V', 'G'): -3, ('T', 'L'): -1, ('V', 'K'): -2,\n    ('K', 'Q'): 1, ('R', 'A'): -1, ('I', 'R'): -3, ('T', 'D'): -1,\n    ('P', 'F'): -4, ('I', 'N'): -3, ('K', 'I'): -3, ('M', 'D'): -3,\n    ('V', 'W'): -3, ('W', 'W'): 11, ('M', 'H'): -2, ('P', 'N'): -2,\n    ('K', 'A'): -1, ('M', 'L'): 2, ('K', 'E'): 1, ('Z', 'E'): 4,\n    ('X', 'N'): -1, ('Z', 'A'): -1, ('Z', 'M'): -1, ('X', 'F'): -1,\n    ('K', 'C'): -3, ('B', 'Q'): 0, ('X', 'B'): -1, ('B', 'M'): -3,\n    ('F', 'C'): -2, ('Z', 'Q'): 3, ('X', 'Z'): -1, ('F', 'G'): -3,\n    ('B', 'E'): 1, ('X', 'V'): -1, ('F', 'K'): -3, ('B', 'A'): -2,\n    ('X', 'R'): -1, ('D', 'D'): 6, ('W', 'G'): -2, ('Z', 'F'): -3,\n    ('S', 'Q'): 0, ('W', 'C'): -2, ('W', 'K'): -3, ('H', 'Q'): 0,\n    ('L', 'C'): -1, ('W', 'N'): -4, ('S', 'A'): 1, ('L', 'G'): -4,\n    ('W', 'S'): -3, ('S', 'E'): 0, ('H', 'E'): 0, ('S', 'I'): -2,\n    ('H', 'A'): -2, ('S', 'M'): -1, ('Y', 'L'): -1, ('Y', 'H'): 2,\n    ('Y', 'D'): -3, ('E', 'R'): 0, ('X', 'P'): -2, ('G', 'G'): 6,\n    ('G', 'C'): -3, ('E', 'N'): 0, ('Y', 'T'): -2, ('Y', 'P'): -3,\n    ('T', 'K'): -1, ('A', 'A'): 4, ('P', 'Q'): -1, ('T', 'C'): -1,\n    ('V', 'H'): -3, ('T', 'G'): -2, ('I', 'Q'): -3, ('Z', 'T'): -1,\n    ('C', 'R'): -3, ('V', 'P'): -2, ('P', 'E'): -1, ('M', 'C'): -1,\n    ('K', 'N'): 0, ('I', 'I'): 4, ('P', 'A'): -1, ('M', 'G'): -3,\n    ('T', 'S'): 1, ('I', 'E'): -3, ('P', 'M'): -2, ('M', 'K'): -1,\n    ('I', 'A'): -1, ('P', 'I'): -3, ('R', 'R'): 5, ('X', 'M'): -1,\n    ('L', 'I'): 2, ('X', 'I'): -1, ('Z', 'B'): 1, ('X', 'E'): -1,\n    ('Z', 'N'): 0, ('X', 'A'): 0, ('B', 'R'): -1, ('B', 'N'): 3,\n    ('F', 'D'): -3, ('X', 'Y'): -1, ('Z', 'R'): 0, ('F', 'H'): -1,\n    ('B', 'F'): -3, ('F', 'L'): 0, ('X', 'Q'): -1, ('B', 'B'): 4\n}\n\ndef calculate_alignment_scores(\n    sequence1: str,\n    sequence2: str,\n    scoring_matrix: Dict[Tuple[str, str], int],\n    gap_penalty: int\n) -&gt; Tuple[List[int], List[int]]:\n    current_scores = list(range(0, (len(sequence1) + 1) * gap_penalty, gap_penalty))\n    backtrack = [0] * (len(sequence1) + 1)\n    \n    for j in range(1, len(sequence2) + 1):\n        previous_scores = current_scores[:]\n        current_scores[0] = previous_scores[0] + gap_penalty\n        for i in range(1, len(sequence1) + 1):\n            options = [\n                previous_scores[i] + gap_penalty,\n                current_scores[i - 1] + gap_penalty,\n                previous_scores[i - 1] + scoring_matrix.get(\n                    (sequence1[i - 1], sequence2[j - 1]),\n                    scoring_matrix.get((sequence2[j - 1], sequence1[i - 1]), 0)\n                ),\n            ]\n            current_scores[i] = max(options)\n            backtrack[i] = options.index(current_scores[i])\n    \n    return current_scores, backtrack\n\ndef find_middle_edge(\n    sequence1: str,\n    sequence2: str,\n    scoring_matrix: Dict[Tuple[str, str], int],\n    gap_penalty: int\n) -&gt; Tuple[Tuple[int, int], Tuple[int, int]]:\n    midpoint = floor(len(sequence2) / 2)\n    \n    forward_scores, _ = calculate_alignment_scores(\n        sequence1, sequence2[:midpoint], scoring_matrix, gap_penalty\n    )\n    reverse_scores, reverse_backtrack = calculate_alignment_scores(\n        sequence1[::-1], sequence2[midpoint:][::-1], scoring_matrix, gap_penalty\n    )\n    \n    total_scores = [f + r for f, r in zip(forward_scores, reverse_scores[::-1])]\n    best_score_index = total_scores.index(max(total_scores))\n    \n    start_node = (best_score_index, midpoint)\n    possible_moves = [\n        (start_node[0], start_node[1] + 1),\n        (start_node[0] + 1, start_node[1]),\n        (start_node[0] + 1, start_node[1] + 1)\n    ]\n    end_node = possible_moves[reverse_backtrack[::-1][best_score_index]]\n    \n    return (start_node, end_node)\n\n# Sample usage\nsample_input: str = \"\"\"\nPLEASANTLY\nMEASNLY\n\"\"\"\n\nsequence1, sequence2 = sample_input.strip().split(\"\\n\")\nresult = find_middle_edge(sequence1, sequence2, BLOSUM62, -5)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-59",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-59",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "59.1 Sample Dataset",
    "text": "59.1 Sample Dataset\nPLEASANTLY\nMEANLY"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-59",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-59",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "59.2 Sample Output",
    "text": "59.2 Sample Output\n8\nPLEASANTLY\n-MEA--N-LY"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-59",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-59",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "59.3 Solution",
    "text": "59.3 Solution\nfrom math import floor\nfrom typing import Dict, Tuple, List\n\nBLOSUM62: Dict[Tuple[str, str], int] = {\n    ('W', 'F'): 1, ('L', 'R'): -2, ('S', 'P'): -1, ('V', 'T'): 0,\n    ('Q', 'Q'): 5, ('N', 'A'): -2, ('Z', 'Y'): -2, ('W', 'R'): -3,\n    ('Q', 'A'): -1, ('S', 'D'): 0, ('H', 'H'): 8, ('S', 'H'): -1,\n    ('H', 'D'): -1, ('L', 'N'): -3, ('W', 'A'): -3, ('Y', 'M'): -1,\n    ('G', 'R'): -2, ('Y', 'I'): -1, ('Y', 'E'): -2, ('B', 'Y'): -3,\n    ('Y', 'A'): -2, ('V', 'D'): -3, ('B', 'S'): 0, ('Y', 'Y'): 7,\n    ('G', 'N'): 0, ('E', 'C'): -4, ('Y', 'Q'): -1, ('Z', 'Z'): 4,\n    ('V', 'A'): 0, ('C', 'C'): 9, ('M', 'R'): -1, ('V', 'E'): -2,\n    ('T', 'N'): 0, ('P', 'P'): 7, ('V', 'I'): 3, ('V', 'S'): -2,\n    ('Z', 'P'): -1, ('V', 'M'): 1, ('T', 'F'): -2, ('V', 'Q'): -2,\n    ('K', 'K'): 5, ('P', 'D'): -1, ('I', 'H'): -3, ('I', 'D'): -3,\n    ('T', 'R'): -1, ('P', 'L'): -3, ('K', 'G'): -2, ('M', 'N'): -2,\n    ('P', 'H'): -2, ('F', 'Q'): -3, ('Z', 'G'): -2, ('X', 'L'): -1,\n    ('T', 'M'): -1, ('Z', 'C'): -3, ('X', 'H'): -1, ('D', 'R'): -2,\n    ('B', 'W'): -4, ('X', 'D'): -1, ('Z', 'K'): 1, ('F', 'A'): -2,\n    ('Z', 'W'): -3, ('F', 'E'): -3, ('D', 'N'): 1, ('B', 'K'): 0,\n    ('X', 'X'): -1, ('F', 'I'): 0, ('B', 'G'): -1, ('X', 'T'): 0,\n    ('F', 'M'): 0, ('B', 'C'): -3, ('Z', 'I'): -3, ('Z', 'V'): -2,\n    ('S', 'S'): 4, ('L', 'Q'): -2, ('W', 'E'): -3, ('Q', 'R'): 1,\n    ('N', 'N'): 6, ('W', 'M'): -1, ('Q', 'C'): -3, ('W', 'I'): -3,\n    ('S', 'C'): -1, ('L', 'A'): -1, ('S', 'G'): 0, ('L', 'E'): -3,\n    ('W', 'Q'): -2, ('H', 'G'): -2, ('S', 'K'): 0, ('Q', 'N'): 0,\n    ('N', 'R'): 0, ('H', 'C'): -3, ('Y', 'N'): -2, ('G', 'Q'): -2,\n    ('Y', 'F'): 3, ('C', 'A'): 0, ('V', 'L'): 1, ('G', 'E'): -2,\n    ('G', 'A'): 0, ('K', 'R'): 2, ('E', 'D'): 2, ('Y', 'R'): -2,\n    ('M', 'Q'): 0, ('T', 'I'): -1, ('C', 'D'): -3, ('V', 'F'): -1,\n    ('T', 'A'): 0, ('T', 'P'): -1, ('B', 'P'): -2, ('T', 'E'): -1,\n    ('V', 'N'): -3, ('P', 'G'): -2, ('M', 'A'): -1, ('K', 'H'): -1,\n    ('V', 'R'): -3, ('P', 'C'): -3, ('M', 'E'): -2, ('K', 'L'): -2,\n    ('V', 'V'): 4, ('M', 'I'): 1, ('T', 'Q'): -1, ('I', 'G'): -4,\n    ('P', 'K'): -1, ('M', 'M'): 5, ('K', 'D'): -1, ('I', 'C'): -1,\n    ('Z', 'D'): 1, ('F', 'R'): -3, ('X', 'K'): -1, ('Q', 'D'): 0,\n    ('X', 'G'): -1, ('Z', 'L'): -3, ('X', 'C'): -2, ('Z', 'H'): 0,\n    ('B', 'L'): -4, ('B', 'H'): 0, ('F', 'F'): 6, ('X', 'W'): -2,\n    ('B', 'D'): 4, ('D', 'A'): -2, ('S', 'L'): -2, ('X', 'S'): 0,\n    ('F', 'N'): -3, ('S', 'R'): -1, ('W', 'D'): -4, ('V', 'Y'): -1,\n    ('W', 'L'): -2, ('H', 'R'): 0, ('W', 'H'): -2, ('H', 'N'): 1,\n    ('W', 'T'): -2, ('T', 'T'): 5, ('S', 'F'): -2, ('W', 'P'): -4,\n    ('L', 'D'): -4, ('B', 'I'): -3, ('L', 'H'): -3, ('S', 'N'): 1,\n    ('B', 'T'): -1, ('L', 'L'): 4, ('Y', 'K'): -2, ('E', 'Q'): 2,\n    ('Y', 'G'): -3, ('Z', 'S'): 0, ('Y', 'C'): -2, ('G', 'D'): -1,\n    ('B', 'V'): -3, ('E', 'A'): -1, ('Y', 'W'): 2, ('E', 'E'): 5,\n    ('Y', 'S'): -2, ('C', 'N'): -3, ('V', 'C'): -1, ('T', 'H'): -2,\n    ('P', 'R'): -2, ('V', 'G'): -3, ('T', 'L'): -1, ('V', 'K'): -2,\n    ('K', 'Q'): 1, ('R', 'A'): -1, ('I', 'R'): -3, ('T', 'D'): -1,\n    ('P', 'F'): -4, ('I', 'N'): -3, ('K', 'I'): -3, ('M', 'D'): -3,\n    ('V', 'W'): -3, ('W', 'W'): 11, ('M', 'H'): -2, ('P', 'N'): -2,\n    ('K', 'A'): -1, ('M', 'L'): 2, ('K', 'E'): 1, ('Z', 'E'): 4,\n    ('X', 'N'): -1, ('Z', 'A'): -1, ('Z', 'M'): -1, ('X', 'F'): -1,\n    ('K', 'C'): -3, ('B', 'Q'): 0, ('X', 'B'): -1, ('B', 'M'): -3,\n    ('F', 'C'): -2, ('Z', 'Q'): 3, ('X', 'Z'): -1, ('F', 'G'): -3,\n    ('B', 'E'): 1, ('X', 'V'): -1, ('F', 'K'): -3, ('B', 'A'): -2,\n    ('X', 'R'): -1, ('D', 'D'): 6, ('W', 'G'): -2, ('Z', 'F'): -3,\n    ('S', 'Q'): 0, ('W', 'C'): -2, ('W', 'K'): -3, ('H', 'Q'): 0,\n    ('L', 'C'): -1, ('W', 'N'): -4, ('S', 'A'): 1, ('L', 'G'): -4,\n    ('W', 'S'): -3, ('S', 'E'): 0, ('H', 'E'): 0, ('S', 'I'): -2,\n    ('H', 'A'): -2, ('S', 'M'): -1, ('Y', 'L'): -1, ('Y', 'H'): 2,\n    ('Y', 'D'): -3, ('E', 'R'): 0, ('X', 'P'): -2, ('G', 'G'): 6,\n    ('G', 'C'): -3, ('E', 'N'): 0, ('Y', 'T'): -2, ('Y', 'P'): -3,\n    ('T', 'K'): -1, ('A', 'A'): 4, ('P', 'Q'): -1, ('T', 'C'): -1,\n    ('V', 'H'): -3, ('T', 'G'): -2, ('I', 'Q'): -3, ('Z', 'T'): -1,\n    ('C', 'R'): -3, ('V', 'P'): -2, ('P', 'E'): -1, ('M', 'C'): -1,\n    ('K', 'N'): 0, ('I', 'I'): 4, ('P', 'A'): -1, ('M', 'G'): -3,\n    ('T', 'S'): 1, ('I', 'E'): -3, ('P', 'M'): -2, ('M', 'K'): -1,\n    ('I', 'A'): -1, ('P', 'I'): -3, ('R', 'R'): 5, ('X', 'M'): -1,\n    ('L', 'I'): 2, ('X', 'I'): -1, ('Z', 'B'): 1, ('X', 'E'): -1,\n    ('Z', 'N'): 0, ('X', 'A'): 0, ('B', 'R'): -1, ('B', 'N'): 3,\n    ('F', 'D'): -3, ('X', 'Y'): -1, ('Z', 'R'): 0, ('F', 'H'): -1,\n    ('B', 'F'): -3, ('F', 'L'): 0, ('X', 'Q'): -1, ('B', 'B'): 4\n}\n\ndef calculate_alignment_scores(\n    sequence1: str,\n    sequence2: str,\n    scoring_matrix: Dict[Tuple[str, str], int],\n    gap_penalty: int\n) -&gt; Tuple[List[int], List[int]]:\n    current_scores = list(range(0, (len(sequence1) + 1) * gap_penalty, gap_penalty))\n    backtrack = [0] * (len(sequence1) + 1)\n    \n    for j in range(1, len(sequence2) + 1):\n        previous_scores = current_scores[:]\n        current_scores[0] = previous_scores[0] + gap_penalty\n        for i in range(1, len(sequence1) + 1):\n            options = [\n                previous_scores[i] + gap_penalty,\n                current_scores[i - 1] + gap_penalty,\n                previous_scores[i - 1] + scoring_matrix.get(\n                    (sequence1[i - 1], sequence2[j - 1]),\n                    scoring_matrix.get((sequence2[j - 1], sequence1[i - 1]), 0)\n                ),\n            ]\n            current_scores[i] = max(options)\n            backtrack[i] = options.index(current_scores[i])\n    \n    return current_scores, backtrack\n\ndef find_middle_edge(\n    sequence1: str,\n    sequence2: str,\n    scoring_matrix: Dict[Tuple[str, str], int],\n    gap_penalty: int\n) -&gt; Tuple[Tuple[int, int], Tuple[int, int]]:\n    midpoint = floor(len(sequence2) / 2)\n    \n    forward_scores, _ = calculate_alignment_scores(\n        sequence1, sequence2[:midpoint], scoring_matrix, gap_penalty\n    )\n    reverse_scores, reverse_backtrack = calculate_alignment_scores(\n        sequence1[::-1], sequence2[midpoint:][::-1], scoring_matrix, gap_penalty\n    )\n    \n    total_scores = [f + r for f, r in zip(forward_scores, reverse_scores[::-1])]\n    best_score_index = total_scores.index(max(total_scores))\n    \n    start_node = (best_score_index, midpoint)\n    possible_moves = [\n        (start_node[0], start_node[1] + 1),\n        (start_node[0] + 1, start_node[1]),\n        (start_node[0] + 1, start_node[1] + 1)\n    ]\n    end_node = possible_moves[reverse_backtrack[::-1][best_score_index]]\n    \n    return (start_node, end_node)\n\ndef calculate_alignment_score(\n    aligned_seq1: str,\n    aligned_seq2: str,\n    scoring_matrix: Dict[Tuple[str, str], int],\n    gap_penalty: int\n) -&gt; int:\n    return sum(\n        gap_penalty if aligned_seq1[i] == \"-\" or aligned_seq2[i] == \"-\" else\n        scoring_matrix.get((aligned_seq1[i], aligned_seq2[i]),\n                           scoring_matrix.get((aligned_seq2[i], aligned_seq1[i]), 0))\n        for i in range(len(aligned_seq1))\n    )\n\ndef find_alignment_path(\n    sequence1: str,\n    sequence2: str,\n    scoring_matrix: Dict[Tuple[str, str], int],\n    gap_penalty: int\n) -&gt; str:\n    def linear_space_alignment(top: int, bottom: int, left: int, right: int) -&gt; str:\n        if left == right:\n            return \"↓\" * (bottom - top)\n        elif top == bottom:\n            return \"→\" * (right - left)\n        else:\n            ((i, j), (i2, j2)) = find_middle_edge(\n                sequence1[top:bottom], sequence2[left:right], scoring_matrix, gap_penalty\n            )\n            edge = \"↓\" if j == j2 else \"→\" if i == i2 else \"↘\"\n            return (\n                linear_space_alignment(top, i + top, left, j + left) +\n                edge +\n                linear_space_alignment(i2 + top, bottom, j2 + left, right)\n            )\n\n    return linear_space_alignment(0, len(sequence1), 0, len(sequence2))\n\ndef construct_alignment(\n    alignment_path: str,\n    sequence1: str,\n    sequence2: str\n) -&gt; Tuple[str, str]:\n    aligned_seq1, aligned_seq2 = \"\", \"\"\n    i, j = 0, 0\n    for direction in alignment_path:\n        if direction == \"↘\":\n            aligned_seq1 += sequence1[i]\n            aligned_seq2 += sequence2[j]\n            i += 1\n            j += 1\n        elif direction == \"↓\":\n            aligned_seq1 += sequence1[i]\n            aligned_seq2 += \"-\"\n            i += 1\n        else:\n            aligned_seq1 += \"-\"\n            aligned_seq2 += sequence2[j]\n            j += 1\n    return aligned_seq1, aligned_seq2\n\n# Sample usage\nsample_input: str = \"\"\"\nPLEASANTLY\nMEANLY\n\"\"\"\n\nsequence1, sequence2 = sample_input.strip().split(\"\\n\")\nscoring_matrix = BLOSUM62\nalignment_path = find_alignment_path(sequence1, sequence2, scoring_matrix, -5)\naligned_seq1, aligned_seq2 = construct_alignment(alignment_path, sequence1, sequence2)\nprint(calculate_alignment_score(aligned_seq1, aligned_seq2, scoring_matrix, -5))\nprint(aligned_seq1, aligned_seq2, sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-60",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-60",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "60.1 Sample Dataset",
    "text": "60.1 Sample Dataset\nATATCCG\nTCCGA\nATGTACTG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-60",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-60",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "60.2 Sample Output",
    "text": "60.2 Sample Output\n3\nATATCC-G-\n---TCC-GA\nATGTACTG-"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-60",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-60",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "60.3 Solution",
    "text": "60.3 Solution\nfrom itertools import product\nfrom typing import List, Tuple, Dict\n\n# Check if the coordinates are non-negative in the alignment matrix\ndef is_valid_coordinate(pointer: Tuple[int, ...], position: Tuple[int, ...]) -&gt; bool:\n    return all([i &gt;= 0 for i in get_previous_position(position, pointer)])\n\n# Get the previous position given a current position and a pointer\ndef get_previous_position(position: Tuple[int, ...], pointer: Tuple[int, ...]) -&gt; Tuple[int, ...]:\n    return tuple([p + d for p, d in zip(position, pointer)])\n\n# Calculate the score for a given position and pointer\ndef calculate_score(sequences: List[str], position: Tuple[int, ...], pointer: Tuple[int, ...]) -&gt; int:\n    if pointer == (-1, -1, -1):\n        bases = [sequences[i][j] for i, j in enumerate(get_previous_position(position, pointer))]\n        return int(all(base == bases[0] for base in bases))\n    else:\n        return 0\n\n# Generate possible previous cell pointers\ndef generate_moves(dimension: int) -&gt; List[Tuple[int, ...]]:\n    return list(product([0, -1], repeat=dimension))[1:]\n\ndef multiple_sequence_alignment(sequences: List[str]) -&gt; Tuple[int, str, str, str]:\n    scores: Dict[Tuple[int, ...], int] = {}\n    pointers: Dict[Tuple[int, ...], Tuple[int, ...]] = {}\n    scores[(0, 0, 0)] = 0\n    \n    ranges = [range(0, len(seq) + 1) for seq in sequences]\n    \n    for position in product(*ranges):\n        valid_pointers = list(filter(lambda x: is_valid_coordinate(x, position), generate_moves(3)))\n        if not valid_pointers:\n            continue\n        \n        possible_scores = [scores[get_previous_position(position, ptr)] + calculate_score(sequences, position, ptr) for ptr in valid_pointers]\n        scores[position] = max(possible_scores)\n        pointers[position] = valid_pointers[possible_scores.index(max(possible_scores))]\n\n    # Traceback to recover alignment\n    total_score = scores[position]\n    aligned_sequences = [\"\", \"\", \"\"]\n    \n    while any([x &gt; 0 for x in position]):\n        pointer = pointers[position]\n        for i, seq in enumerate(sequences):\n            aligned_sequences[i] += seq[position[i] - 1] if pointer[i] == -1 else \"-\"\n        position = get_previous_position(position, pointer)\n\n    return (total_score, \n            aligned_sequences[0][::-1], \n            aligned_sequences[1][::-1], \n            aligned_sequences[2][::-1])\n\nsample_input = \"\"\"\nATATCCG\nTCCGA\nATGTACTG\n\"\"\"\n\nsequences = sample_input.strip().split(\"\\n\")\nalignment_score, seq1, seq2, seq3 = multiple_sequence_alignment(sequences)\nprint(alignment_score)\nprint(seq1)\nprint(seq2)\nprint(seq3)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-61",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-61",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "61.1 Sample Dataset",
    "text": "61.1 Sample Dataset\n1 -&gt; 2\n2 -&gt; 3\n4 -&gt; 2\n5 -&gt; 3"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-61",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-61",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "61.2 Sample Output",
    "text": "61.2 Sample Output\n1, 4, 5, 2, 3"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-61",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-61",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "61.3 Solution",
    "text": "61.3 Solution\nfrom typing import Dict, List, Tuple, Set\n\nNodeLabel = str\nGraph = Dict[NodeLabel, List[NodeLabel]]\n\ndef create_graph(edge_list: List[str]) -&gt; Graph:\n    graph: Graph = {}\n    for edge in edge_list:\n        source, targets = edge.split(\" -&gt; \")\n        target_list = targets.split(\",\")\n        if source not in graph:\n            graph[source] = []\n        graph[source].extend(target_list)\n        for target in target_list:\n            if target not in graph:\n                graph[target] = []\n    return graph\n\ndef depth_first_search(graph: Graph, node: NodeLabel, visited: Set[NodeLabel], stack: List[NodeLabel]) -&gt; None:\n    visited.add(node)\n    for neighbor in graph.get(node, []):\n        if neighbor not in visited:\n            depth_first_search(graph, neighbor, visited, stack)\n    stack.insert(0, node)\n\ndef topological_sort(graph: Graph) -&gt; List[NodeLabel]:\n    visited: Set[NodeLabel] = set()\n    stack: List[NodeLabel] = []\n    for node in graph:\n        if node not in visited:\n            depth_first_search(graph, node, visited, stack)\n    return stack\n\ndef parse_input(input_text: str) -&gt; List[str]:\n    return input_text.strip().split(\"\\n\")\n\n# Sample usage\nsample_input = \"\"\"\n1 -&gt; 2\n2 -&gt; 3\n4 -&gt; 2\n5 -&gt; 3\n\"\"\"\n\nedge_list = parse_input(sample_input)\ngraph = create_graph(edge_list)\nsorted_nodes = topological_sort(graph)\nprint(\", \".join(sorted_nodes))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-62",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-62",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "62.1 Sample Dataset",
    "text": "62.1 Sample Dataset\n(-3 +4 +1 +5 -2)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-62",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-62",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "62.2 Sample Output",
    "text": "62.2 Sample Output\n(-1 -4 +3 +5 -2)\n(+1 -4 +3 +5 -2)\n(+1 +2 -5 -3 +4)\n(+1 +2 +3 +5 +4)\n(+1 +2 +3 -4 -5)\n(+1 +2 +3 +4 -5)\n(+1 +2 +3 +4 +5)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-62",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-62",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "62.3 Solution",
    "text": "62.3 Solution\nfrom typing import List, Tuple\n\ndef perform_k_sorting_reversal(permutation: List[int], k: int) -&gt; List[int]:\n    j = k\n    while abs(permutation[j]) != k + 1:\n        j += 1\n    permutation[k:j+1] = [-x for x in reversed(permutation[k:j+1])]\n    return permutation\n\ndef greedy_sorting(permutation: List[int]) -&gt; List[List[int]]:\n    reversal_sequence: List[List[int]] = []\n    for k in range(len(permutation)):\n        while permutation[k] != k + 1:\n            permutation = perform_k_sorting_reversal(permutation, k)\n            reversal_sequence.append(list(permutation))\n    return reversal_sequence\n\ndef parse_permutation(input_text: str) -&gt; List[int]:\n    cleaned_input = input_text.strip().replace(\"(\", \"\").replace(\")\", \"\")\n    return [int(x) for x in cleaned_input.split()]\n\ndef format_permutation(permutation: List[int]) -&gt; str:\n    formatted_elements = [(\"+\" if x &gt; 0 else \"\") + str(x) for x in permutation]\n    return \"(\" + \" \".join(formatted_elements) + \")\"\n\n# Sample usage\nsample_input = \"\"\"\n(-3 +4 +1 +5 -2)\n\"\"\"\n\ninitial_permutation = parse_permutation(sample_input)\nsorting_sequence = greedy_sorting(initial_permutation)\n\nfor permutation in sorting_sequence:\n    print(format_permutation(permutation))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-63",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-63",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "63.1 Sample Dataset",
    "text": "63.1 Sample Dataset\n(+3 +4 +5 -12 -8 -7 -6 +1 +2 +10 +9 -11 +13 +14)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-63",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-63",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "63.2 Sample Output",
    "text": "63.2 Sample Output\n8"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-63",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-63",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "63.3 Solution",
    "text": "63.3 Solution\nfrom typing import List\n\ndef count_breakpoints(permutation: List[int]) -&gt; int:\n    augmented_permutation = [0] + permutation + [max(permutation) + 1]\n    breakpoint_count = 0\n    \n    for i in range(1, len(augmented_permutation) - 1):\n        if augmented_permutation[i] != augmented_permutation[i - 1] + 1:\n            breakpoint_count += 1\n    \n    return breakpoint_count\n\nsample_input = \"\"\"\n(+3 +4 +5 -12 -8 -7 -6 +1 +2 +10 +9 -11 +13 +14)\n\"\"\"\n\npermutation_string = sample_input.strip()\npermutation_string = permutation_string.replace(\"(\", \"\").replace(\")\", \"\")\npermutation = [int(x) for x in permutation_string.split()]\n\nprint(count_breakpoints(permutation))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-64",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-64",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "64.1 Sample Dataset",
    "text": "64.1 Sample Dataset\n(+1 +2 +3 +4 +5 +6)\n(+1 -3 -6 -5)(+2 -4)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-64",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-64",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "64.2 Sample Output",
    "text": "64.2 Sample Output\n3"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-64",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-64",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "64.3 Solution",
    "text": "64.3 Solution\nimport re\nfrom collections import defaultdict\nfrom typing import List, Dict, Set, DefaultDict\n\ndef find_component(start_node: int, graph: Dict[int, List[int]]) -&gt; Set[int]:\n    queue: List[int] = [start_node]\n    visited: Set[int] = set()\n    while queue:\n        current_node = queue.pop(0)\n        visited.add(current_node)\n        for neighbor in graph[current_node]:\n            if neighbor not in visited:\n                queue.append(neighbor)\n    return visited\n\ndef parse_genome_graph(genome_string: str) -&gt; DefaultDict[int, List[int]]:\n    genome_graph: DefaultDict[int, List[int]] = defaultdict(list)\n    for component in re.findall(r\"\\((.+?)\\)\", genome_string):\n        chromosome = list(map(int, component.split()))\n        for i in range(len(chromosome) - 1):\n            genome_graph[chromosome[i]].append(-chromosome[i + 1])\n            genome_graph[-chromosome[i + 1]].append(chromosome[i])\n        genome_graph[chromosome[-1]].append(-chromosome[0])\n        genome_graph[-chromosome[0]].append(chromosome[-1])\n    return genome_graph\n\ndef breakpoint_graph(genome1: DefaultDict[int, List[int]], genome2: DefaultDict[int, List[int]]) -&gt; Dict[int, List[int]]:\n    combined_graph: Dict[int, List[int]] = {}\n    for node in genome1.keys():\n        combined_graph[node] = genome1[node] + genome2[node]\n    return combined_graph\n\ndef calculate_two_break_distance(genomes: List[DefaultDict[int, List[int]]]) -&gt; int:\n    combined_graph = breakpoint_graph(*genomes)\n    nodes: Set[int] = set(combined_graph.keys())\n    num_blocks = len(nodes) // 2\n    num_components = 0\n    while nodes:\n        component = find_component(next(iter(nodes)), combined_graph)\n        nodes -= component\n        num_components += 1\n    return num_blocks - num_components\n\nsample_input = \"\"\"\n(+1 +2 +3 +4 +5 +6)\n(+1 -3 -6 -5)(+2 -4)\n\"\"\"\n\ninput_lines = sample_input.strip().split(\"\\n\")\ngenomes = [parse_genome_graph(genome_string) for genome_string in input_lines]\nprint(calculate_two_break_distance(genomes))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-65",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-65",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "65.1 Sample Dataset",
    "text": "65.1 Sample Dataset\n(+1 -2 -3 +4)\n(+1 +2 -4 -3)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-65",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-65",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "65.2 Sample Output",
    "text": "65.2 Sample Output\n(+1 -2 -3 +4)\n(+1 +2 -3 +4)\n(+1 +2 -4 +3)\n(+1 +2 -4 -3)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-65",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-65",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "65.3 Solution",
    "text": "65.3 Solution\nimport re\nfrom collections import defaultdict\nfrom typing import List, Dict, Set, Generator, DefaultDict\n\ndef find_component(start_node: int, graph: Dict[int, List[int]]) -&gt; Set[int]:\n    queue: List[int] = [start_node]\n    visited: Set[int] = set()\n    while queue:\n        current_node = queue.pop(0)\n        visited.add(current_node)\n        for neighbor in graph[current_node]:\n            if neighbor not in visited:\n                queue.append(neighbor)\n    return visited\n\ndef parse_genome_graph(genome_string: str) -&gt; DefaultDict[int, List[int]]:\n    genome_graph: DefaultDict[int, List[int]] = defaultdict(list)\n    for component in re.findall(r\"\\((.+?)\\)\", genome_string):\n        chromosome = list(map(int, component.split()))\n        for i in range(len(chromosome) - 1):\n            genome_graph[chromosome[i]].append(-chromosome[i + 1])\n            genome_graph[-chromosome[i + 1]].append(chromosome[i])\n        genome_graph[chromosome[-1]].append(-chromosome[0])\n        genome_graph[-chromosome[0]].append(chromosome[-1])\n    return genome_graph\n\ndef breakpoint_graph(genome1: DefaultDict[int, List[int]], genome2: DefaultDict[int, List[int]]) -&gt; Dict[int, List[int]]:\n    combined_graph: Dict[int, List[int]] = {}\n    for node in genome1.keys():\n        combined_graph[node] = genome1[node] + genome2[node]\n    return combined_graph\n\ndef format_perm(perm: List[int]) -&gt; str:\n    return \"(\" + \" \".join([f\"{x:+}\" for x in perm]) + \")\"\n\ndef find_components(graph: Dict[int, List[int]]) -&gt; Generator[Set[int], None, None]:\n    nodes: Set[int] = set(graph.keys())\n    while nodes:\n        component = find_component(next(iter(nodes)), graph)\n        nodes = nodes - component\n        yield component\n\ndef non_trivial_cycle_nodes(graph: Dict[int, List[int]]) -&gt; List[int] | None:\n    for component in find_components(graph):\n        if len(component) &gt; 2:\n            return list(component)\n    return None\n\ndef find_genome_component(start_node: int, graph: Dict[int, List[int]]) -&gt; List[int]:\n    queue: List[int] = [start_node]\n    visited: List[int] = []\n    while queue:\n        current_node = queue.pop(0)\n        visited.append(current_node)\n        for neighbor in graph[current_node]:\n            if -neighbor not in visited:\n                queue.append(-neighbor)\n    return visited\n\ndef format_genome_graph(genome_graph: Dict[int, List[int]]) -&gt; str:\n    nodes: Set[int] = set(genome_graph.keys())\n    components: List[List[int]] = []\n    while nodes:\n        component = find_genome_component(next(iter(nodes)), genome_graph)\n        nodes = nodes - set(component)\n        nodes = nodes - set(-x for x in component)\n        components.append(component)\n    return \"\".join([format_perm(c) for c in components])\n\ndef add_edge(graph: Dict[int, List[int]], node1: int, node2: int) -&gt; None:\n    graph[node1].append(node2)\n    graph[node2].append(node1)\n\ndef del_edge(graph: Dict[int, List[int]], node1: int, node2: int) -&gt; None:\n    graph[node1].remove(node2)\n    graph[node2].remove(node1)\n\ndef ba6d(genome1: DefaultDict[int, List[int]], genome2: DefaultDict[int, List[int]]) -&gt; Generator[str, None, None]:\n    combined_graph = breakpoint_graph(genome1, genome2)\n    nodes = non_trivial_cycle_nodes(combined_graph)\n    yield format_genome_graph(genome1)\n    while nodes:\n        j = nodes[0]\n        i2 = genome2[nodes[0]][0]\n        i = genome1[j][0]\n        j2 = genome1[i2][0]\n\n        del_edge(genome1, i, j)\n        del_edge(genome1, i2, j2)\n        add_edge(genome1, j, i2)\n        add_edge(genome1, j2, i)\n\n        yield format_genome_graph(genome1)\n        combined_graph = breakpoint_graph(genome1, genome2)\n        nodes = non_trivial_cycle_nodes(combined_graph)\n\nsample_input = \"\"\"\n(+1 -2 -3 +4)\n(+1 +2 -4 -3)\n\"\"\"\n\ngenome1, genome2 = [parse_genome_graph(s) for s in sample_input.strip().split(\"\\n\")]\nfor genome in ba6d(genome1, genome2):\n    print(genome)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-66",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-66",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "66.1 Sample Dataset",
    "text": "66.1 Sample Dataset\n3\nAAACTCATC\nTTTCAAATC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-66",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-66",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "66.2 Sample Output",
    "text": "66.2 Sample Output\n(0, 4)\n(0, 0)\n(4, 2)\n(6, 6)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-66",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-66",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "66.3 Solution",
    "text": "66.3 Solution\nimport re\nfrom typing import List, Tuple, Iterator\n\ndef reverse_complement(sequence: str) -&gt; str:\n    return sequence[::-1].translate(str.maketrans(\"ACGT\", \"TGCA\"))\n\ndef find_overlapping_matches(pattern: str, sequence: str) -&gt; Iterator[re.Match]:\n    return re.finditer(rf\"(?=({pattern}))\", sequence)\n\ndef find_shared_kmers(kmer_length: int, sequence1: str, sequence2: str) -&gt; Iterator[Tuple[int, int]]:\n    for i in range(len(sequence1) - kmer_length + 1):\n        kmer = sequence1[i : (i + kmer_length)]\n        for current_sequence in [kmer, reverse_complement(kmer)]:\n            matches = list(find_overlapping_matches(current_sequence, sequence2))\n            for match in matches:\n                yield (i, match.start())\n\n# Sample usage\nsample_input = \"\"\"\n3\nAAACTCATC\nTTTCAAATC\n\"\"\"\n\nkmer_length, sequence1, sequence2 = sample_input.strip().split(\"\\n\")\nfor match in find_shared_kmers(int(kmer_length), sequence1, sequence2):\n    print(match)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-67",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-67",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "67.1 Sample Dataset",
    "text": "67.1 Sample Dataset\n(+1 -2 -3 +4)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-67",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-67",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "67.2 Sample Output",
    "text": "67.2 Sample Output\n(1 2 4 3 6 5 7 8)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-67",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-67",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "67.3 Solution",
    "text": "67.3 Solution\nfrom typing import List\n\ndef parse_permutation(permutation_string: str) -&gt; List[int]:\n    return list(map(int, permutation_string[1:-1].split()))\n\ndef chromosome_to_cycle(chromosome: List[int]) -&gt; List[int]:\n    nodes: List[int] = []\n    for gene in chromosome:\n        if gene &gt; 0:\n            nodes.extend([2 * gene - 1, 2 * gene])\n        else:\n            nodes.extend([-2 * gene, -2 * gene - 1])\n    return nodes\n\ndef convert_chromosome_to_cycle(chromosome_string: str) -&gt; List[int]:\n    return chromosome_to_cycle(parse_permutation(chromosome_string))\n\ndef format_cycle(cycle: List[int]) -&gt; str:\n    return \"(\" + \" \".join(map(str, cycle)) + \")\"\n\nsample_input: str = \"(+1 -2 -3 +4)\"\n\nchromosome_string: str = sample_input.strip()\nprint(format_cycle(convert_chromosome_to_cycle(chromosome_string)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-68",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-68",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "68.1 Sample Dataset",
    "text": "68.1 Sample Dataset\n(1 2 4 3 6 5 7 8)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-68",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-68",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "68.2 Sample Output",
    "text": "68.2 Sample Output\n(+1 -2 -3 +4)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-68",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-68",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "68.3 Solution",
    "text": "68.3 Solution\nfrom typing import List\n\ndef parse_cycle(cycle_string: str) -&gt; List[int]:\n    return list(map(int, cycle_string[1:-1].split()))\n\ndef format_chromosome(chromosome: List[int]) -&gt; str:\n    return \"(\" + \" \".join([f\"{gene:+}\" for gene in chromosome]) + \")\"\n\ndef cycle_to_chromosome(cycle: List[int]) -&gt; List[int]:\n    chromosome: List[int] = []\n    for j1, j2 in zip(cycle[::2], cycle[1::2]):\n        if j1 &lt; j2:\n            chromosome.append(j2 // 2)\n        else:\n            chromosome.append(-j1 // 2)\n    return chromosome\n\nsample_input: str = \"(1 2 4 3 6 5 7 8)\"\n\ncycle_string: str = sample_input.strip()\nchromosome: List[int] = cycle_to_chromosome(parse_cycle(cycle_string))\nprint(format_chromosome(chromosome))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-69",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-69",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "69.1 Sample Dataset",
    "text": "69.1 Sample Dataset\n(+1 -2 -3)(+4 +5 -6)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-69",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-69",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "69.2 Sample Output",
    "text": "69.2 Sample Output\n(2, 4), (3, 6), (5, 1), (8, 9), (10, 12), (11, 7)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-69",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-69",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "69.3 Solution",
    "text": "69.3 Solution\nimport re\nfrom typing import List, Dict, Tuple\n\ndef chromosome_to_cycle(chromosome: List[int]) -&gt; List[int]:\n    nodes: List[int] = []\n    for gene in chromosome:\n        if gene &gt; 0:\n            nodes.extend([2 * gene - 1, 2 * gene])\n        else:\n            nodes.extend([-2 * gene, -2 * gene - 1])\n    return nodes\n\ndef parse_integers(string: str) -&gt; List[int]:\n    return list(map(int, string.split()))\n\ndef get_colored_edges(genome: List[List[int]]) -&gt; Dict[int, int]:\n    edge_dict: Dict[int, int] = {}\n    for chromosome in genome:\n        nodes = chromosome_to_cycle(chromosome)\n        for j in range(len(chromosome)):\n            start_index = 2 * j + 1\n            end_index = (2 * j + 2) % len(nodes)\n            edge_dict[nodes[start_index]] = nodes[end_index]\n    return edge_dict\n\nsample_input: str = \"(+1 -2 -3)(+4 +5 -6)\"\n\ngenome_string: str = sample_input.strip()\ngenome: List[List[int]] = [parse_integers(x) for x in re.findall(r\"\\((.+?)\\)\", genome_string)]\nedges: List[Tuple[int, int]] = [(k, v) for k, v in get_colored_edges(genome).items()]\nprint(*edges, sep=\", \")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-70",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-70",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "70.1 Sample Dataset",
    "text": "70.1 Sample Dataset\n(2, 4), (3, 6), (5, 1), (7, 9), (10, 12), (11, 8)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-70",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-70",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "70.2 Sample Output",
    "text": "70.2 Sample Output\n(+1 -2 -3)(-4 +5 -6)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-70",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-70",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "70.3 Solution",
    "text": "70.3 Solution\nimport re\nfrom typing import List, Dict, Tuple, Iterator\nfrom copy import copy\n\ndef format_chromosome(chromosome: List[int]) -&gt; str:\n    return \"(\" + \" \".join([f\"{gene:+}\" for gene in chromosome]) + \")\"\n\ndef cycle_to_chromosome(cycle: List[int]) -&gt; List[int]:\n    chromosome: List[int] = []\n    for j1, j2 in zip(cycle[::2], cycle[1::2]):\n        if j1 &lt; j2:\n            chromosome.append(j2 // 2)\n        else:\n            chromosome.append(-j1 // 2)\n    return chromosome\n\ndef parse_integers(string: str) -&gt; List[int]:\n    return list(map(int, string.split(\", \")))\n\ndef get_first_key(dictionary: Dict) -&gt; int:\n    return next(iter(dictionary.keys()))\n\n# Find a single cycle from colored edges\ndef find_node_cycle(graph: Dict[int, int]) -&gt; List[int]:\n    start: int = get_first_key(graph)\n    current: int = start\n    component: List[int] = []\n    while graph:\n        next_node: int = graph.pop(current)\n        graph.pop(next_node)\n        neighbor: int = next_node + 1 if next_node % 2 else next_node - 1\n        if neighbor == start:\n            return [next_node] + component + [current]\n        component.extend([current, next_node])\n        current = neighbor\n\n# find cycles in colored edges\n# to do this, we first make each edge \"undirected\"\ndef find_node_cycles(graph: Dict[int, int]) -&gt; Iterator[List[int]]:\n    graph = copy(graph)\n    for k, v in list(graph.items()):\n        graph[v] = k\n    while graph:\n        yield find_node_cycle(graph)\n\ndef graph_to_genome(genome_graph: Dict[int, int]) -&gt; List[List[int]]:\n    genome: List[List[int]] = []\n    for nodes in find_node_cycles(genome_graph):\n        genome.append(cycle_to_chromosome(nodes))\n    return genome\n\ndef parse_edge_string(edge_string: str) -&gt; Dict[int, int]:\n    graph: Dict[int, int] = {}\n    for x in re.findall(r\"\\((.+?)\\)\", edge_string):\n        a, b = parse_integers(x)\n        graph[a] = b\n    return graph\n\nsample_input: str = \"(2, 4), (3, 6), (5, 1), (7, 9), (10, 12), (11, 8)\"\n\nedge_string: str = sample_input.strip()\ngenome_graph: Dict[int, int] = parse_edge_string(edge_string)\nprint(*[format_chromosome(x) for x in graph_to_genome(genome_graph)], sep=\"\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-71",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-71",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "71.1 Sample Dataset",
    "text": "71.1 Sample Dataset\n(2, 4), (3, 8), (7, 5), (6, 1)\n1, 6, 3, 8"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-71",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-71",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "71.2 Sample Output",
    "text": "71.2 Sample Output\n(2, 4), (3, 1), (7, 5), (6, 8)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-71",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-71",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "71.3 Solution",
    "text": "71.3 Solution\nimport re\nfrom typing import Dict, List, Tuple\nfrom copy import copy\n\ndef parse_edge_string(edge_string: str) -&gt; Dict[int, int]:\n    graph: Dict[int, int] = {}\n    for match in re.findall(r\"\\((.+?)\\)\", edge_string):\n        node1, node2 = parse_integers(match)\n        graph[node1] = node2\n    return graph\n\ndef parse_integers(string: str) -&gt; List[int]:\n    return list(map(int, string.split(\", \")))\n\ndef remove_edge(graph: Dict[int, int], edge: Tuple[int, int]) -&gt; None:\n    if edge[0] in graph:\n        graph.pop(edge[0])\n    else:\n        graph.pop(edge[1])\n\ndef two_break_on_genome_graph(graph: Dict[int, int], i: int, ip: int, j: int, jp: int) -&gt; Dict[int, int]:\n    new_graph: Dict[int, int] = copy(graph)\n    for edge in [(i, ip), (j, jp)]:\n        remove_edge(new_graph, edge)\n    new_graph[i] = j\n    new_graph[ip] = jp\n    return new_graph\n\nsample_input: str = \"\"\"\n(2, 4), (3, 8), (7, 5), (6, 1)\n1, 6, 3, 8\n\"\"\"\n\nedge_string, break_points = sample_input.strip().split(\"\\n\")\ngraph: Dict[int, int] = parse_edge_string(edge_string)\nbreak_points_list: List[int] = parse_integers(break_points)\nnew_graph: Dict[int, int] = two_break_on_genome_graph(graph, *break_points_list)\nedges: List[Tuple[int, int]] = [(k, v) for k, v in new_graph.items()]\nprint(*edges, sep=\", \")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-72",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-72",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "72.1 Sample Dataset",
    "text": "72.1 Sample Dataset\n(+1 -2 -4 +3)\n1, 6, 3, 8"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-72",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-72",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "72.2 Sample Output",
    "text": "72.2 Sample Output\n(+1 -2) (-4 +3)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-72",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-72",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "72.3 Solution",
    "text": "72.3 Solution\nfrom typing import List, Dict, Tuple, Iterator\nfrom copy import copy\n\ndef parse_permutation(s: str) -&gt; List[int]:\n    return list(map(int, s[1:-1].split()))\n\ndef format_permutation(chromosome: List[int]) -&gt; str:\n    return \"(\" + \" \".join([f\"{gene:+}\" for gene in chromosome]) + \")\"\n\ndef chromosome_to_cycle(chromosome: List[int]) -&gt; List[int]:\n    nodes: List[int] = []\n    for gene in chromosome:\n        if gene &gt; 0:\n            nodes.extend([2 * gene - 1, 2 * gene])\n        else:\n            nodes.extend([-2 * gene, -2 * gene - 1])\n    return nodes\n    \ndef colored_edges(genome: List[List[int]]) -&gt; Dict[int, int]:\n    graph: Dict[int, int] = {}\n    for chromosome in genome:\n        nodes = chromosome_to_cycle(chromosome)\n        for j in range(len(chromosome)):\n            start_index = 2 * j + 1\n            end_index = (2 * j + 2) % len(nodes)\n            graph[nodes[start_index]] = nodes[end_index]\n    return graph\n\ndef remove_edge(graph: Dict[int, int], edge: Tuple[int, int]) -&gt; None:\n    if edge[0] in graph:\n        graph.pop(edge[0])\n    else:\n        graph.pop(edge[1])\n        \ndef two_break_on_genome_graph(graph: Dict[int, int], i: int, ip: int, j: int, jp: int) -&gt; Dict[int, int]:\n    new_graph = copy(graph)\n    for edge in [(i, ip), (j, jp)]:\n        remove_edge(new_graph, edge)\n    new_graph[i] = j\n    new_graph[ip] = jp\n    return new_graph\n\ndef get_first_key(dictionary: Dict) -&gt; int:\n    return next(iter(dictionary.keys()))\n    \ndef find_node_cycle(graph: Dict[int, int]) -&gt; List[int]:\n    start = get_first_key(graph)\n    current = start\n    component: List[int] = []\n    while graph:\n        next_node = graph.pop(current)\n        graph.pop(next_node)\n        neighbor = next_node + 1 if next_node % 2 else next_node - 1\n        if neighbor == start:\n            return [next_node] + component + [current]\n        component.extend([current, next_node])\n        current = neighbor\n        \ndef find_node_cycles(graph: Dict[int, int]) -&gt; Iterator[List[int]]:\n    graph = copy(graph)\n    for k, v in list(graph.items()):\n        graph[v] = k\n    while graph:\n        yield find_node_cycle(graph)\n\ndef cycle_to_chromosome(cycle: List[int]) -&gt; List[int]:\n    chromosome: List[int] = []\n    for j1, j2 in zip(cycle[::2], cycle[1::2]):\n        if j1 &lt; j2:\n            chromosome.append(j2 // 2)\n        else:\n            chromosome.append(-j1 // 2)\n    return chromosome\n\ndef graph_to_genome(genome_graph: Dict[int, int]) -&gt; List[List[int]]:\n    genome: List[List[int]] = []\n    for nodes in find_node_cycles(genome_graph):\n        genome.append(cycle_to_chromosome(nodes))\n    return genome\n\ndef parse_integers(x: str) -&gt; List[int]:\n    return list(map(int, x.split(\", \")))\n\ndef two_break_on_genome(chromosome: List[int], i: int, ip: int, j: int, jp: int) -&gt; List[List[int]]:\n    genome_graph = colored_edges([chromosome])\n    genome_graph = two_break_on_genome_graph(genome_graph, i, ip, j, jp)\n    return graph_to_genome(genome_graph)\n\nsample_input: str = \"\"\"\n(+1 -2 -4 +3)\n1, 6, 3, 8\n\"\"\"\n\ngenome_str, indices_str = sample_input.strip().split(\"\\n\")\ngenome = parse_permutation(genome_str)\nnew_genome = two_break_on_genome(genome, *parse_integers(indices_str))\nprint(*[format_permutation(x) for x in new_genome])"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-73",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-73",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "73.1 Sample Dataset",
    "text": "73.1 Sample Dataset\n4\n0-&gt;4:11\n1-&gt;4:2\n2-&gt;5:6\n3-&gt;5:7\n4-&gt;0:11\n4-&gt;1:2\n4-&gt;5:4\n5-&gt;4:4\n5-&gt;3:7\n5-&gt;2:6"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-73",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-73",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "73.2 Sample Output",
    "text": "73.2 Sample Output\n0   13  21  22\n13  0   12  13\n21  12  0   13\n22  13  13  0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-73",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-73",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "73.3 Solution",
    "text": "73.3 Solution\nfrom re import split\nfrom collections import defaultdict\nfrom math import inf\nfrom heapq import heappush, heappop\nfrom typing import Dict, List, Set, Tuple\n\ndef get_all_nodes(graph: Dict[int, List[Dict[str, int]]]) -&gt; Set[int]:\n    source_nodes = set(graph.keys())\n    destination_nodes = {edge[\"n\"] for edges in graph.values() for edge in edges}\n    return source_nodes | destination_nodes\n\n# Dijkstra's algorithm to find distance from start to all other nodes\n# Assumes nodes are integers starting at 0!\ndef dijkstra(start: int, graph: Dict[int, List[Dict[str, int]]]) -&gt; List[float]:\n    distances = [inf for _ in range(len(get_all_nodes(graph)))]\n    distances[start] = 0\n    priority_queue: List[Tuple[float, int]] = []\n    heappush(priority_queue, (0, start))\n    processed_nodes: Set[int] = set()\n\n    while priority_queue:\n        current_node = heappop(priority_queue)[1]\n        processed_nodes.add(current_node)\n        for neighbor in graph[current_node]:\n            if neighbor[\"n\"] not in processed_nodes:\n                distances[neighbor[\"n\"]] = min(distances[current_node] + neighbor[\"w\"], distances[neighbor[\"n\"]])\n                heappush(priority_queue, (distances[neighbor[\"n\"]], neighbor[\"n\"]))\n\n    return distances\n\ndef parse_weighted_graph(edges: List[str]) -&gt; Dict[int, List[Dict[str, int]]]:\n    graph: Dict[int, List[Dict[str, int]]] = defaultdict(list)\n\n    for edge in edges:\n        from_node, to_node, weight = map(int, split(r\"\\D+\", edge))\n        graph[from_node].append({\"n\": to_node, \"w\": weight})\n\n    return graph\n\nsample_input = \"\"\"\n4\n0-&gt;4:11\n1-&gt;4:2\n2-&gt;5:6\n3-&gt;5:7\n4-&gt;0:11\n4-&gt;1:2\n4-&gt;5:4\n5-&gt;4:4\n5-&gt;3:7\n5-&gt;2:6\n\"\"\"\n\nnum_leaves, *edges = sample_input.strip().split(\"\\n\")\nnum_leaves = int(num_leaves)\ngraph = parse_weighted_graph(edges)\nfor i in range(num_leaves):\n    print(*dijkstra(i, graph)[:num_leaves])"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-74",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-74",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "74.1 Sample Dataset",
    "text": "74.1 Sample Dataset\n4\n1\n0   13  21  22\n13  0   12  13\n21  12  0   13\n22  13  13  0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-74",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-74",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "74.2 Sample Output",
    "text": "74.2 Sample Output\n2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-74",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-74",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "74.3 Solution",
    "text": "74.3 Solution\nfrom re import split\nfrom collections import defaultdict\nfrom math import inf\nfrom heapq import heappush, heappop\nfrom typing import Dict, List, Set, Tuple\n\ndef get_all_nodes(graph: Dict[int, List[Dict[str, int]]]) -&gt; Set[int]:\n    source_nodes: Set[int] = set(graph.keys())\n    destination_nodes: Set[int] = {edge[\"to\"] for edges in graph.values() for edge in edges}\n    return source_nodes | destination_nodes\n\n# Dijkstra's algorithm to find distance from start to all other nodes\n# Assumes nodes are integers starting at 0!\ndef dijkstra(start: int, graph: Dict[int, List[Dict[str, int]]]) -&gt; List[float]:\n    distances: List[float] = [inf for _ in range(len(get_all_nodes(graph)))]\n    distances[start] = 0\n    priority_queue: List[Tuple[float, int]] = []\n    heappush(priority_queue, (0, start))\n    processed_nodes: Set[int] = set()\n\n    while priority_queue:\n        current_distance, current_node = heappop(priority_queue)\n        if current_node in processed_nodes:\n            continue\n        processed_nodes.add(current_node)\n        for neighbor in graph[current_node]:\n            neighbor_node: int = neighbor[\"to\"]\n            edge_weight: int = neighbor[\"weight\"]\n            if neighbor_node not in processed_nodes:\n                new_distance: float = current_distance + edge_weight\n                if new_distance &lt; distances[neighbor_node]:\n                    distances[neighbor_node] = new_distance\n                    heappush(priority_queue, (new_distance, neighbor_node))\n\n    return distances\n\ndef parse_weighted_graph(edges: List[str]) -&gt; Dict[int, List[Dict[str, int]]]:\n    graph: Dict[int, List[Dict[str, int]]] = defaultdict(list)\n\n    for edge in edges:\n        from_node, to_node, weight = map(int, split(r\"\\D+\", edge))\n        graph[from_node].append({\"to\": to_node, \"weight\": weight})\n\n    return graph\n\nsample_input: str = \"\"\"\n4\n0-&gt;4:11\n1-&gt;4:2\n2-&gt;5:6\n3-&gt;5:7\n4-&gt;0:11\n4-&gt;1:2\n4-&gt;5:4\n5-&gt;4:4\n5-&gt;3:7\n5-&gt;2:6\n\"\"\"\n\nnum_leaves, *edges = sample_input.strip().split(\"\\n\")\nnum_leaves: int = int(num_leaves)\ngraph: Dict[int, List[Dict[str, int]]] = parse_weighted_graph(edges)\nfor i in range(num_leaves):\n    print(*dijkstra(i, graph)[:num_leaves])"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-75",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-75",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "75.1 Sample Dataset",
    "text": "75.1 Sample Dataset\n4\n0   13  21  22\n13  0   12  13\n21  12  0   13\n22  13  13  0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-75",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-75",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "75.2 Sample Output",
    "text": "75.2 Sample Output\n0-&gt;4:11\n1-&gt;4:2\n2-&gt;5:6\n3-&gt;5:7\n4-&gt;0:11\n4-&gt;1:2\n4-&gt;5:4\n5-&gt;2:6\n5-&gt;3:7\n5-&gt;4:4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-75",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-75",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "75.3 Solution",
    "text": "75.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Dict, Set, Tuple, Generator\n\n# Type aliases for clarity\nDistanceMatrix = List[List[int]]\nGraph = Dict[int, List[Dict[str, int]]]\n\ndef parse_distance_matrix(lines: List[str]) -&gt; DistanceMatrix:\n    \"\"\"Parse integer matrix from set of lines\"\"\"\n    return [[int(x) for x in line.split()] for line in lines]\n\ndef calculate_limb_length(distance_matrix: DistanceMatrix, j: int) -&gt; int:\n    \"\"\"Calculate limb length j for distance matrix\"\"\"\n    min_limb_length = float('inf')\n    n = len(distance_matrix)\n    for k in range(n):\n        for i in range(n):\n            if j != k and i != j:\n                limb_length = (distance_matrix[i][j] + distance_matrix[j][k] - distance_matrix[i][k]) // 2\n                min_limb_length = min(limb_length, min_limb_length)\n    return min_limb_length\n\ndef get_all_nodes(graph: Graph) -&gt; Set[int]:\n    \"\"\"Get all nodes in the graph\"\"\"\n    source_nodes = set(graph.keys())\n    target_nodes = {edge[\"n\"] for edges in graph.values() for edge in edges}\n    return source_nodes | target_nodes\n\ndef find_path(graph: Graph, path: List[Tuple[int, int]], target: int) -&gt; Generator[List[Tuple[int, int]], None, None]:\n    \"\"\"Search tree, returning route to target and cumulative distance\"\"\"\n    if target in [x[0] for x in path]:\n        yield path\n    current_node, current_distance = path[-1]\n    if current_node in graph:\n        for edge in graph[current_node]:\n            if edge[\"n\"] not in [x[0] for x in path]:\n                new_path = path + [(edge[\"n\"], current_distance + edge[\"w\"])]\n                yield from find_path(graph, new_path, target)\n\ndef find_leaves(distance_matrix: DistanceMatrix, n: int) -&gt; Tuple[int, int, int]:\n    \"\"\"Find three leaves i, n, k such that D[i][k] = D[i][n] + D[n][k]\"\"\"\n    for i in range(len(distance_matrix)):\n        for k in range(i + 1, len(distance_matrix)):\n            if distance_matrix[i][k] == distance_matrix[i][n] + distance_matrix[n][k]:\n                return i, n, k\n    raise ValueError(\"No suitable leaves found\")\n\ndef add_node(tree: Graph, i: int, k: int, x: int, n: int) -&gt; Graph:\n    \"\"\"Add node in graph between i and k, distance x from i, labelled n\"\"\"\n    path = list(find_path(tree, [(i, 0)], k))[0]\n    for p, node in enumerate(path):\n        if node[1] &gt; x:\n            break\n    p = p - 1\n    i, d1 = path[p]\n    k, d2 = path[p + 1]\n\n    # Delete old edge and add new edges\n    tree[i] = [edge for edge in tree[i] if edge[\"n\"] != k]\n    tree[i].append({\"n\": n, \"w\": x - d1})\n    tree.setdefault(n, []).append({\"n\": k, \"w\": d2 - x})\n    return tree\n\ndef additive_phylogeny(distance_matrix: DistanceMatrix, m: int) -&gt; Graph:\n    n = len(distance_matrix) - 1\n    if len(distance_matrix) == 2:\n        graph = defaultdict(list)\n        graph[0].append({\"n\": 1, \"w\": distance_matrix[0][1]})\n        return graph\n\n    limb_length = calculate_limb_length(distance_matrix, n)\n    for j in range(len(distance_matrix)):\n        if j != n:\n            distance_matrix[j][n] -= limb_length\n            distance_matrix[n][j] = distance_matrix[j][n]\n\n    i, n, k = find_leaves(distance_matrix, n)\n    x = distance_matrix[i][n]\n\n    # Remove row n and column n from distance_matrix\n    reduced_matrix = [row[:n] + row[n+1:] for row in distance_matrix[:n] + distance_matrix[n+1:]]\n\n    tree = additive_phylogeny(reduced_matrix, m)\n\n    # Label for new internal node\n    v = max(max(get_all_nodes(tree)), m - 1) + 1\n\n    # Break an internal edge adding a new node (possibly) and add the new leaf node\n    tree = add_node(tree, i, k, x, v)\n    tree.setdefault(v, []).append({\"n\": n, \"w\": limb_length})\n    return tree\n\ndef get_edges(graph: Graph) -&gt; List[str]:\n    edges = []\n    for k in sorted(graph):\n        for v in graph[k]:\n            edges.append(f\"{k}-&gt;{v['n']}:{v['w']}\")\n            edges.append(f\"{v['n']}-&gt;{k}:{v['w']}\")\n    return sorted(edges)\n\n# Sample usage\nsample_input = \"\"\"\n4\n0   13  21  22\n13  0   12  13\n21  12  0   13\n22  13  13  0\n\"\"\"\n\nn, *distance_matrix_str = sample_input.strip().split(\"\\n\")\ndistance_matrix = parse_distance_matrix(distance_matrix_str)\ngraph = additive_phylogeny(distance_matrix, int(n))\nfor edge in get_edges(graph):\n    print(edge)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-76",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-76",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "76.1 Sample Dataset",
    "text": "76.1 Sample Dataset\n4\n0   20  17  11\n20  0   20  13\n17  20  0   10\n11  13  10  0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-76",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-76",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "76.2 Sample Output",
    "text": "76.2 Sample Output\n0-&gt;5:7.000\n1-&gt;6:8.833\n2-&gt;4:5.000\n3-&gt;4:5.000\n4-&gt;2:5.000\n4-&gt;3:5.000\n4-&gt;5:2.000\n5-&gt;0:7.000\n5-&gt;4:2.000\n5-&gt;6:1.833\n6-&gt;5:1.833\n6-&gt;1:8.833"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-76",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-76",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "76.3 Solution",
    "text": "76.3 Solution\nfrom typing import List\n\ndef parse_matrix(lines: List[str]) -&gt; List[List[int]]:\n    \"\"\"Parse integer matrix from set of lines\"\"\"\n    return [[int(x) for x in line.split()] for line in lines]\n\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\n\ndef as_edges(graph: Dict[int, List[Dict[str, float]]]) -&gt; List[str]:\n    edges = []\n    for source in sorted(graph):\n        for target in graph[source]:\n            edges.append(f\"{source}-&gt;{target['n']}:{target['w']:f}\")\n            edges.append(f\"{target['n']}-&gt;{source}:{target['w']:f}\")\n    return sorted(edges)\n\ndef closest(distance_matrix: np.ndarray) -&gt; Tuple[int, int]:\n    \"\"\"Find (first) minimum off diagonal index in an array\"\"\"\n    distance_matrix = np.copy(distance_matrix)\n    np.fill_diagonal(distance_matrix, distance_matrix.max() + 1)\n    return divmod(distance_matrix.argmin(), distance_matrix.shape[1])\n\ndef average_ind(distance_matrix: np.ndarray, i: int, j: int, size_i: int, size_j: int) -&gt; np.ndarray:\n    \"\"\"Replace the ith row/col with the average of the ith and jth and remove the jth\"\"\"\n    distance_matrix = np.copy(distance_matrix)\n    average = (distance_matrix[i, :] * size_i + distance_matrix[j, :] * size_j) / (size_i + size_j)\n    distance_matrix[i, :] = average\n    distance_matrix[:, i] = average\n    distance_matrix = np.delete(distance_matrix, j, 0)\n    distance_matrix = np.delete(distance_matrix, j, 1)\n    np.fill_diagonal(distance_matrix, 0)\n    return distance_matrix\n\ndef upgma(distance_matrix: np.ndarray, num_clusters: int) -&gt; Dict[int, List[Dict[str, float]]]:\n    clusters = list(range(0, num_clusters))\n    ages: Dict[int, float] = defaultdict(lambda: 0)  # the \"age\" of a node\n    size: Dict[int, int] = defaultdict(lambda: 1)  # the number of descendants of a node\n    tree: Dict[int, List[Dict[str, float]]] = {}  # the graph / tree we're building\n    node_label = num_clusters  # a label for internal nodes as we add them\n    \n    while len(clusters) &gt; 1:\n        i, j = closest(distance_matrix)\n        a, b = clusters[i], clusters[j]\n\n        tree[node_label] = [\n            {\"n\": a, \"w\": distance_matrix[i, j] / 2 - ages[a]},\n            {\"n\": b, \"w\": distance_matrix[i, j] / 2 - ages[b]},\n        ]\n        size[node_label] = size[a] + size[b]\n        ages[node_label] = distance_matrix[i, j] / 2\n        clusters[i] = node_label\n        del clusters[j]\n        distance_matrix = average_ind(distance_matrix, *closest(distance_matrix), size[a], size[b])\n        node_label += 1\n\n    return tree\n\nsample_input = \"\"\"\n4\n0   20  17  11\n20  0   20  13\n17  20  0   10\n11  13  10  0\n\"\"\"\n\nnum_clusters, *distance_data = sample_input.strip().split(\"\\n\")\ndistance_matrix = np.array(parse_matrix(distance_data), float)\ngraph = upgma(distance_matrix, int(num_clusters))\nfor edge in as_edges(graph):\n    print(edge)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-77",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-77",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "77.1 Sample Dataset",
    "text": "77.1 Sample Dataset\n4\n0   23  27  20\n23  0   30  28\n27  30  0   30\n20  28  30  0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-77",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-77",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "77.2 Sample Output",
    "text": "77.2 Sample Output\n0-&gt;4:8.000\n1-&gt;5:13.500\n2-&gt;5:16.500\n3-&gt;4:12.000\n4-&gt;5:2.000\n4-&gt;0:8.000\n4-&gt;3:12.000\n5-&gt;1:13.500\n5-&gt;2:16.500\n5-&gt;4:2.000"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-77",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-77",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "77.3 Solution",
    "text": "77.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Dict, Union\nimport numpy as np\n\ndef parse_matrix(lines: List[str]) -&gt; List[List[int]]:\n    \"\"\"Parse integer matrix from set of lines\"\"\"\n    return [[int(x) for x in line.split()] for line in lines]\n\ndef format_edges(tree: Dict[int, List[Dict[str, Union[int, float]]]]) -&gt; List[str]:\n    formatted_edges = []\n    for node in sorted(tree):\n        for neighbor in tree[node]:\n            formatted_edges.append(f\"{node}-&gt;{neighbor['node']}:{neighbor['weight']:f}\")\n            formatted_edges.append(f\"{neighbor['node']}-&gt;{node}:{neighbor['weight']:f}\")\n    return sorted(formatted_edges)\n\ndef find_closest_pair(distance_matrix: np.ndarray) -&gt; tuple[int, int]:\n    \"\"\"Find (first) minimum off-diagonal index in an array\"\"\"\n    temp_matrix = np.copy(distance_matrix)\n    np.fill_diagonal(temp_matrix, temp_matrix.max() + 1)\n    return divmod(temp_matrix.argmin(), temp_matrix.shape[1])\n\ndef calculate_neighbor_joining_matrix(distance_matrix: np.ndarray, num_nodes: int) -&gt; np.ndarray:\n    nj_matrix = np.copy(distance_matrix)\n    for i in range(len(distance_matrix)):\n        for j in range(len(distance_matrix)):\n            if i != j:\n                nj_matrix[i, j] = (num_nodes - 2) * distance_matrix[i, j] - sum(distance_matrix[i, :]) - sum(distance_matrix[j, :])\n    return nj_matrix\n\ndef neighbor_joining(distance_matrix: np.ndarray, num_nodes: int, labels: List[int] = None) -&gt; Dict[int, List[Dict[str, Union[int, float]]]]:\n    if not labels:\n        labels = list(range(num_nodes))\n\n    if num_nodes == 2:\n        tree = defaultdict(list)\n        tree[labels[0]].append({\"node\": labels[1], \"weight\": distance_matrix[0][1]})\n        return tree\n\n    nj_matrix = calculate_neighbor_joining_matrix(distance_matrix, num_nodes)\n    i, j = find_closest_pair(nj_matrix)\n    delta = (sum(distance_matrix[i, :]) - sum(distance_matrix[j, :])) / (num_nodes - 2)\n    limb_i = (distance_matrix[i, j] + delta) / 2\n    limb_j = (distance_matrix[i, j] - delta) / 2\n\n    label_i = labels[i]\n    label_j = labels[j]\n\n    distance_matrix = np.append(distance_matrix, np.zeros((1, len(distance_matrix))), axis=0)\n    distance_matrix = np.append(distance_matrix, np.zeros((len(distance_matrix), 1)), axis=1)\n    labels = labels + [max(labels) + 1]\n\n    for k in range(num_nodes):\n        distance_matrix[k, num_nodes] = (distance_matrix[k, i] + distance_matrix[k, j] - distance_matrix[i, j]) / 2\n        distance_matrix[num_nodes, k] = (distance_matrix[k, i] + distance_matrix[k, j] - distance_matrix[i, j]) / 2\n    for x in [j, i]:\n        distance_matrix = np.delete(distance_matrix, x, 0)\n        distance_matrix = np.delete(distance_matrix, x, 1)\n        del labels[x]\n\n    tree = neighbor_joining(distance_matrix, num_nodes - 1, labels)\n\n    tree[labels[-1]].append({\"node\": label_i, \"weight\": limb_i})\n    tree[labels[-1]].append({\"node\": label_j, \"weight\": limb_j})\n    return tree\n\nsample_input = \"\"\"\n4\n0   23  27  20\n23  0   30  28\n27  30  0   30\n20  28  30  0\n\"\"\"\n\nnum_nodes, *distance_matrix = sample_input.strip().split(\"\\n\")\ndistance_matrix = np.array(parse_matrix(distance_matrix), float)\ntree = neighbor_joining(distance_matrix, int(num_nodes))\nfor edge in format_edges(tree):\n    print(edge)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-78",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-78",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "78.1 Sample Dataset",
    "text": "78.1 Sample Dataset\n4\n4-&gt;CAAATCCC\n4-&gt;ATTGCGAC\n5-&gt;CTGCGCTG\n5-&gt;ATGGACGA\n6-&gt;4\n6-&gt;5"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-78",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-78",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "78.2 Sample Output",
    "text": "78.2 Sample Output\n16\nATAGACAA-&gt;ATAGACAC:1\nATAGACAC-&gt;ATAGACAA:1\nATAGACAC-&gt;CAAATCCC:5\nCAAATCCC-&gt;ATAGACAC:5\nATAGACAC-&gt;ATTGCGAC:3\nATTGCGAC-&gt;ATAGACAC:3\nATAGACAA-&gt;ATGGACAA:1\nATGGACAA-&gt;ATAGACAA:1\nATGGACAA-&gt;CTGCGCTG:5\nCTGCGCTG-&gt;ATGGACAA:5\nATGGACAA-&gt;ATGGACGA:1\nATGGACGA-&gt;ATGGACAA:1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-78",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-78",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "78.3 Solution",
    "text": "78.3 Solution"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-79",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-79",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "79.1 Sample Dataset",
    "text": "79.1 Sample Dataset\n4\nTCGGCCAA-&gt;4\n4-&gt;TCGGCCAA\nCCTGGCTG-&gt;4\n4-&gt;CCTGGCTG\nCACAGGAT-&gt;5\n5-&gt;CACAGGAT\nTGAGTACC-&gt;5\n5-&gt;TGAGTACC\n4-&gt;5\n5-&gt;4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-79",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-79",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "79.2 Sample Output",
    "text": "79.2 Sample Output\n17\nTCGGCCAA-&gt;CCAGGCAC:4\nCCTGGCTG-&gt;CCAGGCAC:3\nTGAGTACC-&gt;CAAGGAAC:4\nCCAGGCAC-&gt;CCTGGCTG:3\nCCAGGCAC-&gt;CAAGGAAC:2\nCCAGGCAC-&gt;TCGGCCAA:4\nCACAGGAT-&gt;CAAGGAAC:4\nCAAGGAAC-&gt;CACAGGAT:4\nCAAGGAAC-&gt;TGAGTACC:4\nCAAGGAAC-&gt;CCAGGCAC:2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-79",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-79",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "79.3 Solution",
    "text": "79.3 Solution\nfrom collections import defaultdict\nfrom math import inf\nfrom typing import Dict, List, Set, Tuple\n\n# return all nodes of a simple graph\ndef nodes(graph: Dict[int, List[int]]) -&gt; Set[int]:\n    s = list(graph.keys())\n    e = [y for v in graph.values() for y in v]\n    return set(s) | set(e)\n\n# return all leaves of a simple graph\ndef leaves(graph: Dict[int, List[int]]) -&gt; Set[int]:\n    return set(y for v in list(graph.values()) for y in v if not graph[y])\n\n# return all root node of a simple graph\ndef root(graph: Dict[int, List[int]]) -&gt; int:\n    rev = reverse_graph(graph)\n    node = list(nodes(graph))[0]\n    while node in rev:\n        node = rev[node]\n    return node\n\n# reverse a simple graph (child points to parent)\ndef reverse_graph(graph: Dict[int, List[int]]) -&gt; Dict[int, int]:\n    rev: Dict[int, int] = {}\n    for node in graph:\n        for child in graph[node]:\n            rev[child] = node\n    return rev\n\ndef parse_input(input_string: str) -&gt; Tuple[Dict[int, str], Dict[int, List[int]]]:\n    lines = input_string.strip().split('\\n')\n    n = int(lines[0])\n    seqs: Dict[int, str] = {}\n    graph: Dict[int, List[int]] = defaultdict(list)\n    for i, edge in enumerate(lines[1:n+1]):\n        f, t = edge.rstrip().split(\"-&gt;\")\n        graph[int(f)].append(i)\n        seqs[i] = t\n    for edge in lines[n+1:]:\n        f, t = edge.rstrip().split(\"-&gt;\")\n        graph[int(f)].append(int(t))\n    return seqs, graph\n\n# print (bidirectional) edges\ndef print_edges(graph: Dict[int, List[int]], seqs: Dict[int, str], node: int) -&gt; None:\n    for child in graph[node]:\n        if node in seqs and child in seqs:\n            dist = sum(a != b for a, b in zip(seqs[node], seqs[child]))\n            print(f\"{seqs[node]}-&gt;{seqs[child]}:{dist}\")\n            print(f\"{seqs[child]}-&gt;{seqs[node]}:{dist}\")\n        print_edges(graph, seqs, child)\n\ndef extract_position(graph: Dict[int, List[int]], seqs: Dict[int, str], pos: int) -&gt; Dict[int, str]:\n    chars: Dict[int, str] = {}\n    for n in nodes(graph) - leaves(graph):\n        chars[n] = \"\"\n    for leaf in leaves(graph):\n        chars[leaf] = seqs[leaf][pos]\n    return chars\n\ndef traceback(skp: Dict[int, List[Dict[int, int]]], node: int, ind: int) -&gt; Dict[int, str]:\n    bases = [\"A\", \"C\", \"T\", \"G\"]\n    chars: Dict[int, str] = {}\n    chars[node] = bases[ind]\n    for k, v in skp[node][ind].items():\n        if k in skp:\n            chars = chars | traceback(skp, k, v)\n    return chars\n\ndef small_parsimony(graph: Dict[int, List[int]], chars: Dict[int, str]) -&gt; Tuple[int, Dict[int, str]]:\n    bases = [\"A\", \"C\", \"T\", \"G\"]\n    sk: Dict[int, List[int]] = {}  # minimum parsimony score of the subtree over possible labels\n    skp: Dict[int, List[Dict[int, int]]] = {}  # pointer to selected base for each child over possible labels\n    to_process = nodes(graph)\n\n    # initialise leaves\n    for leaf in leaves(graph):\n        sk[leaf] = [0 if chars[leaf] == c else inf for c in bases]\n        to_process.remove(leaf)\n\n    # iterate over available nodes till all are processed\n    while to_process:\n        for n in list(to_process):\n            if all(v in sk for v in graph[n]):\n                sk[n], skp[n] = [], []\n                for k in bases:\n                    tot = 0\n                    ptr: Dict[int, int] = {}\n                    for d, sk_child in [(d, sk[d]) for d in graph[n]]:\n                        score = []\n                        for i, c in enumerate(bases):\n                            score += [sk_child[i] + (0 if c == k else 1)]\n                        tot += min(score)\n                        ptr[d] = score.index(min(score))\n                    skp[n] += [ptr]\n                    sk[n] += [tot]\n                to_process.remove(n)\n\n    # Recover sequence\n    node = root(graph)\n    score = min(sk[node])\n    return score, traceback(skp, node, sk[node].index(score))\n\ndef ba6f(graph: Dict[int, List[int]], seqs: Dict[int, str]) -&gt; Tuple[int, Dict[int, str]]:\n    # initialise sequences\n    for n in nodes(graph) - leaves(graph):\n        seqs[n] = \"\"\n\n    total_score = 0\n    for pos in range(len(next(iter(seqs.values())))):\n        chars = extract_position(graph, seqs, pos)\n        score, tbchars = small_parsimony(graph, chars)\n        total_score += score\n        for k, v in tbchars.items():\n            seqs[k] += v\n\n    return total_score, seqs\n\nsample_input = \"\"\"\n4\n4-&gt;CAAATCCC\n4-&gt;ATTGCGAC\n5-&gt;CTGCGCTG\n5-&gt;ATGGACGA\n6-&gt;4\n6-&gt;5\n\"\"\"\n\nseqs, graph = parse_input(sample_input)\ntotal_score, seqs = ba6f(graph, seqs)\nprint(total_score)\nprint_edges(graph, seqs, root(graph))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-80",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-80",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "80.1 Sample Dataset",
    "text": "80.1 Sample Dataset\n4\nTCGGCCAA-&gt;4\n4-&gt;TCGGCCAA\nCCTGGCTG-&gt;4\n4-&gt;CCTGGCTG\nCACAGGAT-&gt;5\n5-&gt;CACAGGAT\nTGAGTACC-&gt;5\n5-&gt;TGAGTACC\n4-&gt;5\n5-&gt;4"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-80",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-80",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "80.2 Sample Output",
    "text": "80.2 Sample Output\n17\nTCGGCCAA-&gt;CCAGGCAC:4\nCCTGGCTG-&gt;CCAGGCAC:3\nTGAGTACC-&gt;CAAGGAAC:4\nCCAGGCAC-&gt;CCTGGCTG:3\nCCAGGCAC-&gt;CAAGGAAC:2\nCCAGGCAC-&gt;TCGGCCAA:4\nCACAGGAT-&gt;CAAGGAAC:4\nCAAGGAAC-&gt;CACAGGAT:4\nCAAGGAAC-&gt;TGAGTACC:4\nCAAGGAAC-&gt;CCAGGCAC:2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-80",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-80",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "80.3 Solution",
    "text": "80.3 Solution\nfrom collections import defaultdict\nfrom typing import Dict, List, Set, Tuple, TextIO\nimport io\nfrom math import inf\n\ndef nodes(graph: Dict[int, List[int]]) -&gt; Set[int]:\n    s = list(graph.keys())\n    e = [y for v in graph.values() for y in v]\n    return set(s) | set(e)\n\ndef leaves(graph: Dict[int, List[int]]) -&gt; Set[int]:\n    return set(y for v in list(graph.values()) for y in v if not graph[y])\n\ndef root(graph: Dict[int, List[int]]) -&gt; int:\n    rev = reverse_graph(graph)\n    node = list(nodes(graph))[0]\n    while node in rev:\n        node = rev[node]\n    return node\n\ndef get_all_nodes(graph: Dict[int, List[int]]) -&gt; Set[int]:\n    source_nodes = set(graph.keys())\n    target_nodes = {target for targets in graph.values() for target in targets}\n    return source_nodes | target_nodes\n\ndef reverse_graph(graph: Dict[int, List[int]]) -&gt; Dict[int, int]:\n    reversed_graph: Dict[int, int] = {}\n    for parent, children in graph.items():\n        for child in children:\n            reversed_graph[child] = parent\n    return reversed_graph\n\ndef get_leaves(graph: Dict[int, List[int]]) -&gt; Set[int]:\n    return {child for children in graph.values() for child in children if not graph[child]}\n\ndef find_root(graph: Dict[int, List[int]]) -&gt; int:\n    reversed_graph = reverse_graph(graph)\n    node = next(iter(get_all_nodes(graph)))\n    while node in reversed_graph:\n        node = reversed_graph[node]\n    return node\n\ndef print_edges(graph: Dict[int, List[int]], sequences: Dict[int, str], node: int) -&gt; None:\n    for child in graph[node]:\n        distance = sum(a != b for a, b in zip(sequences[node], sequences[child]))\n        print(f\"{sequences[node]}-&gt;{sequences[child]}:{distance}\")\n        print(f\"{sequences[child]}-&gt;{sequences[node]}:{distance}\")\n        print_edges(graph, sequences, child)\n\ndef root_tree(graph: Dict[int, List[int]], node: int) -&gt; None:\n    for child in graph[node]:\n        if node in graph[child]:\n            graph[child].remove(node)\n        root_tree(graph, child)\n\ndef parse_input(handle: TextIO) -&gt; Tuple[Dict[int, str], Dict[int, List[int]]]:\n    n = int(next(handle))\n    sequences: Dict[int, str] = {}\n    graph: Dict[int, List[int]] = defaultdict(list)\n    for i in range(n):\n        next(handle)\n        from_node, to_seq = next(handle).rstrip().split(\"-&gt;\")\n        graph[int(from_node)].append(i)\n        sequences[i] = to_seq\n\n    lines = handle.readlines()\n    root = int(lines[0].rstrip().split(\"-&gt;\")[0])\n    for edge in lines:\n        from_node, to_node = edge.rstrip().split(\"-&gt;\")\n        graph[int(from_node)].append(int(to_node))\n\n    root_tree(graph, root)\n    return sequences, graph\n\ndef extract_position(graph: Dict[int, List[int]], sequences: Dict[int, str], pos: int) -&gt; Dict[int, str]:\n    chars: Dict[int, str] = {}\n    for n in get_all_nodes(graph) - get_leaves(graph):\n        chars[n] = \"\"\n    for leaf in get_leaves(graph):\n        chars[leaf] = sequences[leaf][pos]\n    return chars\n\ndef ba6f(graph: Dict[int, List[int]], sequences: Dict[int, str]) -&gt; Tuple[int, Dict[int, str]]:\n    for n in get_all_nodes(graph) - get_leaves(graph):\n        sequences[n] = \"\"\n\n    total_score = 0\n    for pos in range(len(next(iter(sequences.values())))):\n        chars = extract_position(graph, sequences, pos)\n        score, tbchars = small_parsimony(graph, chars)\n        total_score += score\n        for k, v in tbchars.items():\n            sequences[k] += v\n\n    return total_score, sequences\n\ndef small_parsimony(graph: Dict[int, List[int]], chars: Dict[int, str]) -&gt; Tuple[int, Dict[int, str]]:\n    bases = [\"A\", \"C\", \"T\", \"G\"]\n    sk: Dict[int, List[int]] = {}\n    skp: Dict[int, List[Dict[int, int]]] = {}\n    to_process = nodes(graph)\n\n    for leaf in leaves(graph):\n        sk[leaf] = [0 if chars[leaf] == c else inf for c in bases]\n        to_process.remove(leaf)\n\n    while to_process:\n        for n in list(to_process):\n            if all(v in sk for v in graph[n]):\n                sk[n], skp[n] = [], []\n                for k in bases:\n                    tot = 0\n                    ptr: Dict[int, int] = {}\n                    for d, sk_child in [(d, sk[d]) for d in graph[n]]:\n                        score = []\n                        for i, c in enumerate(bases):\n                            score += [sk_child[i] + (0 if c == k else 1)]\n                        tot += min(score)\n                        ptr[d] = score.index(min(score))\n                    skp[n] += [ptr]\n                    sk[n] += [tot]\n                to_process.remove(n)\n\n    node = root(graph)\n    score = min(sk[node])\n    return score, traceback(skp, node, sk[node].index(score))\n\ndef traceback(skp: Dict[int, List[Dict[int, int]]], node: int, ind: int) -&gt; Dict[int, str]:\n    bases = [\"A\", \"C\", \"T\", \"G\"]\n    chars: Dict[int, str] = {}\n    chars[node] = bases[ind]\n    for k, v in skp[node][ind].items():\n        if k in skp:\n            chars = chars | traceback(skp, k, v)\n    return chars\n\nsample_input = \"\"\"\n4\nTCGGCCAA-&gt;4\n4-&gt;TCGGCCAA\nCCTGGCTG-&gt;4\n4-&gt;CCTGGCTG\nCACAGGAT-&gt;5\n5-&gt;CACAGGAT\nTGAGTACC-&gt;5\n5-&gt;TGAGTACC\n4-&gt;5\n5-&gt;4\n\"\"\"\n\nsequences, graph = parse_input(io.StringIO(sample_input.strip()))\ntotal_score, sequences = ba6f(graph, sequences)\nprint(total_score)\nprint_edges(graph, sequences, find_root(graph))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-81",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-81",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "81.1 Sample Dataset",
    "text": "81.1 Sample Dataset\n3 2\n0.0 0.0\n5.0 5.0\n0.0 5.0\n1.0 1.0\n2.0 2.0\n3.0 3.0\n1.0 2.0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-81",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-81",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "81.2 Sample Output",
    "text": "81.2 Sample Output\n0.0 0.0\n5.0 5.0\n0.0 5.0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-81",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-81",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "81.3 Solution",
    "text": "81.3 Solution\nfrom math import sqrt\nfrom typing import List, Tuple, Iterator, TextIO, TypeVar\nimport io\n\nT = TypeVar('T')\n\ndef read_types(file_handle: TextIO, data_type: type) -&gt; Iterator[List[T]]:\n    for line in file_handle:\n        yield list(map(data_type, line.split()))\n\ndef euclidean_distance(point_a: List[float], point_b: List[float]) -&gt; float:\n    \"\"\"Euclidean distance between a pair of n-dimensional points\"\"\"\n    return sqrt(sum((coord_a - coord_b) ** 2 for coord_a, coord_b in zip(point_a, point_b)))\n\ndef nearest_center_distance(data_point: List[float], center_points: List[List[float]]) -&gt; float:\n    \"\"\"Euclidean distance from DataPoint to its closest center\"\"\"\n    return min(euclidean_distance(data_point, center) for center in center_points)\n\ndef farthest_first_traversal(data_points: List[List[float]], num_centers: int) -&gt; List[List[float]]:\n    center_points = [data_points[0]]\n    while len(center_points) &lt; num_centers:\n        distances = [(i, nearest_center_distance(point, center_points)) for i, point in enumerate(data_points)]\n        center_points.append(data_points[max(distances, key=lambda x: x[1])[0]])\n    return center_points\n\nimport io\n\nsample_input = \"\"\"\n3 2\n0.0 0.0\n5.0 5.0\n0.0 5.0\n1.0 1.0\n2.0 2.0\n3.0 3.0\n1.0 2.0\n\"\"\"\n\nfile_handle = io.StringIO(sample_input.strip())\nnum_centers, dimensions = next(read_types(file_handle, int))\ndata_points = [point for point in read_types(file_handle, float)]\ncenter_points = farthest_first_traversal(data_points, num_centers)\nfor center in center_points:\n    print(*center)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-82",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-82",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "82.1 Sample Dataset",
    "text": "82.1 Sample Dataset\n2 2\n2.31 4.55\n5.96 9.08\n--------\n3.42 6.03\n6.23 8.25\n4.76 1.64\n4.47 4.33\n3.95 7.61\n8.93 2.97\n9.74 4.03\n1.73 1.28\n9.72 5.01\n7.27 3.77"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-82",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-82",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "82.2 Sample Output",
    "text": "82.2 Sample Output\n18.246"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-82",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-82",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "82.3 Solution",
    "text": "82.3 Solution\nfrom math import sqrt\nimport io\nfrom typing import List, Tuple, Iterator, TextIO, TypeVar\n\nT = TypeVar('T')\n\ndef read_types(file_handle: TextIO, data_type: type) -&gt; Iterator[List[T]]:\n    for line in file_handle:\n        yield list(map(data_type, line.split()))\n\ndef euclidean_distance(point_a: List[float], point_b: List[float]) -&gt; float:\n    \"\"\"Euclidean distance between a pair of n-dimensional points\"\"\"\n    return sqrt(sum((coord_a - coord_b) ** 2 for coord_a, coord_b in zip(point_a, point_b)))\n\ndef nearest_center_distance(data_point: List[float], center_points: List[List[float]]) -&gt; float:\n    \"\"\"Euclidean distance from DataPoint to its closest center\"\"\"\n    return min(euclidean_distance(data_point, center) for center in center_points)\n\ndef calculate_distortion(data_points: List[List[float]], center_points: List[List[float]]) -&gt; float:\n    return (1 / len(data_points)) * sum(nearest_center_distance(point, center_points) ** 2 for point in data_points)\n\nsample_input = \"\"\"\n2 2\n2.31 4.55\n5.96 9.08\n--------\n3.42 6.03\n6.23 8.25\n4.76 1.64\n4.47 4.33\n3.95 7.61\n8.93 2.97\n9.74 4.03\n1.73 1.28\n9.72 5.01\n7.27 3.77\n\"\"\"\n\nfile_handle = io.StringIO(sample_input.strip())\nnum_centers, _ = next(read_types(file_handle, int))\ndata_generator = read_types(file_handle, float)\ncenter_points = [next(data_generator) for _ in range(num_centers)]\n_ = next(file_handle)\ndata_points = [point for point in data_generator]\nprint(round(calculate_distortion(data_points, center_points), 3))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-83",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-83",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "83.1 Sample Dataset",
    "text": "83.1 Sample Dataset\n2 2\n1.3 1.1\n1.3 0.2\n0.6 2.8\n3.0 3.2\n1.2 0.7\n1.4 1.6\n1.2 1.0\n1.2 1.1\n0.6 1.5\n1.8 2.6\n1.2 1.3\n1.2 1.0\n0.0 1.9"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-83",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-83",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "83.2 Sample Output",
    "text": "83.2 Sample Output\n1.800 2.867\n1.060 1.140"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-83",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-83",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "83.3 Solution",
    "text": "83.3 Solution\nimport numpy as np\nimport io\nfrom math import sqrt\nfrom typing import List, Iterator, TypeVar, Any\n\nT = TypeVar('T')\n\ndef read_types(file_handle: io.StringIO, data_type: type) -&gt; Iterator[List[T]]:\n    for line in file_handle:\n        yield list(map(data_type, line.split()))\n\ndef euclidean_distance(point_a: np.ndarray, point_b: np.ndarray) -&gt; float:\n    \"\"\"Euclidean distance between a pair of n-dimensional points\"\"\"\n    return sqrt(sum((x - y) ** 2 for x, y in zip(point_a, point_b)))\n\ndef nearest_center_assignment(data_point: np.ndarray, center_points: List[np.ndarray]) -&gt; int:\n    \"\"\"Center index that minimises Euclidean distance to point\"\"\"\n    distances = [euclidean_distance(data_point, center) for center in center_points]\n    return distances.index(min(distances))\n\ndef compute_center(data_points: List[np.ndarray], assignments: List[int], center_index: int) -&gt; np.ndarray:\n    cluster_points = [p for p, a in zip(data_points, assignments) if a == center_index]\n    return np.mean(np.array(cluster_points), axis=0) if cluster_points else np.zeros_like(data_points[0])\n\ndef k_means(data_points: List[np.ndarray], num_clusters: int, max_iterations: int = 20) -&gt; List[np.ndarray]:\n    center_points = data_points[:num_clusters]\n    for _ in range(max_iterations):\n        assignments = [nearest_center_assignment(point, center_points) for point in data_points]\n        center_points = [compute_center(data_points, assignments, i) for i in range(num_clusters)]\n    return center_points\n\nsample_input = \"\"\"\n2 2\n1.3 1.1\n1.3 0.2\n0.6 2.8\n3.0 3.2\n1.2 0.7\n1.4 1.6\n1.2 1.0\n1.2 1.1\n0.6 1.5\n1.8 2.6\n1.2 1.3\n1.2 1.0\n0.0 1.9\n\"\"\"\n\nfile_handle = io.StringIO(sample_input.strip())\nnum_clusters, dimensions = next(read_types(file_handle, int))\ndata_points = [np.array(point) for point in read_types(file_handle, float)]\nfor center in k_means(data_points, num_clusters):\n    print(*[f\"{coord:f}\" for coord in center])"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-84",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-84",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "84.1 Sample Dataset",
    "text": "84.1 Sample Dataset\n2 2\n2.7\n1.3 1.1\n1.3 0.2\n0.6 2.8\n3.0 3.2\n1.2 0.7\n1.4 1.6\n1.2 1.0\n1.2 1.1\n0.6 1.5\n1.8 2.6\n1.2 1.3\n1.2 1.0\n0.0 1.9"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-84",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-84",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "84.2 Sample Output",
    "text": "84.2 Sample Output\n1.662 2.623\n1.075 1.148"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-84",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-84",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "84.3 Solution",
    "text": "84.3 Solution\nimport numpy as np\nfrom math import sqrt\nimport io\nfrom typing import List, Iterator, TypeVar, Any\n\nT = TypeVar('T')\n\ndef read_types(file_handle: io.StringIO, data_type: type) -&gt; Iterator[List[T]]:\n    for line in file_handle:\n        yield list(map(data_type, line.split()))\n\ndef euclidean_distance(point_a: np.ndarray, point_b: np.ndarray) -&gt; float:\n    \"\"\"Euclidean distance between a pair of n-dimensional points\"\"\"\n    return sqrt(sum((x - y) ** 2 for x, y in zip(point_a, point_b)))\n\ndef partition_function(data_point: np.ndarray, center_points: np.ndarray, beta: float) -&gt; np.ndarray:\n    numerator = [np.exp(-beta * euclidean_distance(data_point, center)) for center in center_points]\n    return np.array(numerator) / sum(numerator)\n\ndef hidden_matrix(data_points: np.ndarray, center_points: np.ndarray, beta: float) -&gt; np.ndarray:\n    return np.array([partition_function(x, center_points, beta) for x in data_points])\n\ndef soft_k_means(data_points: List[np.ndarray], num_clusters: int, beta: float, max_iterations: int = 20) -&gt; np.ndarray:\n    center_points = np.array(data_points[:num_clusters])\n    data_points = np.array(data_points)\n    for _ in range(max_iterations):\n        h_matrix = hidden_matrix(data_points, center_points, beta)\n        center_points = [np.dot(h_matrix[:, i], data_points) for i in range(num_clusters)]\n        sums = np.sum(h_matrix, 0)\n        center_points = np.transpose(np.transpose(center_points) / sums)\n    return center_points\n\nsample_input = \"\"\"\n2 2\n2.7\n1.3 1.1\n1.3 0.2\n0.6 2.8\n3.0 3.2\n1.2 0.7\n1.4 1.6\n1.2 1.0\n1.2 1.1\n0.6 1.5\n1.8 2.6\n1.2 1.3\n1.2 1.0\n0.0 1.9\n\"\"\"\n\nfile_handle = io.StringIO(sample_input.strip())\nnum_clusters, dimensions = next(read_types(file_handle, int))\nbeta = next(read_types(file_handle, float))[0]\ndata_points = [np.array(point) for point in read_types(file_handle, float)]\nfor center in soft_k_means(data_points, num_clusters, beta):\n    print(*[f\"{coord:f}\" for coord in center])"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-85",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-85",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "85.1 Sample Dataset",
    "text": "85.1 Sample Dataset\n7\n0.00 0.74 0.85 0.54 0.83 0.92 0.89\n0.74 0.00 1.59 1.35 1.20 1.48 1.55\n0.85 1.59 0.00 0.63 1.13 0.69 0.73\n0.54 1.35 0.63 0.00 0.66 0.43 0.88\n0.83 1.20 1.13 0.66 0.00 0.72 0.55\n0.92 1.48 0.69 0.43 0.72 0.00 0.80\n0.89 1.55 0.73 0.88 0.55 0.80 0.00"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-85",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-85",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "85.2 Sample Output",
    "text": "85.2 Sample Output\n4 6\n5 7\n3 4 6\n1 2\n5 7 3 4 6\n1 2 5 7 3 4 6"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-85",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-85",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "85.3 Solution",
    "text": "85.3 Solution\nimport numpy as np\nfrom collections import defaultdict\nimport io\nfrom typing import List, Tuple, Dict, Generator\n\n# find (first) minimum off diagonal index in an array\ndef find_closest_pair(distance_matrix: np.ndarray) -&gt; Tuple[int, int]:\n    temp_matrix = np.copy(distance_matrix)\n    np.fill_diagonal(temp_matrix, temp_matrix.max() + 1)\n    return divmod(temp_matrix.argmin(), temp_matrix.shape[1])\n\ndef average_distances(distance_matrix: np.ndarray, i: int, j: int, size_i: int, size_j: int) -&gt; np.ndarray:\n    temp_matrix = np.copy(distance_matrix)\n    average = (temp_matrix[i, :] * size_i + temp_matrix[j, :] * size_j) / (size_i + size_j)\n    temp_matrix[i, :] = average\n    temp_matrix[:, i] = average\n    temp_matrix = np.delete(temp_matrix, j, 0)\n    temp_matrix = np.delete(temp_matrix, j, 1)\n    np.fill_diagonal(temp_matrix, 0)\n    return temp_matrix\n\ndef get_descendants(tree: Dict[int, List[int]], node: int) -&gt; List[int]:\n    queue = [node]\n    descendants = []\n    while queue:\n        current_node = queue.pop(0)\n        if current_node in tree:\n            queue.extend(tree[current_node])\n        else:\n            descendants.append(current_node)\n    return descendants\n\ndef hierarchical_clustering(distance_matrix: np.ndarray, num_elements: int) -&gt; Generator[List[int], None, None]:\n    clusters = list(range(1, num_elements + 1))\n    tree: Dict[int, List[int]] = {}\n    cluster_size = defaultdict(lambda: 1)  # the number of descendants of a node\n    next_node = num_elements\n    while len(clusters) &gt; 1:\n        next_node += 1\n        i, j = find_closest_pair(distance_matrix)\n        cluster_a, cluster_b = clusters[i], clusters[j]\n        tree[next_node] = [cluster_a, cluster_b]\n        cluster_size[next_node] = cluster_size[cluster_a] + cluster_size[cluster_b]\n        distance_matrix = average_distances(distance_matrix, *find_closest_pair(distance_matrix), cluster_size[cluster_a], cluster_size[cluster_b])\n        clusters[i] = next_node\n        del clusters[j]\n        yield get_descendants(tree, cluster_a) + get_descendants(tree, cluster_b)\n\nsample_input = \"\"\"\n7\n0.00 0.74 0.85 0.54 0.83 0.92 0.89\n0.74 0.00 1.59 1.35 1.20 1.48 1.55\n0.85 1.59 0.00 0.63 1.13 0.69 0.73\n0.54 1.35 0.63 0.00 0.66 0.43 0.88\n0.83 1.20 1.13 0.66 0.00 0.72 0.55\n0.92 1.48 0.69 0.43 0.72 0.00 0.80\n0.89 1.55 0.73 0.88 0.55 0.80 0.00\n\"\"\"\n\nnum_elements, *distance_data = io.StringIO(sample_input.strip()).read().splitlines()\ndistance_matrix = np.array([list(map(float, row.split())) for row in distance_data])\nfor step in hierarchical_clustering(distance_matrix, int(num_elements)):\n    print(*step)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-86",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-86",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "86.1 Sample Dataset",
    "text": "86.1 Sample Dataset\nATAGA\nATC\nGAT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-86",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-86",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "86.2 Sample Output",
    "text": "86.2 Sample Output\n0-&gt;1:A\n1-&gt;2:T\n2-&gt;3:A\n3-&gt;4:G\n4-&gt;5:A\n2-&gt;6:C\n0-&gt;7:G\n7-&gt;8:A\n8-&gt;9:T"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-86",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-86",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "86.3 Solution",
    "text": "86.3 Solution\nfrom itertools import count\nfrom typing import List, Dict, Tuple, Any\n\nclass Trie:\n    def __init__(self) -&gt; None:\n        self.id_generator = count(start=0)\n        self.root: Tuple[int, Dict[str, Any]] = (next(self.id_generator), {})\n    \n    def insert(self, sequence: str) -&gt; None:\n        current_node = self.root\n        for character in sequence:\n            if character not in current_node[1]:\n                current_node[1][character] = (next(self.id_generator), {})\n            current_node = current_node[1][character]\n            \ndef build_trie(sequences: List[str]) -&gt; Tuple[int, Dict[str, Any]]:\n    trie = Trie()\n    for sequence in sequences:\n        trie.insert(sequence)\n    return trie.root\n\ndef format_trie(node: Tuple[int, Dict[str, Any]]) -&gt; List[str]:\n    formatted_output: List[str] = []\n    node_id, children = node\n    for char, child_node in children.items():\n        child_id, _ = child_node\n        formatted_output.append(f\"{node_id}-&gt;{child_id}:{char}\")\n        formatted_output.extend(format_trie(child_node))\n    return formatted_output\n\nsample_input: str = \"\"\"\nATAGA\nATC\nGAT\n\"\"\"\n\nsequences: List[str] = sample_input.strip().split(\"\\n\")\n\nfor edge in format_trie(build_trie(sequences)):\n    print(edge)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-87",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-87",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "87.1 Sample Dataset",
    "text": "87.1 Sample Dataset\nAATCGGGTTCAATCGGGGT\nATCG\nGGGT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-87",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-87",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "87.2 Sample Output",
    "text": "87.2 Sample Output\n1 4 11 15"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-87",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-87",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "87.3 Solution",
    "text": "87.3 Solution\nfrom collections import defaultdict\nfrom typing import List, DefaultDict, Set\n\nsample_input: str = \"\"\"\nAATCGGGTTCAATCGGGGT\nATCG\nGGGT\n\"\"\"\n\nsequences: List[str] = sample_input.strip().split(\"\\n\")\nmain_sequence: str = sequences[0]\nkmers: List[str] = sequences[1:]\n\nkmer_positions: DefaultDict[str, Set[int]] = defaultdict(set)\n\nkmer_length: int = len(kmers[0])\n\nfor start_index in range(len(main_sequence) - kmer_length + 1):\n    current_kmer: str = main_sequence[start_index:start_index + kmer_length]\n    kmer_positions[current_kmer].add(start_index)\n\nall_positions: Set[int] = set()\nfor kmer in kmers:\n    all_positions.update(kmer_positions[kmer])\n\nsorted_positions: List[int] = sorted(all_positions)\noutput: str = \" \".join(map(str, sorted_positions))\n\nprint(output)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-88",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-88",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "88.1 Sample Dataset",
    "text": "88.1 Sample Dataset\nATAAATG$"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-88",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-88",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "88.2 Sample Output",
    "text": "88.2 Sample Output\nAAATG$\nG$\nT\nATG$\nTG$\nA\nA\nAAATG$\nG$\nT\nG$\n$"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-88",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-88",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "88.3 Solution",
    "text": "88.3 Solution\nfrom typing import List, Optional\n\nclass Tree:\n    class Node:\n        def __init__(self) -&gt; None:\n            self.label: Optional[int] = None\n            self.edges: List['Tree.Edge'] = []\n            self.indicator: Optional[str] = None\n            self.depth: int = 0\n\n    class Edge:\n        def __init__(self) -&gt; None:\n            self.from_node: Optional['Tree.Node'] = None\n            self.target_node: Optional['Tree.Node'] = None\n            self.position: Optional[int] = None\n            self.length: Optional[int] = None\n\n    def __init__(self) -&gt; None:\n        self.all_nodes: List[Tree.Node] = []\n        self.all_edges: List[Tree.Edge] = []\n        self.root: Tree.Node = self.add_node()\n\n    def add_node(self) -&gt; Node:\n        new_node = Tree.Node()\n        new_node.label = len(self.all_nodes)\n        self.all_nodes.append(new_node)\n        return new_node\n\n    def add_edge(self, from_node: Node, target_node: Node, pos: int, length: int) -&gt; Edge:\n        new_edge = Tree.Edge()\n        new_edge.from_node = from_node\n        new_edge.target_node = target_node\n        new_edge.position = pos\n        new_edge.length = length\n        target_node.depth = from_node.depth + length\n        from_node.edges.append(new_edge)\n        self.all_edges.append(new_edge)\n        return new_edge\n\n    def edge_labels(self, text: str) -&gt; List[str]:\n        return [text[edge.position : edge.position + edge.length] for edge in self.all_edges]\n\n    def return_ripe_nodes(self) -&gt; List[Node]:\n        ripe_nodes = []\n        for node in self.all_nodes:\n            if node.indicator is None and all(edge.target_node.indicator is not None for edge in node.edges):\n                ripe_nodes.append(node)\n        return ripe_nodes\n\n    def add_indicators(self) -&gt; None:\n        ripe_nodes = self.return_ripe_nodes()\n        while ripe_nodes:\n            for node in ripe_nodes:\n                children_indicators = set(edge.target_node.indicator for edge in node.edges)\n                node.indicator = '*' if len(children_indicators) != 1 else next(iter(children_indicators))\n            ripe_nodes = self.return_ripe_nodes()\n\n    def construct_suffix_tree(self, trie_node: 'Trie.Node', tree_node: Node, path: List['Trie.Edge'] = []) -&gt; None:\n        while len(trie_node.edges) == 1:\n            trie_edge = trie_node.edges[0]\n            path.append(trie_edge)\n            trie_node = trie_edge.target_node\n\n        if path:\n            new_tree_node = self.add_node()\n            self.add_edge(tree_node, new_tree_node, path[0].position, len(path))\n            tree_node = new_tree_node\n\n        if not trie_node.edges:\n            tree_node.indicator = trie_node.indicator\n            tree_node.label = trie_node.label\n            return\n\n        for trie_edge in trie_node.edges:\n            self.construct_suffix_tree(trie_edge.target_node, tree_node, [trie_edge])\n\n    def populate_suffix_tree(self, text: str) -&gt; None:\n        suffix_trie = Trie()\n        suffix_trie.construct_suffix_trie(text)\n        self.construct_suffix_tree(suffix_trie.root, self.root)\n\nclass Trie:\n    class Node:\n        def __init__(self) -&gt; None:\n            self.label: Optional[int] = None\n            self.edges: List['Trie.Edge'] = []\n            self.indicator: Optional[str] = None\n\n    class Edge:\n        def __init__(self) -&gt; None:\n            self.from_node: Optional['Trie.Node'] = None\n            self.target_node: Optional['Trie.Node'] = None\n            self.label: Optional[str] = None\n            self.position: Optional[int] = None\n\n    def __init__(self) -&gt; None:\n        self.all_nodes: List[Trie.Node] = []\n        self.all_edges: List[Trie.Edge] = []\n        self.root: Trie.Node = self.add_node()\n\n    def add_node(self) -&gt; Node:\n        new_node = Trie.Node()\n        new_node.label = len(self.all_nodes)\n        self.all_nodes.append(new_node)\n        return new_node\n\n    def add_edge(self, from_node: Node, target_node: Node, label: str, pos: Optional[int] = None) -&gt; Edge:\n        new_edge = Trie.Edge()\n        new_edge.from_node = from_node\n        new_edge.target_node = target_node\n        new_edge.label = label\n        new_edge.position = pos\n        from_node.edges.append(new_edge)\n        self.all_edges.append(new_edge)\n        return new_edge\n\n    def construct_suffix_trie(self, text: str) -&gt; None:\n        indicator = '#'\n        for i in range(len(text)):\n            current_node = self.root\n            for j in range(i, len(text)):\n                current_symbol = text[j]\n                next_node = next((edge.target_node for edge in current_node.edges if edge.label == current_symbol), None)\n                if next_node is None:\n                    new_node = self.add_node()\n                    self.add_edge(current_node, new_node, current_symbol, j)\n                    current_node = new_node\n                else:\n                    current_node = next_node\n            if not current_node.edges:\n                current_node.label = f'L{i}'\n                current_node.indicator = indicator\n            if text[i] == '#':\n                indicator = '$'\n\nsample_input = \"ATAAATG$\"\ninput_lines = sample_input.strip().split()\ntext = input_lines[0]\n\ntree = Tree()\ntree.populate_suffix_tree(text)\n\nresult = tree.edge_labels(text)\nprint(\"\\n\".join(result))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-89",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-89",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "89.1 Sample Dataset",
    "text": "89.1 Sample Dataset\nATATCGTTTTATCGTT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-89",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-89",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "89.2 Sample Output",
    "text": "89.2 Sample Output\nTATCGTT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-89",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-89",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "89.3 Solution",
    "text": "89.3 Solution\nfrom functools import cache\nfrom os.path import commonprefix\nfrom typing import Dict, List, Tuple, Generator, Iterator\n\ndef get_edges(graph: Dict[str, Dict]) -&gt; Generator[str, None, None]:\n    for key in graph.keys():\n        yield key\n        yield from get_edges(graph[key])\n\n@cache\ndef suffix_tree(sequence: str, start_positions: Tuple[int, ...]) -&gt; Dict[str, Dict]:\n    tree: Dict[str, Dict] = {}\n    unique_bases = sorted(set([sequence[start] for start in start_positions]))\n    \n    for base in unique_bases:\n        matching_positions = [start for start in start_positions if sequence[start] == base]\n        subsequences = [sequence[s:] for s in matching_positions]\n        common_prefix = commonprefix(subsequences)\n        prefix_length = len(common_prefix)\n        new_start_positions = [start + prefix_length for start in matching_positions if start + prefix_length &lt; len(sequence)]\n        tree[common_prefix] = suffix_tree(sequence, tuple(new_start_positions))\n    \n    return tree\n\ndef create_suffix_tree(sequence: str) -&gt; Dict[str, Dict]:\n    return suffix_tree(sequence, tuple(range(len(sequence))))\n\ndef internal_edges(tree: Dict[str, Dict]) -&gt; Iterator[str]:\n    for node in tree.keys():\n        if not len(tree[node]):\n            yield \"\"\n        for child_edge in internal_edges(tree[node]):\n            yield node + child_edge\n\ndef longest_shared_substring(tree: Dict[str, Dict]) -&gt; str:\n    return max(internal_edges(tree), key=lambda x: len(x))\n\nsample_input = \"\"\"\nATATCGTTTTATCGTT\n\"\"\"\n\nsequence = sample_input.strip()\nsuffix_tree = create_suffix_tree(sequence)\nprint(longest_shared_substring(suffix_tree))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-90",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-90",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "90.1 Sample Dataset",
    "text": "90.1 Sample Dataset\nTCGGTAGATTGCGCCCACTC\nAGGGGCTCGCAGTGTAAGAA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-90",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-90",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "90.2 Sample Output",
    "text": "90.2 Sample Output\nAGA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-90",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-90",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "90.3 Solution",
    "text": "90.3 Solution\nfrom collections import defaultdict\nimport re\nfrom functools import cache\nfrom os.path import commonprefix\nfrom typing import Dict, List, Set, Tuple, Generator\n\n@cache\ndef build_suffix_tree(sequence: str, start_positions: Tuple[int, ...]) -&gt; Dict[str, Dict]:\n    tree: Dict[str, Dict] = {}\n    unique_bases = sorted(set([sequence[start] for start in start_positions]))\n    for base in unique_bases:\n        matching_positions = [start for start in start_positions if sequence[start] == base]\n        subsequences = [sequence[s:] for s in matching_positions]\n        common_prefix = commonprefix(subsequences)\n        prefix_length = len(common_prefix)\n        new_start_positions = [start + prefix_length for start in matching_positions if start + prefix_length &lt; len(sequence)]\n        tree[common_prefix] = build_suffix_tree(sequence, tuple(new_start_positions))\n    return tree\n\ndef create_suffix_tree(sequence: str) -&gt; Dict[str, Dict]:\n    return build_suffix_tree(sequence, tuple(range(len(sequence))))\n\ndef convert_to_graph(suffix_tree: Dict[str, Dict]) -&gt; Dict[int, List[Dict[str, int]]]:\n    def build_graph_structure(subtree: Dict[str, Dict], graph: Dict[int, List[Dict[str, int]]], node_id: int) -&gt; int:\n        next_node_id = node_id\n        for edge in sorted(subtree):\n            next_node_id += 1\n            graph[node_id].append({\"n\": next_node_id, \"l\": edge})\n            next_node_id = build_graph_structure(subtree[edge], graph, next_node_id)\n        return next_node_id\n\n    graph: Dict[int, List[Dict[str, int]]] = defaultdict(list)\n    build_graph_structure(suffix_tree, graph, 0)\n    return graph\n\ndef find_leaf_nodes(graph: Dict[int, List[Dict[str, int]]]) -&gt; Set[int]:\n    all_child_nodes = set(node[\"n\"] for nodes in graph.values() for node in nodes)\n    parent_nodes = set(graph.keys())\n    return all_child_nodes - parent_nodes\n\ndef reverse_graph_structure(graph: Dict[int, List[Dict[str, int]]]) -&gt; Dict[int, Dict[str, int]]:\n    reversed_graph: Dict[int, Dict[str, int]] = {}\n    for parent_node, children in graph.items():\n        for child in children:\n            reversed_graph[child[\"n\"]] = {\"n\": parent_node, \"l\": child[\"l\"]}\n    return reversed_graph\n\ndef get_purple_edges(tree: Dict[int, List[Dict[str, int]]], colors: Dict[int, str]) -&gt; Generator[str, None, None]:\n    def _get_purple_edges(node: int, sequence: str) -&gt; Generator[str, None, None]:\n        if colors[node] == \"purple\":\n            has_purple_child = any(colors[child[\"n\"]] == \"purple\" for child in tree[node])\n            if has_purple_child:\n                for child in tree[node]:\n                    yield from _get_purple_edges(child[\"n\"], sequence + child[\"l\"])\n            else:\n                yield sequence\n\n    yield from _get_purple_edges(0, \"\")\n\ndef initialize_leaf_colors(tree: Dict[int, List[Dict[str, int]]]) -&gt; Dict[int, str]:\n    reversed_graph = reverse_graph_structure(tree)\n    colors: Dict[int, str] = defaultdict(lambda: None)\n    for leaf in find_leaf_nodes(tree):\n        edge = reversed_graph[leaf][\"l\"]\n        match = re.search(r\"([$#])\", edge)\n        colors[leaf] = \"red\" if match.group() == \"$\" else \"blue\"\n    return colors\n\ndef color_tree_nodes(tree: Dict[int, List[Dict[str, int]]], colors: Dict[int, str]) -&gt; Dict[int, str]:\n    nodes_to_process = list(tree.keys())\n    while nodes_to_process:\n        for node in nodes_to_process:\n            child_colors = [colors[child[\"n\"]] for child in tree[node]]\n            if all(child_colors):\n                unique_colors = set(child_colors)\n                if unique_colors == {\"red\"}:\n                    colors[node] = \"red\"\n                elif unique_colors == {\"blue\"}:\n                    colors[node] = \"blue\"\n                else:\n                    colors[node] = \"purple\"\n                nodes_to_process.remove(node)\n    return colors\n\ndef find_longest_shared_substring(seq1: str, seq2: str) -&gt; str:\n    combined_sequence = seq1 + \"$\" + seq2 + \"#\"\n    suffix_tree = create_suffix_tree(combined_sequence)\n    graph = convert_to_graph(suffix_tree)\n    initial_colors = initialize_leaf_colors(graph)\n    final_colors = color_tree_nodes(graph, initial_colors)\n    return max(get_purple_edges(graph, final_colors), key=len)\n\nsample_input = \"\"\"\nTCGGTAGATTGCGCCCACTC\nAGGGGCTCGCAGTGTAAGAA\n\"\"\"\nsequence1, sequence2 = sample_input.strip().split(\"\\n\")\nprint(find_longest_shared_substring(sequence1, sequence2))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-91",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-91",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "91.1 Sample Dataset",
    "text": "91.1 Sample Dataset\nCCAAGCTGCTAGAGG\nCATGCTGGGCTGGCT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-91",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-91",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "91.2 Sample Output",
    "text": "91.2 Sample Output\nAA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-91",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-91",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "91.3 Solution",
    "text": "91.3 Solution\nfrom functools import cache\nfrom os.path import commonprefix\nfrom typing import Dict, List, Tuple, Set\nfrom collections import defaultdict\nimport re\n\n@cache\ndef build_suffix_tree(sequence: str, start_positions: Tuple[int, ...]) -&gt; Dict[str, Dict]:\n    tree: Dict[str, Dict] = {}\n    unique_bases = sorted(set([sequence[start] for start in start_positions]))\n    for base in unique_bases:\n        matching_positions = [start for start in start_positions if sequence[start] == base]\n        subsequences = [sequence[s:] for s in matching_positions]\n        common_prefix = commonprefix(subsequences)\n        prefix_length = len(common_prefix)\n        new_start_positions = [start + prefix_length for start in matching_positions if start + prefix_length &lt; len(sequence)]\n        tree[common_prefix] = build_suffix_tree(sequence, tuple(new_start_positions))\n    return tree\n\ndef create_suffix_tree(sequence: str) -&gt; Dict[str, Dict]:\n    return build_suffix_tree(sequence, tuple(range(len(sequence))))\n\ndef find_leaf_nodes(graph: Dict[int, List[Dict[str, int]]]) -&gt; Set[int]:\n    all_child_nodes = set(node[\"n\"] for nodes in graph.values() for node in nodes)\n    parent_nodes = set(graph.keys())\n    return all_child_nodes - parent_nodes\n\ndef reverse_graph_structure(graph: Dict[int, List[Dict[str, int]]]) -&gt; Dict[int, Dict[str, int]]:\n    reversed_graph: Dict[int, Dict[str, int]] = {}\n    for parent, children in graph.items():\n        for child in children:\n            reversed_graph[child[\"n\"]] = {\"n\": parent, \"l\": child[\"l\"]}\n    return reversed_graph\n\ndef convert_to_graph(suffix_tree: Dict[str, Dict]) -&gt; Dict[int, List[Dict[str, int]]]:\n    def build_graph_structure(subtree: Dict[str, Dict], graph: Dict[int, List[Dict[str, int]]], node_id: int) -&gt; int:\n        next_node_id = node_id\n        for edge in sorted(subtree):\n            next_node_id += 1\n            graph[node_id].append({\"n\": next_node_id, \"l\": edge})\n            next_node_id = build_graph_structure(subtree[edge], graph, next_node_id)\n        return next_node_id\n\n    graph: Dict[int, List[Dict[str, int]]] = defaultdict(list)\n    build_graph_structure(suffix_tree, graph, 0)\n    return graph\n\ndef initialize_leaf_colors(tree: Dict[int, List[Dict[str, int]]]) -&gt; Dict[int, str]:\n    reversed_graph = reverse_graph_structure(tree)\n    colors: Dict[int, str] = defaultdict(lambda: None)\n    for leaf in find_leaf_nodes(tree):\n        edge = reversed_graph[leaf][\"l\"]\n        match = re.search(r\"([$#])\", edge)\n        colors[leaf] = \"red\" if match and match.group() == \"$\" else \"blue\"\n    return colors\n\ndef color_tree_nodes(tree: Dict[int, List[Dict[str, int]]], colors: Dict[int, str]) -&gt; Dict[int, str]:\n    nodes_to_process = list(tree.keys())\n    while nodes_to_process:\n        for node in nodes_to_process[:]:\n            child_colors = [colors[n[\"n\"]] for n in tree[node]]\n            if all(child_colors):\n                unique_colors = set(child_colors)\n                if unique_colors == {\"red\"}:\n                    colors[node] = \"red\"\n                elif unique_colors == {\"blue\"}:\n                    colors[node] = \"blue\"\n                else:\n                    colors[node] = \"purple\"\n                nodes_to_process.remove(node)\n    return colors\n\ndef find_non_purple_edges(tree: Dict[int, List[Dict[str, int]]], colors: Dict[int, str]):\n    def _find_non_purple_edges(node: int, sequence: List[str], path: List[int]):\n        if colors[node] == \"purple\":\n            for child in tree[node]:\n                yield from _find_non_purple_edges(child[\"n\"], sequence + [child[\"l\"]], path + [node])\n        else:\n            if colors[node] == \"red\":\n                sequence[-1] = sequence[-1][0]\n                joined_sequence = \"\".join(sequence)\n                if \"#\" not in joined_sequence and \"$\" not in joined_sequence:\n                    yield joined_sequence\n\n    yield from _find_non_purple_edges(0, [], [])\n\ndef find_shortest_nonshared_substring(seq1: str, seq2: str) -&gt; str:\n    combined_sequence = seq1 + \"$\" + seq2 + \"#\"\n    suffix_tree = create_suffix_tree(combined_sequence)\n    graph = convert_to_graph(suffix_tree)\n    initial_colors = initialize_leaf_colors(graph)\n    final_colors = color_tree_nodes(graph, initial_colors)\n    return min(find_non_purple_edges(graph, final_colors), key=len)\n\nsample_input = \"\"\"\nCCAAGCTGCTAGAGG\nCATGCTGGGCTGGCT\n\"\"\"\n\nsequence1, sequence2 = sample_input.strip().split(\"\\n\")\nprint(find_shortest_nonshared_substring(sequence1, sequence2))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-92",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-92",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "92.1 Sample Dataset",
    "text": "92.1 Sample Dataset\nAACGATAGCGGTAGA$"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-92",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-92",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "92.2 Sample Output",
    "text": "92.2 Sample Output\n15, 14, 0, 1, 12, 6, 4, 2, 8, 13, 3, 7, 9, 10, 11, 5"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-92",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-92",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "92.3 Solution",
    "text": "92.3 Solution\nfrom typing import List\n\ndef create_suffix_array(text: str) -&gt; List[int]:\n    \"\"\"\n    Create a suffix array for the given text.\n\n    Args:\n    text (str): The input string to create a suffix array for.\n\n    Returns:\n    List[int]: A list of indices representing the suffix array.\n    \"\"\"\n    suffixes = [(i, text[i:]) for i in range(len(text))]\n    sorted_suffixes = sorted(suffixes, key=lambda x: x[1])\n    return [index for index, _ in sorted_suffixes]\n\nsample_input = \"AACGATAGCGGTAGA$\"\nsuffix_array = create_suffix_array(sample_input.strip())\nprint(*suffix_array, sep=\", \")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-93",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-93",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "93.1 Sample Dataset",
    "text": "93.1 Sample Dataset\nAATCGGGTTCAATCGGGGT\nATCG\nGGGT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-93",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-93",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "93.2 Sample Output",
    "text": "93.2 Sample Output\n1 4 11 15"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-93",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-93",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "93.3 Solution",
    "text": "93.3 Solution\nfrom typing import List, Tuple\n\ndef create_suffix_array(text: str) -&gt; List[int]:\n    \"\"\"\n    Create a suffix array for the given text.\n\n    Args:\n    text (str): The input string to create a suffix array for.\n\n    Returns:\n    List[int]: A list of indices representing the suffix array.\n    \"\"\"\n    suffixes: List[Tuple[int, str]] = [(i, text[i:]) for i in range(len(text))]\n    sorted_suffixes: List[Tuple[int, str]] = sorted(suffixes, key=lambda x: x[1])\n    return [index for index, _ in sorted_suffixes]\n\ndef find_pattern_occurrences(text: str, pattern: str, suffix_array: List[int]) -&gt; List[int]:\n    \"\"\"\n    Find all occurrences of a pattern in the text using the suffix array.\n\n    Args:\n    text (str): The input text to search in.\n    pattern (str): The pattern to search for.\n    suffix_array (List[int]): The suffix array of the text.\n\n    Returns:\n    List[int]: A list of indices where the pattern occurs in the text.\n    \"\"\"\n    lower_bound: int = 0\n    upper_bound: int = len(text)\n    \n    while lower_bound &lt; upper_bound:\n        mid_point: int = (lower_bound + upper_bound) // 2\n        if pattern &gt; text[suffix_array[mid_point]:][: len(pattern)]:\n            lower_bound = mid_point + 1\n        else:\n            upper_bound = mid_point\n    \n    first_occurrence: int = lower_bound\n    upper_bound = len(text)\n    \n    while lower_bound &lt; upper_bound:\n        mid_point: int = (lower_bound + upper_bound) // 2\n        if pattern &lt; text[suffix_array[mid_point]:][: len(pattern)]:\n            upper_bound = mid_point\n        else:\n            lower_bound = mid_point + 1\n    \n    last_occurrence: int = upper_bound\n    \n    if first_occurrence &gt; last_occurrence:\n        return []\n    else:\n        return list(range(first_occurrence, last_occurrence))\n\nsample_input: str = \"\"\"\nAATCGGGTTCAATCGGGGT\nATCG\nGGGT\n\"\"\"\n\nsequence, *patterns = sample_input.strip().split(\"\\n\")\nsuffix_array: List[int] = create_suffix_array(sequence)\nmatching_indices: List[int] = []\n\nfor pattern in patterns:\n    for index in find_pattern_occurrences(sequence, pattern, suffix_array):\n        matching_indices.append(suffix_array[index])\n\nprint(*sorted(set(matching_indices)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-94",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-94",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "94.1 Sample Dataset",
    "text": "94.1 Sample Dataset\nGCGTGCCTGGTCA$"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-94",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-94",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "94.2 Sample Output",
    "text": "94.2 Sample Output\nACTGGCT$TGCGGC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-94",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-94",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "94.3 Solution",
    "text": "94.3 Solution\nfrom typing import List\n\ndef create_suffix_array(text: str) -&gt; List[int]:\n    \"\"\"\n    Create a suffix array for the given text.\n\n    Args:\n    text (str): The input string to create a suffix array for.\n\n    Returns:\n    List[int]: A list of indices representing the suffix array.\n    \"\"\"\n    suffixes = sorted(range(len(text)), key=lambda i: text[i:])\n    return suffixes\n\ndef burrows_wheeler_transform(sequence: str) -&gt; str:\n    \"\"\"\n    Perform the Burrows-Wheeler Transform on the input sequence.\n\n    Args:\n    sequence (str): The input string to transform.\n\n    Returns:\n    str: The Burrows-Wheeler Transform of the input sequence.\n    \"\"\"\n    suffix_array = create_suffix_array(sequence)\n    return ''.join(sequence[i - 1] for i in suffix_array)\n\nsample_input = \"GCGTGCCTGGTCA$\"\ntransformed_sequence = burrows_wheeler_transform(sample_input.strip())\nprint(transformed_sequence)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-95",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-95",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "95.1 Sample Dataset",
    "text": "95.1 Sample Dataset\nTTCCTAACG$A"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-95",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-95",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "95.2 Sample Output",
    "text": "95.2 Sample Output\nTACATCACGT$"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-95",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-95",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "95.3 Solution",
    "text": "95.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Tuple, Generator\n\ndef index_characters(sequence: str) -&gt; Generator[Tuple[str, int], None, None]:\n    \"\"\"\n    Generate each character with its occurrence number in the sequence.\n\n    Args:\n    sequence (str): The input string to index.\n\n    Yields:\n    Tuple[str, int]: A tuple containing the character and its occurrence number.\n    \"\"\"\n    char_counts = defaultdict(int)\n    for char in sequence:\n        yield char, char_counts[char]\n        char_counts[char] += 1\n\ndef burrows_wheeler_transform_inverse(bwt_sequence: str) -&gt; str:\n    \"\"\"\n    Perform the inverse Burrows-Wheeler Transform on the input sequence.\n\n    Args:\n    bwt_sequence (str): The Burrows-Wheeler transformed string.\n\n    Returns:\n    str: The original string before BWT.\n    \"\"\"\n    first_column = list(index_characters(sorted(bwt_sequence)))\n    last_column = list(index_characters(bwt_sequence))\n    \n    current_char: Tuple[str, int] = (\"$\", 0)\n    original_sequence: List[str] = []\n    \n    for _ in range(len(bwt_sequence)):\n        current_char = first_column[last_column.index(current_char)]\n        original_sequence.append(current_char[0])\n    \n    return ''.join(original_sequence)\n\nsample_input = \"TTCCTAACG$A\"\noriginal_sequence = burrows_wheeler_transform_inverse(sample_input.strip())\nprint(original_sequence)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-96",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-96",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "96.1 Sample Dataset",
    "text": "96.1 Sample Dataset\nT$GACCA\n3"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-96",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-96",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "96.2 Sample Output",
    "text": "96.2 Sample Output\n1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-96",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-96",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "96.3 Solution",
    "text": "96.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Tuple, Generator\n\ndef index_characters(sequence: str) -&gt; Generator[Tuple[str, int], None, None]:\n    \"\"\"\n    Generate each character with its occurrence number in the sequence.\n\n    Args:\n    sequence (str): The input string to index.\n\n    Yields:\n    Tuple[str, int]: A tuple containing the character and its occurrence number.\n    \"\"\"\n    char_counts = defaultdict(int)\n    for char in sequence:\n        yield char, char_counts[char]\n        char_counts[char] += 1\n\ndef last_to_first_mapping(sequence: str, index: int) -&gt; int:\n    \"\"\"\n    Find the mapping from Last column to First column in the Burrows-Wheeler Transform matrix.\n\n    Args:\n    sequence (str): The input string (Last column of BWT matrix).\n    index (int): The index in the Last column.\n\n    Returns:\n    int: The corresponding index in the First column.\n    \"\"\"\n    first_column = list(index_characters(sorted(sequence)))\n    last_column = list(index_characters(sequence))\n    return first_column.index(last_column[index])\n\nsample_input = \"\"\"\nT$GACCA\n3\n\"\"\"\nsequence, index_str = sample_input.strip().split(\"\\n\")\nindex = int(index_str)\n\nresult = last_to_first_mapping(sequence.strip(), index)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-97",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-97",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "97.1 Sample Dataset",
    "text": "97.1 Sample Dataset\nTCCTCTATGAGATCCTATTCTATGAAACCTTCA$GACCAAAATTCTCCGGC\nCCT CAC GAG CAG ATC"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-97",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-97",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "97.2 Sample Output",
    "text": "97.2 Sample Output\n2 1 1 0 1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-97",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-97",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "97.3 Solution",
    "text": "97.3 Solution\nfrom typing import List, Dict, Tuple\n\ndef BWMatching(bwt: str, patterns: List[str]) -&gt; List[int]:\n    def create_count_and_first_occurrence(bwt: str) -&gt; Tuple[Dict[str, List[int]], Dict[str, int]]:\n        count: Dict[str, List[int]] = {char: [0] * (len(bwt) + 1) for char in set(bwt)}\n        first_occurrence: Dict[str, int] = {}\n        sorted_bwt: List[str] = sorted(bwt)\n        \n        for i, char in enumerate(bwt):\n            for c in count:\n                count[c][i + 1] = count[c][i]\n            count[char][i + 1] += 1\n        \n        for i, char in enumerate(sorted_bwt):\n            if char not in first_occurrence:\n                first_occurrence[char] = i\n        \n        return count, first_occurrence\n\n    def count_matches(pattern: str) -&gt; int:\n        top: int = 0\n        bottom: int = len(bwt) - 1\n        while top &lt;= bottom:\n            if pattern:\n                symbol: str = pattern[-1]\n                pattern = pattern[:-1]\n                if symbol in bwt[top:bottom+1]:\n                    top = first_occurrence[symbol] + count[symbol][top]\n                    bottom = first_occurrence[symbol] + count[symbol][bottom+1] - 1\n                else:\n                    return 0\n            else:\n                return bottom - top + 1\n\n    count, first_occurrence = create_count_and_first_occurrence(bwt)\n    return [count_matches(pattern) for pattern in patterns]\n\n# Sample input processing\nsample_input: str = \"\"\"\nTCCTCTATGAGATCCTATTCTATGAAACCTTCA$GACCAAAATTCTCCGGC\nCCT CAC GAG CAG ATC\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nbwt: str = input_lines[0]\npatterns: List[str] = input_lines[1].split()\n\n# Run the BWMatching algorithm\nresult: List[int] = BWMatching(bwt, patterns)\nprint(\" \".join(map(str, result)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-98",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-98",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "98.1 Sample Dataset",
    "text": "98.1 Sample Dataset\nGGCGCCGC$TAGTCACACACGCCGTA\nACC CCG CAG"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-98",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-98",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "98.2 Sample Output",
    "text": "98.2 Sample Output\n1 2 1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-98",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-98",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "98.3 Solution",
    "text": "98.3 Solution\nfrom typing import List, Dict, Tuple\n\ndef BetterBWMatching(bwt: str, patterns: List[str]) -&gt; List[int]:\n    def preprocess_bwt(bwt: str) -&gt; Tuple[List[str], Dict[str, List[int]], Dict[str, int]]:\n        first_column: List[str] = sorted(bwt)\n        count: Dict[str, List[int]] = {char: [0] * (len(bwt) + 1) for char in set(bwt)}\n        start_index: Dict[str, int] = {char: first_column.index(char) for char in set(bwt)}\n        \n        for i, char in enumerate(bwt):\n            for c in count:\n                count[c][i + 1] = count[c][i]\n            count[char][i + 1] += 1\n        \n        return first_column, count, start_index\n\n    def count_occurrences(pattern: str) -&gt; int:\n        top: int = 0\n        bottom: int = len(bwt) - 1\n        \n        while top &lt;= bottom:\n            if pattern:\n                symbol: str = pattern[-1]\n                pattern = pattern[:-1]\n                if symbol in bwt[top:bottom+1]:\n                    top = start_index[symbol] + count[symbol][top]\n                    bottom = start_index[symbol] + count[symbol][bottom+1] - 1\n                else:\n                    return 0\n            else:\n                return bottom - top + 1\n\n    first_column, count, start_index = preprocess_bwt(bwt)\n    return [count_occurrences(pattern) for pattern in patterns]\n\n# Sample input processing\nsample_input: str = \"\"\"\nGGCGCCGC$TAGTCACACACGCCGTA\nACC CCG CAG\n\"\"\"\n\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nbwt: str = input_lines[0]\npatterns: List[str] = input_lines[1].split()\n\n# Run the BetterBWMatching algorithm\nresult: List[int] = BetterBWMatching(bwt, patterns)\nprint(\" \".join(map(str, result)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-99",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-99",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "99.1 Sample Dataset",
    "text": "99.1 Sample Dataset\nAATCGGGTTCAATCGGGGT\nATCG\nGGGT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-99",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-99",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "99.2 Sample Output",
    "text": "99.2 Sample Output\n1 4 11 15"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-99",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-99",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "99.3 Solution",
    "text": "99.3 Solution\nfrom typing import List, Dict, Tuple\n\ndef burrows_wheeler_transform(text: str) -&gt; str:\n    n = len(text)\n    rotations = sorted([text[i:] + text[:i] for i in range(n)])\n    bwt = ''.join([rot[-1] for rot in rotations])\n    return bwt\n\ndef create_checkpoint_array(bwt: str, checkpoint_interval: int) -&gt; Dict[int, Dict[str, int]]:\n    symbols = list(set(bwt))\n    checkpoint_array = {}\n    for idx in range(0, len(bwt), checkpoint_interval):\n        checkpoint_array[idx] = {symbol: bwt[:idx].count(symbol) for symbol in symbols}\n    return checkpoint_array\n\ndef count_symbol(checkpoint_array: Dict[int, Dict[str, int]], idx: int, last_column: str, symbol: str) -&gt; int:\n    checkpoints = [x for x in checkpoint_array.keys() if x &lt;= idx]\n    nearest_checkpoint = max(checkpoints)\n    count = checkpoint_array[nearest_checkpoint][symbol]\n    count += last_column[nearest_checkpoint:idx].count(symbol)\n    return count\n\ndef create_partial_suffix_array(text: str, k: int) -&gt; Dict[int, int]:\n    suffixes = [(text[i:], i) for i in range(len(text))]\n    sorted_suffixes = sorted(suffixes)\n    return {i: pos for i, (_, pos) in enumerate(sorted_suffixes) if pos % k == 0}\n\ndef multiple_pattern_matching(first_occurrence: Dict[str, int], last_column: str, pattern: str, checkpoint_array: Dict[int, Dict[str, int]]) -&gt; Tuple[int, int]:\n    top = 0\n    bottom = len(last_column) - 1\n\n    while top &lt;= bottom:\n        if pattern:\n            symbol = pattern[-1]\n            pattern = pattern[:-1]\n            if symbol in last_column[top: bottom + 1]:\n                top = first_occurrence[symbol] + count_symbol(checkpoint_array, top, last_column, symbol)\n                bottom = first_occurrence[symbol] + count_symbol(checkpoint_array, bottom + 1, last_column, symbol) - 1\n            else:\n                return -1, -1\n        else:\n            return top, bottom\n    return -1, -1\n\ndef find_pattern_occurrences(text: str, patterns: List[str], checkpoint_interval: int = 100) -&gt; List[int]:\n    bwt = burrows_wheeler_transform(text + '$')\n\n    first_occurrence = {}\n    for idx, symbol in enumerate(sorted(bwt)):\n        if symbol not in first_occurrence:\n            first_occurrence[symbol] = idx\n\n    checkpoint_array = create_checkpoint_array(bwt, checkpoint_interval)\n    partial_suffix_array = create_partial_suffix_array(text + '$', checkpoint_interval)\n\n    positions = []\n    for pattern in patterns:\n        top, bottom = multiple_pattern_matching(first_occurrence, bwt, pattern, checkpoint_array)\n        if top != -1:\n            for idx in range(top, bottom + 1):\n                offset = 0\n                while idx not in partial_suffix_array:\n                    idx = first_occurrence[bwt[idx]] + count_symbol(checkpoint_array, idx, bwt, bwt[idx])\n                    offset += 1\n                positions.append(partial_suffix_array[idx] + offset)\n\n    return sorted(positions)\n\n# Sample input processing\nsample_input: str = \"\"\"\nAATCGGGTTCAATCGGGGT\nATCG\nGGGT\n\"\"\"\n\ninput_lines = sample_input.strip().split(\"\\n\")\ntext = input_lines[0]\npatterns = input_lines[1:]\n\nresult = find_pattern_occurrences(text, patterns)\nprint(' '.join(str(pos) for pos in result))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-100",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-100",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "100.1 Sample Dataset",
    "text": "100.1 Sample Dataset\nACATGCTACTTT\nATT GCC GCTA TATT\n1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-100",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-100",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "100.2 Sample Output",
    "text": "100.2 Sample Output\n2 4 4 6 7 8 9"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-100",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-100",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "100.3 Solution",
    "text": "100.3 Solution\nfrom collections import defaultdict\nfrom copy import copy\nfrom typing import Dict, List, Tuple, Iterator\n\ndef suffix_array(text: str) -&gt; List[int]:\n    suffixes: Dict[int, str] = {i: text[i:] for i in range(len(text))}\n    return sorted(suffixes.keys(), key=lambda x: suffixes[x])\n\ndef partial_suffix_array(sequence: str, k: int) -&gt; List[Tuple[int, int]]:\n    return [(i, x) for i, x in enumerate(suffix_array(sequence)) if x % k == 0]\n\ndef burrows_wheeler_transform(sequence: str) -&gt; str:\n    return \"\".join(sequence[i - 1] for i in suffix_array(sequence))\n\nfrom itertools import accumulate\n\ndef first_occurrence(sequence: str) -&gt; Dict[str, int]:\n    unique_letters: List[str] = sorted(set(sequence))\n    counts: List[int] = [0] + list(accumulate(sequence.count(x) for x in unique_letters))\n    return dict(zip(unique_letters, counts))\n\ndef count_symbols(sequence: str) -&gt; List[Dict[str, int]]:\n    count: List[Dict[str, int]] = []\n    count.append(defaultdict(int))\n    for i, symbol in enumerate(sequence):\n        count.append(copy(count[i]))\n        count[i + 1][symbol] += 1\n    return count\n\ndef find_location(row: int, psa: Dict[int, int], last_column: str, fo: Dict[str, int], cs: List[Dict[str, int]]) -&gt; int:\n    steps: int = 0\n    while row not in psa:\n        predecessor: str = last_column[row]\n        row = fo[predecessor] + cs[row][predecessor]\n        steps += 1\n    return steps + psa[row]\n\nclass BWMatch:\n    def __init__(self, sequence: str, k: int = 10):\n        self.psa: Dict[int, int] = dict(partial_suffix_array(sequence + \"$\", k))\n        self.sequence: str = burrows_wheeler_transform(sequence + \"$\")\n        self.fo: Dict[str, int] = first_occurrence(self.sequence)\n        self.cs: List[Dict[str, int]] = count_symbols(self.sequence)\n        self.max_mismatches: int = 0\n\n    def update(self, pointers: Tuple[int, int], x: str) -&gt; Tuple[int, int]:\n        top, bottom = pointers\n        return (self.fo[x] + self.cs[top][x], self.fo[x] + self.cs[bottom + 1][x] - 1)\n\n    def bwm(self, pattern: str, pointers: Tuple[int, int], mismatch_count: int) -&gt; List[int]:\n        if not pattern:\n            return list(range(pointers[0], pointers[1] + 1))\n        matches: List[int] = []\n        pattern, symbol = pattern[:-1], pattern[-1]\n        if symbol in self.sequence[pointers[0] : pointers[1] + 1]:\n            matches += self.bwm(pattern, self.update(pointers, symbol), mismatch_count)\n        if mismatch_count &lt; self.max_mismatches:\n            for mismatch in [\"A\", \"C\", \"G\", \"T\"]:\n                if mismatch != symbol:\n                    matches += self.bwm(pattern, self.update(pointers, mismatch), mismatch_count + 1)\n        return matches\n\n    def match_patterns(self, patterns: List[str], max_mismatches: int) -&gt; Iterator[int]:\n        self.max_mismatches = max_mismatches\n        for pattern in patterns:\n            for match in self.bwm(pattern, (0, len(self.sequence) - 1), 0):\n                yield find_location(match, self.psa, self.sequence, self.fo, self.cs)\n\nsample_input: str = \"\"\"\nACATGCTACTTT\nATT GCC GCTA TATT\n1\n\"\"\"\nsequence, patterns, mismatches = sample_input.strip().split(\"\\n\")\npatterns: List[str] = patterns.split()\nmismatches: int = int(mismatches)\nmatcher: BWMatch = BWMatch(sequence)\nprint(*sorted(matcher.match_patterns(patterns, mismatches)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-101",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-101",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "101.1 Sample Dataset",
    "text": "101.1 Sample Dataset\n0 -&gt; {}\n1 -&gt; {}\n2 -&gt; 0,1\n3 -&gt; {}\n4 -&gt; {}\n5 -&gt; 3,2\n6 -&gt; {}\n7 -&gt; 4,5,6\n-\n0: red\n1: red\n3: blue\n4: blue\n6: red"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-101",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-101",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "101.2 Sample Output",
    "text": "101.2 Sample Output\n0: red\n1: red\n2: red\n3: blue\n4: blue\n5: purple\n6: red\n7: purple"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-101",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-101",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "101.3 Solution",
    "text": "101.3 Solution\nfrom collections import defaultdict\nimport io\nfrom typing import Dict, List, Union, TextIO\n\ndef color_tree(tree: Dict[int, List[Dict[str, int]]], node_colors: Dict[int, Union[str, None]]) -&gt; Dict[int, Union[str, None]]:\n    uncolored_nodes = list(tree.keys())\n    while uncolored_nodes:\n        for node in list(uncolored_nodes):  # Create a copy of uncolored_nodes to iterate over\n            child_colors = [node_colors[child[\"n\"]] for child in tree[node] if node_colors[child[\"n\"]] is not None]\n            if len(child_colors) == len(tree[node]):  # Check if all children are colored\n                if all(color == \"red\" for color in child_colors):\n                    node_colors[node] = \"red\"\n                elif all(color == \"blue\" for color in child_colors):\n                    node_colors[node] = \"blue\"\n                else:\n                    node_colors[node] = \"purple\"\n                uncolored_nodes.remove(node)\n    return node_colors\n\ndef parse_input(input_data: Union[str, TextIO]) -&gt; Tuple[Dict[int, List[Dict[str, int]]], Dict[int, Union[str, None]]]:\n    if isinstance(input_data, str):\n        if '\\n' in input_data:\n            # If input_data contains newlines, treat it as a string input\n            file_obj = io.StringIO(input_data)\n        else:\n            # Otherwise, treat it as a filename\n            file_obj = open(input_data)\n    elif isinstance(input_data, TextIO):\n        file_obj = input_data\n    else:\n        raise ValueError(\"Input must be a filename, a string, or a file-like object\")\n\n    tree: Dict[int, List[Dict[str, int]]] = defaultdict(list)\n    node_colors: Dict[int, Union[str, None]] = defaultdict(lambda: None)\n    \n    parsing_tree = True\n    for line in file_obj:\n        line = line.strip()\n        if line == \"-\":\n            parsing_tree = False\n            continue\n        \n        if parsing_tree:\n            if \" -&gt; \" in line:\n                parent, children = line.split(\" -&gt; \")\n                if children != \"{}\":\n                    for child in children.split(\",\"):\n                        tree[int(parent)].append({\"n\": int(child)})\n        else:\n            node, color = line.split(\": \")\n            node_colors[int(node)] = color\n\n    if isinstance(input_data, str) and '\\n' not in input_data:\n        file_obj.close()\n    \n    return tree, node_colors\n\nsample_input = \"\"\"\n0 -&gt; {}\n1 -&gt; {}\n2 -&gt; 0,1\n3 -&gt; {}\n4 -&gt; {}\n5 -&gt; 3,2\n6 -&gt; {}\n7 -&gt; 4,5,6\n-\n0: red\n1: red\n3: blue\n4: blue\n6: red\n\"\"\"\n\ntree, node_colors = parse_input(sample_input)\ncolored_tree = color_tree(tree, node_colors)\nfor node in sorted(colored_tree.keys()):\n    print(f\"{node}: {colored_tree[node]}\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-102",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-102",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "102.1 Sample Dataset",
    "text": "102.1 Sample Dataset\nPANAMABANANAS$\n5"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-102",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-102",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "102.2 Sample Output",
    "text": "102.2 Sample Output\n1,5\n11,10\n12,0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-102",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-102",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "102.3 Solution",
    "text": "102.3 Solution\nfrom typing import List, Dict, Tuple\n\ndef suffix_array(text: str) -&gt; List[int]:\n    suffixes: Dict[int, str] = {i: text[i:] for i in range(len(text))}\n    return sorted(suffixes.keys(), key=lambda x: suffixes[x])\n\ndef partial_suffix_array(sequence: str, step: int) -&gt; List[Tuple[int, int]]:\n    return [(index, position) for index, position in enumerate(suffix_array(sequence)) if position % step == 0]\n\nsample_input: str = \"\"\"\nPANAMABANANAS$\n5\n\"\"\"\n\nsequence, step_str = sample_input.strip().split(\"\\n\")\nstep: int = int(step_str)\n\nfor entry in partial_suffix_array(sequence, step):\n    print(*entry, sep=\",\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-103",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-103",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "103.1 Sample Dataset",
    "text": "103.1 Sample Dataset\nGTAGT$\n5, 2, 3, 0, 4, 1\n0, 0, 0, 2, 0, 1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-103",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-103",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "103.2 Sample Output",
    "text": "103.2 Sample Output\n$\n$\n$\nAGT$\nAGT$\nAGT$\nGT\nT"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-103",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-103",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "103.3 Solution",
    "text": "103.3 Solution\nfrom typing import Dict, List, Union\n\nclass Node:\n    def __init__(self, parent: Union[int, str, None] = None, label: str = \"\"):\n        self.parent: Union[int, str, None] = parent\n        self.label: str = label\n\ndef calculate_depth(tree: Dict[Union[int, str], Node], node: Union[int, str]) -&gt; int:\n    \"\"\"\n    Calculate the length of the concatenation of all path labels from the root to node\n    \"\"\"\n    depth: int = len(tree[node].label)\n    while tree[node].parent is not None:\n        node = tree[node].parent\n        depth += len(tree[node].label)\n    return depth\n\ndef construct_suffix_tree(text: str, suffix_array: List[int], lcp_array: List[int]) -&gt; Dict[Union[int, str], Node]:\n    tree: Dict[Union[int, str], Node] = {-1: Node()}\n\n    for i in range(len(text)):\n        current_node: int = i - 1\n        while tree[current_node].parent is not None and calculate_depth(tree, current_node) &gt; lcp_array[i]:\n            current_node = tree[current_node].parent\n        current_depth: int = calculate_depth(tree, current_node)\n\n        if lcp_array[i] == current_depth:\n            tree[i] = Node(current_node, text[suffix_array[i] + lcp_array[i]:])\n        else:\n            temp_node: int = i - 1\n            while tree[temp_node].parent is not None and tree[temp_node].parent != current_node:\n                temp_node = tree[temp_node].parent\n\n            new_node_key: str = f\"y{i}\"\n            tree[new_node_key] = Node(current_node, text[suffix_array[i - 1] + current_depth : suffix_array[i - 1] + lcp_array[i]])\n            tree[temp_node] = Node(new_node_key, text[suffix_array[i - 1] + lcp_array[i] : suffix_array[i - 1] + calculate_depth(tree, temp_node)])\n            tree[i] = Node(new_node_key, text[suffix_array[i] + lcp_array[i]:])\n\n    del tree[-1]\n    return tree\n\nsample_input: str = \"\"\"\nGTAGT$\n5, 2, 3, 0, 4, 1\n0, 0, 0, 2, 0, 1\n\"\"\"\n\ntext, suffix_array_str, lcp_array_str = sample_input.strip().split(\"\\n\")\nsuffix_array: List[int] = [int(x) for x in suffix_array_str.split(\", \")]\nlcp_array: List[int] = [int(x) for x in lcp_array_str.split(\", \")]\n\nsuffix_tree: Dict[Union[int, str], Node] = construct_suffix_tree(text, suffix_array, lcp_array)\nlabels: List[str] = [suffix_tree[key].label for key in suffix_tree.keys()]\nprint(*sorted(labels), sep=\"\\n\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-104",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-104",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "104.1 Sample Dataset",
    "text": "104.1 Sample Dataset\nAABBBAABABAAAABBBBAABBABABBBAABBAAAABABAABBABABBAB\n--------\nA   B\n--------\n    A   B\nA   0.194   0.806\nB   0.273   0.727"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-104",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-104",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "104.2 Sample Output",
    "text": "104.2 Sample Output\n5.01732865318e-19"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-104",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-104",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "104.3 Solution",
    "text": "104.3 Solution\nfrom math import prod\nfrom io import StringIO\n\ndef parse_hidden_path_data(input_lines):\n    hidden_path = next(input_lines).strip()\n    next(input_lines)  # Skip separator\n    states = next(input_lines).split()\n    next(input_lines)  # Skip separator\n    next(input_lines)  # Skip column headers\n    \n    transition_probabilities = {}\n    for line in input_lines:\n        if line.strip():\n            row_data = line.split()\n            current_state = row_data[0]\n            for next_state, probability in zip(states, row_data[1:]):\n                transition_probabilities[(current_state, next_state)] = float(probability)\n    \n    return hidden_path, transition_probabilities\n\ndef calculate_hidden_path_probability(hidden_path, transition_probabilities):\n    initial_probability = 0.5\n    path_probability = initial_probability * prod(\n        transition_probabilities[state_pair] \n        for state_pair in zip(hidden_path, hidden_path[1:])\n    )\n    return path_probability\n\nsample_input = \"\"\"\nAABBBAABABAAAABBBBAABBABABBBAABBAAAABABAABBABABBAB\n--------\nA B\n--------\n    A   B\nA   0.194   0.806\nB   0.273   0.727\n\"\"\"\n\ninput_lines = iter(StringIO(sample_input.strip()).readlines())\nhidden_path, transition_probabilities = parse_hidden_path_data(input_lines)\nresult_probability = calculate_hidden_path_probability(hidden_path, transition_probabilities)\nprint(f\"{result_probability:e}\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-105",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-105",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "105.1 Sample Dataset",
    "text": "105.1 Sample Dataset\nxxyzyxzzxzxyxyyzxxzzxxyyxxyxyzzxxyzyzxzxxyxyyzxxzx\n--------\nx   y   z\n--------\nBBBAAABABABBBBBBAAAAAABAAAABABABBBBBABAABABABABBBB\n--------\nA   B\n--------\n    x   y   z\nA   0.612   0.314   0.074 \nB   0.346   0.317   0.336"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-105",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-105",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "105.2 Sample Output",
    "text": "105.2 Sample Output\n1.93157070893e-28"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-105",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-105",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "105.3 Solution",
    "text": "105.3 Solution\nfrom math import prod\nfrom io import StringIO\n\ndef parse_emission_data(input_lines):\n    emission_sequence = next(input_lines).strip()\n    next(input_lines)  # Skip separator\n    symbols = next(input_lines).split()\n    next(input_lines)  # Skip separator\n    hidden_path = next(input_lines).strip()\n    next(input_lines)  # Skip separator\n    states = next(input_lines).split()\n    next(input_lines)  # Skip separator\n    next(input_lines)  # Skip column headers\n    \n    emission_probabilities = {}\n    for line in input_lines:\n        if line.strip():\n            row_data = line.split()\n            state = row_data[0]\n            for symbol, probability in zip(symbols, row_data[1:]):\n                emission_probabilities[(state, symbol)] = float(probability)\n    \n    return emission_sequence, hidden_path, emission_probabilities\n\ndef calculate_emission_probability(emission_sequence, hidden_path, emission_probabilities):\n    return prod(emission_probabilities[state_symbol] for state_symbol in zip(hidden_path, emission_sequence))\n\nsample_input = \"\"\"\nxxyzyxzzxzxyxyyzxxzzxxyyxxyxyzzxxyzyzxzxxyxyyzxxzx\n--------\nx y z\n--------\nBBBAAABABABBBBBBAAAAAABAAAABABABBBBBABAABABABABBBB\n--------\nA B\n--------\n    x   y   z\nA   0.612   0.314   0.074 \nB   0.346   0.317   0.336\n\"\"\"\n\ninput_lines = iter(StringIO(sample_input.strip()).readlines())\nemission_sequence, hidden_path, emission_probabilities = parse_emission_data(input_lines)\nresult_probability = calculate_emission_probability(emission_sequence, hidden_path, emission_probabilities)\nprint(f\"{result_probability:e}\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-106",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-106",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "106.1 Sample Dataset",
    "text": "106.1 Sample Dataset\nxyxzzxyxyy\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.641   0.359\nB   0.729   0.271\n--------\n    x   y   z\nA   0.117   0.691   0.192   \nB   0.097   0.42    0.483"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-106",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-106",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "106.2 Sample Output",
    "text": "106.2 Sample Output\nAAABBAAAAA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-106",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-106",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "106.3 Solution",
    "text": "106.3 Solution\nfrom io import StringIO\nfrom math import log\nfrom typing import List, Dict, Tuple, Iterator\nimport numpy as np\n\ndef parse_input(input_iterator: Iterator[str]) -&gt; Tuple[str, List[str], Dict[Tuple[str, str], float], Dict[Tuple[str, str], float]]:\n    sequence = next(input_iterator).rstrip()\n    next(input_iterator)\n    alphabet = next(input_iterator).split()\n    next(input_iterator)\n    states = next(input_iterator).split()\n    next(input_iterator)\n    \n    transition_lines = [next(input_iterator) for _ in range(len(states) + 1)]\n    transition_matrix = {\n        (states[i], states[j]): float(value)\n        for i, row in enumerate(transition_lines[1:])\n        for j, value in enumerate(row.split()[1:])\n    }\n    \n    next(input_iterator)\n    emission_lines = [next(input_iterator) for _ in range(len(states) + 1)]\n    emission_matrix = {\n        (states[i], alphabet[j]): float(value)\n        for i, row in enumerate(emission_lines[1:])\n        for j, value in enumerate(row.split()[1:])\n    }\n    \n    return sequence, states, transition_matrix, emission_matrix\n\ndef viterbi(sequence: str, states: List[str], transition_matrix: Dict[Tuple[str, str], float], emission_matrix: Dict[Tuple[str, str], float]) -&gt; str:\n    num_states = len(states)\n    sequence_length = len(sequence)\n    viterbi_matrix = np.zeros((sequence_length, num_states))\n    backpointer = np.zeros((sequence_length, num_states), dtype=int)\n\n    # Initialize the first column of the viterbi matrix\n    for i, state in enumerate(states):\n        viterbi_matrix[0, i] = log(emission_matrix[state, sequence[0]] / num_states)\n\n    # Fill in the rest of the viterbi matrix\n    for t in range(1, sequence_length):\n        for j, current_state in enumerate(states):\n            probabilities = [\n                log(transition_matrix[previous_state, current_state]) + \n                log(emission_matrix[current_state, sequence[t]]) + \n                viterbi_matrix[t-1, k]\n                for k, previous_state in enumerate(states)\n            ]\n            max_prob_index = probabilities.index(max(probabilities))\n            backpointer[t, j] = max_prob_index\n            viterbi_matrix[t, j] = max(probabilities)\n\n    best_path_index = np.argmax(viterbi_matrix[-1, :])\n    best_path = states[best_path_index]\n    for t in range(sequence_length - 1, 0, -1):\n        best_path_index = backpointer[t, best_path_index]\n        best_path = states[best_path_index] + best_path\n\n    return best_path\n\n# Example usage\nsample_input = \"\"\"\nxyxzzxyxyy\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.641   0.359\nB   0.729   0.271\n--------\n    x   y   z\nA   0.117   0.691   0.192   \nB   0.097   0.42    0.483\n\"\"\"\n\ninput_lines = iter(StringIO(sample_input.strip()).readlines())\nsequence, states, transition_matrix, emission_matrix = parse_input(input_lines)\nresult = viterbi(sequence, states, transition_matrix, emission_matrix)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-107",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-107",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "107.1 Sample Dataset",
    "text": "107.1 Sample Dataset\nxzyyzzyzyy\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.303   0.697 \nB   0.831   0.169 \n--------\n    x   y   z\nA   0.533   0.065   0.402 \nB   0.342   0.334   0.324"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-107",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-107",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "107.2 Sample Output",
    "text": "107.2 Sample Output\n1.1005510319694847e-06"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-107",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-107",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "107.3 Solution",
    "text": "107.3 Solution\nfrom io import StringIO\nimport numpy as np\n\ndef parse_hmm_input(input_iterator):\n    sequence = next(input_iterator).rstrip()\n    next(input_iterator)\n    alphabet = next(input_iterator).split()\n    next(input_iterator)\n    states = next(input_iterator).split()\n    next(input_iterator)\n    \n    transition_lines = [next(input_iterator) for _ in range(len(states) + 1)]\n    transition_matrix = {\n        (states[i], states[j]): float(value)\n        for i, row in enumerate(transition_lines[1:])\n        for j, value in enumerate(row.split()[1:])\n    }\n    \n    next(input_iterator)\n    emission_lines = [next(input_iterator) for _ in range(len(states) + 1)]\n    emission_matrix = {\n        (states[i], alphabet[j]): float(value)\n        for i, row in enumerate(emission_lines[1:])\n        for j, value in enumerate(row.split()[1:])\n    }\n    \n    return sequence, states, transition_matrix, emission_matrix\n\ndef calculate_hmm_likelihood(sequence, states, transition_matrix, emission_matrix):\n    probability_matrix = np.ones((len(sequence) + 1, len(states)))\n\n    for i, state in enumerate(states):\n        probability_matrix[0, i] = emission_matrix[state, sequence[0]] / len(states)\n\n    for i, emission in enumerate(sequence[1:], start=1):\n        for j, current_state in enumerate(states):\n            probability_matrix[i, j] = sum(\n                transition_matrix[previous_state, current_state] * \n                emission_matrix[current_state, emission] * \n                probability_matrix[i - 1, k]\n                for k, previous_state in enumerate(states)\n            )\n\n    return sum(probability_matrix[i, :])\n\nsample_input = \"\"\"\nxzyyzzyzyy\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.303   0.697 \nB   0.831   0.169 \n--------\n    x   y   z\nA   0.533   0.065   0.402 \nB   0.342   0.334   0.324\n\"\"\"\n\ninput_lines = iter(StringIO(sample_input.strip()).readlines())\nsequence, states, transition_matrix, emission_matrix = parse_hmm_input(input_lines)\nresult = calculate_hmm_likelihood(sequence, states, transition_matrix, emission_matrix)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-108",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-108",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "108.1 Sample Dataset",
    "text": "108.1 Sample Dataset\n0.289\n--------\nA   B   C   D   E\n--------\nEBA\nEBD\nEB-\nEED\nEBD\nEBE\nE-D\nEBD"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-108",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-108",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "108.2 Sample Output",
    "text": "108.2 Sample Output\nS   I0  M1  D1  I1  M2  D2  I2  M3  D3  I3  E\nS   0   0   1.0 0   0   0   0   0   0   0   0   0\nI0  0   0   0   0   0   0   0   0   0   0   0   0\nM1  0   0   0   0   0   0.875   0.125   0   0   0   0   0\nD1  0   0   0   0   0   0   0   0   0   0   0   0\nI1  0   0   0   0   0   0   0   0   0   0   0   0\nM2  0   0   0   0   0   0   0   0   0.857   0.143   0   0\nD2  0   0   0   0   0   0   0   0   1.0 0   0   0\nI2  0   0   0   0   0   0   0   0   0   0   0   0\nM3  0   0   0   0   0   0   0   0   0   0   0   1.0\nD3  0   0   0   0   0   0   0   0   0   0   0   1.0\nI3  0   0   0   0   0   0   0   0   0   0   0   0\nE   0   0   0   0   0   0   0   0   0   0   0   0\n--------\n    A   B   C   D   E\nS   0   0   0   0   0\nI0  0   0   0   0   0\nM1  0   0   0   0   1.0\nD1  0   0   0   0   0\nI1  0   0   0   0   0\nM2  0   0.857   0   0   0.143\nD2  0   0   0   0   0\nI2  0   0   0   0   0\nM3  0.143   0   0   0.714   0.143\nD3  0   0   0   0   0\nI3  0   0   0   0   0\nE   0   0   0   0   0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-108",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-108",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "108.3 Solution",
    "text": "108.3 Solution\nimport numpy as np\nfrom io import StringIO\nfrom typing import List, Tuple, Iterator\n\ndef parse_hmm_input(input_iterator: Iterator[str]) -&gt; Tuple[float, List[str], np.ndarray]:\n    threshold = float(next(input_iterator).rstrip())\n    next(input_iterator)\n    alphabet = next(input_iterator).split()\n    next(input_iterator)\n    alignment = np.array([list(sequence.strip()) for sequence in input_iterator])\n    return threshold, alphabet, alignment\n\ndef calculate_state_index(position: int, state_type: str) -&gt; int:\n    if state_type == \"ins\":\n        return (position + 1) * 3 + 1\n    else:\n        return {\"match\": 0, \"del\": 1}[state_type] + 3 * position + 2\n\ndef normalize_row(row: np.ndarray, include_zeros: bool = False, min_value: float = 0.0) -&gt; np.ndarray:\n    if include_zeros and sum(row) == 0:\n        row[:] = 1\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        normalized = row / sum(row)\n        normalized[row == 0.0] = min_value\n        return normalized\n\ndef normalize_matrix(matrix: np.ndarray, include_zeros: bool = False, min_value: float = 0.0) -&gt; np.ndarray:\n    return np.array([normalize_row(row, include_zeros=include_zeros, min_value=min_value) for row in matrix])\n\ndef print_matrix(matrix: np.ndarray, row_labels: List[str], column_labels: List[str]) -&gt; None:\n    print(*column_labels, sep=\"\\t\")\n    for i, row in enumerate(matrix):\n        formatted_row = [row_labels[i]] + [round(x, 3) if x &gt; 0.0 else \"0\" for x in row]\n        print(*formatted_row, sep=\"\\t\")\n\ndef print_transition_probabilities(transition_matrix: np.ndarray) -&gt; None:\n    n = (transition_matrix.shape[0] - 3) // 3\n    print_matrix(transition_matrix, generate_state_labels(n), generate_state_labels(n))\n\ndef print_emission_probabilities(emission_matrix: np.ndarray, alphabet: List[str]) -&gt; None:\n    n = (emission_matrix.shape[0] - 3) // 3\n    print_matrix(emission_matrix, generate_state_labels(n), alphabet)\n\ndef generate_state_labels(n: int) -&gt; List[str]:\n    labels = [\"S\", \"I0\"]\n    for i in range(1, n + 1):\n        labels += [f\"M{i}\", f\"D{i}\", f\"I{i}\"]\n    labels.append(\"E\")\n    return labels\n\ndef create_transition_matrix(n: int) -&gt; np.ndarray:\n    return np.zeros((n * 3 + 3, n * 3 + 3), dtype=float)\n\ndef create_emission_matrix(n: int, m: int) -&gt; np.ndarray:\n    return np.zeros((n * 3 + 3, m), dtype=float)\n\ndef build_profile_hmm(threshold: float, alphabet: List[str], alignment: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    valid_columns = np.mean(alignment == \"-\", axis=0) &lt; threshold\n    valid_column_count = sum(valid_columns)\n    end_state = valid_column_count * 3 + 2\n    transition_probs = create_transition_matrix(valid_column_count)\n    emission_probs = create_emission_matrix(valid_column_count, len(alphabet))\n\n    for sequence in alignment:\n        prev_index = 0\n        column_index = -1\n        for i, char in enumerate(sequence):\n            if valid_columns[i]:\n                column_index += 1\n                if char == \"-\":\n                    current_index = calculate_state_index(column_index, \"del\")\n                else:\n                    current_index = calculate_state_index(column_index, \"match\")\n                transition_probs[prev_index, current_index] += 1\n                prev_index = current_index\n            else:\n                if char != \"-\":\n                    current_index = calculate_state_index(column_index, \"ins\")\n                    transition_probs[prev_index, current_index] += 1\n                    prev_index = current_index\n            if char != \"-\":\n                emission_probs[current_index, alphabet.index(char)] += 1\n        transition_probs[prev_index, end_state] += 1\n\n    transition_probs = normalize_matrix(transition_probs)\n    emission_probs = normalize_matrix(emission_probs)\n\n    return transition_probs, emission_probs\n\nsample_input = \"\"\"\n0.289\n--------\nA   B   C   D   E\n--------\nEBA\nEBD\nEB-\nEED\nEBD\nEBE\nE-D\nEBD\n\"\"\"\n\ninput_lines = iter(StringIO(sample_input.strip()).readlines())\nthreshold, alphabet, alignment = parse_hmm_input(input_lines)\ntransition_probs, emission_probs = build_profile_hmm(threshold, alphabet, alignment)\nprint_transition_probabilities(transition_probs)\nprint(\"--------\")\nprint_emission_probabilities(emission_probs, alphabet)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-109",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-109",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "109.1 Sample Dataset",
    "text": "109.1 Sample Dataset\n0.358   0.01\n--------\nA   B   C   D   E\n--------\nADA\nADA\nAAA\nADC\n-DA\nD-A"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-109",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-109",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "109.2 Sample Output",
    "text": "109.2 Sample Output\nS   I0  M1  D1  I1  M2  D2  I2  M3  D3  I3  E\nS   0   0.01    0.819   0.172   0   0   0   0   0   0   0   0\nI0  0   0.333   0.333   0.333   0   0   0   0   0   0   0   0\nM1  0   0   0   0   0.01    0.786   0.204   0   0   0   0   0\nD1  0   0   0   0   0.01    0.981   0.01    0   0   0   0   0\nI1  0   0   0   0   0.333   0.333   0.333   0   0   0   0   0\nM2  0   0   0   0   0   0   0   0.01    0.981   0.01    0   0\nD2  0   0   0   0   0   0   0   0.01    0.981   0.01    0   0\nI2  0   0   0   0   0   0   0   0.333   0.333   0.333   0   0\nM3  0   0   0   0   0   0   0   0   0   0   0.01    0.99\nD3  0   0   0   0   0   0   0   0   0   0   0.5 0.5\nI3  0   0   0   0   0   0   0   0   0   0   0.5 0.5\nE   0   0   0   0   0   0   0   0   0   0   0   0\n--------\n    A   B   C   D   E\nS   0   0   0   0   0\nI0  0.2 0.2 0.2 0.2 0.2\nM1  0.771   0.01    0.01    0.2 0.01\nD1  0   0   0   0   0\nI1  0.2 0.2 0.2 0.2 0.2\nM2  0.2 0.01    0.01    0.771   0.01\nD2  0   0   0   0   0\nI2  0.2 0.2 0.2 0.2 0.2\nM3  0.803   0.01    0.168   0.01    0.01\nD3  0   0   0   0   0\nI3  0.2 0.2 0.2 0.2 0.2\nE   0   0   0   0   0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-109",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-109",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "109.3 Solution",
    "text": "109.3 Solution\nimport numpy as np\nfrom io import StringIO\nfrom typing import List, Tuple\n\ndef normalize_matrix(matrix: np.ndarray, include_zeros: bool = False, min_value: float = 0.0) -&gt; np.ndarray:\n    return np.array([normalize_row(row, include_zeros=include_zeros, min_value=min_value) for row in matrix])\n\ndef normalize_row(row: np.ndarray, include_zeros: bool = False, min_value: float = 0.0) -&gt; np.ndarray:\n    if include_zeros and sum(row) == 0:\n        row[:] = 1\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        normalized = row / sum(row)\n        normalized[row == 0.0] = min_value\n        return normalized\n\ndef print_matrix(matrix: np.ndarray, row_labels: List[str], col_labels: List[str]) -&gt; None:\n    print(*col_labels, sep=\"\\t\")\n    for i, row in enumerate(matrix):\n        formatted_row = [row_labels[i]] + [round(x, 3) if x &gt; 0.0 else \"0\" for x in row]\n        print(*formatted_row, sep=\"\\t\")\n\ndef print_transition_probs(transition_matrix: np.ndarray) -&gt; None:\n    n = (transition_matrix.shape[0] - 3) // 3\n    print_matrix(transition_matrix, generate_state_labels(n), generate_state_labels(n))\n\ndef print_emission_probs(emission_matrix: np.ndarray, alphabet: List[str]) -&gt; None:\n    n = (emission_matrix.shape[0] - 3) // 3\n    print_matrix(emission_matrix, generate_state_labels(n), alphabet)\n\ndef generate_state_labels(n: int) -&gt; List[str]:\n    labels = [\"S\", \"I0\"]\n    for i in range(1, n + 1):\n        labels += [f\"M{i}\", f\"D{i}\", f\"I{i}\"]\n    labels.append(\"E\")\n    return labels\n\ndef create_transition_matrix(n: int) -&gt; np.ndarray:\n    return np.zeros((n * 3 + 3, n * 3 + 3), dtype=float)\n\ndef create_emission_matrix(n: int, m: int) -&gt; np.ndarray:\n    return np.zeros((n * 3 + 3, m), dtype=float)\n\ndef calculate_state_index(position: int, state_type: str) -&gt; int:\n    if state_type == \"ins\":\n        return (position + 1) * 3 + 1\n    else:\n        return {\"match\": 0, \"del\": 1}[state_type] + 3 * position + 2\n\ndef build_profile_hmm(threshold: float, alphabet: List[str], alignment: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    valid_columns = np.mean(alignment == \"-\", axis=0) &lt; threshold\n    valid_column_count = sum(valid_columns)\n    end_state = valid_column_count * 3 + 2\n    transition_probs = create_transition_matrix(valid_column_count)\n    emission_probs = create_emission_matrix(valid_column_count, len(alphabet))\n\n    for sequence in alignment:\n        prev_index = 0\n        column_index = -1\n        for i, char in enumerate(sequence):\n            if valid_columns[i]:\n                column_index += 1\n                if char == \"-\":\n                    current_index = calculate_state_index(column_index, \"del\")\n                else:\n                    current_index = calculate_state_index(column_index, \"match\")\n                transition_probs[prev_index, current_index] += 1\n                prev_index = current_index\n            else:\n                if char != \"-\":\n                    current_index = calculate_state_index(column_index, \"ins\")\n                    transition_probs[prev_index, current_index] += 1\n                    prev_index = current_index\n            if char != \"-\":\n                emission_probs[current_index, alphabet.index(char)] += 1\n        transition_probs[prev_index, end_state] += 1\n\n    transition_probs = normalize_matrix(transition_probs)\n    emission_probs = normalize_matrix(emission_probs)\n\n    return transition_probs, emission_probs\n\ndef parse_input(input_handle: Iterator[str]) -&gt; Tuple[float, float, List[str], np.ndarray]:\n    threshold, pseudocount = map(float, next(input_handle).rstrip().split())\n    next(input_handle)\n    alphabet = next(input_handle).split()\n    next(input_handle)\n    alignment = np.array([list(sequence.strip()) for sequence in input_handle])\n    return threshold, pseudocount, alphabet, alignment\n\ndef add_transition_pseudocounts(transition_matrix: np.ndarray, pseudocount: float) -&gt; np.ndarray:\n    n = (transition_matrix.shape[0] - 3) // 3\n    transition_matrix[0, 1:4] += pseudocount\n    transition_matrix[1, 1:4] += pseudocount\n    for i in range(n):\n        transition_matrix[i * 3 + 2 : i * 3 + 5, (i + 1) * 3 + 1 : (i + 1) * 3 + 4] += pseudocount\n    return normalize_matrix(transition_matrix)\n\ndef add_emission_pseudocounts(emission_matrix: np.ndarray, pseudocount: float) -&gt; np.ndarray:\n    n = (emission_matrix.shape[0] - 3) // 3\n    emission_matrix[1, :] += pseudocount\n    for i in range(n):\n        emission_matrix[i * 3 + 2, :] += pseudocount\n        emission_matrix[i * 3 + 4, :] += pseudocount\n    return normalize_matrix(emission_matrix)\n\ndef build_pseudocount_profile_hmm(threshold: float, pseudocount: float, alphabet: List[str], alignment: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    transition_probs, emission_probs = build_profile_hmm(threshold, alphabet, alignment)\n    transition_probs = add_transition_pseudocounts(transition_probs, pseudocount)\n    emission_probs = add_emission_pseudocounts(emission_probs, pseudocount)\n    return transition_probs, emission_probs\n\nsample_input = \"\"\"\n0.358   0.01\n--------\nA   B   C   D   E\n--------\nADA\nADA\nAAA\nADC\n-DA\nD-A\n\"\"\"\n\ninput_lines = iter(StringIO(sample_input.strip()).readlines())\nthreshold, pseudocount, alphabet, alignment = parse_input(input_lines)\ntransition_probs, emission_probs = build_pseudocount_profile_hmm(threshold, pseudocount, alphabet, alignment)\nprint_transition_probs(transition_probs)\nprint(\"--------\")\nprint_emission_probs(emission_probs, alphabet)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-110",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-110",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "110.1 Sample Dataset",
    "text": "110.1 Sample Dataset\nAEFDFDC\n--------\n0.4 0.01\n--------\nA   B   C   D   E   F\n--------\nACDEFACADF\nAFDA---CCF\nA--EFD-FDC\nACAEF--A-C\nADDEFAAADF"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-110",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-110",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "110.2 Sample Output",
    "text": "110.2 Sample Output\nM1 D2 D3 M4 M5 I5 M6 M7 M8"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-110",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-110",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "110.3 Solution",
    "text": "110.3 Solution\nfrom io import StringIO\nfrom collections import defaultdict\nfrom typing import List, Tuple, Dict, Iterator\nimport numpy as np\nfrom math import inf, log\n\ndef generate_state_labels(num_states: int) -&gt; List[str]:\n    labels = [\"S\", \"I0\"]\n    for i in range(1, num_states + 1):\n        labels.extend([f\"M{i}\", f\"D{i}\", f\"I{i}\"])\n    labels.append(\"E\")\n    return labels\n\ndef normalize_row(row: np.ndarray, include_zeros: bool = False, minimum_value: float = 0.0) -&gt; np.ndarray:\n    if include_zeros and sum(row) == 0:\n        row[:] = 1\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        normalized = row / sum(row)\n        normalized[row == 0.0] = minimum_value\n        return normalized\n\ndef normalize_matrix(matrix: np.ndarray, include_zeros: bool = False, minimum_value: float = 0.0) -&gt; np.ndarray:\n    return np.array([normalize_row(r, include_zeros=include_zeros, minimum_value=minimum_value) for r in matrix])\n\ndef create_transition_matrix(num_states: int) -&gt; np.ndarray:\n    return np.zeros((num_states * 3 + 3, num_states * 3 + 3), dtype=float)\n\ndef create_emission_matrix(num_states: int, num_symbols: int) -&gt; np.ndarray:\n    return np.zeros((num_states * 3 + 3, num_symbols), dtype=float)\n\ndef calculate_index(state_num: int, state_type: str) -&gt; int:\n    if state_type == \"ins\":\n        return (state_num + 1) * 3 + 1\n    else:\n        return {\"match\": 0, \"del\": 1}[state_type] + 3 * state_num + 2\n\ndef build_profile_hmm(threshold: float, alphabet: List[str], alignment: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    valid_columns = np.mean(alignment == \"-\", axis=0) &lt; threshold\n    valid_length = sum(valid_columns)\n    end_state = valid_length * 3 + 2\n    transition_probs = create_transition_matrix(valid_length)\n    emission_probs = create_emission_matrix(valid_length, len(alphabet))\n\n    for sequence in alignment:\n        prev_index = 0\n        valid_col_count = -1\n        for col, char in enumerate(sequence):\n            if valid_columns[col]:\n                valid_col_count += 1\n                if char == \"-\":\n                    current_index = calculate_index(valid_col_count, \"del\")\n                else:\n                    current_index = calculate_index(valid_col_count, \"match\")\n                transition_probs[prev_index, current_index] += 1\n                prev_index = current_index\n            else:\n                if char != \"-\":\n                    current_index = calculate_index(valid_col_count, \"ins\")\n                    transition_probs[prev_index, current_index] += 1\n                    prev_index = current_index\n            if char != \"-\":\n                emission_probs[current_index, alphabet.index(char)] += 1\n        transition_probs[prev_index, end_state] += 1\n\n    transition_probs = normalize_matrix(transition_probs)\n    emission_probs = normalize_matrix(emission_probs)\n\n    return transition_probs, emission_probs\n\ndef add_pseudocounts_to_transitions(matrix: np.ndarray, pseudocount: float) -&gt; np.ndarray:\n    num_states = (matrix.shape[0] - 3) // 3\n    matrix[0, 1:4] += pseudocount\n    matrix[1, 1:4] += pseudocount\n    for i in range(num_states):\n        matrix[i*3+2:i*3+5, (i+1)*3+1:(i+1)*3+4] += pseudocount\n    return normalize_matrix(matrix)\n\ndef add_pseudocounts_to_emissions(matrix: np.ndarray, pseudocount: float) -&gt; np.ndarray:\n    num_states = (matrix.shape[0] - 3) // 3\n    matrix[1, :] += pseudocount\n    for i in range(num_states):\n        matrix[i*3+2, :] += pseudocount\n        matrix[i*3+4, :] += pseudocount\n    return normalize_matrix(matrix)\n\ndef build_profile_hmm_with_pseudocounts(threshold: float, pseudocount: float, alphabet: List[str], alignment: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    transition_probs, emission_probs = build_profile_hmm(threshold, alphabet, alignment)\n    transition_probs = add_pseudocounts_to_transitions(transition_probs, pseudocount)\n    emission_probs = add_pseudocounts_to_emissions(emission_probs, pseudocount)\n    return transition_probs, emission_probs\n\ndef parse_input_data(input_iterator: Iterator[str]) -&gt; Tuple[str, float, float, List[str], np.ndarray]:\n    sequence = next(input_iterator).rstrip()\n    next(input_iterator)\n    threshold, pseudocount = map(float, next(input_iterator).rstrip().split())\n    next(input_iterator)\n    alphabet = next(input_iterator).split()\n    next(input_iterator)\n    alignment = np.array([list(x.strip()) for x in input_iterator])\n    return sequence, threshold, pseudocount, alphabet, alignment\n\ndef convert_transition_probs_to_dict(matrix: np.ndarray) -&gt; Dict[Tuple[str, str], float]:\n    prob_dict = defaultdict(float)\n    num_states = (matrix.shape[0] - 3) // 3\n    labels = generate_state_labels(num_states)\n    for i in range(matrix.shape[0]):\n        for j in range(matrix.shape[0]):\n            prob_dict[labels[i], labels[j]] = matrix[i][j]\n    return prob_dict\n\ndef convert_emission_probs_to_dict(matrix: np.ndarray, alphabet: List[str]) -&gt; Dict[Tuple[str, str], float]:\n    prob_dict = defaultdict(float)\n    num_states = (matrix.shape[0] - 3) // 3\n    labels = generate_state_labels(num_states)\n    for i in range(matrix.shape[0]):\n        for j, symbol in enumerate(alphabet):\n            prob_dict[labels[i], symbol] = matrix[i][j]\n    return prob_dict\n\ndef build_hmm_graph(transition_probs: Dict[Tuple[str, str], float], num_states: int) -&gt; Dict[str, List[Dict[str, float]]]:\n    def add_edge(source: str, target: str) -&gt; None:\n        graph[source].append({\"node\": target, \"weight\": transition_probs[source, target]})\n\n    graph = defaultdict(list)\n    for target in [\"I0\", \"M1\", \"D1\"]:\n        add_edge(\"S\", target)\n    for i in range(num_states):\n        source = f\"I{i}\"\n        for target in [source, f\"M{i+1}\", f\"D{i+1}\"]:\n            add_edge(source, target)\n    for i in range(1, num_states):\n        for source in [f\"M{i}\", f\"D{i}\"]:\n            for target in [f\"M{i+1}\", f\"I{i}\", f\"D{i+1}\"]:\n                add_edge(source, target)\n    for source in [f\"I{num_states}\", f\"M{num_states}\", f\"D{num_states}\"]:\n        for target in [f\"I{num_states}\", \"E\"]:\n            add_edge(source, target)\n\n    return graph\n\ndef generate_topological_order(num_states: int, seq_length: int) -&gt; Iterator[Tuple[str, int]]:\n    yield (\"S\", 0)\n    for j in range(num_states):\n        yield (f\"D{j+1}\", 0)\n    for i in range(seq_length):\n        yield (\"I0\", i + 1)\n        for j in range(num_states):\n            for state_type in [\"M\", \"D\", \"I\"]:\n                yield (f\"{state_type}{j+1}\", i + 1)\n    yield (\"E\", seq_length + 1)\n\ndef get_previous_nodes(current_node: str, current_col: int, num_states: int, seq_length: int) -&gt; List[Tuple[str, int]]:\n    if current_node[0] == \"E\":\n        return [(f\"D{num_states}\", seq_length), (f\"M{num_states}\", seq_length), (f\"I{num_states}\", seq_length)]\n    state_num = int(current_node[1:])\n    if current_col == 0:\n        return [(\"S\", 0)] if state_num == 1 else [(f\"D{state_num-1}\", 0)]\n    elif current_node == \"I0\":\n        return [(\"S\", 0)] if current_col == 1 else [(\"I0\", current_col - 1)]\n    elif current_node == \"M1\":\n        return [(\"S\", 0)] if current_col == 1 else [(\"I0\", current_col - 1)]\n    elif current_node[0] == \"I\":\n        return [(f\"D{state_num}\", 0)] if current_col == 1 else [(f\"D{state_num}\", current_col - 1), (f\"M{state_num}\", current_col - 1), (f\"I{state_num}\", current_col - 1)]\n    elif current_node[0] == \"M\":\n        return [(f\"D{state_num-1}\", 0)] if current_col == 1 else [(f\"D{state_num-1}\", current_col - 1), (f\"M{state_num-1}\", current_col - 1), (f\"I{state_num-1}\", current_col - 1)]\n    elif current_node[0] == \"D\":\n        return [(\"I0\", current_col)] if state_num == 1 else [(f\"D{state_num-1}\", current_col), (f\"M{state_num-1}\", current_col), (f\"I{state_num-1}\", current_col)]\n    else:\n        print(f\"Unhandled node: {current_node}\")\n        return []\n\ndef simplify_graph(graph: Dict[str, List[Dict[str, float]]]) -&gt; Dict[str, Dict[str, float]]:\n    return {k: {x[\"node\"]: x[\"weight\"] for x in v} for k, v in graph.items()}\n\n# Main execution\ndef main(sample_input):\n    input_lines = iter(StringIO(sample_input.strip()).readlines())\n    sequence, threshold, pseudocount, alphabet, alignment = parse_input_data(input_lines)\n    transition_probs, emission_probs = build_profile_hmm_with_pseudocounts(threshold, pseudocount, alphabet, alignment)\n    num_states = (transition_probs.shape[0] - 3) // 3\n    transition_probs_dict = convert_transition_probs_to_dict(transition_probs)\n    emission_probs_dict = convert_emission_probs_to_dict(emission_probs, alphabet)\n\n    graph = build_hmm_graph(transition_probs_dict, num_states)\n    topological_order = generate_topological_order(num_states, len(sequence))\n    simplified_graph = simplify_graph(graph)\n\n    # Dynamic programming to find the most probable path\n    previous_node = next(topological_order)\n    scores = {previous_node: 0}\n    backpointers = {previous_node: (None, None)}\n\n    for current_node, current_col in topological_order:\n        backpointers[(current_node, current_col)] = 0\n        scores[(current_node, current_col)] = -inf\n        for prev_node, prev_col in get_previous_nodes(current_node, current_col, num_states, len(sequence)):\n            if prev_col &lt; current_col and current_node != \"E\":\n                emission_prob = emission_probs_dict[current_node, sequence[current_col - 1]]\n            else:\n                emission_prob = 1\n            log_prob = log(simplified_graph[prev_node][current_node]) + log(emission_prob) + scores[(prev_node, prev_col)]\n            if log_prob &gt; scores[(current_node, current_col)]:\n                scores[(current_node, current_col)] = log_prob\n                backpointers[(current_node, current_col)] = (prev_node, prev_col)\n\n    # Traceback to find the path\n    path = []\n    position = (\"E\", len(sequence) + 1)\n    while position[0]:\n        path.append(backpointers[position][0])\n        position = backpointers[position]\n\n    print(*path[::-1][2:])\n\nsample_input = \"\"\"\nAEFDFDC\n--------\n0.4 0.01\n--------\nA   B   C   D   E   F\n--------\nACDEFACADF\nAFDA---CCF\nA--EFD-FDC\nACAEF--A-C\nADDEFAAADF\n\"\"\"\n\nmain(sample_input)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-111",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-111",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "111.1 Sample Dataset",
    "text": "111.1 Sample Dataset\nyzzzyxzxxx\n--------\nx   y   z\n--------\nBBABABABAB\n--------\nA   B   C"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-111",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-111",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "111.2 Sample Output",
    "text": "111.2 Sample Output\nA   B   C\nA   0.0 1.0 0.0\nB   0.8 0.2 0.0\nC   0.333   0.333   0.333\n--------\n    x   y   z\nA   0.25    0.25    0.5\nB   0.5 0.167   0.333\nC   0.333   0.333   0.333"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-111",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-111",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "111.3 Solution",
    "text": "111.3 Solution\nfrom io import StringIO\nfrom collections import defaultdict\nfrom typing import List, Tuple, Dict, Iterator, Union\nimport numpy as np\n\ndef normalize_row(row: np.ndarray, include_zeros: bool = False, minimum_value: float = 0.0) -&gt; np.ndarray:\n    if include_zeros and sum(row) == 0:\n        row[:] = 1\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        normalized = row / sum(row)\n        normalized[row == 0.0] = minimum_value\n        return normalized\n\ndef normalize_matrix(matrix: np.ndarray, include_zeros: bool = False, minimum_value: float = 0.0) -&gt; np.ndarray:\n    return np.array([normalize_row(r, include_zeros=include_zeros, minimum_value=minimum_value) for r in matrix])\n\ndef print_matrix(matrix: np.ndarray, row_labels: List[str], column_labels: List[str]) -&gt; None:\n    print(*column_labels, sep=\"\\t\")\n    for i, row in enumerate(matrix):\n        r = [row_labels[i]] + [round(x, 3) if x &gt; 0.0 else \"0\" for x in row]\n        print(*r, sep=\"\\t\")\n\ndef parse_input(handle: Iterator[str]) -&gt; Tuple[str, List[str], str, List[str]]:\n    sequence = next(handle).rstrip()\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    path = next(handle).rstrip()\n    next(handle)\n    states = next(handle).split()\n    return sequence, alphabet, path, states\n\ndef convert_to_dict(matrix: np.ndarray, row_labels: List[str], column_labels: List[str]) -&gt; Dict[Tuple[str, str], float]:\n    result = defaultdict(float)\n    for i in range(matrix.shape[0]):\n        for j in range(matrix.shape[1]):\n            result[row_labels[i], column_labels[j]] = matrix[i][j]\n    return result\n\ndef estimate_transition_matrix(path: str, states: List[str], to_dict: bool = False) -&gt; Union[np.ndarray, Dict[Tuple[str, str], float]]:\n    transition_matrix = np.zeros((len(states), len(states)), dtype=float)\n    for current_state, next_state in zip(path, path[1:]):\n        transition_matrix[states.index(current_state)][states.index(next_state)] += 1\n    transition_matrix = normalize_matrix(transition_matrix, include_zeros=True, minimum_value=1e-16)\n    if to_dict:\n        return convert_to_dict(transition_matrix, states, states)\n    else:\n        return transition_matrix\n\ndef estimate_emission_matrix(sequence: str, alphabet: List[str], path: str, states: List[str], to_dict: bool = False) -&gt; Union[np.ndarray, Dict[Tuple[str, str], float]]:\n    emission_matrix = np.zeros((len(states), len(alphabet)), dtype=float)\n    for state, symbol in zip(path, sequence):\n        emission_matrix[states.index(state)][alphabet.index(symbol)] += 1\n    emission_matrix = normalize_matrix(emission_matrix, include_zeros=True, minimum_value=1e-16)\n    if to_dict:\n        return convert_to_dict(emission_matrix, states, alphabet)\n    else:\n        return emission_matrix\n\ndef main(sample_input: str) -&gt; None:\n    input_lines = iter(StringIO(sample_input.strip()).readlines())\n    sequence, alphabet, path, states = parse_input(input_lines)\n    transition_matrix = estimate_transition_matrix(path, states)\n    emission_matrix = estimate_emission_matrix(sequence, alphabet, path, states)\n    print_matrix(transition_matrix, states, states)\n    print(\"--------\")\n    print_matrix(emission_matrix, states, alphabet)\n\nsample_input = \"\"\"\nyzzzyxzxxx\n--------\nx   y   z\n--------\nBBABABABAB\n--------\nA   B   C\n\"\"\"\n\nmain(sample_input)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-112",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-112",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "112.1 Sample Dataset",
    "text": "112.1 Sample Dataset\n100\n--------\nxxxzyzzxxzxyzxzxyxxzyzyzyyyyzzxxxzzxzyzzzxyxzzzxyzzxxxxzzzxyyxzzzzzyzzzxxzzxxxyxyzzyxzxxxyxzyxxyzyxz\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.582   0.418\nB   0.272   0.728\n--------\n    x   y   z\nA   0.129   0.35    0.52\nB   0.422   0.151   0.426"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-112",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-112",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "112.2 Sample Output",
    "text": "112.2 Sample Output\nA   B\nA   0.875   0.125\nB   0.011   0.989\n--------\n    x   y   z\nA   0.0 0.75    0.25\nB   0.402   0.174   0.424"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-112",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-112",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "112.3 Solution",
    "text": "112.3 Solution\nfrom io import StringIO\nfrom typing import List, Dict, Tuple\nimport numpy as np\nfrom math import log\n\ndef viterbi(sequence: str, states: List[str], transition_matrix: Dict[Tuple[str, str], float], emission_matrix: Dict[Tuple[str, str], float]) -&gt; str:\n    mat = np.zeros((len(sequence), len(states)))\n    ptr = np.zeros((len(sequence), len(states)), dtype=int)\n\n    for i, state in enumerate(states):\n        mat[0, i] = log(emission_matrix[state, sequence[0]] / len(states))\n\n    for i, emission in enumerate(sequence[1:], start=1):\n        for j, state in enumerate(states):\n            opt = [\n                log(transition_matrix[prev, state]) + log(emission_matrix[state, emission]) + mat[i - 1, k]\n                for k, prev in enumerate(states)\n            ]\n            p = opt.index(max(opt))\n            ptr[i, j] = p\n            mat[i, j] = max(opt)\n    ind = np.argmax(mat[i, :])\n\n    state_sequence = states[ind]\n    while i &gt; 0:\n        state_sequence = states[ptr[i, ind]] + state_sequence\n        ind = ptr[i, ind]\n        i -= 1\n    return state_sequence\n\ndef print_matrix(matrix: np.ndarray, row_labels: List[str], column_labels: List[str]) -&gt; None:\n    print(*column_labels, sep=\"\\t\")\n    for i, row in enumerate(matrix):\n        r = [row_labels[i]] + [round(x, 3) if x &gt; 0.0 else \"0\" for x in row]\n        print(*r, sep=\"\\t\")\n\ndef normalize_matrix(matrix: np.ndarray, include_zeros: bool = True, min_val: float = 1e-16) -&gt; np.ndarray:\n    normalized = matrix / matrix.sum(axis=1, keepdims=True)\n    if include_zeros:\n        normalized[normalized == 0] = min_val\n    return normalized\n\ndef estimate_transition_matrix(path: str, states: List[str], to_dict: bool = False) -&gt; Dict[Tuple[str, str], float]:\n    tmat = np.zeros((len(states), len(states)), dtype=float)\n    for a, b in zip(path, path[1:]):\n        tmat[states.index(a)][states.index(b)] += 1\n    tmat = normalize_matrix(tmat)\n    if to_dict:\n        return {(states[i], states[j]): tmat[i, j] for i in range(len(states)) for j in range(len(states))}\n    return tmat\n\ndef estimate_emission_matrix(sequence: str, alphabet: List[str], path: str, states: List[str], to_dict: bool = False) -&gt; Dict[Tuple[str, str], float]:\n    emat = np.zeros((len(states), len(alphabet)), dtype=float)\n    for a, b in zip(path, sequence):\n        emat[states.index(a)][alphabet.index(b)] += 1\n    emat = normalize_matrix(emat)\n    if to_dict:\n        return {(states[i], alphabet[j]): emat[i, j] for i in range(len(states)) for j in range(len(alphabet))}\n    return emat\n\ndef print_dict(d: Dict[Tuple[str, str], float], row_labels: List[str], column_labels: List[str]) -&gt; None:\n    mat = np.zeros((len(row_labels), len(column_labels)), dtype=float)\n    for i, r in enumerate(row_labels):\n        for j, c in enumerate(column_labels):\n            mat[i, j] = d[r, c]\n    print_matrix(mat, row_labels, column_labels)\n\ndef parse_input(handle: StringIO) -&gt; Tuple[int, str, List[str], List[str], Dict[Tuple[str, str], float], Dict[Tuple[str, str], float]]:\n    niter = int(next(handle).rstrip())\n    next(handle)\n    sequence = next(handle).rstrip()\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    states = next(handle).split()\n    next(handle)\n    lines = [next(handle) for _ in range(len(states) + 1)]\n    transition_matrix = {\n        (states[i], states[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    next(handle)\n    lines = [next(handle) for i in range(len(states) + 1)]\n    emission_matrix = {\n        (states[i], alphabet[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    return niter, sequence, states, alphabet, transition_matrix, emission_matrix\n\ndef main(sample_input: str) -&gt; None:\n    input_lines = StringIO(sample_input.strip())\n    niter, sequence, states, alphabet, transition_matrix, emission_matrix = parse_input(input_lines)\n    for _ in range(niter):\n        path = viterbi(sequence, states, transition_matrix, emission_matrix)\n        transition_matrix = estimate_transition_matrix(path, states, to_dict=True)\n        emission_matrix = estimate_emission_matrix(sequence, alphabet, path, states, to_dict=True)\n    print_dict(transition_matrix, states, states)\n    print(\"--------\")\n    print_dict(emission_matrix, states, alphabet)\n\nsample_input = \"\"\"\n100\n--------\nxxxzyzzxxzxyzxzxyxxzyzyzyyyyzzxxxzzxzyzzzxyxzzzxyzzxxxxzzzxyyxzzzzzyzzzxxzzxxxyxyzzyxzxxxyxzyxxyzyxz\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.582   0.418\nB   0.272   0.728\n--------\n    x   y   z\nA   0.129   0.35    0.52\nB   0.422   0.151   0.426\n\"\"\"\n\nmain(sample_input)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-113",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-113",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "113.1 Sample Dataset",
    "text": "113.1 Sample Dataset\nzyxxxxyxzz\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.911   0.089\nB   0.228   0.772\n--------\n    x   y   z\nA   0.356   0.191   0.453 \nB   0.04    0.467   0.493"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-113",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-113",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "113.2 Sample Output",
    "text": "113.2 Sample Output\nA   B \n0.5438  0.4562 \n0.6492  0.3508 \n0.9647  0.0353 \n0.9936  0.0064 \n0.9957  0.0043 \n0.9891  0.0109 \n0.9154  0.0846 \n0.964   0.036 \n0.8737  0.1263 \n0.8167  0.1833"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-113",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-113",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "113.3 Solution",
    "text": "113.3 Solution\nfrom typing import List, Dict, Tuple, Iterator\nfrom io import StringIO\nimport numpy as np\n\ndef parse_input(handle: Iterator[str]) -&gt; Tuple[str, List[str], Dict[Tuple[str, str], float], Dict[Tuple[str, str], float]]:\n    seq: str = next(handle).rstrip()\n    next(handle)\n    alphabet: List[str] = next(handle).split()\n    next(handle)\n    states: List[str] = next(handle).split()\n    next(handle)\n    lines: List[str] = [next(handle) for _ in range(len(states) + 1)]\n    tmat: Dict[Tuple[str, str], float] = {\n        (states[i], states[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    next(handle)\n    lines = [next(handle) for i in range(len(states) + 1)]\n    emat: Dict[Tuple[str, str], float] = {\n        (states[i], alphabet[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    return seq, states, tmat, emat\n\ndef forward(seq: str, states: List[str], tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float]) -&gt; np.ndarray:\n    mat: np.ndarray = np.ones((len(seq), len(states)))\n\n    for i, state in enumerate(states):\n        mat[0, i] = emat[state, seq[0]]\n    for i, emission in enumerate(seq[1:], start=1):\n        for j, state in enumerate(states):\n            mat[i, j] = sum(\n                tmat[prev, state] * emat[state, emission] * mat[i - 1, k]\n                for k, prev in enumerate(states)\n            )\n\n    return mat\n\ndef backward(seq: str, states: List[str], tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float]) -&gt; np.ndarray:\n    mat: np.ndarray = np.ones((len(seq), len(states)))\n\n    for i, emission in enumerate(seq[::-1][:-1], start=1):\n        for j, state in enumerate(states):\n            mat[len(seq) - i - 1, j] = sum(\n                tmat[state, prev] * emat[prev, emission] * mat[len(seq) - i, k]\n                for k, prev in enumerate(states)\n            )\n    return mat\n\ndef soft_decode(seq: str, states: List[str], tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float], normalise: bool = True) -&gt; np.ndarray:\n    tot: np.ndarray = forward(seq, states, tmat, emat) * backward(seq, states, tmat, emat)\n    if normalise:\n        tot = tot / np.sum(tot, axis=1, keepdims=True)\n    return tot\n\ndef main(sample_input: str) -&gt; None:\n    input_lines: Iterator[str] = StringIO(sample_input.strip())\n    seq, states, tmat, emat = parse_input(input_lines)\n    tot: np.ndarray = soft_decode(seq, states, tmat, emat)\n    print(*states, sep=\"\\t\")\n    for r in np.round(tot, 4):\n        print(*r, sep=\"\\t\")\n\nsample_input: str = \"\"\"\nzyxxxxyxzz\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.911   0.089\nB   0.228   0.772\n--------\n    x   y   z\nA   0.356   0.191   0.453 \nB   0.04    0.467   0.493\n\"\"\"\n\nmain(sample_input)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-114",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-114",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "114.1 Sample Dataset",
    "text": "114.1 Sample Dataset\n10\n--------\nxzyyzyzyxy\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.019   0.981 \nB   0.668   0.332 \n--------\nx   y   z\nA   0.175   0.003   0.821 \nB   0.196   0.512   0.293"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-114",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-114",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "114.2 Sample Output",
    "text": "114.2 Sample Output\nA   B\nA   0.000   1.000   \nB   0.786   0.214   \n--------\n    x   y   z\nA   0.242   0.000   0.758   \nB   0.172   0.828   0.000"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-114",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-114",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "114.3 Solution",
    "text": "114.3 Solution\nfrom typing import List, Dict, Tuple, Iterator\nfrom io import StringIO\nimport numpy as np\nfrom collections import defaultdict\n\ndef print_matrix(matrix: np.ndarray, row_labels: List[str], col_labels: List[str]) -&gt; None:\n    print(*col_labels, sep=\"\\t\")\n    for i, row in enumerate(matrix):\n        r = [row_labels[i]] + [round(x, 3) if x &gt; 0.0 else \"0\" for x in row]\n        print(*r, sep=\"\\t\")\n\ndef print_dict(d: Dict[Tuple[str, str], float], row_labels: List[str], col_labels: List[str]) -&gt; None:\n    mat = np.zeros((len(row_labels), len(col_labels)), dtype=float)\n    for i, r in enumerate(row_labels):\n        for j, c in enumerate(col_labels):\n            mat[i, j] = d[r, c]\n    print_matrix(mat, row_labels, col_labels)\n\ndef parse_input(handle: Iterator[str]) -&gt; Tuple[int, str, List[str], List[str], Dict[Tuple[str, str], float], Dict[Tuple[str, str], float]]:\n    niter = int(next(handle).rstrip())\n    next(handle)\n    seq = next(handle).rstrip()\n    next(handle)\n    alphabet = next(handle).split()\n    next(handle)\n    states = next(handle).split()\n    next(handle)\n    lines = [next(handle) for _ in range(len(states) + 1)]\n    tmat = {\n        (states[i], states[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    next(handle)\n    lines = [next(handle) for _ in range(len(states) + 1)]\n    emat = {\n        (states[i], alphabet[j]): float(v)\n        for i, x in enumerate(lines[1:])\n        for j, v in enumerate(x.split()[1:])\n    }\n    return niter, seq, states, alphabet, tmat, emat\n\ndef forward(seq: str, states: List[str], tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float]) -&gt; np.ndarray:\n    mat = np.ones((len(seq), len(states)))\n    for i, state in enumerate(states):\n        mat[0, i] = emat[state, seq[0]]\n    for i, emission in enumerate(seq[1:], start=1):\n        for j, state in enumerate(states):\n            mat[i, j] = sum(\n                tmat[prev, state] * emat[state, emission] * mat[i - 1, k]\n                for k, prev in enumerate(states)\n            )\n    return mat\n\ndef backward(seq: str, states: List[str], tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float]) -&gt; np.ndarray:\n    mat = np.ones((len(seq), len(states)))\n    for i, emission in enumerate(seq[::-1][:-1], start=1):\n        for j, state in enumerate(states):\n            mat[len(seq) - i - 1, j] = sum(\n                tmat[state, prev] * emat[prev, emission] * mat[len(seq) - i, k]\n                for k, prev in enumerate(states)\n            )\n    return mat\n\ndef soft_decode(seq: str, states: List[str], tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float], normalise: bool = True) -&gt; np.ndarray:\n    tot = forward(seq, states, tmat, emat) * backward(seq, states, tmat, emat)\n    if normalise:\n        tot = tot / np.sum(tot, axis=1, keepdims=True)\n    return tot\n\ndef as_dict(x: np.ndarray, r: List[str], c: List[str]) -&gt; Dict[Tuple[str, str], float]:\n    g = defaultdict(float)\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            g[r[i], c[j]] = x[i][j]\n    return g\n\ndef estimate_pi2(seq: str, fwd: np.ndarray, bak: np.ndarray, tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float], states: List[str]) -&gt; np.ndarray:\n    rep_mat = np.zeros((fwd.shape[0] - 1, len(states), len(states)), dtype=float)\n    for i in range(0, fwd.shape[0] - 1):\n        for j, s1 in enumerate(states):\n            for k, s2 in enumerate(states):\n                weight = tmat[s1, s2] * emat[s2, seq[i + 1]]\n                rep_mat[i, j, k] = (\n                    fwd[i, j] * bak[i + 1, k] * weight / sum(fwd[i, :] * bak[i, :])\n                )\n    return rep_mat\n\ndef estimate_tmat(seq: str, st: List[str], tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float]) -&gt; Dict[Tuple[str, str], float]:\n    fwd = forward(seq, st, tmat, emat)\n    bak = backward(seq, st, tmat, emat)\n    pi2 = estimate_pi2(seq, fwd, bak, tmat, emat, st)\n    tmat_new = np.sum(pi2, 0)\n    tmat_new = tmat_new / np.sum(tmat_new, axis=1, keepdims=True)\n    return as_dict(tmat_new, st, st)\n\ndef estimate_emat(seq: str, al: List[str], st: List[str], tmat: Dict[Tuple[str, str], float], emat: Dict[Tuple[str, str], float]) -&gt; Dict[Tuple[str, str], float]:\n    pi1 = soft_decode(seq, st, tmat, emat)\n    emat_new = np.zeros((len(st), len(al)), dtype=float)\n    for i, emission in enumerate(al):\n        ind = np.array(list(seq)) == emission\n        emat_new[:, i] = np.sum(pi1[ind, :], 0)\n    emat_new = emat_new / np.sum(emat_new, axis=1, keepdims=True)\n    return as_dict(emat_new, st, al)\n\ndef main(sample_input: str) -&gt; None:\n    input_lines = StringIO(sample_input.strip())\n    niter, seq, st, al, tmat, emat = parse_input(input_lines)\n    for _ in range(niter):\n        tmat2 = estimate_tmat(seq, st, tmat, emat)\n        emat2 = estimate_emat(seq, al, st, tmat, emat)\n        emat, tmat = emat2, tmat2\n    print_dict(tmat, st, st)\n    print(\"--------\")\n    print_dict(emat, st, al)\n\nsample_input: str = \"\"\"\n10\n--------\nxzyyzyzyxy\n--------\nx   y   z\n--------\nA   B\n--------\n    A   B\nA   0.019   0.981 \nB   0.668   0.332 \n--------\nx   y   z\nA   0.175   0.003   0.821 \nB   0.196   0.512   0.293\n\"\"\"\n\nmain(sample_input)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-115",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-115",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "115.1 Sample Dataset",
    "text": "115.1 Sample Dataset\n57 71 154 185 301 332 415 429 486"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-115",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-115",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "115.2 Sample Output",
    "text": "115.2 Sample Output\n0-&gt;57:G\n0-&gt;71:A\n57-&gt;154:P\n57-&gt;185:K\n71-&gt;185:N\n154-&gt;301:F\n185-&gt;332:F\n301-&gt;415:N\n301-&gt;429:K\n332-&gt;429:P\n415-&gt;486:A\n429-&gt;486:G"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-115",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-115",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "115.3 Solution",
    "text": "115.3 Solution\nfrom collections import defaultdict\nfrom typing import List, Dict, DefaultDict\n\n# Amino acid weights dictionary\namino_acid_weights: Dict[str, int] = {\n    'G': 57, 'A': 71, 'S': 87, 'P': 97, 'V': 99,\n    'T': 101, 'C': 103, 'I': 113, 'L': 113, 'N': 114,\n    'D': 115, 'K': 128, 'Q': 128, 'E': 129, 'M': 131,\n    'H': 137, 'F': 147, 'R': 156, 'Y': 163, 'W': 186\n}\n\ndef spectrum_graph(masses: List[int]) -&gt; DefaultDict[int, List[Dict[str, int]]]:\n    # Reverse mapping of weights to amino acids\n    weight_to_amino_acid: Dict[int, str] = {weight: aa for aa, weight in amino_acid_weights.items()}\n    \n    graph: DefaultDict[int, List[Dict[str, int]]] = defaultdict(list)\n    \n    # Create graph based on mass differences\n    for i in range(len(masses)):\n        for j in range(i + 1, len(masses)):\n            difference: int = masses[j] - masses[i]\n            if difference in weight_to_amino_acid:\n                graph[masses[i]].append({\"n\": masses[j], \"l\": weight_to_amino_acid[difference]})\n    \n    return graph\n\n# Sample input\nsample_input: str = \"57 71 154 185 301 332 415 429 486\"\nmasses: List[int] = [0] + list(map(int, sample_input.split()))\n\n# Print the spectrum graph\nfor start_mass, edges in spectrum_graph(masses).items():\n    for edge in edges:\n        print(f\"{start_mass}-&gt;{edge['n']}:{edge['l']}\")"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-116",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-116",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "116.1 Sample Dataset",
    "text": "116.1 Sample Dataset\n57 71 154 185 301 332 415 429 486"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-116",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-116",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "116.2 Sample Output",
    "text": "116.2 Sample Output\nGPFNA"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-116",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-116",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "116.3 Solution",
    "text": "116.3 Solution\nfrom typing import Dict, List, Tuple\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\nmass_to_amino_acid: Dict[int, str] = {v: k for k, v in amino_acid_masses.items()}\n\n\ndef create_spectrum_graph(spectrum: List[int]) -&gt; List[List[int]]:\n    adjacency_list: List[List[int]] = []\n    for i in range(len(spectrum)):\n        for j in range(i, len(spectrum)):\n            if spectrum[j] - spectrum[i] in mass_to_amino_acid.keys():\n                adjacency_list.append([spectrum[i], spectrum[j], mass_to_amino_acid[spectrum[j] - spectrum[i]]])\n    return adjacency_list\n\n\ndef calculate_ideal_spectrum(peptide: str) -&gt; List[int]:\n    prefix_mass: List[int] = [0]\n    for i in range(len(peptide)):\n        temp = prefix_mass[i] + amino_acid_masses[peptide[i]]\n        prefix_mass.append(temp)\n    linear_spectrum: List[int] = [0]\n    for i in range(len(peptide)):\n        for j in range(i + 1, len(peptide) + 1):\n            linear_spectrum.append(prefix_mass[j] - prefix_mass[i])\n    linear_spectrum.sort()\n    return linear_spectrum\n\n\ndef find_paths(adjacency_list: List[List[int]]) -&gt; List[str]:\n    node: int = 0\n    peptide_list: List[str] = []\n    tmp_edges: List[List[List[int]]] = []\n    peptide: str = ''\n    tmp_peps: List[str] = []\n\n    while any([len(x) != 0 for x in tmp_edges]) or len(tmp_edges) == 0:\n        next_edges: List[List[int]] = [e for e in adjacency_list if e[0] == node]\n        if len(next_edges) &gt; 1:\n            tmp = next_edges[1:]\n            tmp_edges.append(tmp)\n            tmp_peps.append(peptide)\n\n        next_edge = next_edges[0]\n        peptide += next_edge[2]\n        node = next_edge[1]\n\n        if len([e for e in adjacency_list if e[0] == node]) == 0:\n            tmp = [x for x in tmp_edges if len(x) != 0][-1]\n            next_edge = tmp.pop()\n            node = next_edge[1]\n            peptide_list.append(peptide)\n            tmp_pep = tmp_peps.pop()\n            peptide = tmp_pep + next_edge[2]\n\n    return peptide_list\n\n\ndef decode_ideal_spectrum(spectrum: List[int]) -&gt; str:\n    adjacency_list = create_spectrum_graph(spectrum)\n    all_paths = find_paths(adjacency_list)\n    for peptide in all_paths:\n        if set(spectrum).issubset(calculate_ideal_spectrum(peptide)):\n            return peptide\n\n\nsample_input: str = \"\"\"\n57 71 154 185 301 332 415 429 486\n\"\"\"\n\nspectrum: List[int] = [int(s) for s in sample_input.strip().split()]\nspectrum = [0] + spectrum\n\nprint(decode_ideal_spectrum(spectrum))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-117",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-117",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "117.1 Sample Dataset",
    "text": "117.1 Sample Dataset\nXZZXX"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-117",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-117",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "117.2 Sample Output",
    "text": "117.2 Sample Output\n0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-117",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-117",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "117.3 Solution",
    "text": "117.3 Solution\nfrom typing import Dict, List\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131, 'L': 113,\n    'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163, 'X': 4, 'Z': 5\n}\n\n\ndef create_peptide_vector(peptide: str) -&gt; List[int]:\n    prefix_masses: List[int] = []\n    for i in range(len(peptide)):\n        prefix = peptide[:i+1]\n        mass = sum(amino_acid_masses[aa] for aa in prefix)\n        prefix_masses.append(mass)\n\n    vector: List[int] = [0] * prefix_masses[-1]\n    for mass in prefix_masses:\n        vector[mass - 1] = 1\n    return vector\n\n\nsample_input: str = \"\"\"\nXZZXX\n\"\"\"\npeptide: str = sample_input.strip()\nprint(' '.join(str(x) for x in create_peptide_vector(peptide)))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-118",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-118",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "118.1 Sample Dataset",
    "text": "118.1 Sample Dataset\n0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-118",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-118",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "118.2 Sample Output",
    "text": "118.2 Sample Output\nXZZXX"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-118",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-118",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "118.3 Solution",
    "text": "118.3 Solution\nfrom typing import Dict, List\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\nmass_to_amino_acid: Dict[int, str] = {v: k for k, v in amino_acid_masses.items()}\nmass_to_amino_acid[4] = 'X'\nmass_to_amino_acid[5] = 'Z'\n\n\ndef convert_peptide_vector_to_sequence(vector: List[int]) -&gt; str:\n    prefix_masses: List[int] = [i + 1 for i, v in enumerate(vector) if v == 1]\n\n    peptide: str = mass_to_amino_acid[prefix_masses[0]]\n    for i in range(1, len(prefix_masses)):\n        mass = prefix_masses[i] - prefix_masses[i - 1]\n        peptide += mass_to_amino_acid[mass]\n\n    return peptide\n\n\nsample_input: str = \"\"\"\n0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1\n\"\"\"\nvector: List[int] = [int(x) for x in sample_input.strip().split()]\n\nprint(convert_peptide_vector_to_sequence(vector))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-119",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-119",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "119.1 Sample Dataset",
    "text": "119.1 Sample Dataset\n0 0 0 4 -2 -3 -1 -7 6 5 3 2 1 9 3 -8 0 3 1 2 1 0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-119",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-119",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "119.2 Sample Output",
    "text": "119.2 Sample Output\nXZZXX"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-119",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-119",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "119.3 Solution",
    "text": "119.3 Solution\nfrom typing import Dict, List, Tuple\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\nmass_to_amino_acid: Dict[int, str] = {v: k for k, v in amino_acid_masses.items()}\n\n\ndef sequence_peptide(spectral_vector: List[int]) -&gt; str:\n    spectral_vector = [0] + spectral_vector\n\n    adjacency_list: List[List[int]] = []\n    for i in range(len(spectral_vector)):\n        for j in range(i, len(spectral_vector)):\n            if (j - i) in mass_to_amino_acid.keys():\n                adjacency_list.append([i, j])\n\n    adjacency_dict: Dict[int, List[List[Union[int, str]]]] = {}\n    for i in range(len(spectral_vector)):\n        for j in range(i, len(spectral_vector)):\n            if (j - i) in mass_to_amino_acid.keys():\n                edge = [i, mass_to_amino_acid[j - i]]\n                if j not in adjacency_dict:\n                    adjacency_dict[j] = [edge]\n                else:\n                    adjacency_dict[j].append(edge)\n\n    scores: Dict[int, List[Union[float, str]]] = {0: [0, '-']}\n    for node in adjacency_dict.keys():\n        scores[node] = [-float('inf'), '-']\n        edges = adjacency_dict[node]\n        for edge in edges:\n            if edge[0] != 0:\n                scores[edge[0]] = [-float('inf'), '-']\n\n    for node in adjacency_dict.keys():\n        max_score: float = -float('inf')\n        best_edge: Union[str, List[Union[int, str]]] = '-'\n        for parent in adjacency_dict[node]:\n            score = scores[parent[0]][0]\n            if score &gt; max_score:\n                max_score = score\n                best_edge = parent\n        scores[node] = [max_score + spectral_vector[node], best_edge]\n\n    node: int = list(scores.keys())[-1]\n    peptide: str = ''\n    while node != 0:\n        peptide = scores[node][1][1] + peptide\n        node = scores[node][1][0]\n\n    return peptide\n\n\nsample_input: str = \"\"\"\n0 0 0 4 -2 -3 -1 -7 6 5 3 2 1 9 3 -8 0 3 1 2 1 0\n\"\"\"\n\nspectral_vector: List[int] = [int(x) for x in sample_input.split()]\n\nprint(sequence_peptide(spectral_vector))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-120",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-120",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "120.1 Sample Dataset",
    "text": "120.1 Sample Dataset\n0 0 0 4 -2 -3 -1 -7 6 5 3 2 1 9 3 -8 0 3 1 2 1 8\nXZZXZXXXZXZZXZXXZ"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-120",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-120",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "120.2 Sample Output",
    "text": "120.2 Sample Output\nZXZXX"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-120",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-120",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "120.3 Solution",
    "text": "120.3 Solution\nfrom typing import Dict, List\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163,\n    'X': 4, 'Z': 5  # Added 'X' and 'Z' with arbitrary masses\n}\n\ndef create_peptide_vector(peptide: str) -&gt; List[int]:\n    prefix_masses: List[int] = [0]\n    for amino_acid in peptide:\n        prefix_masses.append(prefix_masses[-1] + amino_acid_masses[amino_acid])\n    vector: List[int] = [0] * prefix_masses[-1]\n    for mass in prefix_masses[1:]:\n        vector[mass - 1] = 1\n    return vector\n\ndef identify_peptide(spectral_vector: List[int], proteome: str) -&gt; str:\n    max_score: float = float('-inf')\n    best_peptide: str = \"\"\n\n    for i in range(len(proteome)):\n        for j in range(i + 1, len(proteome) + 1):\n            peptide: str = proteome[i:j]\n            peptide_vector: List[int] = create_peptide_vector(peptide)\n            \n            if len(peptide_vector) &gt; len(spectral_vector):\n                break\n            \n            if len(peptide_vector) == len(spectral_vector):\n                score: float = sum(s * v for s, v in zip(spectral_vector, peptide_vector))\n                if score &gt; max_score:\n                    max_score = score\n                    best_peptide = peptide\n\n    return best_peptide\n\nsample_input: str = \"\"\"\n0 0 0 4 -2 -3 -1 -7 6 5 3 2 1 9 3 -8 0 3 1 2 1 8\nXZZXZXXXZXZZXZXXZ\n\"\"\"\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nspectral_vector: List[int] = [int(x) for x in input_lines[0].strip().split()]\nproteome: str = input_lines[1].strip()\nprint(identify_peptide(spectral_vector, proteome))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-121",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-121",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "121.1 Sample Dataset",
    "text": "121.1 Sample Dataset\n-1 5 -4 5 3 -1 -4 5 -1 0 0 4 -1 0 1 4 4 4\n-4 2 -2 -4 4 -5 -1 4 -1 2 5 -3 -1 3 2 -3\nXXXZXZXXZXZXXXZXXZX\n5"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-121",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-121",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "121.2 Sample Output",
    "text": "121.2 Sample Output\nXZXZ"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-121",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-121",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "121.3 Solution",
    "text": "121.3 Solution\nfrom typing import Dict, List, Set, Tuple\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163,\n    'X': 4, 'Z': 5  # Added 'X' and 'Z' with arbitrary masses\n}\n\ndef is_number(n: str) -&gt; bool:\n    try:\n        float(n)\n    except ValueError:\n        return False\n    return True\n\ndef create_peptide_vector(peptide: str) -&gt; List[int]:\n    prefix_masses: List[int] = [0]\n    for amino_acid in peptide:\n        prefix_masses.append(prefix_masses[-1] + amino_acid_masses[amino_acid])\n    vector: List[int] = [0] * prefix_masses[-1]\n    for mass in prefix_masses[1:]:\n        vector[mass - 1] = 1\n    return vector\n\ndef identify_peptide(spectral_vector: List[int], proteome: str) -&gt; Tuple[str, float]:\n    max_score: float = float('-inf')\n    best_peptide: str = ''\n\n    for i in range(len(proteome)):\n        for j in range(i + 1, len(proteome) + 1):\n            peptide: str = proteome[i:j]\n            peptide_vector: List[int] = create_peptide_vector(peptide)\n            \n            if len(peptide_vector) &gt; len(spectral_vector):\n                break\n            \n            if len(peptide_vector) == len(spectral_vector):\n                score: float = sum(s * v for s, v in zip(spectral_vector, peptide_vector))\n                if score &gt; max_score:\n                    max_score = score\n                    best_peptide = peptide\n\n    return best_peptide, max_score\n\ndef search_peptide_spectrum_matches(spectral_vectors: List[List[int]], proteome: str, threshold: float) -&gt; Set[str]:\n    psm_set: Set[str] = set()\n    for vector in spectral_vectors:\n        peptide, score = identify_peptide(vector, proteome)\n        if score &gt;= threshold:\n            psm_set.add(peptide)\n    return psm_set\n\nsample_input: str = \"\"\"\n-1 5 -4 5 3 -1 -4 5 -1 0 0 4 -1 0 1 4 4 4\n-4 2 -2 -4 4 -5 -1 4 -1 2 5 -3 -1 3 2 -3\nXXXZXZXXZXZXXXZXXZX\n5\n\"\"\"\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\n\nspectral_vectors: List[List[int]] = []\nidx: int = 0\nwhile idx &lt; len(input_lines) and (is_number(input_lines[idx][0]) or is_number(input_lines[idx][:2])):\n    vector: List[int] = [int(x) for x in input_lines[idx].strip().split()]\n    spectral_vectors.append(vector)\n    idx += 1\n\nproteome: str = input_lines[idx].strip()\nthreshold: int = int(input_lines[idx + 1])\n\nresult: Set[str] = search_peptide_spectrum_matches(spectral_vectors, proteome, threshold)\n\nfor peptide in result:\n    print(peptide)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-122",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-122",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "122.1 Sample Dataset",
    "text": "122.1 Sample Dataset\n4 -3 -2 3 3 -4 5 -3 -1 -1 3 4 1 3\n1\n8"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-122",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-122",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "122.2 Sample Output",
    "text": "122.2 Sample Output\n0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-122",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-122",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "122.3 Solution",
    "text": "122.3 Solution\nfrom typing import Dict, List\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\nmass_values: List[int] = list(amino_acid_masses.values())\n\ndef calculate_spectral_dictionary_size(spectral_vector: List[int], threshold: int, max_score: int) -&gt; int:\n    vector_length: int = len(spectral_vector)\n\n    size_matrix: Dict[int, Dict[int, int]] = {0: {0: 1}}\n    for t in range(1, max_score + 1):\n        size_matrix[0][t] = 0\n\n    for i in range(1, vector_length + 1):\n        size_matrix[i] = {}\n        for t in range(max_score + 1):\n            size_matrix[i][t] = 0\n            for mass in mass_values:\n                if (i - mass) &gt;= 0 and (t - spectral_vector[i - 1]) &gt;= 0 and (t - spectral_vector[i - 1]) &lt;= max_score:\n                    size_matrix[i][t] += size_matrix[i - mass][t - spectral_vector[i - 1]]\n\n    final_size: int = sum(size_matrix[vector_length][t] for t in range(threshold, max_score + 1))\n\n    return final_size\n\nsample_input: str = \"\"\"\n4 -3 -2 3 3 -4 5 -3 -1 -1 3 4 1 3\n1\n8\n\"\"\"\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nspectral_vector: List[int] = [int(x) for x in input_lines[0].strip().split()]\nthreshold: int = int(input_lines[1])\nmax_score: int = int(input_lines[2])\n\nprint(calculate_spectral_dictionary_size(spectral_vector, threshold, max_score))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-123",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-123",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "123.1 Sample Dataset",
    "text": "123.1 Sample Dataset\n4 -3 -2 3 3 -4 5 -3 -1 -1 3 4 1 3\n1\n8"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-123",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-123",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "123.2 Sample Output",
    "text": "123.2 Sample Output\n0"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-123",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-123",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "123.3 Solution",
    "text": "123.3 Solution\nfrom typing import Dict, List\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163\n}\nmass_values: List[int] = list(amino_acid_masses.values())\n\ndef calculate_spectral_dictionary_probability(spectral_vector: List[int], threshold: int, max_score: int) -&gt; float:\n    vector_length: int = len(spectral_vector)\n\n    probability_matrix: Dict[int, Dict[int, float]] = {0: {0: 1.0}}\n    for t in range(1, max_score + 1):\n        probability_matrix[0][t] = 0.0\n\n    for i in range(1, vector_length + 1):\n        probability_matrix[i] = {}\n        for t in range(max_score + 1):\n            probability_matrix[i][t] = 0.0\n            for mass in mass_values:\n                if (i - mass) &gt;= 0 and (t - spectral_vector[i - 1]) &gt;= 0 and (t - spectral_vector[i - 1]) &lt;= max_score:\n                    probability_matrix[i][t] += probability_matrix[i - mass][t - spectral_vector[i - 1]]\n            probability_matrix[i][t] /= 20\n\n    final_probability: float = sum(probability_matrix[vector_length][t] for t in range(threshold, max_score + 1))\n\n    return final_probability\n\nsample_input: str = \"\"\"\n4 -3 -2 3 3 -4 5 -3 -1 -1 3 4 1 3\n1\n8\n\"\"\"\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\nspectral_vector: List[int] = [int(x) for x in input_lines[0].strip().split()]\nthreshold: int = int(input_lines[1])\nmax_score: int = int(input_lines[2])\n\nprint(calculate_spectral_dictionary_probability(spectral_vector, threshold, max_score))"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-dataset-124",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-dataset-124",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "124.1 Sample Dataset",
    "text": "124.1 Sample Dataset\nXXZ\n4 -3 -2 3 3 -4 5 -3 -1 -1 3 4 1 3\n2"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#sample-output-124",
    "href": "posts/md/Rosalind_textbookTrack.html#sample-output-124",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "124.2 Sample Output",
    "text": "124.2 Sample Output\nXX(-1)Z(+2)"
  },
  {
    "objectID": "posts/md/Rosalind_textbookTrack.html#solution-124",
    "href": "posts/md/Rosalind_textbookTrack.html#solution-124",
    "title": "Rosalind Textbook track 문제풀이",
    "section": "124.3 Solution",
    "text": "124.3 Solution\nfrom typing import Dict, List\n\namino_acid_masses: Dict[str, int] = {\n    'A': 71, 'C': 103, 'E': 129, 'D': 115, 'G': 57, 'F': 147, 'I': 113, 'H': 137, 'K': 128, 'M': 131,\n    'L': 113, 'N': 114, 'Q': 128, 'P': 97, 'S': 87, 'R': 156, 'T': 101, 'W': 186, 'V': 99, 'Y': 163,\n    'X': 4, 'Z': 5  # Added 'X' and 'Z' with arbitrary masses\n}\n\ndef print_score_matrix(score_matrix: Dict[int, Dict[int, Dict[int, float]]], prefix_masses: List[int], spectral_vector: List[int], max_k: int) -&gt; None:\n    for t in range(-2, max_k + 1):\n        if t == -2:\n            formatted_vector = [str(x).rjust(3) for x in spectral_vector]\n            print('   ', *formatted_vector)\n            print(' ')\n            print(' ')\n        elif t == -1:\n            header = [str(i).rjust(3) for i in range(len(spectral_vector))]\n            print('   ', *header)\n        else:\n            for mass in prefix_masses:\n                row = []\n                for j in range(-1, len(spectral_vector)):\n                    if j == -1:\n                        row.append(f\"{mass:&gt;3} \")\n                    else:\n                        score = score_matrix[mass][j][t]\n                        if score &lt; -1e5:\n                            score = 'XX'\n                        row.append(f\"{score:&gt;3}\")\n                print(' '.join(row))\n            print(' ')\n    return None\n\ndef spectral_alignment(peptide: str, spectral_vector: List[int], max_k: int) -&gt; str:\n    spectral_vector.insert(0, 0)\n\n    # Calculate prefix masses\n    prefix_masses = [0]\n    for i in range(len(peptide)):\n        prefix = peptide[:i + 1]\n        mass = sum(amino_acid_masses[aa] for aa in prefix)\n        prefix_masses.append(mass)\n\n    # Create diff array\n    mass_differences = {}\n    for i in range(1, len(prefix_masses)):\n        mass_differences[prefix_masses[i]] = prefix_masses[i] - prefix_masses[i - 1]\n\n    # Initialize scores\n    score_matrix: Dict[int, Dict[int, Dict[int, float]]] = {}\n    for mass in prefix_masses:\n        score_matrix[mass] = {}\n        for j in range(len(spectral_vector)):\n            score_matrix[mass][j] = {t: -float(\"inf\") for t in range(max_k + 1)}\n    score_matrix[0][0][0] = 0\n\n    # Calculate scores\n    for mass in prefix_masses[1:]:\n        for j in range(len(spectral_vector)):\n            for t in range(max_k + 1):\n                if (t == 0) and (mass - mass_differences[mass] &gt;= 0) and (j - mass_differences[mass] &gt;= 0):\n                    score_matrix[mass][j][t] = spectral_vector[j] + score_matrix[mass - mass_differences[mass]][j - mass_differences[mass]][t]\n                elif (t &gt; 0) and (mass - mass_differences[mass] &gt;= 0) and (j - mass_differences[mass] &gt;= 0):\n                    score_matrix[mass][j][t] = spectral_vector[j] + max(score_matrix[mass - mass_differences[mass]][j - mass_differences[mass]][t], \n                                                                         max(score_matrix[mass - mass_differences[mass]][j_star][t - 1] for j_star in range(j)))\n                elif (t &gt; 0) and (mass - mass_differences[mass] &gt;= 0) and (j &gt; 0):\n                    score_matrix[mass][j][t] = spectral_vector[j] + max(score_matrix[mass - mass_differences[mass]][j_star][t - 1] for j_star in range(j))\n\n    # Find max score layer\n    max_score = -float(\"inf\")\n    max_layer = 0\n    for t in range(max_k + 1):\n        current_score = score_matrix[prefix_masses[-1]][len(spectral_vector) - 1][t]\n        if current_score &gt; max_score:\n            max_score = current_score\n            max_layer = t\n\n    # Backtrace\n    layer = max_layer\n    column_index = len(spectral_vector) - 1\n\n    result_peptide = ''\n    for i in range(len(peptide), 0, -1):\n        pre_mass = prefix_masses[i]\n        if (column_index - mass_differences[pre_mass] &gt;= 0) and (\n                score_matrix[pre_mass][column_index][layer] == spectral_vector[column_index] + score_matrix[pre_mass - mass_differences[pre_mass]][column_index - mass_differences[pre_mass]][layer]):\n            column_index -= mass_differences[pre_mass]\n            result_peptide = peptide[i - 1] + result_peptide\n        else:\n            temp_scores = [score_matrix[pre_mass - mass_differences[pre_mass]][j_star][layer - 1] for j_star in range(column_index)]\n            idx_max_score = temp_scores.index(max(temp_scores))\n            modification_amount = column_index - idx_max_score - mass_differences[pre_mass]\n            if modification_amount &gt; 0:\n                result_peptide = peptide[i - 1] + '(+' + str(modification_amount) + ')' + result_peptide\n            else:\n                result_peptide = peptide[i - 1] + '(' + str(modification_amount) + ')' + result_peptide\n            column_index = idx_max_score\n            layer -= 1\n\n    return result_peptide\n\nsample_input: str = \"\"\"\nXXZ\n4 -3 -2 3 3 -4 5 -3 -1 -1 3 4 1 3\n2\n\"\"\"\ninput_lines: List[str] = sample_input.strip().split(\"\\n\")\npeptide_sequence: str = input_lines[0]\nspectral_vector_values: List[int] = [int(x) for x in input_lines[1].strip().split()]\nmax_k_value: int = int(input_lines[2])\n\nprint(spectral_alignment(peptide_sequence, spectral_vector_values, max_k_value))"
  },
  {
    "objectID": "posts/md/How_Ds_Store.html",
    "href": "posts/md/How_Ds_Store.html",
    "title": ".DS_Store 파일 관리",
    "section": "",
    "text": ".DS_Store 파일은 다른 OS 로 데이터를 공유하는 과정에서 전달되는 경우가 많고 불필요한 정보까지 옮겨 질 수 있습니다. 그래서 아래와 같이 삭제해서 데이터를 옮기거나 자동 생성을 방지하는 작업이 필요합니다.\n.DS_Store(Desktop Services Store) 는 애플에서 정의한 파일 포맷으로, macOS 에서 아래와 같은 역할을 합니다.1"
  },
  {
    "objectID": "posts/md/How_Ds_Store.html#footnotes",
    "href": "posts/md/How_Ds_Store.html#footnotes",
    "title": ".DS_Store 파일 관리",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://chanhhh.tistory.com/209↩︎\nhttps://jonhyuk0922.tistory.com/116↩︎\nhttps://wooono.tistory.com/251↩︎"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html",
    "href": "posts/md/Stat_clinical_trial.html",
    "title": "임상 통계학",
    "section": "",
    "text": "다음은 책 ” 강승호, 신약개발에 필요한 임상통계학, 자유 아카데미, 2022” 을 읽고 요약 정리한 것으로 많은 설명은 perplexity 를 사용해 덧붙였습니다. 책 전체 내용을 요약하지는 않았기 때문에 더 자세한 내용에 관심이 있으다면 책을 한번 읽어보시길 바랍니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#임상적-유의성과-통계적-유의성",
    "href": "posts/md/Stat_clinical_trial.html#임상적-유의성과-통계적-유의성",
    "title": "임상 통계학",
    "section": "1.1 임상적 유의성과 통계적 유의성",
    "text": "1.1 임상적 유의성과 통계적 유의성\n임상적 유의성과 통계적 유의성은 임상 연구에서 중요한 개념이지만 서로 다른 의미를 가집니다.\n\n통계적 유의성: 연구 결과가 우연에 의해 발생할 가능성이 낮다는 것을 의미합니다. 일반적으로 p- 값이 미리 정해진 유의수준 (보통 5%) 보다 작을 때 통계적으로 유의하다고 판단합니다. 통계적 유의성은 데이터의 변동성을 고려하여 결과가 우연이 아닐 확률을 평가합니다.1\n임상적 유의성: 통계적으로 유의한 결과가 실제 임상 환경에서 환자에게 의미 있는 변화를 가져오는지를 임상의가 평가합니다. 즉, 통계적으로 유의한 차이가 임상적으로도 중요한 차이인지 판단하는 것입니다. 예를 들어, 고혈압 치료제의 경우, 통계적으로 유의한 혈압 감소가 실제로 환자의 건강에 긍정적인 영향을 미칠 만큼 충분히 큰지 평가해야 합니다.2\n\n따라서, 통계적으로 유의한 결과가 항상 임상적으로 유의한 것은 아닙니다. 다만 통계적 유의성은 객관적인 자료를 통해 설명되지만 임상적 유의성은 임상의 주관이 들어가게 되는 차이가 있습니다. 하지만 무엇보다도 허가 기관으로 부터 품목 허가를 받기 위해서는 위의 두가지 유의성이 모두 적절히 설명되어야 합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#통계분석계획-statistical-analysis-plan-sap-과-맹검-검토-blind-review-의-필요성",
    "href": "posts/md/Stat_clinical_trial.html#통계분석계획-statistical-analysis-plan-sap-과-맹검-검토-blind-review-의-필요성",
    "title": "임상 통계학",
    "section": "1.2 통계분석계획 (Statistical Analysis Plan, SAP) 과 맹검 검토 (Blind review) 의 필요성",
    "text": "1.2 통계분석계획 (Statistical Analysis Plan, SAP) 과 맹검 검토 (Blind review) 의 필요성\n\n1.2.1 통계분석계획의 필요성\n사전에 작성된 임상 시험 계획서의 통계 분석법이 적절하지 않은 경우가 많습니다. 따라서 통계 분석 계획은 임상 시험을 통해 자료를 모두 수집한 뒤 분석법을 수정할 수 있는 마지막 기회입니다.\n\n규제 요구사항 충족: 규제 기관은 임상시험의 신뢰성과 타당성을 평가하기 위해 통계분석계획을 요구합니다. 이는 허가 심사 과정에서 중요한 근거 자료로 사용됩니다.3\n\n\n\n1.2.2 맹검 검토의 필요성\n\n편향 최소화: 맹검 검토는 분석자가 시험 데이터를 해석할 때 발생할 수 있는 편향 (연구자의 사전 지식이나 기대가 결과에 영향) 을 최소화합니다.4\n객관성 확보: 맹검 검토는 연구 결과가 의도하지 않은 외부 요인에 의해 왜곡되지 않도록 보장해 임상시험의 객관성을 높입니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#임상-시험에-자주-쓰이는-시험-디자인",
    "href": "posts/md/Stat_clinical_trial.html#임상-시험에-자주-쓰이는-시험-디자인",
    "title": "임상 통계학",
    "section": "1.3 임상 시험에 자주 쓰이는 시험 디자인",
    "text": "1.3 임상 시험에 자주 쓰이는 시험 디자인\n\n1.3.1 평행 설계 (Parallel Design)\n| group A  | group B  | group C  |\n| -------- | -------- | -------- |\n| patient1 | patient2 | patiant3 |\n| -------- | -------- | -------- |\n| patient4 | patient5 | patiant6 |\n평행 설계는 임상시험 참가자들을 두 개 이상의 그룹으로 무작위로 배정하여, 각 그룹이 서로 다른 치료를 받도록 하는 방식입니다. 각 그룹은 시험 기간 동안 동일한 치료를 계속 받습니다.\n\n장점: 이 설계는 단순하고 이해하기 쉬우며, 각 그룹 간의 비교가 명확합니다. 또한, 교차 설계에 비해 참가자가 적게 필요할 수 있습니다.\n적용: 주로 만성 질환의 장기 치료 효과를 평가할 때 사용됩니다.5\n\n\n\n1.3.2 교차 설계 (Cross-over Design)\npatient1 —-&gt; treatment A —-&gt; wash out —&gt; treatment B\npartien2 —-&gt; treatment B —-&gt; wash out —-&gt; treatment A\n교차 설계는 참가자들이 두 개 이상의 치료를 순차적으로 받도록 하는 방식입니다. 각 참가자는 모든 치료를 받으며, 치료 사이에 세척 기간 (washout period) 을 두어 이전 치료의 영향을 최소화합니다.\n\n장점: 각 참가자가 자신의 대조군 역할을 하므로, 개인 간 변동을 줄일 수 있습니다. 이는 통계적 검정력을 높이고, 필요한 참가자 수를 줄일 수 있습니다.\n적용: 주로 급성 질환이나 일시적인 증상에 대한 치료 효과를 평가할 때 사용됩니다. 그러나 질병의 자연 경과가 빠르게 변하거나, 치료의 장기적 효과가 있는 경우에는 부적합할 수 있습니다\n\n\n\n1.3.3 요인 설계?\n| patient  | treatment A | treatment B | treatment A+B | no treatment |\n| -------- | ----------- | ----------- | ------------- | ------------ |\n| patient1 | yes         |             |               |              |\n| patient2 |             | yes         |               |              |\n| partien3 |             |             | yes           |              |"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#우위성-검정",
    "href": "posts/md/Stat_clinical_trial.html#우위성-검정",
    "title": "임상 통계학",
    "section": "1.4 우위성 검정",
    "text": "1.4 우위성 검정\n우위성 검정 (Superiority Test) 은 임상시험에서 시험약이 대조약보다 더 나은 효과를 보임을 증명하기 위해 수행되는 통계적 검정입니다. 이 검정은 새로운 치료법이나 약물이 기존의 표준 치료법보다 효과적이라는 것을 입증하는 데 사용됩니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#우위성-검정의-주요-특징",
    "href": "posts/md/Stat_clinical_trial.html#우위성-검정의-주요-특징",
    "title": "임상 통계학",
    "section": "1.5 우위성 검정의 주요 특징",
    "text": "1.5 우위성 검정의 주요 특징\n\n목적: 시험약이 대조약보다 통계적으로 유의하게 더 나은 효과를 보이는지를 확인하는 것입니다. 이는 새로운 치료가 기존 치료보다 임상적으로 의미 있는 개선을 제공하는지를 평가합니다.\n설계: 일반적으로 무작위 대조시험 (Randomized Controlled Trial, RCT) 에서 수행되며, 두 그룹 간의 효과 차이를 비교합니다. 이 과정에서 통계적 유의성을 확보하기 위해 적절한 표본 크기와 설계가 필요합니다.\n결과 해석: 통계적으로 유의한 결과가 도출되면, 이는 시험약이 대조약보다 우수하다는 증거로 해석됩니다. 그러나 통계적 유의성이 임상적 유의성을 항상 보장하는 것은 아니므로, 임상적 의미도 함께 고려해야 합니다.\n\n우위성 검정은 신약 개발 및 임상시험에서 중요한 역할을 하며, 새로운 치료법의 도입을 위한 과학적 근거를 제공합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#동등성-검정",
    "href": "posts/md/Stat_clinical_trial.html#동등성-검정",
    "title": "임상 통계학",
    "section": "1.6 동등성 검정",
    "text": "1.6 동등성 검정\n동등성 검정 (Equivalence Test) 은 두 치료제 사이에 임상적으로 의미 있는 차이가 없음을 증명하기 위해 수행되는 통계적 검정입니다. 이는 주로 두 치료제가 서로 동등한 효과를 가진다는 것을 입증하려는 경우에 사용됩니다.\n\n목적: 두 치료제 간의 효과 차이가 사전에 정의된 허용 가능한 범위 내에 있음을 증명하는 것입니다. 이는 두 치료제가 임상적으로 동등한 효과를 나타낸다는 것을 보여주려는 것입니다.\n설계: 동등성 검정에서는 양측 신뢰구간을 사용하여 두 치료제 간의 차이가 사전에 설정된 동등성 경계 내에 있는지를 평가합니다. 신뢰구간이 동등성 경계 내에 완전히 포함되면 두 치료제가 동등하다고 결론지을 수 있습니다.6\n결과 해석: 동등성 검정의 결과는 두 치료제가 통계적으로 유의한 차이가 없다는 것을 의미하며, 이는 임상적으로도 의미 있는 차이가 없다는 것을 나타냅니다. 이는 새로운 치료제가 기존 치료제와 유사한 효과를 제공함을 입증하는 데 사용됩니다.\n\n동등성 검정은 제네릭 의약품의 승인이나 기존 치료제와의 비교 연구에서 자주 사용됩니다. 이를 통해 새로운 치료제의 효과가 기존 치료제와 동등함을 입증함으로써, 임상적 유용성을 확보할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#비열등성-검정",
    "href": "posts/md/Stat_clinical_trial.html#비열등성-검정",
    "title": "임상 통계학",
    "section": "1.7 비열등성 검정",
    "text": "1.7 비열등성 검정\n비열등성 검정 (Non-inferiority Test) 은 임상시험에서 실험적 치료가 대조약보다 열등하지 않음을 증명하기 위해 수행되는 통계적 검정입니다. 이는 특히 위약 대조가 비윤리적이거나 실용적이지 않은 경우에 사용되며, 시험약이 대조약과 비교하여 임상적으로 의미 있는 차이가 없음을 입증하려는 목적을 가집니다.\n\n목적: 비열등성 검정은 새로운 치료제가 기존의 활성 대조약보다 열등하지 않다는 것을 입증하는 데 목적이 있습니다. 이를 통해 새로운 치료제가 기존 치료와 유사한 효과를 제공하면서도 다른 장점 (예: 부작용 감소, 비용 절감 등) 을 가질 수 있음을 보여줍니다.\n설계: 비열등성 검정에서는 비열등성 마진 (non-inferiority margin) 을 사전에 정의해야 합니다. 이 마진은 두 치료제 간의 허용 가능한 최대 효과 차이를 나타내며, 신뢰구간이 이 마진 내에 있으면 비열등하다고 결론지을 수 있습니다.7\n결과 해석: 비열등성 검정의 결과는 실험적 치료가 대조약보다 통계적으로 유의하게 열등하지 않다는 것을 의미합니다. 이를 통해, 실험적 치료가 위약보다 효과적임을 간접적으로 증명할 수 있습니다.\n\n비열등성 검정은 특히 새로운 치료법이 기존 치료법과 유사한 효과를 가지면서도 다른 임상적 이점을 제공할 때 유용합니다. 이는 임상시험에서 중요한 설계 전략으로 활용됩니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#임상-시험에서-심사기관의-가장-큰-관심사",
    "href": "posts/md/Stat_clinical_trial.html#임상-시험에서-심사기관의-가장-큰-관심사",
    "title": "임상 통계학",
    "section": "1.8 임상 시험에서 심사기관의 가장 큰 관심사",
    "text": "1.8 임상 시험에서 심사기관의 가장 큰 관심사\n약효가 없는 신약을 허가해주는 것 (False positive; 제 1 종 오류) 이 약효가 있는 신약을 허가하지 않는 것 (제 2 종 오류) 보다 큰 문제이다. 따라서 임상 시험에서 제 1 종 오류의 비율이 5% 이하로 통제되었는지를 가장 관심 있게 본다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#차-유효성-평가-변수-primary-endpoint",
    "href": "posts/md/Stat_clinical_trial.html#차-유효성-평가-변수-primary-endpoint",
    "title": "임상 통계학",
    "section": "2.1 1 차 유효성 평가 변수 (Primary endpoint)",
    "text": "2.1 1 차 유효성 평가 변수 (Primary endpoint)\n1 차 유효성 평가 변수 (Primary Endpoint) 는 임상시험의 성공 여부를 판단하는 데 사용되는 주요 변수입니다. 이는 임상시험의 목표를 달성했는지를 평가하는 핵심 기준으로, 임상시험계획서에 명확하게 명시되어야 하며, 선택한 이유에 대한 설명도 포함되어야 합니다. 일반적으로 하나의 1 차 유효성 평가 변수가 설정되며, 이는 표본 크기를 계산하는 데도 사용될 수 있습니다. 이러한 변수는 임상시험의 결과를 해석하고, 시험약의 효과를 평가하는 데 중요한 역할을 합니다.\n\n목적: 임상시험의 주요 목표를 달성했는지를 평가하는 핵심 지표입니다. 이는 임상시험의 성공 여부를 판단하는 데 가장 중요한 변수로, 시험의 설계 단계에서 명확하게 정의되어야 합니다.\n특징: 일반적으로 하나의 1 차 유효성 평가 변수가 설정되며, 표본 크기 계산과 통계적 검정의 주요 기준으로 사용됩니다.\n예시: 암 치료제의 경우, 무진행 생존기간 (Progression-Free Survival) 이나 전체 생존기간 (Overall Survival) 이 1 차 유효성 평가 변수가 될 수 있습니다.\n\n\n2.1.1 1 차 유효성 평가 변수 (Primary Endpoint) 의 예시\n\n무진행 생존기간 (Progression-Free Survival, PFS): 이는 암 치료제 임상시험에서 자주 사용되는 1 차 유효성 평가 변수로, 환자가 질병의 진행 없이 생존한 기간을 측정합니다. PFS 는 치료의 효과를 평가하는 중요한 지표로 활용됩니다.10\n최적 전체 반응 (Best Overall Response, BOR): 이 변수는 치료 후 종양의 크기 변화나 반응을 평가하는 데 사용됩니다. BOR 은 암 치료의 효과를 정량화하는 데 중요한 역할을 합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#차-유효성-평가-변수-secondary-endpoint",
    "href": "posts/md/Stat_clinical_trial.html#차-유효성-평가-변수-secondary-endpoint",
    "title": "임상 통계학",
    "section": "2.2 2 차 유효성 평가 변수 (Secondary Endpoint)",
    "text": "2.2 2 차 유효성 평가 변수 (Secondary Endpoint)\n2 차 유효성 평가 변수 (Secondary Endpoint) 는 임상시험에서 보조적으로 시험약의 효과를 측정하는 변수입니다. 이는 1 차 유효성 평가 변수 외에 추가적인 정보를 제공하며, 치료의 부가적인 효과나 안전성을 평가하는 데 사용됩니다. 2 차 유효성 평가 변수는 임상시험의 보조적인 목표를 달성했는지를 평가하며 일반적으로 2 차 유효성 평가 변수는 다음과 같은 6 가지 유형으로 나뉩니다:\n\n증상 개선: 치료가 환자의 증상을 얼마나 개선하는지를 평가합니다.\n삶의 질: 치료가 환자의 전반적인 삶의 질에 미치는 영향을 측정합니다.\n부작용 발생률: 치료와 관련된 부작용의 빈도와 심각성을 평가합니다.\n생체표지자 변화: 치료가 특정 생체표지자에 미치는 영향을 측정합니다.\n경제적 평가: 치료의 비용 효과성을 평가합니다.\n장기적 효과: 치료의 장기적인 효과와 안전성을 평가합니다.\n\n\n목적: 1 차 유효성 평가 변수 외에 추가적인 정보를 제공하며, 치료의 부가적인 효과나 안전성을 평가하는 데 사용됩니다. 이는 임상시험의 보조적인 목표를 달성했는지를 평가합니다.\n특징: 여러 개의 2 차 유효성 평가 변수가 설정될 수 있으며, 1 차 유효성 평가 변수에 비해 상대적으로 중요도가 낮습니다. 그러나, 2 차 유효성 평가 변수는 치료의 전체적인 효과를 이해하는 데 중요한 역할을 합니다.\n예시: 위의 암 치료제 사례에서, 삶의 질 (Quality of Life) 이나 부작용 발생률 등이 2 차 유효성 평가 변수가 될 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#주요-편향-유형",
    "href": "posts/md/Stat_clinical_trial.html#주요-편향-유형",
    "title": "임상 통계학",
    "section": "3.1 주요 편향 유형",
    "text": "3.1 주요 편향 유형\n\n선정 편향 (Selection Bias): 연구 대상자를 선택하는 과정에서 발생하는 편향으로, 연구 결과가 모집단 전체를 대표하지 못하게 합니다. 무작위 배정 (randomization) 을 통해 최소화할 수 있습니다.\n정보 편향 (Information Bias): 데이터를 수집하거나 측정하는 과정에서 발생하는 오류로, 잘못된 정보가 수집되어 결과에 영향을 미칩니다. 맹검 (blinding) 과 표준화된 측정 방법을 통해 줄일 수 있습니다.\n관찰자 편향 (Observer Bias): 연구자가 결과를 해석하거나 측정할 때 주관적인 판단이 개입되는 경우 발생합니다. 이중 맹검 (double-blind) 설계를 통해 최소화할 수 있습니다.\n출판 편향 (Publication Bias): 긍정적인 결과가 부정적인 결과보다 더 자주 출판되는 경향이 있어, 연구 결과의 전반적인 해석에 영향을 미칩니다.\n회귀 편향 (Regression Bias): 극단적인 값이 평균으로 회귀하는 경향으로 인해 발생하는 편향입니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#편향-최소화-전략",
    "href": "posts/md/Stat_clinical_trial.html#편향-최소화-전략",
    "title": "임상 통계학",
    "section": "3.2 편향 최소화 전략",
    "text": "3.2 편향 최소화 전략\n\n무작위 배정: 연구 대상자를 무작위로 배정하여 선정 편향을 줄입니다.\n맹검: 연구자와 참가자가 어떤 치료를 받는지 모르게 하여 관찰자와 정보 편향을 줄입니다.\n표준화된 프로토콜: 데이터 수집과 측정 방법을 표준화하여 정보 편향을 줄입니다.\n완전한 데이터 보고: 긍정적, 부정적 결과 모두를 보고하여 출판 편향을 줄입니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#편향의-발생을-막는-임상-디자인",
    "href": "posts/md/Stat_clinical_trial.html#편향의-발생을-막는-임상-디자인",
    "title": "임상 통계학",
    "section": "3.3 편향의 발생을 막는 임상 디자인",
    "text": "3.3 편향의 발생을 막는 임상 디자인\n편향을 방지하기 위한 임상시험 디자인 방법 중 하나는 맹검 (blinding) 입니다. 맹검은 연구자가 의도치 않게 결과에 영향을 미치는 것을 방지하기 위해 피험자와 연구자 모두에게 특정 정보를 숨기는 방법입니다. 맹검에는 여러 유형이 있으며, 그 중 흔히 사용되는 두 가지는 다음과 같습니다:\n\n3.3.1 맹검\n\n단일 맹검 (Single-Blind)\n\n\n설명: 피험자만이 자신이 어떤 치료를 받고 있는지 모르는 경우입니다. 연구자나 시험자는 어떤 치료가 피험자에게 투여되는지 알고 있습니다.\n목적: 피험자가 자신이 받는 치료에 대해 알지 못하게 하여, 심리적 요인이나 기대가 결과에 미치는 영향을 최소화합니다.\n\n\n이중 맹검 (Double-Blind)\n\n\n설명: 피험자와 연구자 모두가 어떤 치료가 투여되고 있는지 모르는 경우입니다.\n목적: 피험자와 연구자 모두의 편향을 방지하여, 연구 결과의 객관성과 신뢰성을 높입니다. 이는 편향을 최소화하는 가장 강력한 방법 중 하나로 간주됩니다.11\n\n이러한 맹검 방법은 임상시험에서 편향을 줄이고, 결과의 타당성을 높이는 데 중요한 역할을 합니다.\n\n\n3.3.2 무작위 배정\n무작위 배정 (Randomization) 은 임상시험에서 편향을 줄이고 결과의 신뢰성을 높이기 위해 피험자를 무작위로 배정하는 방법입니다. 무작위 배정의 주요 유형과 각각의 특징은 다음과 같습니다:\n\n단순 무작위 배정 (Simple Randomization)\n\n\n설명: 난수발생기를 사용하여 피험자를 무작위로 배정하는 방법입니다.\n장점: 가장 기본적이고 이해하기 쉬운 방법입니다.\n단점: 불균등한 배정이 발생할 수 있는 가능성이 있어, 특히 작은 표본 크기에서 그룹 간 불균형이 생길 수 있습니다.\n\n\n블록 무작위 배정 (Block Randomization)\n\n\n설명: 피험자들을 몇 개의 블록으로 나누고, 각 블록 내에서 단순 무작위 배정을 수행하는 방법입니다.\n장점: 각 치료군에 균등한 수의 피험자가 배정되도록 보장하여, 시간 경과에 따른 외부 요인의 영향을 최소화할 수 있습니다. 이는 특히 중간 분석이 필요한 경우에 유리합니다.\n\n\n층화 무작위 배정 (Stratified Randomization)\n\n\n설명: 피험자들을 특정 특성 (예: 질병의 중증도, 연령 등) 에 따라 층으로 나눈 후, 각 층 내에서 블록 무작위 배정을 수행하는 방법입니다.\n장점: 중요한 인구통계학적 또는 임상적 특성에 따라 피험자를 균등하게 배정하여, 이러한 특성들이 결과에 미치는 영향을 최소화할 수 있습니다.\n단점: 층의 수가 많아질수록 복잡성이 증가하며, 각 층에 충분한 수의 피험자가 필요합니다. 이는 시험 설계와 관리에 있어 추가적인 부담이 될 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#검정력-power",
    "href": "posts/md/Stat_clinical_trial.html#검정력-power",
    "title": "임상 통계학",
    "section": "4.1 검정력 (Power)",
    "text": "4.1 검정력 (Power)\n\n정의: 검정력은 실제로 효과가 존재할 때, 연구가 그 효과를 발견할 수 있는 확률을 의미합니다. 이는 1 - 제 2종 오류 확률(β) 로 정의됩니다.\n의미: 검정력이 높을수록 연구가 실제 효과를 발견할 가능성이 높아집니다. 일반적으로 80% 이상의 검정력이 목표로 설정됩니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#제-1-종-오류와-제-2-종-오류",
    "href": "posts/md/Stat_clinical_trial.html#제-1-종-오류와-제-2-종-오류",
    "title": "임상 통계학",
    "section": "4.2 제 1 종 오류와 제 2 종 오류",
    "text": "4.2 제 1 종 오류와 제 2 종 오류\n\n제 1 종 오류 (Type I Error, α): 실제로 효과가 없는데도 효과가 있다고 잘못 결론 내리는 오류입니다. 일반적으로 α는 5% 이하로 설정됩니다.\n제 2 종 오류 (Type II Error, β): 실제로 효과가 있는데도 효과가 없다고 잘못 결론 내리는 오류입니다. β가 낮을수록 검정력이 높아집니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#제-1-종-오류와-제-2-종-오류-간의-트레이드-오프",
    "href": "posts/md/Stat_clinical_trial.html#제-1-종-오류와-제-2-종-오류-간의-트레이드-오프",
    "title": "임상 통계학",
    "section": "4.3 제 1 종 오류와 제 2 종 오류 간의 트레이드 오프",
    "text": "4.3 제 1 종 오류와 제 2 종 오류 간의 트레이드 오프\n\n트레이드 오프: 표본의 크기가 동일한 경우, 제 1 종 오류 (α) 를 줄이면 제 2 종 오류 (β) 가 증가하는 경향이 있습니다. 이는 연구 설계에서 두 오류 간의 균형을 맞추는 것이 중요함을 의미합니다.\n해결 방법: 이 트레이드 오프를 해결하기 위해서는 표본의 크기를 증가시키는 것이 일반적인 방법입니다. 표본 크기를 늘리면 제 2 종 오류를 줄여 검정력을 높일 수 있습니다. 다시 말해 임상 시험에서 제 1 종 오류는 5% 이하로 고정되어 있기 때문에 제 2 종 오류를 줄여서 검정력을 높이기 위해서는 표본의 크기를 늘리는 수 밖에 없다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#표본의-크기를-늘리지-않고-검정력을-높이는-방법",
    "href": "posts/md/Stat_clinical_trial.html#표본의-크기를-늘리지-않고-검정력을-높이는-방법",
    "title": "임상 통계학",
    "section": "4.4 표본의 크기를 늘리지 않고 검정력을 높이는 방법",
    "text": "4.4 표본의 크기를 늘리지 않고 검정력을 높이는 방법\n표본의 크기를 늘리지 않고 검정력을 높이는 방법에는 다음은 두 가지 방법이 있습니다.\n\n환자 선정을 엄격하게 해서 동질적인 환자를 모집:\n\n설명: 연구에 포함되는 환자군을 더 동질적으로 만들기 위해 엄격한 선정 기준을 적용합니다. 이는 환자 간 변동성을 줄여 결과의 변동성을 감소시킵니다.\n장점: 동질적인 환자군을 통해 통계적 검정력이 높아질 수 있습니다.\n단점: 환자 모집에 시간이 더 걸릴 수 있으며, 신약의 적응증이 줄어들 수 있습니다.\n\n시험약의 약효 크기를 높임:\n\n설명: 시험약의 효과 크기를 증가시키면, 통계적으로 유의한 결과를 얻을 가능성이 높아집니다.\n장점: 더 큰 효과 크기는 검정력을 증가시킵니다.\n단점: 임상 시험이 이미 진행 중인 경우에는 이 방법을 적용하기 어려울 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#허가-기관에서-권장하는-검정력",
    "href": "posts/md/Stat_clinical_trial.html#허가-기관에서-권장하는-검정력",
    "title": "임상 통계학",
    "section": "4.5 허가 기관에서 권장하는 검정력",
    "text": "4.5 허가 기관에서 권장하는 검정력\n허가 기관에서 권장하는 검정력은 일반적으로 80% 에서 90% 사이입니다. ICH E9 가이드라인에 따르면 이정도 수준의 검정력은 임상시험의 의미 있는 결과를 도출하는 데 필수적입니다.12"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#표본의-크기를-결정하는-방법",
    "href": "posts/md/Stat_clinical_trial.html#표본의-크기를-결정하는-방법",
    "title": "임상 통계학",
    "section": "4.6 표본의 크기를 결정하는 방법",
    "text": "4.6 표본의 크기를 결정하는 방법\n표본의 크기를 결정하는 방법은 임상시험의 설계에서 중요한 요소로, 1 차 유효성 평가 변수에 의해 크게 영향을 받습니다. 특히, 통계 분석법이 two-sample t-test 인 경우, 표본 크기 결정은 다음과 같은 요소들을 고려합니다:\n\n효과 크기 (Effect Size): 두 그룹 간의 차이를 측정하는 데 사용됩니다. 효과 크기가 클수록 필요한 표본 크기는 작아질 수 있습니다.\n유의 수준 (Significance Level, α): 일반적으로 5% 로 설정되며, 이는 제 1 종 오류의 허용 확률을 의미합니다.\n검정력 (Power, 1-β): 일반적으로 80% 에서 90% 로 설정되며, 이는 실제 효과가 있을 때 이를 발견할 수 있는 확률입니다.\n표준 편차 (Standard Deviation): 모집단의 변동성을 나타내며, 변동성이 클수록 더 많은 표본이 필요합니다.\n\n이러한 요소들은 표본 크기를 결정하는 데 중요한 역할을 하며, 연구의 목표와 설계에 따라 신중하게 고려되어야 합니다. 표본 크기를 적절히 설정함으로써 연구의 신뢰성과 타당성을 높일 수 있습니다.\n1 차 유효성 평가 변수에 의해 결정된다. 통계 분석법이 two-sample t-test 인 경우는 다음과 같다.\n\\[ n = \\frac{Z_{alpha/2} + Z_{beta} * 2 * \\delta^2}{ (\\mu_1 - \\mu_2)^2}\\]"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#표본의-크기-계산에-사용된-값의-정확도",
    "href": "posts/md/Stat_clinical_trial.html#표본의-크기-계산에-사용된-값의-정확도",
    "title": "임상 통계학",
    "section": "4.7 표본의 크기 계산에 사용된 값의 정확도",
    "text": "4.7 표본의 크기 계산에 사용된 값의 정확도\n표본의 크기를 계산할 때 사용되는 값들은 일반적으로 임상시험이 끝나야 정확히 알 수 있는 미지의 값이기 때문에, 초기에는 추정치를 사용하여 계산합니다. 이러한 추정치는 과거의 연구 데이터나 파일럿 연구의 결과를 바탕으로 추정됩니다. 이로 인해, 표본 크기 계산은 항상 어느 정도의 불확실성을 내포하고 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#그룹간-표본-크기",
    "href": "posts/md/Stat_clinical_trial.html#그룹간-표본-크기",
    "title": "임상 통계학",
    "section": "4.8 그룹간 표본 크기",
    "text": "4.8 그룹간 표본 크기\n\n동일한 표본 크기: 두 그룹 간의 표본 크기를 동일하게 설정하면, 통계적 검정력이 최대화됩니다. 이는 두 그룹의 비교가 더 균형 잡히고, 결과의 해석이 용이해지기 때문입니다.\n비대칭 표본 크기: 실질적인 이유 (예: 모집의 어려움, 비용 제한 등) 로 인해 두 그룹의 표본 크기가 동일하지 않을 수 있습니다. 비대칭 표본 크기는 연구의 검정력을 감소시킬 수 있으며, 이를 보완하기 위해서는 더 많은 총 표본이 필요할 수 있습니다.\n\n결론적으로, 표본 크기를 설계할 때는 가능한 한 두 그룹의 크기를 동일하게 유지하는 것이 이상적입니다. 그러나, 연구의 실질적인 제약을 고려하여 비대칭적인 크기를 사용할 수도 있으며, 이 경우 검정력에 미치는 영향을 충분히 고려해야 합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#임상-시험이-실패하는-이유",
    "href": "posts/md/Stat_clinical_trial.html#임상-시험이-실패하는-이유",
    "title": "임상 통계학",
    "section": "4.9 임상 시험이 실패하는 이유",
    "text": "4.9 임상 시험이 실패하는 이유\n임상시험이 실패하는 이유는 다양하며 다음은 임상시험 실패의 일반적인 원인들입니다:\n\n시험약의 효과가 감소되는 편향이 발생한 경우: 연구 설계나 실행 과정에서 편향이 발생하면 시험약의 실제 효과가 왜곡되어 나타날 수 있습니다.\n시험약의 용량 선정이 잘못된 경우: 적절한 용량을 선택하지 못하면, 약효가 충분히 나타나지 않거나 안전성 문제가 발생할 수 있습니다. 용량이 너무 낮으면 약효가 줄어들고, 너무 높으면 부작용이 증가할 수 있습니다.\n검정력이 부족한 경우: 충분한 표본 크기를 확보하지 못하면 검정력이 부족하게 되어, 실제로 효과가 있는 경우에도 이를 발견하지 못할 수 있습니다.\n환자 모집단을 잘못 선택한 경우: 시험약이 특정 환자군에서만 효과가 있을 수 있는데, 잘못된 모집단을 선택하면 약효를 제대로 평가하지 못할 수 있습니다.\n1 차 유효성 평가 변수를 잘못 선택한 경우: 적절한 1 차 유효성 평가 변수를 선택하지 못하면, 시험의 주요 목표를 정확히 평가하지 못할 수 있습니다.\n우연에 의해 실패한 경우: 검정력 80% 라는 의미는, 실제로 효과가 있을 때 이를 발견할 확률이 80% 라는 뜻이며, 이는 우연에 의해 실패할 확률이 20% 임을 의미합니다.\n시험약의 약효가 없는 경우: 시험약 자체에 효과가 없으면 임상시험은 실패할 수밖에 없습니다. 이는 시험약의 기전이 잘못 이해되었거나, 예상과 달리 효과가 없는 경우에 해당합니다.\n\n이러한 요인들은 임상시험 설계와 실행 단계에서 신중하게 고려되어야 하며, 각 요인을 최소화하기 위한 전략이 필요합니다. 이를 통해 임상시험의 성공 가능성을 높일 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#기관별-약효의-크기",
    "href": "posts/md/Stat_clinical_trial.html#기관별-약효의-크기",
    "title": "임상 통계학",
    "section": "5.1 기관별 약효의 크기",
    "text": "5.1 기관별 약효의 크기\n다기관 임상시험에서 약효가 존재한다고 판단되면, 각 기관별로 약효의 크기에 대한 교호작용 검정을 수행합니다. 이는 각 기관에서의 약효가 일관되게 나타나는지를 평가하는 과정입니다. 교호작용 검정은 특정 기관에서의 약효 차이가 시험의 전반적인 결과에 미치는 영향을 평가해 결과의 해석에 중요한 역할을 합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#교호-작용-검정",
    "href": "posts/md/Stat_clinical_trial.html#교호-작용-검정",
    "title": "임상 통계학",
    "section": "5.2 교호 작용 검정",
    "text": "5.2 교호 작용 검정\n교호작용 검정은 통계학적으로 두 변수 간의 상호작용이 존재하는지를 평가하는 방법입니다. 통계학적으로 교호작용이 존재한다면, 이는 두 가지 종류로 나뉩니다:\n\n양적인 교호작용: 두 변수 간의 상호작용이 효과의 크기에 영향을 미치지만, 효과의 방향은 동일하게 유지됩니다. 예를 들어, 약물 A 와 B 가 함께 사용될 때 효과가 더 커지지만, 두 약물이 모두 긍정적인 효과를 나타내는 경우입니다.\n질적인 교호작용: 두 변수 간의 상호작용이 효과의 방향을 변화시킵니다. 예를 들어, 약물 A 가 단독으로는 긍정적인 효과를 나타내지만, 약물 B 와 함께 사용될 때는 부정적인 효과를 나타내는 경우입니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#임상-설계-단계에서-보정",
    "href": "posts/md/Stat_clinical_trial.html#임상-설계-단계에서-보정",
    "title": "임상 통계학",
    "section": "6.1 임상 설계 단계에서 보정",
    "text": "6.1 임상 설계 단계에서 보정\n층화 무작위 배정:14 - 공변량에 대해 층화하여 무작위 배정을 실시합니다. 이는 치료군과 대조군 간 공변량 분포의 불균형을 줄여 편향을 감소시키며 검정력 향상에도 기여합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#분석-단계에서-보정",
    "href": "posts/md/Stat_clinical_trial.html#분석-단계에서-보정",
    "title": "임상 통계학",
    "section": "6.2 분석 단계에서 보정",
    "text": "6.2 분석 단계에서 보정\n기저치 공변량 보정을 주 분석에서 실시했는지 명확히 기술해야 합니다. 탐색적 분석의 결과는 신중하게 해석해야 하며, 이에 근거한 결론은 일반적으로 받아들여지기 어렵습니다. 1 차 유효성 평가변수의 형태에 따라 다른 통계적 방법을 사용합니다.\n\n연속형 변수: 공분산분석 (ANCOVA) 을 사용합니다.\n이진형 또는 범주형 변수: 로지스틱 회귀분석을 적용합니다.\n시간형 변수 (생존시간): Cox 비례위험모형 (proportional hazard mode) 을 활용합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#보정할-기저치-공변량의-기준",
    "href": "posts/md/Stat_clinical_trial.html#보정할-기저치-공변량의-기준",
    "title": "임상 통계학",
    "section": "6.3 보정할 기저치 공변량의 기준",
    "text": "6.3 보정할 기저치 공변량의 기준\n\n1 차 유효성 평가변수와의 상관관계: 1 차 유효성 평가변수와 상관관계가 있는 기저치 공변량을 선택하는 것이 중요합니다.15\n층화 무작위배정 변수: 층화 무작위배정에 사용된 변수는 보정 대상으로 고려해야 합니다.\n무작위 배정 이전 변수: 무작위 배정 이후에 관찰되는 변수는 보정 대상에서 제외해야 합니다.\n교호작용 부재: 교호작용이 존재하지 않는 변수를 선택하는 것이 바람직합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#무작위-배정-후-관측된-불균형-기저치-공변량-보정",
    "href": "posts/md/Stat_clinical_trial.html#무작위-배정-후-관측된-불균형-기저치-공변량-보정",
    "title": "임상 통계학",
    "section": "6.4 무작위 배정 후 관측된 불균형 기저치 공변량 보정",
    "text": "6.4 무작위 배정 후 관측된 불균형 기저치 공변량 보정\n원칙적으로 무작위 배정 후 관측된 불균형 기저치 공변량은 보정하지 않는 것이 바람직합니다. 이에 대한 근거는 다음과 같습니다:\n\n편향 가능성: 사후에 관찰된 불균형을 보정하는 것은 새로운 편향을 도입할 수 있습니다.16\n통계적 타당성: 원래 사전에 명시되지 않은 공변량을 무작위배정 이후에 보정하는 것은 통계적으로 부적절할 수 있습니다.\n연구 계획의 중요성: 임상시험 계획서의 통계 부분에서 주 분석에 포함될 임상시험대상자 집단을 사전에 정의하는 것이 중요합니다.\n객관성 유지: 무작위 배정 후 관찰된 불균형을 보정하면 연구의 객관성이 훼손될 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#itt-intention-to-treat-분석-16",
    "href": "posts/md/Stat_clinical_trial.html#itt-intention-to-treat-분석-16",
    "title": "임상 통계학",
    "section": "7.1 ITT (Intention to Treat) 분석 17",
    "text": "7.1 ITT (Intention to Treat) 분석 17\n\n무작위 배정된 모든 참가자를 원래 배정된 그룹대로 분석에 포함시킵니다.\n프로토콜 위반, 중도 탈락, 치료 비순응 등에 관계없이 모든 참가자를 포함합니다.\n실제 임상 현장의 상황을 더 잘 반영하여 치료의 실제 효과를 추정할 수 있습니다.\n무작위 배정의 이점을 유지하고 편향을 최소화합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#pp-per-protocol-분석-17",
    "href": "posts/md/Stat_clinical_trial.html#pp-per-protocol-분석-17",
    "title": "임상 통계학",
    "section": "7.2 PP (Per Protocol) 분석 18",
    "text": "7.2 PP (Per Protocol) 분석 18\nPP(Per Protocol) 분석은 임상시험에서 중요한 분석 방법 중 하나입니다. PP 분석의 주요 특징은 다음과 같습니다:\n\nPP 분석은 full analysis set(전체 분석 집합) 의 부분집합으로 임상시험계획서를 보다 잘 준수한 피험자들만을 포함하는 분석 집합입니다.\n주요 프로토콜 위반, 치료 비순응, 그룹 전환, 측정 누락 등의 사유로 제외된 피험자들은 분석에서 제외됩니다 19\n이상적인 조건에서의 치료 효과를 식별하는 것이 목적이며,” 환자들이 완전히 순응할 경우 효과가 어떻게 나타나는가?” 라는 질문에 답하고자 합니다.20"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#itt-분석과의-차이",
    "href": "posts/md/Stat_clinical_trial.html#itt-분석과의-차이",
    "title": "임상 통계학",
    "section": "7.3 ITT 분석과의 차이",
    "text": "7.3 ITT 분석과의 차이\n\nPP 분석은 치료를 완료하지 않은 환자들을 제외하므로 치료 차이를 더 잘 반영할 수 있습니다.\nITT 분석이 두 치료법을 비슷하게 보이게 하는 경향이 있는 반면, PP 분석은 치료 차이를 더 명확히 보여줄 수 있습니다.\nITT 는 보수적인 추정치를 제공하지만, 실제 임상 상황을 더 잘 반영합니다.\nPP 는 이상적인 조건에서의 효과를 보여주지만, 선택 편향의 위험이 있습니다.\nPP 분석은 임상시험 결과를 해석하는 데 중요한 역할을 하지만, 선택 편향의 위험이 있으므로 ITT 분석과 함께 고려되어야 합니다. 특히 비열등성 시험에서는 ITT 와 PP 분석 모두가 중요하며, 두 분석 결과가 유사한 결론을 도출할 때 연구 결과의 신뢰성이 높아집니다.\nITT 가 일반적으로 우선적으로 권장되는 분석 방법입니다.21"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#임상시험-자료가-불완전해지는-이유",
    "href": "posts/md/Stat_clinical_trial.html#임상시험-자료가-불완전해지는-이유",
    "title": "임상 통계학",
    "section": "7.4 임상시험 자료가 불완전해지는 이유",
    "text": "7.4 임상시험 자료가 불완전해지는 이유\n임상시험에서 자료가 불완전해지는 이유는 다양하지만 크게 4 가지 주요 이유에 대해 자세히 살펴보겠습니다:\n\n7.4.1 선정기준 위반\n\n일부 피험자가 연구 시작 후 선정기준에 부합하지 않는 것으로 밝혀질 수 있습니다.\n이는 초기 스크리닝 과정에서의 오류나 피험자 상태의 변화로 인해 발생할 수 있습니다.\n이러한 경우, 해당 피험자의 데이터를 포함시킬지 여부에 대한 결정이 필요합니다.\n\n\n\n7.4.2 윤리적 사유 또는 단순 실수로 인한 치료법 변경\n\n피험자의 안전이나 윤리적 고려사항으로 인해 원래 계획된 치료법을 변경해야 할 수 있습니다.\n의료진의 실수로 인해 잘못된 치료가 제공될 수도 있습니다.\n이러한 변경은 데이터의 일관성을 해치고 결과 해석을 복잡하게 만들 수 있습니다.\n\n\n\n7.4.3 임상시험계획서 위반 또는 낮은 순응도\n\n일부 피험자가 임상시험계획서를 정확히 따르지 않거나 처방된 치료에 대한 순응도가 낮을 수 있습니다.\n이는 약물 복용 누락, 방문 일정 미준수, 금지된 병용 약물 사용 등의 형태로 나타날 수 있습니다.\n이러한 위반은 치료 효과의 정확한 평가를 어렵게 만듭니다.\n\n\n\n7.4.4 1 차 유효성 평가변수의 결측치 발생\n\n피험자가 추적 관찰에서 이탈하거나, 특정 검사를 거부하거나, 데이터 수집 과정에서 오류가 발생할 수 있습니다.\n결측치는 통계 분석의 정확성과 신뢰성을 저하시킬 수 있습니다.\n결측 데이터 처리 방법 (예: 다중 대체법, 마지막 관찰값 전달법 등) 의 선택이 중요해집니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#다중-검정의-문제",
    "href": "posts/md/Stat_clinical_trial.html#다중-검정의-문제",
    "title": "임상 통계학",
    "section": "8.1 다중 검정의 문제",
    "text": "8.1 다중 검정의 문제\n\n8.1.1 Familywise Type I 오류율 증가\n\n개별 검정의 유의수준을 5% 로 설정하더라도, 여러 검정을 수행하면 전체적인 Type I 오류율 (familywise error rate) 이 5% 이상으로 증가합니다.\n이는 실제로 효과가 없는 경우에도 우연히 유의한 결과를 얻을 확률이 높아짐을 의미합니다.\n\n\n\n8.1.2 거짓 양성 결과 증가\n\n검정 횟수가 증가할수록 최소한 하나의 거짓 양성 결과를 얻을 확률이 높아집니다.\n예를 들어, 20 개의 독립적인 검정을 5% 유의수준에서 수행할 경우, 최소 하나의 거짓 양성 결과를 얻을 확률은 64% 에 달합니다.\n\n\n\n8.1.3 연구 결과의 신뢰성 저하\n\n다중 검정으로 인해 우연히 유의한 결과를 얻을 가능성이 높아지면, 연구 결과의 전반적인 신뢰성이 저하됩니다.\n\n\n\n8.1.4 임상적 영향\n\n약효가 없는 약물이 시판 허가를 받을 확률이 증가합니다.\n이는 환자 안전과 공중 보건에 부정적인 영향을 미칠 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#다중-검정-문제의-해결-방법",
    "href": "posts/md/Stat_clinical_trial.html#다중-검정-문제의-해결-방법",
    "title": "임상 통계학",
    "section": "8.2 다중 검정 문제의 해결 방법",
    "text": "8.2 다중 검정 문제의 해결 방법\n다중 검정 문제를 해결하는 여러 방법과 그 장단점에 대해 설명하겠습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#bonferroni-method",
    "href": "posts/md/Stat_clinical_trial.html#bonferroni-method",
    "title": "임상 통계학",
    "section": "8.3 Bonferroni method",
    "text": "8.3 Bonferroni method\n장점: - 간단하고 직관적인 방법 - FWER(Family-Wise Error Rate) 을 효과적으로 통제\n단점: - 매우 보수적인 방법으로, 가설의 수가 많아지면 개별 검정의 유의수준이 작아져 귀무가설을 잘 기각하지 못함 22 - Type II 오류 (false negative) 가 증가하여 검정력이 감소함"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#holm-method",
    "href": "posts/md/Stat_clinical_trial.html#holm-method",
    "title": "임상 통계학",
    "section": "8.4 Holm method",
    "text": "8.4 Holm method\n장점: - Bonferroni 방법보다 덜 보수적 - FWER 을 통제하면서도 더 많은 귀무가설을 기각시킬 수 있음 - Type II 오류를 줄이고 검정력을 증가시킴 23\n단점: - Bonferroni 방법에 비해 계산이 복잡함"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#hochberg-method",
    "href": "posts/md/Stat_clinical_trial.html#hochberg-method",
    "title": "임상 통계학",
    "section": "8.5 Hochberg method",
    "text": "8.5 Hochberg method\n이 방법은 Holm 방법과 유사하지만 역순으로 진행됩니다.\n장점: - Holm 방법보다 더 강력한 검정력을 제공할 수 있음 - FWER 을 통제하면서도 더 많은 유의한 결과를 찾을 수 있음\n단점: - 특정 조건 (검정 통계량들이 독립적이거나 양의 의존성을 가질 때) 에서만 FWER 을 정확히 통제함"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#prospective-alpha-allocation-scheme",
    "href": "posts/md/Stat_clinical_trial.html#prospective-alpha-allocation-scheme",
    "title": "임상 통계학",
    "section": "8.6 Prospective alpha allocation scheme",
    "text": "8.6 Prospective alpha allocation scheme\n이 방법은 사전에 각 가설에 대한 유의수준을 할당합니다.\n장점: - 연구자가 각 가설의 중요도에 따라 유의수준을 조정할 수 있음 - 유연한 접근 방식 제공\n단점: - 주관적인 판단이 개입될 수 있음 - 사전 계획이 필요하여 유연성이 제한될 수 있음"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#the-fixed-sequence-method",
    "href": "posts/md/Stat_clinical_trial.html#the-fixed-sequence-method",
    "title": "임상 통계학",
    "section": "8.7 The fixed-sequence method",
    "text": "8.7 The fixed-sequence method\n이 방법은 사전에 정해진 순서대로 가설을 검정합니다.\n장점: - 중요한 가설을 우선적으로 검정할 수 있음 - FWER 을 효과적으로 통제함\n단점: - 순서가 고정되어 있어 유연성이 떨어짐 - 초기 가설이 기각되지 않으면 후속 가설을 검정할 수 없음"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#the-fallback-method",
    "href": "posts/md/Stat_clinical_trial.html#the-fallback-method",
    "title": "임상 통계학",
    "section": "8.8 The fallback method",
    "text": "8.8 The fallback method\n이 방법은 fixed-sequence method 의 변형으로, 이전 가설이 기각되지 않아도 다음 가설을 검정할 수 있습니다.\n장점: - Fixed-sequence method 보다 유연함 - 모든 가설에 대해 검정 기회를 제공함\n단점: - 계산이 복잡할 수 있음 - 초기 가설의 중요성이 여전히 강조됨"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#gate-keeping",
    "href": "posts/md/Stat_clinical_trial.html#gate-keeping",
    "title": "임상 통계학",
    "section": "8.9 Gate keeping",
    "text": "8.9 Gate keeping\n이 방법은 가설을 그룹으로 나누고, 특정 그룹의 가설이 기각되어야 다음 그룹의 가설을 검정할 수 있게 합니다.\n장점: - 구조화된 접근 방식으로 가설의 우선순위를 반영할 수 있음 - FWER 을 효과적으로 통제함\n단점: - 복잡한 구조를 설계해야 함 - 초기 그룹의 가설이 기각되지 않으면 후속 그룹의 가설을 검정할 수 없음\n이러한 다양한 방법들은 각각의 장단점이 있으며, 연구 상황과 목적에 따라 적절한 방법을 선택해야 합니다.\n다중 검정 문제를 해결하기 위한 여러 방법이 있습니다:\n\nBonferroni 교정: 유의수준을 검정 횟수로 나누어 조정합니다.\nHolm-Bonferroni 방법: 단계적으로 유의수준을 조정합니다.\nFalse Discovery Rate (FDR) 제어: 거짓 발견 비율을 제어합니다.\n사전에 주요 결과 변수 지정: 분석 계획 단계에서 주요 결과 변수를 미리 지정합니다.\n통합 검정 방법 사용: 여러 결과를 하나의 통계량으로 통합하여 분석합니다.\n\n\n8.9.1 다중 검정 문제를 해결하는 방법과 각각의 장단점\n\nBonferroni method: 장단점?\nHolm method: 장단점?\nHochberg method:\nProspective alpha allocation scheme\nThe fixed-sequence method\nThe fallback method\nGate keeping"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#확증적-하위군-분석",
    "href": "posts/md/Stat_clinical_trial.html#확증적-하위군-분석",
    "title": "임상 통계학",
    "section": "9.1 확증적 하위군 분석",
    "text": "9.1 확증적 하위군 분석\n확증적 하위군 분석은 매우 드문 경우에 수행되며, 다음 두 가지 조건이 임상시험 계획서에 명확히 명시되어 있어야 합니다:\n\n분석할 하위군이 사전에 정의되어 있어야 함\n다중성 문제를 해결하기 위한 통계적 방법이 명시되어 있어야 함\n\n이러한 조건을 충족시키는 확증적 하위군 분석은 신뢰성 있는 결과를 제공할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#탐색적-하위군-분석",
    "href": "posts/md/Stat_clinical_trial.html#탐색적-하위군-분석",
    "title": "임상 통계학",
    "section": "9.2 탐색적 하위군 분석",
    "text": "9.2 탐색적 하위군 분석\n탐색적 하위군 분석은 다음과 같은 특징을 가집니다:\n\n시험약의 효능 존재 여부를 직접적으로 검증하지 않습니다.\n주로 치료제와 공변량 사이의 교호작용 존재 여부를 판단하는 데 중점을 둡니다.\n이 분석은 향후 연구를 위한 가설을 생성하거나 특정 하위군에서의 치료 효과를 탐색하는 데 유용합니다.\n\n탐색적 하위군 분석의 결과는 주의해서 해석해야 하며, 일반적으로 확증적 증거로 간주되지 않습니다 [1]."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#주의사항",
    "href": "posts/md/Stat_clinical_trial.html#주의사항",
    "title": "임상 통계학",
    "section": "9.3 주의사항",
    "text": "9.3 주의사항\n하위군 분석, 특히 탐색적 분석을 수행할 때는 다음 사항에 주의해야 합니다:\n\n다중성 문제: 여러 하위군을 동시에 분석할 경우 제 1 종 오류 (false positive) 가능성이 증가합니다.\n검정력 감소: 하위군으로 나누면 각 그룹의 표본 크기가 작아져 통계적 검정력이 감소할 수 있습니다.\n해석의 주의: 특히 탐색적 분석 결과는 추가 연구의 필요성을 제시하는 정도로 해석해야 합니다 [1]."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#assay-sensitivity-의-개념",
    "href": "posts/md/Stat_clinical_trial.html#assay-sensitivity-의-개념",
    "title": "임상 통계학",
    "section": "10.1 Assay sensitivity 의 개념",
    "text": "10.1 Assay sensitivity 의 개념\n변별력있는 시험. 과거 임상 결과의 재현성에 대한 문제를 피하기 위한 선행요건. 3 가지 간접적인 방법이 있다.\n\nHESDE\n현재 비열등성 임상과 과거 임상의 유효성 입증\n현재 비열등성 임상을 높은 수준으로 관리\n\n\n과거 임상시험의 결과가 재현되지 않을 수도 있는가? 그렇다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#비열등성-마진을-결정하는-방법",
    "href": "posts/md/Stat_clinical_trial.html#비열등성-마진을-결정하는-방법",
    "title": "임상 통계학",
    "section": "10.2 비열등성 마진을 결정하는 방법",
    "text": "10.2 비열등성 마진을 결정하는 방법\nFDA 권고에 따르면 통계적 마진과 임상적 마진이 있다. 1. 통계적 마진 2. 임상적 마진: 통계적 마진보다 작아야 한다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#비열등성-시험은-보통-2-개의-그룹이지만-위약을-쓸-수-있다면-3-개의-그룹으로도-진행할-수-있다",
    "href": "posts/md/Stat_clinical_trial.html#비열등성-시험은-보통-2-개의-그룹이지만-위약을-쓸-수-있다면-3-개의-그룹으로도-진행할-수-있다",
    "title": "임상 통계학",
    "section": "10.3 비열등성 시험은 보통 2 개의 그룹이지만 위약을 쓸 수 있다면 3 개의 그룹으로도 진행할 수 있다",
    "text": "10.3 비열등성 시험은 보통 2 개의 그룹이지만 위약을 쓸 수 있다면 3 개의 그룹으로도 진행할 수 있다\n비열등성 임상시험은 위약군 없이 진행되며, 두 가지 주요 목적이 있습니다:\n\n시험약이 대조약보다 나쁘지 않음을 증명\n시험약이 위약보다 약효가 우위에 있음을 보여 시판허가를 받는 것 (더 중요한 목적)"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#assay-sensitivity-의-개념-1",
    "href": "posts/md/Stat_clinical_trial.html#assay-sensitivity-의-개념-1",
    "title": "임상 통계학",
    "section": "10.4 Assay sensitivity 의 개념",
    "text": "10.4 Assay sensitivity 의 개념\nAssay sensitivity 는 변별력 있는 시험을 의미하며, 과거 임상 결과의 재현성 문제를 피하기 위한 선행요건입니다. 이를 확보하기 위한 3 가지 간접적인 방법이 있습니다:\n\nHESDE (Historical Evidence of Sensitivity to Drug Effects): 과거의 임상시험 데이터를 활용하여 약물 효과에 대한 민감도를 평가하는 방법.\n현재 비열등성 임상과 과거 임상의 유효성 입증: 현재 진행 중인 비열등성 임상시험과 과거 임상시험 결과를 비교하는 접근법.\n현재 비열등성 임상을 높은 수준으로 관리: 현재 진행 중인 비열등성 임상시험의 품질을 높이는 데 중점\n\n\n가장 주의할 점은 과거 임상시험의 결과가 다시 재현되지 않을 수도 있다는 것입니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#비열등성-마진-결정-방법",
    "href": "posts/md/Stat_clinical_trial.html#비열등성-마진-결정-방법",
    "title": "임상 통계학",
    "section": "10.5 비열등성 마진 결정 방법",
    "text": "10.5 비열등성 마진 결정 방법\nFDA 권고에 따르면 비열등성 마진을 결정하는 데 두 가지 접근 방식이 있습니다:\n\n통계적 마진: 과거 임상시험 데이터를 기반으로 통계적 분석을 통해 결정.\n임상적 마진: 마진은 임상적 중요성과 실용성을 고려하여 결정되며, 통계적 마진보다 작아야 합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#비열등성-시험에서-주의할-점",
    "href": "posts/md/Stat_clinical_trial.html#비열등성-시험에서-주의할-점",
    "title": "임상 통계학",
    "section": "10.6 비열등성 시험에서 주의할 점",
    "text": "10.6 비열등성 시험에서 주의할 점\n\n비열등성 시험에서는 눈가림 (blinding) 이 우월성 시험만큼 편향을 방지하지 못할 수 있습니다.\n비열등성 마진을 명시하는 것은 매우 중요하며, 임상시험 계획서에 명확히 정의되어야 합니다.\n통계적 처벌 없이 동일한 임상시험에서 비열등성과 우월성을 모두 평가할 수 있습니다. 따라서 가능하다면 임상시험 계획서에 두 가지 가설을 모두 포함하는 것이 좋습니다.24\n비열등성 시험은 일반적으로 2 개의 그룹 (시험약과 대조약) 으로 진행됩니다. 그러나 윤리적으로 허용되고 과학적으로 필요한 경우, 위약을 포함한 3 개의 그룹으로도 진행할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#결측치-제거의-문제점",
    "href": "posts/md/Stat_clinical_trial.html#결측치-제거의-문제점",
    "title": "임상 통계학",
    "section": "11.1 결측치 제거의 문제점",
    "text": "11.1 결측치 제거의 문제점\n단순히 결측치를 제거하는 것은 다음과 같은 심각한 문제를 야기할 수 있습니다:\n\nITT(Intent-to-Treat) 원칙 위배: 모든 무작위 배정된 대상자를 분석에 포함해야 함, 결측치 제거는 이 원칙을 위배하여 임상시험의 무작위화 이점을 손상시킴\n검정력 감소: 표본 크기 감소로 인한 통계적 검정력 저하, 따라서 중요한 치료 효과를 발견하지 못할 위험 증가.\n편향 발생: 결측 데이터가 무작위가 아닌 경우, 남은 데이터가 전체 모집단을 대표하지 못할 수 있으며 따라서 치료 효과 추정치의 왜곡 가능성이 증가.\n\n\n\n\n\n\n\nNote\n\n\n\nComplete Case Analysis(완전 사례 분석) 는 SAS 에서 가장 간단하게 사용할 수 있는 결측치 처리 방법입니다. FDA 에서 요구하는 기본적인 분석 방법으로 PROC 문에서 NOMISS 옵션을 사용해 사용 가능합니다. 결측치가 있는 관측치를 모두 제외하고 분석하는 것이며 단점으로는 데이터 손실로 인한 검정력 감소와 편향 가능성이 증가합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#결측치의-분류",
    "href": "posts/md/Stat_clinical_trial.html#결측치의-분류",
    "title": "임상 통계학",
    "section": "11.2 결측치의 분류",
    "text": "11.2 결측치의 분류\n결측치의 유형을 정확히 파악하는 것은 어렵지만, 적절한 처리 방법을 선택하는 데 중요합니다. 결측치는 발생 메커니즘에 따라 3 가지로 분류됩니다:\n\nMCAR (Missing Completely At Random): 결측 발생이 완전히 무작위적이며 구별하기 쉽지만 실제 발생 비율은 매우 낮음. 예: 실험실 샘플의 우발적인 파손\nMAR (Missing At Random): 결측 발생이 관찰된 다른 변수와 관련되어 있지만, 결측된 변수 자체의 값과는 무관. 예: 특정 연령대에서 설문 응답률이 낮은 경우\nMNAR (Missing Not At Random): 결측 발생이 결측된 변수의 값 자체와 관련, 대부분의 임상시험 결측치가 이 범주에 해당. 예: 부작용으로 인한 중도 탈락\n\n\nMCAR 의 경우 완전 사례 분석을 통해 결측치를 제외하여도 편향되지 않을 수 있지만, MAR 과 MNAR 의 경우 더 복잡한 방법 (예: 다중대체법, 혼합모형 등) 이 필요할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#결측치-처리방법",
    "href": "posts/md/Stat_clinical_trial.html#결측치-처리방법",
    "title": "임상 통계학",
    "section": "11.3 결측치 처리방법",
    "text": "11.3 결측치 처리방법\n\nLOCF (Last Observation Carried Forward): 마지막 관찰값을 이후 결측치에 대입\nBOCF (Baseline Observation Carried Forward): 기준선 관찰값을 결측치에 대입\nUnconditional and Conditional Mean Imputation:\n\nUnconditional: 전체 평균으로 대체\nConditional: 특정 조건 하의 평균으로 대체\n\nBest or Worst Case Imputation: 최선 또는 최악의 시나리오 값으로 대체\nRegression Method: 회귀 모델을 사용하여 결측치 예측\nHot-deck Imputation: 유사한 특성을 가진 다른 관측치의 값으로 대체\nSingle Imputation: 단일 값으로 모든 결측치 대체"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#marmissing-at-random-의-결측치-처리",
    "href": "posts/md/Stat_clinical_trial.html#marmissing-at-random-의-결측치-처리",
    "title": "임상 통계학",
    "section": "11.4 MAR(Missing At Random) 의 결측치 처리",
    "text": "11.4 MAR(Missing At Random) 의 결측치 처리\n\nMultiple Imputation: 여러 번의 대체를 통해 불확실성 반영\nImputation 하지 않는 방법: 혼합 모형 또는 가중 GEE 등 사용\n\nMAR 은 결측치가 무작위로 발생하지만, 결측 여부가 다른 관측된 변수와 관련이 있는 경우를 말합니다. MAR 상황에서는 다음과 같은 결측치 처리 방법을 고려할 수 있습니다:\n\n다중 대체법 (Multiple Imputation)\n\nMAR 가정 하에서 가장 널리 사용되는 방법입니다.\n결측치의 불확실성을 고려하여 여러 번 대체를 수행합니다.\n3 단계로 진행됩니다:\n\nImputation step: 자료의 분포를 토대로 결측치를 대체\nAnalysis step: 대체된 데이터셋을 분석\nPooling step: 여러 분석 결과를 종합\n\n\n최대 우도법 (Maximum Likelihood Estimation)\n\n관측된 데이터를 기반으로 결측 데이터의 분포를 추정합니다.\nEM 알고리즘 등을 사용하여 모수를 추정할 수 있습니다.\n\n가중치 기반 방법 (Weighted Estimation)\n\n결측 확률의 역수를 가중치로 사용하여 편향을 보정합니다.\n\n핫덱 대체법 (Hot-deck Imputation)\n\n유사한 특성을 가진 다른 관측치의 값으로 결측치를 대체합니다.\n\n회귀 대체법 (Regression Imputation)\n\n다른 변수들을 사용하여 결측값을 예측하는 회귀 모델을 구축합니다.\n\nK-NN 알고리즘\n\n가장 유사한 특성을 가진 k 개의 이웃 데이터를 기반으로 결측치를 추정합니다.\n\n\nMAR 상황에서는 단순한 삭제나 평균 대체 등의 방법보다 위의 방법들이 더 적절할 수 있습니다. 특히 다중 대체법은 MAR 가정 하에서 편향을 줄이고 불확실성을 고려할 수 있어 널리 사용됩니다. 결측치 처리 방법 선택 시에는 데이터의 특성, 결측 메커니즘, 분석 목적 등을 종합적으로 고려해야 합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#mnar-missing-not-at-random-의-결측치-처리",
    "href": "posts/md/Stat_clinical_trial.html#mnar-missing-not-at-random-의-결측치-처리",
    "title": "임상 통계학",
    "section": "11.5 MNAR (Missing Not At Random) 의 결측치 처리",
    "text": "11.5 MNAR (Missing Not At Random) 의 결측치 처리\nMNAR(Missing Not At Random) 상황에서의 결측치 처리는 매우 복잡하고 어려운 문제입니다. MNAR 의 경우 결측 메커니즘이 관찰되지 않은 데이터와 관련이 있어, 단순한 대체 방법으로는 편향된 결과를 초래할 수 있습니다. MNAR 상황에서 고려할 수 있는 결측치 처리 방법은 다음과 같습니다:\n\n패턴 혼합 모델 (Pattern Mixture Models)\n\n결측 패턴에 따라 데이터를 그룹화하고, 각 그룹에 대해 별도의 모델을 적용합니다.\n결측 메커니즘을 명시적으로 모델링할 수 있습니다.\n\n선택 모델 (Selection Models)\n\n결측 확률과 관심 변수 간의 관계를 모델링합니다.\n결측 메커니즘에 대한 가정을 명시적으로 포함합니다.\n\n공유 파라미터 모델 (Shared Parameter Models)\n\n결측 과정과 관심 변수를 동시에 모델링합니다.\n두 과정 사이의 의존성을 포착할 수 있습니다.\n\n민감도 분석 (Sensitivity Analysis)\n\n다양한 가정 하에서 여러 분석을 수행하여 결과의 안정성을 평가합니다.\nMNAR 상황에서 특히 중요한 접근 방법입니다.\n\n다중 대체법의 변형 (Modified Multiple Imputation)\n\n표준 다중 대체법을 수정하여 MNAR 메커니즘을 고려합니다.\n대체 모델에 추가적인 파라미터를 포함시킬 수 있습니다.\n\n보조 변수 활용 (Auxiliary Variables)\n\n결측 메커니즘과 관련된 추가 변수를 모델에 포함시켜 MAR 가정에 더 가깝게 만듭니다.\n\n\nMNAR 상황에서는 단일 방법으로 완벽한 해결책을 제공하기 어렵습니다. 따라서 여러 방법을 조합하고, 결과의 안정성을 평가하는 것이 중요합니다. 또한, 도메인 전문가와의 협력을 통해 결측 메커니즘에 대한 이해를 깊이 있게 하는 것이 필수적입니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#결측치-처리의-최선책",
    "href": "posts/md/Stat_clinical_trial.html#결측치-처리의-최선책",
    "title": "임상 통계학",
    "section": "11.6 결측치 처리의 최선책",
    "text": "11.6 결측치 처리의 최선책\n\n결측치 발생 최소화: 연구 설계 및 수행 단계에서 예방\n결측치 처리 방법을 계획서에 명기: 사전에 방법 결정 및 문서화\n민감도 분석 수행: 여러 방법으로 분석하여 결과의 견고성 확인\n결측치 발생 이유 보고: 결측 패턴 및 원인에 대한 상세한 보고"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#신뢰구간을-사용해-생동성-입증",
    "href": "posts/md/Stat_clinical_trial.html#신뢰구간을-사용해-생동성-입증",
    "title": "임상 통계학",
    "section": "12.1 신뢰구간을 사용해 생동성 입증",
    "text": "12.1 신뢰구간을 사용해 생동성 입증\n생물학적 동등성을 입증하기 위해 주로 TOST(Two One-Sided Test procedure) 를 사용합니다. TOST 는 두 제제의 생체이용률이 통계적으로 동등함을 입증하는 데 특화된 방법으로 명확한 기준 제시합니다 (일반적으로 80-125% 범위의 신뢰구간을 사용하여 동등성 여부를 판단할 수 있어, 결과 해석이 명확). TOST 방법에 대해서 간략히 설명하면 아래와 같습니다.\n\n두 제제의 약동학적 파라미터 (예: AUC, Cmax) 의 비율에 대한 90% 신뢰구간을 계산합니다.\n일반적으로 이 신뢰구간이 80-125% 범위 내에 들어가면 생물학적 동등성이 입증된 것으로 간주합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#생동성시험에서-이상-발생-사유",
    "href": "posts/md/Stat_clinical_trial.html#생동성시험에서-이상-발생-사유",
    "title": "임상 통계학",
    "section": "12.2 생동성시험에서 이상 발생 사유",
    "text": "12.2 생동성시험에서 이상 발생 사유\n\n생산과정의 문제\n\n제조 공정의 차이\n원료 약물의 품질 차이\n제형 설계의 문제\n\n환자 집단에 대한 세부적인 교호작용 존재\n\n특정 환자 그룹에서 약물 흡수나 대사의 차이\n유전적 다형성에 의한 약물 반응의 차이\n식이나 다른 약물과의 상호작용"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#동등성-시험이란",
    "href": "posts/md/Stat_clinical_trial.html#동등성-시험이란",
    "title": "임상 통계학",
    "section": "13.1 동등성 시험이란?",
    "text": "13.1 동등성 시험이란?\n동등성 시험은 바이오시밀러가 오리지널 의약품과 임상적으로 의미 있는 차이가 없음을 증명하는 과정입니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#동등성-마진의-결정",
    "href": "posts/md/Stat_clinical_trial.html#동등성-마진의-결정",
    "title": "임상 통계학",
    "section": "13.2 동등성 마진의 결정",
    "text": "13.2 동등성 마진의 결정\n동등성 마진은 1 차 유효성 평가변수에서 바이오시밀러와 오리지널 의약품 간 허용 가능한 차이의 범위를 나타냅니다. 이 마진 내의 차이는 임상적으로 의미 있지 않다고 간주됩니다. 동등성 마진 결정에 대한 구체적인 기준은 약전에 명확하게 명시되어 있지 않습니다. 그러나:\n\nFDA 는 과거 임상 결과를 바탕으로 일반적으로 12% 의 마진을 요구합니다.\n마진 설정은 해당 약물의 특성, 적응증, 과거 임상 데이터 등을 종합적으로 고려하여 결정됩니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#adaptive-design-의-문제점",
    "href": "posts/md/Stat_clinical_trial.html#adaptive-design-의-문제점",
    "title": "임상 통계학",
    "section": "14.1 Adaptive design 의 문제점",
    "text": "14.1 Adaptive design 의 문제점\n\n편향발생: 중간 결과에 따른 설계 변경이 연구 결과에 영향을 미칠 수 있습니다.\n다중검정: 반복적인 분석으로 인해 제 1 종 오류가 증가할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#adaptive-design-의-성공조건",
    "href": "posts/md/Stat_clinical_trial.html#adaptive-design-의-성공조건",
    "title": "임상 통계학",
    "section": "14.2 Adaptive design 의 성공조건",
    "text": "14.2 Adaptive design 의 성공조건\n\n무작위배정 코드를 엄격하게 관리: 배정 정보의 기밀성을 유지합니다.\n자료관리 수준을 높인다: 데이터의 품질과 신뢰성을 확보합니다.\n독립적 자료모니터 위원회를 통해 운영한다: 객관성과 투명성을 보장합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#독립적-자료-모니터링-위원회",
    "href": "posts/md/Stat_clinical_trial.html#독립적-자료-모니터링-위원회",
    "title": "임상 통계학",
    "section": "15.1 독립적 자료 모니터링 위원회",
    "text": "15.1 독립적 자료 모니터링 위원회\n독립적 자료 모니터링 위원회 (Independent Data Monitoring Committee, IDMC) 설립을 통해 임상시험의 객관성과 안전성을 높일 수 있으며, 이는 임상시험 결과의 신뢰도 향상에 기여합니다. 설립 절차는 아래와 같습니다.\n\n헌장 작성:\n\nIDMC 의 설립 목적, 역할, 책임을 명확히 정의한 헌장을 작성합니다.\n대상 임상시험을 구체적으로 명시합니다.\n위원회의 구성, 운영 방식, 의사결정 과정 등을 상세히 기술합니다.\n\n위원 선정:\n\n임상시험과 이해관계가 없는 독립적인 전문가들로 구성합니다.\n통계학자, 임상의, 윤리 전문가 등 다양한 분야의 전문가를 포함시킵니다.\n\n운영 절차 수립:\n\n회의 주기, 데이터 검토 방법, 보고 체계 등을 명확히 정립합니다.\n비밀 유지 및 이해상충 관리 방안을 마련합니다.\n\n스폰서와의 관계 정립:\n\nIDMC 의 독립성을 보장하기 위한 스폰서와의 관계를 명확히 합니다.\n데이터 접근 권한, 의사결정의 자율성 등을 보장받습니다.\n\n규제 기관 승인:\n\n필요한 경우, 규제 기관의 승인을 받습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#동시-대조군이란",
    "href": "posts/md/Stat_clinical_trial.html#동시-대조군이란",
    "title": "임상 통계학",
    "section": "16.1 동시 대조군이란?",
    "text": "16.1 동시 대조군이란?\n동시 대조군은 같은 임상시험 내에서 시험약과 대조약을 각각 다른 그룹에 동시에 투여하는 방식을 말합니다.동시 대조군은 임상시험의 신뢰성과 타당성을 높이는 데 중요한 역할을 합니다.\n주요 특징: - 시간적 일치: 시험약 그룹과 대조군 그룹이 동일한 시간대에 임상시험에 참여합니다. - 환경적 동질성: 두 그룹이 동일한 임상 환경에서 시험을 받습니다. - 무작위 배정: 참가자들을 무작위로 시험약 그룹과 대조군 그룹에 배정합니다.\n장점: - 시간에 따른 변화 요인을 통제할 수 있습니다. - 두 그룹 간의 직접적인 비교가 가능합니다. - 편향을 최소화할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#대조군의-목적과-필요성",
    "href": "posts/md/Stat_clinical_trial.html#대조군의-목적과-필요성",
    "title": "임상 통계학",
    "section": "16.2 대조군의 목적과 필요성",
    "text": "16.2 대조군의 목적과 필요성\n대조군은 시험약에 의해 피험자에게 생긴 결과와 다른 요소에 의한 결과를 구별하기 위해 필요합니다. 주요 목적은 다음과 같습니다:\n\n시험약의 실제 효과 평가: 대조군을 통해 시험약의 순수한 효과를 분리하여 평가할 수 있습니다.\n편향 감소: 대조군 설정은 다양한 편향을 줄이는 데 도움이 됩니다.\n자연치유 효과 구분: 일부 질병은 시간이 지나면 자연적으로 호전될 수 있습니다. 대조군을 통해 이러한 자연치유 효과와 시험약의 효과를 구분할 수 있습니다.\n위약효과 고려: 위약효과 (placebo effect) 는 실제 약물 효과가 아닌 심리적 요인에 의한 증상 개선을 말합니다. 대조군을 사용함으로써 이러한 위약효과와 실제 약물 효과를 구분할 수 있습니다.\n외부 요인의 영향 평가: 임상시험 기간 동안 발생할 수 있는 환경적 변화나 기타 외부 요인의 영향을 평가할 수 있습니다.\n통계적 비교 가능: 대조군은 시험약 그룹과의 통계적 비교를 가능하게 하여, 결과의 유의성을 평가할 수 있게 합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#대조군-선택의-고려사항",
    "href": "posts/md/Stat_clinical_trial.html#대조군-선택의-고려사항",
    "title": "임상 통계학",
    "section": "16.3 대조군 선택의 고려사항",
    "text": "16.3 대조군 선택의 고려사항\n임상시험에서 대조군을 선택할 때는 다음 세 가지 주요 사항을 고려해야 합니다:\n\n현재 표준치료법이 있는지 여부:\n\n표준치료법이 있는 경우: 일반적으로 이를 대조군으로 사용합니다.\n표준치료법이 없는 경우: 위약 대조군이나 무처치 대조군을 고려할 수 있습니다.\n중요성: 현재의 의료 표준을 반영하여 새로운 치료법의 상대적 효과를 평가할 수 있습니다.\n\n특정 대조군 선택에 대한 적절성:\n\n연구 목적과의 부합성: 선택된 대조군이 연구 질문에 적절히 답할 수 있어야 합니다.\n통계적 고려: 대조군은 시험약과의 유의미한 비교를 가능하게 해야 합니다.\n실행 가능성: 선택된 대조군이 실제 임상 환경에서 구현 가능해야 합니다.\n중요성: 적절한 대조군 선택은 연구 결과의 신뢰성과 해석 가능성을 높입니다.\n\n윤리적 적절성:\n\n위약 사용의 윤리성: 심각한 질병에서 위약 사용이 윤리적으로 적절한지 검토해야 합니다.\n최선의 이용 가능한 치료: 대조군 참가자들에게도 최선의 치료가 제공되어야 합니다.\n위험 - 이익 균형: 대조군 참가자들에게 과도한 위험이 가해지지 않아야 합니다.\n중요성: 윤리적 고려는 참가자의 안전과 권리를 보호하고, 연구의 사회적 수용성을 확보합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#대조군선택이-영향을-주는-8-가지",
    "href": "posts/md/Stat_clinical_trial.html#대조군선택이-영향을-주는-8-가지",
    "title": "임상 통계학",
    "section": "16.4 대조군선택이 영향을 주는 8 가지",
    "text": "16.4 대조군선택이 영향을 주는 8 가지\n\n임상결과 해석: 대조군의 특성에 따라 시험약의 효과를 해석하는 방식이 달라집니다. 예를 들어, 위약 대조군과 비교하면 절대적 효과를, 활성 대조군과 비교하면 상대적 효과를 평가할 수 있습니다.\n임상의 윤리성: 특히 중증 질환에서 표준치료가 있는 경우, 위약 대조군 사용은 윤리적 문제를 야기할 수 있습니다. 환자의 안전과 복지를 고려한 대조군 선택이 필요합니다.\n분석의 편향 여부: 적절한 대조군 선택은 연구 결과의 편향을 최소화하는 데 중요합니다. 예를 들어, 역사적 대조군 사용은 시간에 따른 변화로 인한 편향을 초래할 수 있습니다.\n환자 종류와 모집의 속도: 대조군의 유형에 따라 참여 가능한 환자군이 달라지며, 이는 환자 모집 속도에 영향을 줍니다. 예를 들어, 위약 대조군 사용은 일부 환자의 참여를 제한할 수 있습니다.\n연구할 평가 변수의 종류: 대조군 선택은 평가할 수 있는 변수의 범위에 영향을 줍니다. 활성 대조군을 사용하면 안전성 프로필 비교가 가능하지만, 위약 대조군에서는 이러한 비교가 제한적일 수 있습니다.\n임상 결과의 신뢰성: 적절한 대조군 선택은 연구 결과의 신뢰성을 높입니다. 예를 들어, 무작위 배정된 동시 대조군은 역사적 대조군보다 더 신뢰할 수 있는 결과를 제공합니다.\n시판허가의 여부: 규제 기관의 요구사항에 부합하는 대조군 선택은 시판 허가 획득에 중요합니다. 일부 상황에서는 특정 유형의 대조군 사용이 필수적일 수 있습니다.\n잘못된 결론: 부적절한 대조군 선택은 잘못된 결론으로 이어질 수 있습니다. 예를 들어, 활성 대조군의 효과가 일관되지 않은 경우, 비열등성 시험 결과의 해석이 어려워질 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#위약-동시-대조군",
    "href": "posts/md/Stat_clinical_trial.html#위약-동시-대조군",
    "title": "임상 통계학",
    "section": "17.1 위약 동시 대조군",
    "text": "17.1 위약 동시 대조군\n위약 동시 대조군은 임상시험에서 시험약의 효과를 객관적으로 평가하기 위해 사용되는 중요한 대조군으로 치료 효과가 없는 위약 (시험약과 물리적으로 동일하지만 유효성분이 없는 약물) 을 투여하는 환자군입니다. 시험약 그룹과 위약 그룹이 같은 임상시험 기간 동안 동시에 평가되며 참가자들은 무작위로 시험약 그룹 또는 위약 그룹에 배정됩니다. 또한 이중맹검 (참가자와 연구자 모두 누가 위약을 받는지 모르는 상태) 으로 진행됩니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#무처치-no-treatment-동시-대조군",
    "href": "posts/md/Stat_clinical_trial.html#무처치-no-treatment-동시-대조군",
    "title": "임상 통계학",
    "section": "17.2 무처치 (no-treatment) 동시 대조군",
    "text": "17.2 무처치 (no-treatment) 동시 대조군\n\n어떠한 치료도 받지 않는 대조군으로 자연 경과를 관찰하는 데 유용합니다. 다만 윤리적 문제로 사용이 제한될 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#용량반응-동시대조군",
    "href": "posts/md/Stat_clinical_trial.html#용량반응-동시대조군",
    "title": "임상 통계학",
    "section": "17.3 용량반응 동시대조군",
    "text": "17.3 용량반응 동시대조군\n\n피험자들이 다양한 용량에 무작위 배정됨\n약물의 최적 용량을 찾는 데 도움\n용량 - 반응 관계를 평가할 수 있음"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#활성동시대조군",
    "href": "posts/md/Stat_clinical_trial.html#활성동시대조군",
    "title": "임상 통계학",
    "section": "17.4 활성동시대조군",
    "text": "17.4 활성동시대조군\n\n이미 효과가 입증된 표준 치료를 받는 대조군\n새로운 치료법의 상대적 효과를 평가\n윤리적 문제를 최소화할 수 있음"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#외부-대조군",
    "href": "posts/md/Stat_clinical_trial.html#외부-대조군",
    "title": "임상 통계학",
    "section": "17.5 외부 대조군",
    "text": "17.5 외부 대조군\n\n현재 진행 중인 임상시험 외부의 대조군\n과거 데이터나 다른 연구의 대조군 사용\n비용 효율적이지만 편향 위험이 높음"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#다중-대조군",
    "href": "posts/md/Stat_clinical_trial.html#다중-대조군",
    "title": "임상 통계학",
    "section": "17.6 다중 대조군",
    "text": "17.6 다중 대조군\n\n여러 유형의 대조군을 동시에 사용\n다양한 관점에서 치료 효과를 평가\n복잡하지만 포괄적인 결과를 얻을 수 있음"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#시간의-경과에-따른-질병의-변화",
    "href": "posts/md/Stat_clinical_trial.html#시간의-경과에-따른-질병의-변화",
    "title": "임상 통계학",
    "section": "18.1 시간의 경과에 따른 질병의 변화",
    "text": "18.1 시간의 경과에 따른 질병의 변화\n발생할 수 있는 편향: 시간이 지남에 따라 자연적으로 발생하는 질병의 변화가 중재 효과로 오인될 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#평균으로의-회귀",
    "href": "posts/md/Stat_clinical_trial.html#평균으로의-회귀",
    "title": "임상 통계학",
    "section": "18.2 평균으로의 회귀",
    "text": "18.2 평균으로의 회귀\n발생할 수 있는 편향: 극단적인 측정값이 후속 측정에서 평균으로 회귀하는 현상으로, 이를 중재 효과로 잘못 해석할 수 있습니다.\n여러 편향이 시험군과 위약군에 모두 같은 양으로 발생 –&gt; 군간 비교만 편향을 제외한 효과를 추정할 수 있는 방법\n이는 대조군을 포함한 연구 설계의 중요성을 강조합니다. 대조군이 있으면 이러한 편향들이 양 군에 동일하게 작용하므로, 군간 비교를 통해 실제 중재 효과를 더 정확히 추정할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#항암제-임상-시험에서-단일군-전후-비교-디자인",
    "href": "posts/md/Stat_clinical_trial.html#항암제-임상-시험에서-단일군-전후-비교-디자인",
    "title": "임상 통계학",
    "section": "18.3 항암제 임상 시험에서 단일군 전후 비교 디자인",
    "text": "18.3 항암제 임상 시험에서 단일군 전후 비교 디자인\n항암제 개발의 초기 단계에서는 윤리적 문제나 실행 가능성 때문에 단일군 전후 비교가 사용될 수 있습니다. 그러나 이는 예비적인 결과로 간주되며, 후속 연구에서는 보다 엄격한 연구 설계가 필요합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#가교시험",
    "href": "posts/md/Stat_clinical_trial.html#가교시험",
    "title": "임상 통계학",
    "section": "19.1 가교시험",
    "text": "19.1 가교시험\n가교시험은 다른 지역에서 이미 시판허가를 받은 신약을 새로운 지역에 도입할 때 수행하는 소규모 임상시험입니다. 주요 목적은 다음과 같습니다:\n\n새로운 지역의 인구집단에 대한 약물의 안전성과 유효성 확인\n기존 임상 데이터의 외삽 가능성 평가\n지역 특이적 요인 (유전적, 환경적, 문화적 차이 등) 의 영향 평가"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#다지역-임상시험의-장점과-단점",
    "href": "posts/md/Stat_clinical_trial.html#다지역-임상시험의-장점과-단점",
    "title": "임상 통계학",
    "section": "19.2 다지역 임상시험의 장점과 단점",
    "text": "19.2 다지역 임상시험의 장점과 단점\n장점: 1. 시간 단축: 여러 지역에서 동시에 진행되어 전체 개발 기간 단축 2. 비용 효율성: 한 번의 대규모 시험으로 여러 지역의 승인 가능 3. 다양한 인구집단 포함: 결과의 일반화 가능성 증가\n단점: 1. 복잡한 관리: 여러 국가의 규제 요구사항 충족 필요 2. 지역 간 차이로 인한 해석의 어려움 3. 높은 초기 비용"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#다지역-임상시험에서-발생하는-통계적-이슈",
    "href": "posts/md/Stat_clinical_trial.html#다지역-임상시험에서-발생하는-통계적-이슈",
    "title": "임상 통계학",
    "section": "19.3 다지역 임상시험에서 발생하는 통계적 이슈",
    "text": "19.3 다지역 임상시험에서 발생하는 통계적 이슈\n\n지역 간 이질성: 치료 효과의 일관성 평가 필요\n표본 크기 산정: 전체 및 지역별 적절한 표본 크기 결정\n하위그룹 분석: 지역별 효과 차이 분석\n통계적 방법론 선택: 지역 간 차이를 고려한 적절한 분석 방법 선택"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#효과가-각-지역에서-일정한지-확인하는-통계",
    "href": "posts/md/Stat_clinical_trial.html#효과가-각-지역에서-일정한지-확인하는-통계",
    "title": "임상 통계학",
    "section": "19.4 효과가 각 지역에서 일정한지 확인하는 통계",
    "text": "19.4 효과가 각 지역에서 일정한지 확인하는 통계\n\n기술통계: 각 지역별 효과 크기, 표준편차 등 기본적인 통계량을 제공합니다.\nForest plot: 각 지역의 효과 크기와 신뢰구간을 시각적으로 표현하여 지역 간 차이를 쉽게 파악할 수 있게 합니다.\n공변량 보정을 포함하는 모형에 기반한 추정: 지역 외 다른 요인들의 영향을 고려하여 더 정확한 지역별 효과를 추정합니다.\n치료제와 지역간의 교호작용 검정: 치료 효과가 지역에 따라 유의미하게 다른지 통계적으로 검증합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#각-지역에-피험자-할당-방법",
    "href": "posts/md/Stat_clinical_trial.html#각-지역에-피험자-할당-방법",
    "title": "임상 통계학",
    "section": "19.5 각 지역에 피험자 할당 방법",
    "text": "19.5 각 지역에 피험자 할당 방법\n\n비례할당: 각 지역의 인구 비율에 따라 피험자를 할당합니다. 대표성을 확보할 수 있습니다.\n균등할당: 모든 지역에 동일한 수의 피험자를 할당합니다. 지역 간 비교가 용이합니다.\n시험약 효과의 일부 보존: 전체 효과의 일정 비율을 각 지역에서 입증할 수 있도록 할당합니다.\n지역유의성: 각 지역에서 독립적으로 통계적 유의성을 확보할 수 있도록 할당합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#증거의-수준",
    "href": "posts/md/Stat_clinical_trial.html#증거의-수준",
    "title": "임상 통계학",
    "section": "20.1 증거의 수준",
    "text": "20.1 증거의 수준\n시험약 효과에 대한 신뢰구간은 치료 효과의 불확실성을 나타내는 중요한 지표입니다. 희귀질환의 경우, 환자 수가 적어 넓은 신뢰구간이 허용될 수 있으나, 임상적 의미를 신중히 해석해야 합니다.\n희귀질환 임상시험에서의 주요 고려사항에 대해 설명하겠습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#약리적-고려",
    "href": "posts/md/Stat_clinical_trial.html#약리적-고려",
    "title": "임상 통계학",
    "section": "20.2 약리적 고려",
    "text": "20.2 약리적 고려\n희귀질환 치료제 개발에서는 질병의 특성과 약물의 작용 기전을 세밀히 고려해야 합니다. 희귀질환의 병태생리학적 특성을 반영한 맞춤형 약리 연구가 필요하며, 약물 상호작용과 부작용에 대한 면밀한 관찰이 중요합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#평가변수의-선택",
    "href": "posts/md/Stat_clinical_trial.html#평가변수의-선택",
    "title": "임상 통계학",
    "section": "20.3 평가변수의 선택",
    "text": "20.3 평가변수의 선택\n희귀질환의 특성을 반영하는 적절한 평가변수 선택이 중요합니다. 질병 특이적 바이오마커나 환자 보고 결과 (PRO) 를 활용할 수 있습니다. 또한, 대리 평가변수 (surrogate endpoint) 를 사용하여 시험 기간을 단축하고 효율성을 높일 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#대조군의-선택",
    "href": "posts/md/Stat_clinical_trial.html#대조군의-선택",
    "title": "임상 통계학",
    "section": "20.4 대조군의 선택",
    "text": "20.4 대조군의 선택\n희귀질환의 특성상 위약 대조군 설정이 윤리적 문제를 야기할 수 있습니다. 따라서 표준 치료군이나 역사적 대조군 사용을 고려할 수 있습니다. 경우에 따라 환자 자신을 대조군으로 사용하는 N-of-1 시험 설계도 가능합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#방법론-및-통계학적-고려사항",
    "href": "posts/md/Stat_clinical_trial.html#방법론-및-통계학적-고려사항",
    "title": "임상 통계학",
    "section": "20.5 방법론 및 통계학적 고려사항",
    "text": "20.5 방법론 및 통계학적 고려사항\n\n유연한 유의수준 적용: 전통적인 0.05 유의수준 대신 더 유연한 기준을 적용할 수 있습니다. FDA 에서는 희귀질환 임상시험에서 p-value 0.05 이상도 고려할 수 있다고 언급하고 있습니다.\n적응적 임상시험 설계: 중간 분석 결과에 따라 시험 설계를 조정하는 적응적 설계를 통해 효율성을 높일 수 있습니다. 이는 FDA 에서도 권장하는 방법입니다.\n베이지안 접근법: 사전 정보를 활용하여 적은 표본으로도 의미 있는 결론을 도출할 수 있는 베이지안 통계 방법을 고려할 수 있습니다. 특히 항암제 개발에서 이 방법론의 도입이 증가하고 있습니다.\n다중 평가변수 분석: 여러 평가변수를 종합적으로 분석하여 치료 효과를 평가하는 방법을 고려할 수 있습니다. 이는 희귀질환의 복잡한 특성을 반영하는 데 도움이 될 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#용어-정리",
    "href": "posts/md/Stat_clinical_trial.html#용어-정리",
    "title": "임상 통계학",
    "section": "21.1 용어 정리",
    "text": "21.1 용어 정리\n\n이상반응: 시험약과의 연관성을 반드시 규명할 수 없거나, 피험자에 발생한 예상치 못한 의학적 발생을 말합니다. 이는 임상시험 중 관찰된 모든 바람직하지 않은 의학적 사건을 포함합니다.\n약물 이상반응: 의약품과 인과관계가 있다고 의심되는 유해하고 의도하지 않은 반응을 의미합니다. 이는 시험약과의 연관성이 있다고 판단되는 이상반응을 지칭합니다.\n예상하지 못한 약물이상반응: 의약품의 허가사항이나 임상시험계획서에 기술되지 않은 성질이나 중증도의 이상반응을 말합니다. 이는 새로운 안전성 정보로 간주될 수 있습니다.\n중대한 이상반응 또는 이상약물 반응: 사망, 생명을 위협하는 상황, 입원 또는 입원 기간 연장, 지속적인 장애나 기능 저하, 선천적 기형 등을 초래하는 반응을 의미합니다. 이는 즉각적인 보고와 평가가 필요한 중요한 안전성 정보입니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#시험약의-안전성을-평가하는데-필요한-표본의-크기",
    "href": "posts/md/Stat_clinical_trial.html#시험약의-안전성을-평가하는데-필요한-표본의-크기",
    "title": "임상 통계학",
    "section": "21.2 시험약의 안전성을 평가하는데 필요한 표본의 크기",
    "text": "21.2 시험약의 안전성을 평가하는데 필요한 표본의 크기\n시험약의 안전성 평가를 위한 표본 크기 결정은 매우 중요한 과정으로 아래 두 가지 참고할 만한 규칙이 있습니다. 다만 이런 규칙은 임상시험 설계 시 필요한 최소 표본 크기에 도움을 주는 것이며 실제 임상시험에서는 더 많은 요인들을 고려하여 표본 크기를 결정해야 합니다.\n\nrule of 3: 이 규칙은 드문 이상반응을 탐지하는 데 유용합니다. 어떤 약물 이상반응의 실제 발생률이 n 명 중 한 명일 때, 3n 명의 환자를 관찰하면 적어도 95% 의 확률로 한 명 이상에서 해당 이상반응을 관찰할 수 있다는 것입니다. 이는 드문 이상반응의 발생 가능성을 추정하는 데 도움이 됩니다.26\n300 명 규칙: 300 명에게 시험약을 투여했을 때 특정 약물 이상반응이 관찰되지 않았다면, 95% 의 신뢰도로 해당 이상반응의 실제 발생률이 1% 미만이라고 말할 수 있습니다. 이는 상대적으로 흔하지 않은 이상반응의 발생률 상한을 추정하는 데 유용합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#개별-환자에서의-인과관계",
    "href": "posts/md/Stat_clinical_trial.html#개별-환자에서의-인과관계",
    "title": "임상 통계학",
    "section": "21.3 개별 환자에서의 인과관계",
    "text": "21.3 개별 환자에서의 인과관계\n개별 환자에서 약물 이상반응과 시험약 간의 인과관계를 평가할 때는 다음 조건들을 고려합니다. 이 조건들이 많이 만족될수록 인과관계의 가능성이 높아집니다:\n\n시간적 연관성: 약물 투여와 이상반응 발생 사이의 시간적 관계가 적절한지 평가합니다.\n용량관계: 약물 용량 증가에 따라 이상반응의 심각도나 빈도가 증가하는지 확인합니다.\n시험약 재투여 혹은 투여중지: 약물 중단 시 이상반응이 사라지거나, 재투여 시 다시 나타나는지 관찰합니다.\n생물학적 개연성: 약물의 작용 기전과 이상반응 발생 사이에 생물학적으로 설명 가능한 연관성이 있는지 검토합니다.\n특이성: 해당 이상반응이 특정 약물에 특징적인 것인지 평가합니다.\n일치성: 다른 연구나 보고에서도 유사한 이상반응이 관찰되었는지 확인합니다.\n설명 방법이 없는 경우: 다른 요인으로는 이상반응을 설명할 수 없는 경우, 약물과의 인과관계 가능성이 높아집니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#안전성-자료-평가법",
    "href": "posts/md/Stat_clinical_trial.html#안전성-자료-평가법",
    "title": "임상 통계학",
    "section": "21.4 안전성 자료 평가법",
    "text": "21.4 안전성 자료 평가법\n안전성 자료 평가는 임상시험에서 수집된 안전성 정보를 체계적으로 분석하고 해석하는 과정입니다. 주요 평가 방법은 다음과 같습니다:\n\n빈도 분석 : 이상반응의 발생 빈도를 계산하고 비교합니다. 치료군과 대조군 간의 이상반응 발생률 차이를 통계적으로 분석합니다.\n중증도 평가 : 이상반응의 중증도를 등급화하여 분석합니다. 일반적으로 CTCAE(Common Terminology Criteria for Adverse Events) 기준을 사용합니다.\n시간 - 사건 분석 : Kaplan-Meier 곡선이나 Cox 비례위험모형을 이용하여 이상반응 발생까지의 시간을 분석합니다.\n인과관계 평가 : 개별 이상반응과 시험약 간의 인과관계를 평가합니다. 앞서 언급한 7 가지 기준 (시간적 연관성, 용량관계 등) 을 고려합니다.\n하위그룹 분석 : 연령, 성별, 기저질환 등 환자 특성에 따른 안전성 프로파일 차이를 분석합니다.\n누적 분석 : 장기 안전성을 평가하기 위해 이상반응의 누적 발생률을 분석합니다.\n비교 위험도 분석 : 상대위험도 (Relative Risk) 나 오즈비 (Odds Ratio) 를 계산하여 치료군과 대조군의 위험을 비교합니다.\n신호 탐지 : 예상치 못한 이상반응 패턴을 식별하기 위해 데이터 마이닝 기법을 사용할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#임상에서-통계",
    "href": "posts/md/Stat_clinical_trial.html#임상에서-통계",
    "title": "임상 통계학",
    "section": "22.1 임상에서 통계",
    "text": "22.1 임상에서 통계\n\n전수조사와 표본 조사: 전수조사는 모든 대상을 조사하는 방법이지만 현실적으로 어려워 일부만 조사하는 표본 조사를 주로 사용합니다.\n추정:\n\n점추정: 단일 값으로 모수를 추정합니다.\n구간추정 (신뢰구간): 모수가 속할 것으로 예상되는 구간을 제시합니다.\n\n가설검정: 귀무가설의 기각 여부를 통계적으로 판단합니다.\n제 1 종 오류, 2 종 오류, 검정력:\n\n제 1 종 오류: 귀무가설이 참일 때 이를 기각할 확률\n제 2 종 오류: 대립가설이 참일 때 귀무가설을 기각하지 못할 확률\n검정력: 1 - 제 2 종 오류 확률"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#어떤-통계검정방법이-가장-좋은-방법인가",
    "href": "posts/md/Stat_clinical_trial.html#어떤-통계검정방법이-가장-좋은-방법인가",
    "title": "임상 통계학",
    "section": "22.2 어떤 통계검정방법이 가장 좋은 방법인가?",
    "text": "22.2 어떤 통계검정방법이 가장 좋은 방법인가?\n제 1 종 오류를 5% 이하로 유지하면서 검정력을 최대화하는 방법이 가장 좋은 방법으로 간주됩니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#양측-검정과-단측검정",
    "href": "posts/md/Stat_clinical_trial.html#양측-검정과-단측검정",
    "title": "임상 통계학",
    "section": "22.3 양측 검정과 단측검정",
    "text": "22.3 양측 검정과 단측검정\n\n양측검정: 효과의 방향성을 모를 때 사용합니다.\n단측검정: 효과의 방향성을 예측할 수 있을 때 사용합니다. 예를 들어, 시험약이 위약보다 우월한 경우를 검정할 때 사용합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#통계적-유의성과-임상정-유의성",
    "href": "posts/md/Stat_clinical_trial.html#통계적-유의성과-임상정-유의성",
    "title": "임상 통계학",
    "section": "22.4 통계적 유의성과 임상정 유의성",
    "text": "22.4 통계적 유의성과 임상정 유의성\n\n통계적 유의성만 있는 경우: 효과가 작아도 표본 크기가 큰 경우 발생할 수 있습니다.\n임상적 유의성만 있는 경우: 효과가 크더라도 통계적으로 유의하지 않을 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#임상결과해석에-신뢰구간이-더-중요한-이유",
    "href": "posts/md/Stat_clinical_trial.html#임상결과해석에-신뢰구간이-더-중요한-이유",
    "title": "임상 통계학",
    "section": "22.5 임상결과해석에 신뢰구간이 더 중요한 이유",
    "text": "22.5 임상결과해석에 신뢰구간이 더 중요한 이유\n신뢰구간은 통계적 유의성과 임상적 유의성을 동시에 평가할 수 있어 가설검정보다 더 많은 정보를 제공합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#표준편차와-표준오차의-차이",
    "href": "posts/md/Stat_clinical_trial.html#표준편차와-표준오차의-차이",
    "title": "임상 통계학",
    "section": "22.6 표준편차와 표준오차의 차이",
    "text": "22.6 표준편차와 표준오차의 차이\n\n표준편차: 자료의 퍼짐 정도를 나타내는 지표입니다.\n표준오차: 추정량의 정확도를 나타내는 지표로, 여러 공식이 존재할 수 있습니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#실험계획법",
    "href": "posts/md/Stat_clinical_trial.html#실험계획법",
    "title": "임상 통계학",
    "section": "22.7 실험계획법",
    "text": "22.7 실험계획법\n\n무작위 배정: 편향을 줄이기 위해 사용합니다.\n반복: 정확도를 높이기 위해 사용합니다.\n블록화: 외부 요인의 영향을 통제하기 위해 사용합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#two-sample-t-test",
    "href": "posts/md/Stat_clinical_trial.html#two-sample-t-test",
    "title": "임상 통계학",
    "section": "22.8 Two-sample t-test",
    "text": "22.8 Two-sample t-test\n두 독립적인 집단의 평균을 비교하는 방법입니다. 이 검정은 두 집단이 서로 독립적이며, 각 집단의 데이터가 정규분포를 따른다고 가정합니다. 검정 통계량은 두 집단의 평균 차이를 표준오차로 나눈 값으로 계산됩니다.27 예시: 새로운 혈압 강하제의 효과를 평가하기 위해, 치료군과 대조군의 혈압 감소 정도를 비교합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#paired-t-test",
    "href": "posts/md/Stat_clinical_trial.html#paired-t-test",
    "title": "임상 통계학",
    "section": "22.9 Paired t-test",
    "text": "22.9 Paired t-test\n동일한 대상의 전후 비교 등에 사용되는 방법입니다. 이 검정은 각 개체에 대해 두 번의 측정이 이루어질 때 사용됩니다. 예를 들어, 치료 전후의 효과를 비교할 때 유용합니다. 두 측정값의 차이가 정규분포를 따른다고 가정합니다.28 예시: 동일한 환자 그룹에서 특정 약물 복용 전후의 콜레스테롤 수치 변화를 비교합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#one-way-anova",
    "href": "posts/md/Stat_clinical_trial.html#one-way-anova",
    "title": "임상 통계학",
    "section": "22.10 One-way ANOVA",
    "text": "22.10 One-way ANOVA\n세 개 이상의 집단을 비교할 때 사용하는 방법으로, 사후 검정 방법에는 Scheffe, Tukey-Kramer, Dunnett’s 방법 등이 있습니다. 이 방법은 분산분석 (ANOVA) 의 한 형태로, 여러 집단 간의 평균 차이를 동시에 비교할 수 있습니다.29 예시: 세 가지 다른 용량 (저용량, 중간용량, 고용량) 의 약물 효과를 비교하여 최적의 용량을 결정합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#공분산-분석",
    "href": "posts/md/Stat_clinical_trial.html#공분산-분석",
    "title": "임상 통계학",
    "section": "22.11 공분산 분석",
    "text": "22.11 공분산 분석\n공변량의 영향을 보정하여 집단 간 차이를 분석하는 방법입니다. 이 방법은 실험 처리 효과와 함께 연속형 변수 (공변량) 의 영향을 동시에 고려할 수 있어, 더 정확한 처리 효과 추정이 가능합니다. 예시: 새로운 당뇨병 치료제의 효과를 평가할 때, 환자의 나이와 체중을 공변량으로 고려하여 분석합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#랜덤화-블록-설계",
    "href": "posts/md/Stat_clinical_trial.html#랜덤화-블록-설계",
    "title": "임상 통계학",
    "section": "22.12 랜덤화 블록 설계",
    "text": "22.12 랜덤화 블록 설계\n임상 시험의 효율을 높이기 위해 사용되는 실험 설계 방법입니다. 이 방법은 알려진 변동 요인을 블록으로 구분하여 처리하므로, 처리 효과를 더 정확히 추정할 수 있습니다. 예시: 항암제 임상시험에서 환자의 암 병기를 블록으로 사용하여 각 병기 내에서 치료군과 대조군에 균등하게 배정합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#회귀분석",
    "href": "posts/md/Stat_clinical_trial.html#회귀분석",
    "title": "임상 통계학",
    "section": "22.13 회귀분석",
    "text": "22.13 회귀분석\n변수 간의 관계를 모델링하는 방법입니다. 독립변수와 종속변수 간의 관계를 수학적 모델로 표현하여, 변수 간의 관계를 이해하고 예측에 활용할 수 있습니다. 예시: 환자의 나이, 체중, 운동량 등이 혈압에 미치는 영향을 모델링합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#범주형-자료-분석",
    "href": "posts/md/Stat_clinical_trial.html#범주형-자료-분석",
    "title": "임상 통계학",
    "section": "22.14 범주형 자료 분석",
    "text": "22.14 범주형 자료 분석\n카테고리 데이터를 분석하는 방법입니다. 주로 카이제곱 검정, Fisher 의 정확 검정 등이 사용되며, 범주형 변수 간의 관계나 분포의 차이를 분석합니다. 예시: 흡연 여부 (흡연자/비흡연자) 와 폐암 발생 (있음/없음) 사이의 관계를 분석합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#오즈비-odds-ratio",
    "href": "posts/md/Stat_clinical_trial.html#오즈비-odds-ratio",
    "title": "임상 통계학",
    "section": "22.15 오즈비 (odds ratio)",
    "text": "22.15 오즈비 (odds ratio)\n두 사건의 발생 확률의 비율을 나타내는 지표입니다. 주로 case-control 연구에서 위험 요인과 질병 발생의 관련성을 평가하는 데 사용됩니다. 예시: 특정 유전자 변이가 있는 사람이 없는 사람에 비해 특정 질병에 걸릴 확률이 몇 배 높은지 계산합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#로지스틱-회귀분석",
    "href": "posts/md/Stat_clinical_trial.html#로지스틱-회귀분석",
    "title": "임상 통계학",
    "section": "22.16 로지스틱 회귀분석",
    "text": "22.16 로지스틱 회귀분석\n이진 결과변수에 대한 회귀분석 방법으로, 오즈비 계산이 가능합니다. 종속변수가 이진형 (예: 성공/실패) 일 때 사용되며, 독립변수와 종속변수의 확률 간의 관계를 모델링합니다. 예시: 환자의 나이, 성별, 혈압 등의 요인이 심장병 발생 확률에 미치는 영향을 모델링합니다."
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#생존분석",
    "href": "posts/md/Stat_clinical_trial.html#생존분석",
    "title": "임상 통계학",
    "section": "22.17 생존분석",
    "text": "22.17 생존분석\n특정 사건 (예: 사망, 재발) 까지의 시간을 분석하는 방법입니다. Kaplan-Meier 곡선과 Log-rank 검정이 자주 사용됩니다. 예시: 새로운 항암제 치료를 받은 환자군과 기존 치료를 받은 환자군의 5 년 생존율을 비교합니다.\n\n생존함수와 위험함수: 생존함수는 특정 시점까지 사건이 발생하지 않을 확률을, 위험함수는 특정 시점에서의 순간적인 사건 발생 확률을 나타냅니다. 위험함수가 커지면 사망 가능성이 높아집니다.\nKaplan-Meier 곡선: 시간에 따른 생존 확률을 그래프로 나타내는 방법으로, 중도 탈락한 데이터를 처리할 수 있어 자주 사용됩니다.\nLog-rank test: 두 개 이상의 생존함수가 통계적으로 유의한 차이가 있는지 검정하는 방법입니다. 전체 관찰 기간에 걸친 생존 기간을 비교합니다.30"
  },
  {
    "objectID": "posts/md/Stat_clinical_trial.html#footnotes",
    "href": "posts/md/Stat_clinical_trial.html#footnotes",
    "title": "임상 통계학",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://m.medigatenews.com/news/3671926126↩︎\nhttp://www.jcr.kr/Upload/Post_Case/1450158131_phpkvoj7J_-2.pdf↩︎\nhttp://www.docdocdoc.co.kr/news/articleView.html?idxno=1062297↩︎\nhttps://pipetcpt.github.io/book-stat/analysis.html↩︎\nhttps://pipetcpt.github.io/book-stat/design.html↩︎\nhttps://support.minitab.com/ko-kr/minitab/help-and-how-to/statistics/equivalence-tests/how-to/2-sample-equivalence-test/before-you-start/example/↩︎\nhttp://www.kmdianews.com/news/articleView.html?idxno=28425↩︎\nhttps://ko.wikipedia.org/wiki/%EC%9E%84%EC%83%81%EC%8B%9C%ED%97%98↩︎\nhttps://pipetcpt.github.io/book-stat/design.html↩︎\nhttp://www.hitnews.co.kr/news/articleView.html?idxno=46635↩︎\nhttps://ko.wikipedia.org/wiki/%EC%9E%84%EC%83%81%EC%8B%9C%ED%97%98↩︎\nhttps://ekja.org/upload/pdf/kjae-68-106_ko.pdf↩︎\nhttps://pipetcpt.github.io/book-stat/analysis.html↩︎\nhttps://3months.tistory.com/506↩︎\nhttps://rimint02.tistory.com/m/153↩︎\nhttps://blog.naver.com/PostView.naver?blogId=julcho&isHttpsRedirect=true&logNo=40207295594↩︎\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5654877/↩︎\nhttps://en.wikipedia.org/wiki/Intention-to-treat_analysis↩︎\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3159210/↩︎\nhttps://www.clinfo.eu/itt-vs-pp↩︎\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3159210/↩︎\nhttps://greatjoy.tistory.com/78↩︎\nhttps://syj9700.tistory.com/6↩︎\nhttp://www.kmdianews.com/news/articleView.html?idxno=28425↩︎\nhttp://www.mediclic.co.kr/info/bio↩︎\nhttps://pipetcpt.github.io/book-stat/design.html↩︎\nhttps://www.statology.org/two-sample-t-test/↩︎\nhttps://en.wikipedia.org/wiki/Student’s_t-test↩︎\nhttps://www.jmp.com/en_ca/statistics-knowledge-portal/t-test/two-sample-t-test.html)↩︎\nhttps://www.graphpad.com/quickcalcs/ttest1/↩︎"
  },
  {
    "objectID": "posts/md/How_FixEncoding.html#footnotes",
    "href": "posts/md/How_FixEncoding.html#footnotes",
    "title": "문자열 인코딩 문제",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://realpython.com/python-encodings-guide/↩︎"
  },
  {
    "objectID": "posts/md/Primer_of_scRNAseq.html",
    "href": "posts/md/Primer_of_scRNAseq.html",
    "title": "scRNA-seq 소개",
    "section": "",
    "text": "얼마전 뉴욕 타임즈에 A.I. Is Learning What It Means to Be Alive 라는 제목의 기사가 공개되었습니다. 해당 기사에서 에 scRNA-seq 기초 모델이 단일 세포 RNA 시퀀싱 (scRNA) 데이터를 해석, 사용, 적용하는 방식을 어떻게 변화시킬 수 있는지에 대한 이야기가 있었는데 이 포스팅에서 더 깊이 파고들어 살펴보고자 합니다."
  },
  {
    "objectID": "posts/md/Primer_of_scRNAseq.html#역사",
    "href": "posts/md/Primer_of_scRNAseq.html#역사",
    "title": "scRNA-seq 소개",
    "section": "4.1 역사",
    "text": "4.1 역사\n기반 모델 (Foundation Models) 은 일반적으로 광범위한 데이터에 대해 학습된 모델로 다양한 다운스트림 분석을 적용 (예: 미세 조정) 할 수 있는 모델입니다. 이런 기술은 자연어 처리 분야에서 가장 널리 사용되지만 최근에는 이미지 (DALL-E), 오디오 (MusicGen), 심지어 아미노산 서열 (ESM2) 등 다양한 분야로 확장되었습니다.\n거의 모든 다른 개념과 마찬가지로 임베딩 개념은 2010 년대 초까지 거슬러 올라가는 등 이전에도 광범위하게 시도되어 왔으며 기반 모델이 임베딩 개념을 처음 고안한 것은 아닙니다. 하지만 단순한 선형 임베딩 방식에 의존하거나 훈련된 정확한 섭동에 국한되거나 비교적 작은 데이터 세트에 대해 훈련되는 등 그 범위는 제한적이었습니다. 마크오브바이오의 블로그 게시물 에서 이런 단점에 대해 자세히 설명합니다. scVI 와 같이 다양한 데이터 세트에서 훈련된 비선형 데이터 변환을 다룬 논문이 몇 편 있었지만, 그 결과는 논문으로서 흥미롭기는 하지만 실제로 유용하기에는 여전히 모호한 영역에 머물러 있었습니다.\n하지만 혁신적인 Attention Is All You Need 논문에 의해 거대 모델과 방대한 양의 데이터가 특별한 모델을 만들 수 있다는 믿음이 점점 커지면서 생물학자들은 자체적인 scRNA-seq 기반 모델을 구축하기 시작했습니다. 2024 년 3 월 현재, scFormer, scFoundation, GeneFormer, scBERT, scGPT, Universal Cell Embeddings 등 여러 가지의 기반 모델이 공개되었습니다. 이런 모델들이 만들어질 수 있었던 원동력은 앞서 이야기한 바 있는 세포 지도입니다. 각각의 모델은 지난 10 년 동안 모아왔던 여러 개의 세포 지도 데이터를 학습 데이터로 사용해 그 어떤 모델보다 세포 상태를 더 잘 이해할 수 있게 되었습니다."
  },
  {
    "objectID": "posts/md/Primer_of_scRNAseq.html#universal-cell-embeddings",
    "href": "posts/md/Primer_of_scRNAseq.html#universal-cell-embeddings",
    "title": "scRNA-seq 소개",
    "section": "4.2 Universal Cell Embeddings",
    "text": "4.2 Universal Cell Embeddings\nUCE 는 2023 년 11 월에 공개되었으며 scRNA-seq 데이터를 분석하는 사람들에게 정말 중요한 도구가 될 것으로 보입니다. 아래와 같은 특징이 있기 때문이죠.\n\n다른 기반 모델들은 인간 세포에 한정된 반면에 UCE 모델은 인간, 생쥐, 여우원숭이, 제브라피쉬, 돼지, 붉은털원숭이, 필리핀원숭이 (cynomolgus monkey), 발톱개구리등의 총 8 가지 종의 데이터셋을 학습에 사용하였습니다.\nUCE 모델은 미세 조정이 필요 없는 유일한 모델입니다. 즉, 추가적인 학습없이도 103 개의 조직 및 다양한 종의 세포를 분류 할 수 있습니다.\nChan Zuckerberg Biohub Network 의 지원을 받고 있습니다. Chan Zuckerberg Biohub Network 은 타불라 사피엔스, 타불라 무리스, 타불라 무리스 세니스, 타불라 마이크로세버스, CellXGene 등 scRNA-seq 데이터를 가장 많이 생성하고 공개하는 곳입니다.\n다른 기반 모델보다 성능이 뛰어납니다.\n완전히 새로운 종에서 수집한 세포의 임베딩을 UMAP 으로 그려볼 수 있습니다. UCE 는 종간의 정보도 학습했기 때문에 (물론 임베딩 플롯은 항상 약간의 논란이 있을 수 있지만) 영장류와 닭 세포 유형을 분리할 수도 있습니다. 자세한 알아보기.\n새로운 세포 유형을 찾을 수 있습니다. 희귀한 세포 유형이 포함된 scRNA 데이터셋을 통해 제로 샷 임베딩을 수행하면 새로운 세포 유형을 구분하는 간단한 이진 분류기를 만들 수 있습니다. 이 분류기를 사용하면 그동안 데이터 세트에 흩어져 있는 세포들을 찾아 낼 수 있는 것입니다.\n마지막으로 암조직(COPD 및 IPF)과 정상 조직의 유전자 발현 차이를 조사할 수 도 있습니다. 여기서 흥미로운 점은 이 유전자 발현의 차이가 새롭게 발견한 희귀 세포 유형에서 발생했다는 것입니다.\n\n3 년이 지난 후에도 여전히 강세를 보이는 알파폴드 같은 모델처럼 UCE 가 기반 모델의 승자가 될까요? 아직 확신하긴 어렵습니다. 다만 UCE이 현재 가장 좋은 성능을 보이는 모델이라는 것은 확실해 보입니다."
  },
  {
    "objectID": "book.html",
    "href": "book.html",
    "title": "Book",
    "section": "",
    "text": "1 생명정보학 알고리즘\n파이썬으로 구현하는 생명정보학 알고리즘\n\n\n\n2 목차\n\n1장. 서문\n\n1.1 들어가며\n1.2 생명정보학이란?\n1.3 책의 구성\n\n2장. 파이썬 소개\n\n2.1 파이썬의 특징\n2.2 변수와 미리 정의된 함수\n2.3 파이썬 코드 작성하기\n2.4 파이썬 프로그램 개발\n2.5 객체지향 프로그래밍\n2.6 사전 정의된 클래스 및 메서드\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n3장. 세포 및 분자생물학의 기초\n\n3.1 세포: 생명의 기본 단위\n3.2 유전자 정보: 핵산\n3.3 유전자: 유전 정보의 이산 단위\n3.4 인간 유전체\n3.5 생물 자원 및 데이터베이스\n참고 문헌과 추가 자료\n연습 문제\n\n4장. 생물학적 서열의 기본적 처리\n\n4.1 생물학적 서열: 표현과 기본 알고리즘\n4.2 전사와 역상보\n4.3 번역\n4.4 가능성 있는 유전자 찾기: 오픈 리딩 프레임\n4.5 하나로 합체\n4.6 생물학 서열의 클래스\n4.7 바이오파이썬으로 서열 처리\n4.8 바이오파이썬의 서열 주석 객체\n연습 문제와 프로그래밍 프로젝트\n\n5장. 서열 데이터에서 패턴 찾기\n\n5.1 소개: 생명정보학에서 패턴 찾기의 중요성\n5.2 고정된 패턴을 찾는 단순한 알고리즘\n5.3 휴리스틱 알고리즘: 보이어-무어\n5.4 결정적 유한 오토마타\n5.5 정규표현식으로 유연한 패턴 찾기\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n6장. 쌍 서열 정렬\n\n6.1 소개: 서열 비교와 서열 정렬\n6.2 시각화 정렬: 점 도표\n6.3 서열 정렬의 최적화 문제\n6.4 전역 정렬을 위한 동적 프로그래밍 알고리즘\n6.5 지역 정렬을 위한 동적 프로그래밍 알고리즘\n6.6 서열 정렬의 특별한 경우\n6.7 바이오파이썬을 활용한 쌍 서열 정렬\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n7장. 데이터베이스에서 유사한 서열 찾기\n\n7.1 소개\n7.2 BLAST 알고리즘과 프로그램\n7.3 구현한 BLAST 이식\n7.4 바이오파이썬을 통한 BLAST 사용\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n8장. 다중 서열 정렬\n\n8.1 소개: 문제 정의와 복잡도\n8.2 다중 서열 정렬의 알고리즘 최적화 클래스\n8.3 점진적 정렬을 파이썬에서 구현\n8.4 바이오파이썬으로 정렬 다루기\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n9장. 계통학 분석\n\n9.1 소개: 문제 정의 및 연관성\n9.2 계통학적 분석을 위한 알고리즘 클래스\n9.3 파이썬으로 거리 기반 알고리즘 구현\n9.4 계통학 분석을 위한 바이오파이썬\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n10장. 모티프 발견 알고리즘\n\n10.1 소개: 문제 정의와 관련성\n10.2 브루트 포스 알고리즘: 완전 탐색\n10.3 분기 및 경계 알고리즘\n10.4 휴리스틱 알고리즘\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n11장. 확률적 모티프와 알고리즘\n\n11.1 확률 모티프 표현 및 검색\n11.2 확률 알고리즘: 기댓값 최대화\n11.3 모티프 발견을 위한 깁스 샘플링\n11.4 바이오파이썬의 확률 모티프\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n12장. 은닉 마르코프 모델\n\n12.1 소개: 은닉 마르코프 모델이란 무엇인가?\n12.2 파이썬으로 알고리즘 구현\n12.3 데이터베이스 검색을 위한 은닉 마르코프 모델\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n13장. 그래프: 개념과 알고리즘\n\n13.1 그래프: 정의와 표현\n13.2 파이썬 클래스 그래프\n13.3 인접 노드와 차수\n13.4 경로, 탐색, 거리\n13.5 사이클\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n14장. 그래프와 생물학적 네트워크\n\n14.1 소개\n14.2 네트워크를 그래프로 표현\n14.3 네트워크 위상 분석\n14.4 대사작용 가능성 평가\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n15장. 게놈으로 리드 어셈블리: 그래프 기반 알고리즘\n\n15.1 게놈 어셈블리 소개 및 관련한 도전들\n15.2 오버랩 그래프와 해밀턴 사이클\n15.3 드브루인 그래프와 오일러 경로\n15.4 실제 게놈 어셈블리\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n16장. 참조 유전자 서열에 리드 어셈블리\n\n16.1 소개: 서열 일치 문제의 정의와 응용법\n16.2 패턴 전처리: 트라이\n16.3 서열의 전처리: 접미사 트리\n16.4 버로우즈 휠러 변환\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n17장. 더 읽을거리\n\n17.1 추천하는 생명정보학 서적\n17.2 논문 및 학회\n17.3 정규 교육 과정\n17.4 온라인 교육 자료\n\n\n\n\n\n3 생명과학을 위한 딥러닝\n생물학, 유전체학, 신약 개발에 적용하는 실무 딥러닝\n\n\n분자 데이터에 머신러닝을 적용하는 방법\n딥러닝으로 유전학/유전체학 분석하기\n딥러닝으로 생물물리학 시스템 이해\nDeepChem 라이브러리 소개\n딥러닝을 사용한 현미경 이미지 분석\n딥러닝을 사용한 의료 이미지 분석\nVAE와 GAN 모델\n머신러닝 모델의 작동 원리 해석\n\n\n\n4 목차\n\n1장. 왜 생명과학인가?\n\n딥러닝은 왜 필요한가?\n현대 생명과학은 빅데이터를 다룬다\n무엇을 배우는가?\n\n2장. 딥러닝 소개\n\n선형 모델\n다층 퍼셉트론\n모델 학습하기\n검증하기\n정규화\n하이퍼파라미터 최적화\n다른 유형의 모델들\n\n합성곱 신경망\n순환 신경망\n\n\n3장. DeepChem을 이용한 머신러닝\n\nDeepChem의 기본 데이터셋\n독성 분자 예측 모델 만들기\nMNIST 데이터셋으로 필기 인식 모델 만들기\n\nMNIST 필기 인식 데이터셋\n합성곱 신경망으로 필기 인식하기\n\n소프트맥스와 소프트맥스 교차 엔트로피\n\n4장. 분자 수준 데이터 다루기\n\n분자란 무엇인가?\n\n분자 간 결합\n분자 그래프\n분자 구조\n분자 카이랄성\n\n분자 데이터 피처화\n\nSMILES 문자열과 RDKit\n확장 연결 지문\n분자 표현자\n\n그래프 합성곱\n용해도 예측 모델\nMoleculeNet\nSMARTS 문자열\n\n5장. 생물물리학과 머신러닝\n\n단백질의 구조\n\n단백질 서열\n\n단백질 3차원 구조를 예측할 수 있을까?\n\n단백질-리간드 결합\n\n생물물리학적 피처화\n\n그리드 피처화\n원자 피처화\n\n생물물리학 데이터 사례 연구\n\nPDBBind 데이터셋\nPDBBind 데이터셋 피처화\n\n\n6장. 유전학과 딥러닝\n\nDNA, RNA, 단백질\n실제 세포 내에서 일어나는 일\n전사인자의 결합\n\n전사인자의 결합을 예측하는 합성곱 모델\n\n염색질 접근성\nRNA 간섭\n\n7장. 현미경을 위한 딥러닝\n\n현미경에 대한 간략한 소개\n\n현대의 광학현미경\n\n회절 한계\n\n전자현미경과 원자현미경\n초고해상도 현미경\n딥러닝과 회절 한계\n\n현미경을 위한 시료 준비\n\n시료 염색하기\n시료 고정\n\n시료 절편 가공\n\n형광현미경\n시료 준비 과정의 영향\n\n딥러닝 활용법\n\n세포수 측정\n\n세포주란 무엇인가?\n\n세포 구별하기\n머신러닝과 실험\n\n\n8장. 의료 체계를 위한 딥러닝\n\n컴퓨터 지원 질병 진단\n베이즈 네트워크를 이용한 불확실성 예측\n전자 건강 기록\nICD-10 코드\n비지도 학습이란 무엇인가?\n\n거대 전자 건강 기록 데이터베이스의 위험성\n\n방사선학을 위한 딥러닝\n\nX선 촬영과 CT 촬영\n조직학\nMRI 촬영\n\n치료법으로서의 머신러닝\n당뇨망막병증\n\n9장. 생성 모델\n\nVAE\nGAN\n생명과학에 생성 모델 응용하기\n\n신약 후보 물질 찾기\n단백질 엔지니어링\n과학적 발견을 위한 도구\n\n생성 모델의 미래\n생성 모델 사용하기\n\n생성 모델 결과 분석\n\n\n10장. 딥러닝 모델의 해석\n\n예측값 설명하기\n입력값 최적화하기\n불확실성 예측하기\n해석 가능성, 설명 가능성, 실제 결과\n\n11장. 가상 선별검사\n\n예측 모델을 위한 데이터셋 준비\n머신러닝 모델 학습하기\n예측을 위한 데이터셋 준비하기\n예측 모델 적용하기\n\n12장. 딥러닝의 미래와 전망\n\n질병 진단\n맞춤 의학\n신약 개발\n생물학 연구\n\n\n\n\n5 구입처\n\n알라딘\nYES24\n교보문고\n인터파크\n반디앤루니스\n\n\n\n\n6 파이썬을 활용한 생명정보학 2/e\n\n\n지은이: 티아구 안타오\n옮긴이: 김태윤\n출판사: 에이콘 출판\n\n원제 : Bioinformatics with Python Cookbook - Second Edition\n생명정보학 데이터를 파이썬 프로그래밍 기법과 프레임워크를 사용해 처리한다. 차세대 염기서열 분석, 유전체학, 메타지노믹스(metagenomics), 집단 유전학, 계통 발생학, 프로테오믹스(proteomics)의 내용을 다룬다. 다양한 파이썬 도구와 라이브러리로 데이터를 변환, 분석, 시각화하는 최신 프로그래밍 기법을 배운다. 차세대 염기서열 분석 데이터의 필터링(filtering) 기술과 병렬처리 프레임워크(framework)인 대스크(Dask)와 스파크(Spark)도 소개한다.\n\n\n7 목차\n\n1장. 파이썬과 주변 생태계\n\n소개\n아나콘다를 사용한 필요 소프트웨어 설치\n도커를 사용한 필요 소프트웨어 설치\nrpy2를 통해 R과 인터페이스 만들기\n주피터 노트북에서 R 매직 명령어 사용하기\n\n2장. 차세대 염기서열 분석\n\n소개\nNCBI와 진뱅크 데이터베이스 둘러보기\n염기서열 분석의 기초\n배우기\nFASTQ 파일 다루기\n정렬 데이터 다루기\nVCF 파일 데이터 분석하기\n게놈 접근성과 SNP 데이터 필터하기\nHTSeq로 NGS 데이터 처리하기\n\n3장. 게놈 데이터 다루기\n\n소개\n좋은 품질의 참조 게놈 다루기\n낮은 품질의 참조 게놈 다루기\n게놈 주석 살펴보기\n게놈 주석으로 원하는 유전자 추출하기\nEnsembl REST API로 오소로그검색\nEnsembl REST API로 유전자 온톨로지 정보 검색\n\n4장. 집단유전학\n\n소개\nPLINK 형식 데이터셋 관리하기\nGenepop 파일 형식 소개\nBio.PopGen으로 데이터셋 탐색하기\nF - 통계 계산하기\n주성분 분석하기\nADMIXTURE 프로그램으로 집단 구조 조사하기\n\n5장. 집단유전학 시뮬레이션\n\n소개\n순방향 시뮬레이터 소개\n선택 시뮬레이션\n섬 모델과 디딤돌 모델을 사용한 시뮬레이션\n복잡한 집단 통계 모델 만들기\n\n6장. 계통 발생학\n\n소개\n계통 발생학 분석을 위한 데이터셋 준비\n유전자와 게놈 데이터 정렬\n서열 데이터 비교하기\n계통수 그리기\n재귀적으로 계통수 다루기\n계통수 시각화하기\n\n7장. 단백질 데이터 뱅크 사용하기\n\n소개\n데이터베이스에서 단백질 정보 찾기\nBio.PDB 소개\nPDB 파일에서 더 많은 정보 추출하기\nPDB 파일에서 분자간 거리 계산\n기하학적 계산하기\nPyMOL로 애니메이션 만들기\nBiopython을 사용해 mmCIF 파일 파싱하기\n\n8장. 생명정보학 파이프라인\n\n소개\n갤럭시 서버 소개\nAPI를 사용해 갤럭시 사용하기\n갤럭시 도구 개발\n일반적인 파이프라인 사용법\nAirflow를 사용해 유전변이 분석 파이프라인 만들기\n\n9장. 파이썬으로 유전체 빅데이터 다루기\n\n소개\nHDF5 데이터 형식\n대스크 라이브러리로 병렬분산처리\n파케이 데이터 형식\n스파크 라이브러리로 병렬분산처리\n사이썬과 눔바로 코드 최적화\n\n10장. 생명정보학의 다른 주제들\n\n소개\nQIIME2로 메타지노믹스 분석하기\n생식세포계열로 공통 염색체 찾기\nREST API로 GBIF 데이터베이스 사용하기\nGBIF의 지리 참조 데이터 다루기\n사이토스케이프로 단백질 네트워크 시각화\n\n11장. 고급 차세대 염기서열 분석\n\n소개\n분석을 위한 데이터셋 준비하기\n멘델리언 오류로 데이터 품질 관리\n의사 결정 나무를 사용한 데이터 탐색\n표준 통계로 데이터 탐색\n주석 데이터로 생물학적 특성 찾기\n\n\n\n\n8 구입처\n\n알라딘\nYES24\n교보문고\n인터파크\n반디앤루니스"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 짧은 소개\n다음은 자연스럽게 수정한 글입니다:\n안녕하세요. 이 블로그는 제가 생물학자이자 개발자로서 흥미롭고 유용하다고 생각되는 내용들을 공유하는 공간입니다. 저는 과학자의 삶이 흥미진진한 모험으로 가득해야 한다고 믿습니다. 그래서 이 블로그의 내용이 때로는 다소 산만해 보일 수 있지만, 사실 나름의 고민과 정리를 거친 결과물입니다.\n앙리 푸앵카레의 말처럼, “과학자는 연구의 유용성 때문이 아니라, 그 과정에서 느끼는 희열 때문에 연구합니다.” 이 말은 제 연구 철학과도 일맥상통합니다.\n사실 이 블로그를 시작하기까지 많은 고민이 있었습니다. 완벽주의 성향 때문에 글쓰기를 미뤄왔고, 제 글이 기대에 미치지 못할까 두려워했죠. 하지만 글쓰기 실력을 향상시키고 제 생각을 정리하는 좋은 기회라 생각해 이 블로그를 시작하게 되었습니다. 거창한 목표나 엄격한 계획 없이, 그저 제가 알고 있고 관심 있는 분야에 대해 자유롭게 글을 쓰려고 합니다. 때로는 열심히, 때로는 여유롭게, 과학과 기술에 대한 제 생각과 경험을 나누고자 합니다. 부족한 점이 많겠지만, 이 과정을 통해 조금씩 성장해 나가길 기대합니다.\n\n\n2 작업 환경\n제 작업 환경에 대해서 간략히 적어볼게요.\n\n\n코딩할 때는 Visual studio code 를 쓰고, 파이펫팅 할 때는 Eppendorf 파이펫을 씁니다. 독일제 명품이거든요.\n주로 쓰는 프로그래밍 언어는 Python과 R이고 지금은 Rust 공부하고 있습니다.\n이 블로그는 Github 에서 무료 호스팅되고, Static site generator 인 Quarto 를 사용했습니다.\n\n\n\n\n3 내 블로그의 역사\n오래전부터 존재감 없는 블로그를 만들어왔습니다. 제가 기억하는 선에서 한번 나열하면 아래와 같죠.\n\n\nhttp://netsphere.codex.kr/ : 호스팅 업체가 망해버렸습니다.\nhttp://partrita.posterous.com/ : 서비스가 종료되었습니다.\nhttps://partrita.blogspot.kr/ : 구글검색어 노출에 유리했지만, 제약이 많아서 워드프레스로 이사\nhttps://partrita.wordpress.com/ : 처음에는 설치형을 사용하다. 무료호스팅을 제공하는 wordpress으로 이사했었습니다.\nhttps://partrita.github.io/ : 워드프레스는 너무 올드한 것 같아서 최신 유행인 깃헙으로 이전했습니다.\n\n\n\n\n4 저작권에 대해\n게시되는 글들은 대부분 제 개인적인 연구나 독창적인 내용보다는, 다양한 출처(주로 인터넷)에서 얻은 정보를 제 나름대로 정리하고 해석한 것입니다. 이 과정에서 때로는 저작권이 있는 자료가 포함될 수 있음을 인지하고 있습니다. 저는 항상 정보의 정확성과 출처의 신뢰성을 확인하려 노력하지만 혹시라도 저작권 침해 문제가 발생한다면 즉시 조치를 취하겠습니다. 만약 제 글에서 저작권 침해 사항을 발견하셨다면, partrita@gmail.com으로 연락 주시기 바랍니다. 관련 내용을 신속히 확인하고 필요한 조치를 취하겠습니다. 이 블로그는 지식 공유와 학습을 목적으로 운영되며, 모든 분들의 지적 재산권을 존중합니다. 여러분의 이해에 감사드립니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n학술 논문을 효율적으로 읽는 법\n\n\n\n\n\n\nData science\n\n\nTip\n\n\nProductivity\n\n\n\n\n\n\n\n\n\n2024-11-30\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n파이썬으로 Sankey diagram그리기\n\n\n\n\n\n\nPython\n\n\nVisualization\n\n\nBioinformatics\n\n\n\n\n\n\n\n\n\n2024-11-29\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nSeurat 라이브러리 치트시트(Cheatsheet)\n\n\n\n\n\n\nR\n\n\nTip\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nSeurat\n\n\nCheatsheet\n\n\n\n\n\n\n\n\n\n2024-11-12\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nGSE에서 sc-RNAseq 데이터 가져오기\n\n\n\n\n\n\nPython\n\n\nGene Expression Omnibus\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\n\n\n\n\n\n\n\n2024-11-09\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n임상 통계학\n\n\n\n\n\n\nClinical trial\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n2024-10-25\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n인과 관계 추론을 위한 기본 규칙\n\n\n\n\n\n\nPython\n\n\nCausal inference\n\n\n\n\n\n\n\n\n\n2024-10-19\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nscRNA-seq 소개\n\n\n\n\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-10-16\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n.DS_Store 파일 관리\n\n\n\n\n\n\nProductivity\n\n\nTip\n\n\nMacOS\n\n\nGit\n\n\n\n\n\n\n\n\n\n2024-10-07\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n사보타지 매뉴얼\n\n\n\n\n\n\nProductivity\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-10-07\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRosalind Armory 문제풀이\n\n\n\n\n\n\nPython\n\n\nRosalind\n\n\nBioinformatics\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-09-28\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n실험실 프로토콜 모음\n\n\n\n\n\n\nWet lab\n\n\nTip\n\n\nProtocols\n\n\n\n\n\n\n\n\n\n2024-09-28\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRosalind Textbook track 문제풀이\n\n\n\n\n\n\nPython\n\n\nRosalind\n\n\nBioinformatics\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-09-28\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRosalind Stronghold 문제풀이\n\n\n\n\n\n\nPython\n\n\nRosalind\n\n\nBioinformatics\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-09-28\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRosalind Algorithmic Heights 문제풀이\n\n\n\n\n\n\nPython\n\n\nRosalind\n\n\nBioinformatics\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-09-28\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n문자열 인코딩 문제\n\n\n\n\n\n\nPython\n\n\nR\n\n\nCharacter encoding\n\n\nText encoding\n\n\nData science\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-09-21\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 위한 수학\n\n\n\n\n\n\nMachine Learning\n\n\nMathmatics\n\n\nData science\n\n\n\n\n\n\n\n\n\n2024-09-10\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nDockQ로 도킹 모델 품질 측정하기\n\n\n\n\n\n\nBioinformatics\n\n\nTip\n\n\nDocking\n\n\n\n\n\n\n\n\n\n2024-09-09\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n마음챙김과 효율을 위한 생산성 디버깅\n\n\n\n\n\n\nProductivity\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-09-08\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n오픈 소스 PyMOL 설치하기\n\n\n\n\n\n\nPython\n\n\nProtein engineering\n\n\nVisualization\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-09-01\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\npip 대신에 uv 사용하기\n\n\n\n\n\n\nPython\n\n\nPackage manager\n\n\nVirtual environment\n\n\nData science\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-08-24\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nScanpy로 scRNA-seq 분석 07\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n2024-08-15\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nScanpy로 scRNA-seq 분석 06\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n2024-08-11\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nScanpy로 scRNA-seq 분석 05\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n2024-08-10\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nScanpy로 scRNA-seq 분석 04\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n2024-07-22\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nScanpy로 scRNA-seq 분석 03\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n2024-06-28\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nScanpy로 scRNA-seq 분석 02\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n2024-06-22\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nScanpy로 scRNA-seq 분석 01\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n2024-06-17\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n어떻게 데이터 과학은 작동하는가?\n\n\n\n\n\n\nData science\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-05-30\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nMarsilea로 복잡한 시각화 처리하기\n\n\n\n\n\n\nPython\n\n\nData science\n\n\nBioinformatics\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n2024-04-20\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nScanpy로 gene_id를 gene_symbol로 변경하기\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\n\n\n\n\n\n\n\n2024-04-12\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n미니멀한 폴더 정리: PARA method\n\n\n\n\n\n\nProductivity\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-04-04\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRDS 객체를 10X MEX 형식으로 저장하기\n\n\n\n\n\n\nR\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nSeurat\n\n\n\n\n\n\n\n\n\n2024-03-26\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nAnnData 객체를 10X MEX 형식으로 저장하기\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\n\n\n\n\n\n\n\n2024-03-21\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의존성 지옥에 빠진 당신을 구하러 온 Pixi\n\n\n\n\n\n\nPython\n\n\nPackage manager\n\n\nVirtual environment\n\n\nR\n\n\nTip\n\n\n\n\n\n\n\n\n\n2024-02-28\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nscRNA-seq 데이터 QC하기\n\n\n\n\n\n\nPython\n\n\nscRNA-seq\n\n\nBioinformatics\n\n\nScanpy\n\n\n\n\n\n\n\n\n\n2024-02-22\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n10X genomics scRNA-seq alignment\n\n\n\n\n\n\nPython\n\n\nBioinformatics\n\n\nscRNA-seq\n\n\nCellranger\n\n\n\n\n\n\n\n\n\n2024-02-21\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nminiforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구\n\n\n\n\n\n\nPython\n\n\nPackage manager\n\n\nVirtual environment\n\n\nTip\n\n\nR\n\n\n\n\n\n\n\n\n\n2024-01-28\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n파이썬 statsmodels로 통계분석\n\n\n\n\n\n\nPython\n\n\nStatistics\n\n\nStat\n\n\n\n\n\n\n\n\n\n2023-10-07\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n파이썬 통계분석하기\n\n\n\n\n\n\nPython\n\n\nStat\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n2023-09-17\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nSeurat으로 scRNA seq데이터 분석하기\n\n\n\n\n\n\nR\n\n\nData science\n\n\nBioinformatics\n\n\nSeurat\n\n\n\n\n\n\n\n\n\n2023-04-01\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nggpubr로 논문에 사용할 플랏그리기\n\n\n\n\n\n\nR\n\n\nData science\n\n\nBioinformatics\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n2023-03-01\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n파이썬 Pingouin으로 통계 분석하기\n\n\n\n\n\n\nNews\n\n\nPython\n\n\nData science\n\n\nStat\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n2023-01-23\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter notebook 소개\n\n\n\n\n\n\nNews\n\n\nJupyter\n\n\nPython\n\n\n\n\n\n\n\n\n\n2023-01-10\n\n\nTaeyoon kim\n\n\n\n\n\n\n\n\n\n\n\n\n파이썬과 R 개발 환경 설정하기\n\n\n\n\n\n\nR\n\n\nPackage manager\n\n\nVirtual environment\n\n\nBioconductor\n\n\nPython\n\n\nTip\n\n\nData science\n\n\n\n\n\n\n\n\n\n2023-01-04\n\n\nTaeyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n깔끔하게 데이터 정리하기\n\n\n\n\n\n\nNews\n\n\nPython\n\n\nData science\n\n\nTidy data\n\n\n\n\n\n\n\n\n\n2023-01-03\n\n\nTaeyoon Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/md/PARA_methods.html#project",
    "href": "posts/md/PARA_methods.html#project",
    "title": "미니멀한 폴더 정리: PARA method",
    "section": "2.1 PROJECT",
    "text": "2.1 PROJECT\nPROJECT 는 특정 목표를 가지고 시작한 짧은 기간의 작업을 나타냅니다. 몇 가지 예시는 아래와 같습니다.\n\n웹페이지 디자인\n새로운 컴퓨터 구매\n연구 보고서 작성\n욕실 리모델링\n스페인어 학습"
  },
  {
    "objectID": "posts/md/PARA_methods.html#area",
    "href": "posts/md/PARA_methods.html#area",
    "title": "미니멀한 폴더 정리: PARA method",
    "section": "2.2 AREA",
    "text": "2.2 AREA\nAREA 는 지속적인 관심이 필요한 업무와 삶의 중요한 부분을 나타냅니다. 다음은 몇 가지 예시입니다.\n\n업무적인 책임: 마케팅, 인사, 제품 관리, 연구 및 개발, 엔지니어링\n개인적인 책임: 건강, 재정, 가족, 글쓰기, 자동차, 주택"
  },
  {
    "objectID": "posts/md/PARA_methods.html#resource",
    "href": "posts/md/PARA_methods.html#resource",
    "title": "미니멀한 폴더 정리: PARA method",
    "section": "2.3 RESOURCE",
    "text": "2.3 RESOURCE\nRESOURCE 는 관심이 있는 주제나 배우고 싶은 자료가 포함합니다. 예시는 다음과 같습니다.\n\n그래픽 디자인\n가드닝\n커피\n웹 디자인\n일본어\n습관 형성\n사진 촬영"
  },
  {
    "objectID": "posts/md/PARA_methods.html#archive",
    "href": "posts/md/PARA_methods.html#archive",
    "title": "미니멀한 폴더 정리: PARA method",
    "section": "2.4 ARCHIVE",
    "text": "2.4 ARCHIVE\n마지막으로, 이전 세 가지 범주에 속하지 않는 항목을 포함하는 ARCHIVE 가 있습니다. 이 항목에는 향후 필요할 것으로 예상되는 것들이 포함됩니다. 몇 가지 예시는 다음과 같습니다.\n\n완료한 프로젝트나 보류 중인 프로젝트\n이제 관심이 없는 자료\n\n이것이 전부입니다! PROJECT, AREA, RESOURCE, ARCHIVE 네 가지 상위 폴더면 됩니다.\n삶을 네 가지 범주로 축소하는 것이 어렵겠지만 그러나 이것이 핵심입니다. 정리 시스템이 복잡하다면 그건 유지하기도 어렵고 정리하느라 더 많은 시간을 쓰게 될 것입니다. 그러니 최대한 간단한 시스템이어야 합니다. 그래야 문서 정리 시스템이 당신에게 여유 시간을 만들어 줄 수 있습니다."
  },
  {
    "objectID": "posts/md/PARA_methods.html#프로젝트에-대하여",
    "href": "posts/md/PARA_methods.html#프로젝트에-대하여",
    "title": "미니멀한 폴더 정리: PARA method",
    "section": "3.1 프로젝트에 대하여",
    "text": "3.1 프로젝트에 대하여\n프로젝트 목록을 만드는 것은 일의 성격, 업무량, 그리고 우선순위와 결과을 예측하는데 필요합니다. 아래 항목을 살펴보세요.\n    내 프로젝트 목록\n    1. 채용/인력 확보\n    2. 이벤트\n    3. 직속 보고\n    4. 전략적 계획\n    5. 연구\n    6. 휴가\n    7. 전문 개발\n    8. 생산성\n\n\n\n\n\n\nImportant\n\n\n\n문제점이 뭔지 아시겠나요?\n\n\n이 목록의 항목 하나하나는 사실은 프로젝트가 아닙니다. ” 전략적 계획 ” 이 언제 완료되는지를 정확히 말할 수 있나요? ” 휴가 ” 를 목록에서 지워버릴 수 있나요? 확실하지가 않습니다.\n사실 이 목록에 있는 모든 항목은 책임 영역입니다. 이것은 단순한 언어적인 문제로만 보이지만, 사실 그렇지 않습니다. 얼마나 똑똑하고 열정적인 사람이더라도, 책임 영역을 구체적인 프로젝트로 세분화하지 않는 한, 중요한 작업을 제때에 수행하기는 어렵습니다.\n\n3.1.1 문제점 1. 범위가 명확하지 않습니다\n위 목록을 보면 실제로 얼마나 많은 일을 했는지 정확히 알기 어렵습니다. ” 채용 ” 이라는 항목이 얼마나 많은 작업을 포함하는지 어떻게 알 수 있을까요? 이것은 6 개월마다 파트타임 채용부터 이번 분기에 50 명의 사원을 채용하는 것까지 다양할 수 있습니다. 이러한 불확실성은 업무를 실제보다 더 부담스럽게 만듭니다. 만약 ” 채용 ” 이라는 책임 영역에서 여러 프로젝트를 식별하고 그 목록을 매일 앞에 두면 어떤 작업을 더 해야할지 알 수 있을 것입니다. 예를 들어 아래와 같이요.\n     채용 프로젝트\n     1. \"기계공학 연구원\" 고용\n     2. \"프로젝트 분석가\" 고용\n     3. \"마케팅 책임자\" 고용\n     4. \"현장 요원\" 고용\n     5. \"재무 관리자\" 고용\n\n\n3.1.2 문제점 2. 현재 노력과 장기적인 목표가 연결되지 않습니다\n지식근로자의 가장 어려운 측면 중 하나는 창의성이 필요하다는 것입니다. 그리고 창의성은 동기 부여 없이는 지속할 수 없습니다. 다시 말해 의욕을 잃으면 최선의 결과를 얻을 수 없습니다. 그렇다면 동기는 어떻게 부여할 수 있을까요? 대부분은 진전에 달려 있습니다. 우리가 어딘가로 향하는 것을 안다면, 우리는 단기간의 스트레스와 좌절을 견딜 수 있습니다.\n다시 위 목록을 살펴보세요. 채용 프로젝트 목록에 있는 항목 중 하나도 종료일자나 기간에 대해 언급되어 있지 않습니다. 이러한 상황은 마치 끝없는 지평선과도 같아서 어떻게 노력하든 도달할 수 없는 목표로 보입니다. 솔직히 말해서, 이것보다 일의 의욕을 상실시키는 더 효과적인 방법은 없을 것 같습니다.\n그러니 큰 프로젝트를 더 작은 프로젝트로 만들고 기간을 정해야 합니다. 그래야 작은 성공을 통한 동기 부여가 가능합니다. 예를 들면 ” 이벤트 ” 라는 넓은 범주의 프로젝트를 아래와 같이 더 작은 개별 프로젝트로 나눌 수 있을 겁니다.\n     이벤트 프로젝트\n     1. 분기별 직원 워크샵\n     2. 연례 이해관계자 컨퍼런스\n     3. 연구 방법 워크샵\n     4. 연말 채용 박람회\n     5. 임원 여름 워크샵\n이렇게 하면 현재의 노력이 어떻게 장기적인 목표와 연결되는지 알 수 있을 것입니다. 작업 영역이 얼마나 광범위하든, 시간을 들여서 항상 그것들을 작은 프로젝트로 나눌 수 있습니다. 장기 목표에 대한 진전 상황을 알고 싶다면, 반드시 꼭 그렇게 해야 합니다."
  },
  {
    "objectID": "posts/md/Rosalind_armory.html",
    "href": "posts/md/Rosalind_armory.html",
    "title": "Rosalind Armory 문제풀이",
    "section": "",
    "text": "생물 정보학 분석을 위해 사용할 수 있는 소프트웨어는 이미 많습니다. Rosalind_Stronghold 에서는 알고리즘을 직접 구현했다면, 여기 Rosalind_Armory 에서는 이미 존재하는 도구를 사용하여 비슷한 문제를 풀어봅니다.\nRosalind 는 프로젝트 오일러, 구글 코드 잼 에서 영감을 얻었습니다. 이 프로젝트의 이름은 DNA 이중나선을 발견하는 데 기여한 로잘린드 프랭클린 에서 따왔습니다. Rosalind 는 프로그래밍 실력을 키우고자 하는 생물학자와 분자생물학의 계산 문제를 접해본 적이 없는 프로그래머들에게 도움이 될 것입니다."
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset",
    "href": "posts/md/Rosalind_armory.html#sample-dataset",
    "title": "Rosalind Armory 문제풀이",
    "section": "1.1 Sample Dataset",
    "text": "1.1 Sample Dataset\nAGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGC"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output",
    "href": "posts/md/Rosalind_armory.html#sample-output",
    "title": "Rosalind Armory 문제풀이",
    "section": "1.2 Sample Output",
    "text": "1.2 Sample Output\n20 12 17 21"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution",
    "href": "posts/md/Rosalind_armory.html#solution",
    "title": "Rosalind Armory 문제풀이",
    "section": "1.3 Solution",
    "text": "1.3 Solution\nfrom collections import Counter\n\ndef count_dna_symbols(dna):\n    # Define the symbols we're interested in\n    symbols = [\"A\", \"C\", \"G\", \"T\"]\n    \n    # Use Counter to count occurrences of each symbol in the DNA\n    dna_counter = Counter(dna)\n    \n    # Create a dictionary with counts for each symbol of interest\n    symbols_count = {symbol: dna_counter.get(symbol, 0) for symbol in symbols}\n    \n    return symbols_count\n\nsample_input = \"\"\"\nAGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGC\n\"\"\"\n\n# Get the symbols count\nsymbols_count = count_dna_symbols(sample_input)\n\n# Print the counts in the desired format\nprint(\" \".join(str(symbols_count[symbol]) for symbol in [\"A\", \"C\", \"G\", \"T\"]))"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-1",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-1",
    "title": "Rosalind Armory 문제풀이",
    "section": "2.1 Sample Dataset",
    "text": "2.1 Sample Dataset\nAnthoxanthum\n2003/7/25\n2005/12/27"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-1",
    "href": "posts/md/Rosalind_armory.html#sample-output-1",
    "title": "Rosalind Armory 문제풀이",
    "section": "2.2 Sample Output",
    "text": "2.2 Sample Output\n7"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-1",
    "href": "posts/md/Rosalind_armory.html#solution-1",
    "title": "Rosalind Armory 문제풀이",
    "section": "2.3 Solution",
    "text": "2.3 Solution\nfrom Bio import Entrez\n\ndef get_nucleotide_genbank_entries(genus_name, start_date, end_date):\n    \"\"\"\n    Retrieve the count of Nucleotide GenBank entries for a given genus and date range.\n    \n    Args:\n    genus_name (str): The name of the genus to search for.\n    start_date (str): The start date of the publication range (format: YYYY/MM/DD).\n    end_date (str): The end date of the publication range (format: YYYY/MM/DD).\n    \n    Returns:\n    int: The count of entries found.\n    \"\"\"\n    Entrez.email = \"byterube@gmail.com\"  # Replace with your email\n    \n    query = f'\"{genus_name}\"[Organism] AND (\"{start_date}\"[Publication Date] : \"{end_date}\"[Publication Date])'\n    \n    try:\n        with Entrez.esearch(db=\"nucleotide\", term=query) as handle:\n            record = Entrez.read(handle)\n            return int(record[\"Count\"])\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0\n\n# Sample input data\nsample_input = \"\"\"\nAnthoxanthum\n2003/7/25\n2005/12/27\n\"\"\".strip().split(\"\\n\")\n\n# Extracting input values\ngenus_name = sample_input[0]\nstart_date = sample_input[1]\nend_date = sample_input[2]\n\n# Get the count of GenBank entries\ncount = get_nucleotide_genbank_entries(genus_name, start_date, end_date)\n\n# Print the result\nprint(count)"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-2",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-2",
    "title": "Rosalind Armory 문제풀이",
    "section": "3.1 Sample Dataset",
    "text": "3.1 Sample Dataset\nFJ817486 JX069768 JX469983"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-2",
    "href": "posts/md/Rosalind_armory.html#sample-output-2",
    "title": "Rosalind Armory 문제풀이",
    "section": "3.2 Sample Output",
    "text": "3.2 Sample Output\n&gt;JX469983.1 Zea mays subsp. mays clone UT3343 G2-like transcription factor mRNA, partial cds\nATGATGTATCATGCGAAGAATTTTTCTGTGCCCTTTGCTCCGCAGAGGGCACAGGATAATGAGCATGCAA\nGTAATATTGGAGGTATTGGTGGACCCAACATAAGCAACCCTGCTAATCCTGTAGGAAGTGGGAAACAACG\nGCTACGGTGGACATCGGATCTTCATAATCGCTTTGTGGATGCCATCGCCCAGCTTGGTGGACCAGACAGA\nGCTACACCTAAAGGGGTTCTCACTGTGATGGGTGTACCAGGGATCACAATTTATCATGTGAAGAGCCATC\nTGCAGAAGTATCGCCTTGCAAAGTATATACCCGACTCTCCTGCTGAAGGTTCCAAGGACGAAAAGAAAGA\nTTCGAGTGATTCCCTCTCGAACACGGATTCGGCACCAGGATTGCAAATCAATGAGGCACTAAAGATGCAA\nATGGAGGTTCAGAAGCGACTACATGAGCAACTCGAGGTTCAAAGACAACTGCAACTAAGAATTGAAGCAC\nAAGGAAGATACTTGCAGATGATCATTGAGGAGCAACAAAAGCTTGGTGGATCAATTAAGGCTTCTGAGGA\nTCAGAAGCTTTCTGATTCACCTCCAAGCTTAGATGACTACCCAGAGAGCATGCAACCTTCTCCCAAGAAA\nCCAAGGATAGACGCATTATCACCAGATTCAGAGCGCGATACAACACAACCTGAATTCGAATCCCATTTGA\nTCGGTCCGTGGGATCACGGCATTGCATTCCCAGTGGAGGAGTTCAAAGCAGGCCCTGCTATGAGCAAGTC\nA"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-2",
    "href": "posts/md/Rosalind_armory.html#solution-2",
    "title": "Rosalind Armory 문제풀이",
    "section": "3.3 Solution",
    "text": "3.3 Solution\nfrom Bio import Entrez, SeqIO\nfrom typing import List\n\ndef get_shortest_sequence(entry_ids: List[str]) -&gt; None:\n    \"\"\"\n    Fetch nucleotide sequences for given entry IDs and print the shortest one in FASTA format.\n\n    Args:\n    entry_ids (List[str]): A list of GenBank entry IDs.\n\n    Raises:\n    Exception: If there's an error fetching or parsing the sequences.\n    \"\"\"\n    Entrez.email = \"byterube@gmail.com\"  # Replace with your email\n\n    try:\n        # Fetch sequences\n        with Entrez.efetch(db=\"nucleotide\", id=entry_ids, rettype=\"fasta\", retmode=\"text\") as handle:\n            records = list(SeqIO.parse(handle, \"fasta\"))\n\n        if not records:\n            print(\"No sequences found for the given entry IDs.\")\n            return\n\n        # Find and print the shortest sequence\n        shortest_record = min(records, key=lambda record: len(record.seq))\n        print(shortest_record.format(\"fasta\"))\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Sample input data\nsample_input = \"\"\"\nFJ817486 JX069768 JX469983\n\"\"\"\nentry_ids = sample_input.split()\n\n# Get and print the shortest sequence\nget_shortest_sequence(entry_ids)"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-3",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-3",
    "title": "Rosalind Armory 문제풀이",
    "section": "4.1 Sample Dataset",
    "text": "4.1 Sample Dataset\n&gt;Rosalind_7142\nPFTADSMDTSNMAQCRVEDLWWCWIPVHKNPHSFLKTWSPAAGHRGWQFDHNFFVYMMGQ\nFYMTKYNHGYAPARRKRFMCQTFFILTFMHFCFRRAHSMVEWCPLTTVSQFDCTPCAIFE\nWGFMMEFPCFRKQMHHQSYPPQNGLMNFNMTISWYQMKRQHICHMWAEVGILPVPMPFNM\nSYQIWEKGMSMGCENNQKDNEVMIMCWTSDIKKDGPEIWWMYNLPHYLTATRIGLRLALY\n&gt;Rosalind_4494\nVPHRVNREGFPVLDNTFHEQEHWWKEMHVYLDALCHCPEYLDGEKVYFNLYKQQISCERY\nPIDHPSQEIGFGGKQHFTRTEFHTFKADWTWFWCEPTMQAQEIKIFDEQGTSKLRYWADF\nQRMCEVPSGGCVGFEDSQYYENQWQREEYQCGRIKSFNKQYEHDLWWCWIPVHKKPHSFL\nKTWSPAAGHRGWQFDHNFFSTKCSCIMSNCCQPPQQCGQYLTSVCWCCPEYEYVTKREEM\n&gt;Rosalind_3636\nETCYVSQLAYCRGPLLMNDGGYGPLLMNDGGYTISWYQAEEAFPLRWIFMMFWIDGHSCF\nNKESPMLVTQHALRGNFWDMDTCFMPNTLNQLPVRIVEFAKELIKKEFCMNWICAPDPMA\nGNSQFIHCKNCFHNCFRQVGMDLWWCWIPVHKNPHSFLKTWSPAAGHRGWQFDHNFFQMM\nGHQDWGTQTFSCMHWVGWMGWVDCNYDARAHPEFYTIREYADITWYSDTSSNFRGRIGQN"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-3",
    "href": "posts/md/Rosalind_armory.html#sample-output-3",
    "title": "Rosalind Armory 문제풀이",
    "section": "4.2 Sample Output",
    "text": "4.2 Sample Output\nDLWWCWIPVHK[NK]PHSFLKTWSPAAGHRGWQFDHNFF"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-3",
    "href": "posts/md/Rosalind_armory.html#solution-3",
    "title": "Rosalind Armory 문제풀이",
    "section": "4.3 Solution",
    "text": "4.3 Solution\ninstall MM"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-4",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-4",
    "title": "Rosalind Armory 문제풀이",
    "section": "5.1 Sample Dataset",
    "text": "5.1 Sample Dataset\nJX205496.1 JX469991.1"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-4",
    "href": "posts/md/Rosalind_armory.html#sample-output-4",
    "title": "Rosalind Armory 문제풀이",
    "section": "5.2 Sample Output",
    "text": "5.2 Sample Output\n257"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-4",
    "href": "posts/md/Rosalind_armory.html#solution-4",
    "title": "Rosalind Armory 문제풀이",
    "section": "5.3 Solution",
    "text": "5.3 Solution\nfrom Bio import Entrez, SeqIO\nfrom Bio import Align\nfrom Bio.Seq import Seq\nfrom typing import List, Optional\n\ndef fetch_sequences(genbank_ids: List[str]) -&gt; List[Seq]:\n    \"\"\"\n    Fetch sequences from GenBank given a list of IDs.\n    \n    Args:\n    genbank_ids (List[str]): List of GenBank IDs.\n    \n    Returns:\n    List[Seq]: List of Seq objects representing the fetched sequences.\n    \"\"\"\n    Entrez.email = \"byterube@gmail.com\"  # Replace with your email\n    try:\n        with Entrez.efetch(db=\"nucleotide\", id=genbank_ids, rettype=\"fasta\", retmode=\"text\") as handle:\n            return [record.seq for record in SeqIO.parse(handle, \"fasta\")]\n    except Exception as e:\n        print(f\"Error fetching sequences: {e}\")\n        return []\n\ndef align_sequences(seq1: Seq, seq2: Seq) -&gt; Optional[float]:\n    \"\"\"\n    Perform global alignment of two sequences using Bio.Align.PairwiseAligner.\n    \n    Args:\n    seq1 (Seq): First sequence.\n    seq2 (Seq): Second sequence.\n    \n    Returns:\n    Optional[float]: Alignment score, or None if alignment fails.\n    \"\"\"\n    try:\n        aligner = Align.PairwiseAligner()\n        aligner.mode = 'global'\n        aligner.match_score = 5\n        aligner.mismatch_score = -4\n        aligner.open_gap_score = -10\n        aligner.extend_gap_score = -1\n        \n        alignments = aligner.align(seq1, seq2)\n        return alignments.score\n    except Exception as e:\n        print(f\"Error during alignment: {e}\")\n        return None\n\n# Sample input data\nsample_input = \"\"\"\nJX462666.1 NM_001251956.1\n\"\"\"\ngenbank_ids = sample_input.strip().split()\n\n# Fetch sequences\nsequences = fetch_sequences(genbank_ids)\n\n# Perform alignment\nalignment_score = align_sequences(sequences[0], sequences[1])\n\nif alignment_score is not None:\n    print(f\"{alignment_score}\")"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-5",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-5",
    "title": "Rosalind Armory 문제풀이",
    "section": "6.1 Sample Dataset",
    "text": "6.1 Sample Dataset\n@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!*((((***+))%%%++)(%%%%)***-+*****))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-5",
    "href": "posts/md/Rosalind_armory.html#sample-output-5",
    "title": "Rosalind Armory 문제풀이",
    "section": "6.2 Sample Output",
    "text": "6.2 Sample Output\n&gt;SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-5",
    "href": "posts/md/Rosalind_armory.html#solution-5",
    "title": "Rosalind Armory 문제풀이",
    "section": "6.3 Solution",
    "text": "6.3 Solution\nfrom Bio import SeqIO\nfrom io import StringIO\n\ndef convert_fastq_to_fasta(fastq_string):\n    \"\"\"\n    Convert a FASTQ string to a FASTA string.\n\n    Args:\n    fastq_string (str): Input FASTQ formatted string.\n\n    Returns:\n    str: FASTA formatted string.\n    \"\"\"\n    # Create file-like objects for input and output\n    fastq_handle = StringIO(fastq_string)\n    fasta_handle = StringIO()\n\n    # Perform the conversion\n    SeqIO.convert(fastq_handle, 'fastq', fasta_handle, 'fasta')\n\n    # Get the FASTA string and reset the StringIO\n    fasta_handle.seek(0)\n    fasta_string = fasta_handle.read()\n\n    return fasta_string\n\n# Sample FASTQ input\nfastq = \"\"\"\n@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!*((((***+))%%%++)(%%%%)***-+*****))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65\n\"\"\".strip()\n\n# Convert FASTQ to FASTA\nfasta = convert_fastq_to_fasta(fastq)\n\n# Print the resulting FASTA\nprint(fasta)"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-6",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-6",
    "title": "Rosalind Armory 문제풀이",
    "section": "7.1 Sample Dataset",
    "text": "7.1 Sample Dataset\n28\n@Rosalind_0041\nGGCCGGTCTATTTACGTTCTCACCCGACGTGACGTACGGTCC\n+\n6.3536354;&lt;211/0?::6/-2051)-*\"40/.,+%)\n@Rosalind_0041\nTCGTATGCGTAGCACTTGGTACAGGAAGTGAACATCCAGGAT\n+\nAH@FGGGJ&lt;GB&lt;&lt;9:GD=D@GG9=?A@DC=;:?&gt;839/4856\n@Rosalind_0041\nATTCGGTAATTGGCGTGAATCTGTTCTGACTGATAGAGACAA\n+\n@DJEJEA?JHJ@8?F?IA3=;8@C95=;=?;&gt;D/:;74792."
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-6",
    "href": "posts/md/Rosalind_armory.html#sample-output-6",
    "title": "Rosalind Armory 문제풀이",
    "section": "7.2 Sample Output",
    "text": "7.2 Sample Output\n1"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-6",
    "href": "posts/md/Rosalind_armory.html#solution-6",
    "title": "Rosalind Armory 문제풀이",
    "section": "7.3 Solution",
    "text": "7.3 Solution\nfrom Bio import SeqIO\nfrom io import StringIO\n\ndef phre(data):\n    count = 0\n    # Convert the string input to a file-like object\n    with StringIO(data) as f:\n        threshold = int(f.readline().strip())\n        # Parse the FASTQ data from the string\n        for record in SeqIO.parse(f, \"fastq\"):\n            quality = record.letter_annotations[\"phred_quality\"]\n            average_quality = sum(quality) / len(quality)\n            if average_quality &lt; threshold:\n                count += 1\n    return count\n\n# Sample input data\nsample_input = \"\"\"\n28\n@Rosalind_0041\nGGCCGGTCTATTTACGTTCTCACCCGACGTGACGTACGGTCC\n+\n6.3536354;&lt;211/0?::6/-2051)-*\"40/.,+%)\n@Rosalind_0041\nTCGTATGCGTAGCACTTGGTACAGGAAGTGAACATCCAGGAT\n+\nAH@FGGGJ&lt;GB&lt;&lt;9:GD=D@GG9=?A@DC=;:?&gt;839/4856\n@Rosalind_0041\nATTCGGTAATTGGCGTGAATCTGTTCTGACTGATAGAGACAA\n+\n@DJEJEA?JHJ@8?F?IA3=;8@C95=;=?;&gt;D/:;74792.\n\"\"\".strip()\n\nresult = phre(sample_input)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-7",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-7",
    "title": "Rosalind Armory 문제풀이",
    "section": "8.1 Sample Dataset",
    "text": "8.1 Sample Dataset\nATGGCCATGGCGCCCAGAACTGAGATCAATAGTACCCGTATTAACGGGTGA\nMAMAPRTEINSTRING"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-7",
    "href": "posts/md/Rosalind_armory.html#sample-output-7",
    "title": "Rosalind Armory 문제풀이",
    "section": "8.2 Sample Output",
    "text": "8.2 Sample Output\n1"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-7",
    "href": "posts/md/Rosalind_armory.html#solution-7",
    "title": "Rosalind Armory 문제풀이",
    "section": "8.3 Solution",
    "text": "8.3 Solution\nfrom Bio.Seq import translate\n\ndef find_genetic_code(dna, protein):\n    # List of genetic code table IDs to check\n    table_ids = [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15,]\n    \n    for table in table_ids:\n        try:\n            # Translate DNA using the current table\n            translated = translate(dna, table=table, to_stop=True)\n            \n            # Check if the translated protein matches the given protein\n            if translated == protein:\n                return table\n        except:\n            # If there's an error with a particular table, skip it\n            continue\n    \n    # If no matching table is found\n    return None\n\n# Read input\nsample_input = \"\"\"\nATGGCCATGGCGCCCAGAACTGAGATCAATAGTACCCGTATTAACGGGTGA\nMAMAPRTEINSTRING\n\"\"\".strip().split(\"\\n\")\n\ndna = sample_input[0]\nprotein = sample_input[1]\n\n# Find the genetic code\nresult = find_genetic_code(dna, protein)\n\n# Print the result\nif result:\n    print(result)\nelse:\n    print(\"No matching genetic code found.\")"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-8",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-8",
    "title": "Rosalind Armory 문제풀이",
    "section": "9.1 Sample Dataset",
    "text": "9.1 Sample Dataset\n20 90\n@Rosalind_0049_1\nGCAGAGACCAGTAGATGTGTTTGCGGACGGTCGGGCTCCATGTGACACAG\n+\nFD@@;C&lt;AI?4BA:=&gt;C&lt;G=:AE=&gt;&lt;A??&gt;764A8B797@A:58:527+,\n@Rosalind_0049_2\nAATGGGGGGGGGAGACAAAATACGGCTAAGGCAGGGGTCCTTGATGTCAT\n+\n1&lt;&lt;65:793967&lt;4:92568-34:.&gt;1;2752)24')*15;1,*3+*!\n@Rosalind_0049_3\nACCCCATACGGCGAGCGTCAGCATCTGATATCCTCTTTCAATCCTAGCTA\n+\nB:EI&gt;JDB5=&gt;DA?E6B@@CA?C;=;@@C:6D:3=@49;@87;::;;?8+"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-8",
    "href": "posts/md/Rosalind_armory.html#sample-output-8",
    "title": "Rosalind Armory 문제풀이",
    "section": "9.2 Sample Output",
    "text": "9.2 Sample Output\n2"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-8",
    "href": "posts/md/Rosalind_armory.html#solution-8",
    "title": "Rosalind Armory 문제풀이",
    "section": "9.3 Solution",
    "text": "9.3 Solution\nfrom Bio import SeqIO\nfrom Bio.SeqRecord import SeqRecord\nfrom Bio.Seq import Seq\n\ndef quality_filtration(data):\n    # Read the threshold and percentage from the first line\n    t, p = map(int, data[0].strip().split())\n    \n    count = 0\n    # Process the FASTQ data\n    for i in range(1, len(data), 4):\n        sequence = data[i+1].strip()\n        quality_string = data[i+3].strip()\n        \n        # Convert quality string to Phred scores\n        phred_quality = [ord(char) - 33 for char in quality_string]\n        \n        # Perform quality check\n        passes = sum(ph &gt;= t for ph in phred_quality)\n        if (passes / len(phred_quality)) * 100 &gt;= p:\n            count += 1\n\n    return count\n\n# Sample input\nsample_input = \"\"\"\n20 90\n@Rosalind_0049_1\nGCAGAGACCAGTAGATGTGTTTGCGGACGGTCGGGCTCCATGTGACACAG\n+\nFD@@;C&lt;AI?4BA:=&gt;C&lt;G=:AE=&gt;&lt;A??&gt;764A8B797@A:58:527+,\n@Rosalind_0049_2\nAATGGGGGGGGGAGACAAAATACGGCTAAGGCAGGGGTCCTTGATGTCAT\n+\n1&lt;&lt;65:793967&lt;4:92568-34:.&gt;1;2752)24')*15;1,*3+*!\n@Rosalind_0049_3\nACCCCATACGGCGAGCGTCAGCATCTGATATCCTCTTTCAATCCTAGCTA\n+\nB:EI&gt;JDB5=&gt;DA?E6B@@CA?C;=;@@C:6D:3=@49;@87;::;;?8+\n\"\"\".strip().split(\"\\n\")\n\n# Call the function with the sample input\ncount = quality_filtration(sample_input)\nprint(count)"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-9",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-9",
    "title": "Rosalind Armory 문제풀이",
    "section": "10.1 Sample Dataset",
    "text": "10.1 Sample Dataset\n&gt;Rosalind_64\nATAT\n&gt;Rosalind_48\nGCATA"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-9",
    "href": "posts/md/Rosalind_armory.html#sample-output-9",
    "title": "Rosalind Armory 문제풀이",
    "section": "10.2 Sample Output",
    "text": "10.2 Sample Output\n1"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-9",
    "href": "posts/md/Rosalind_armory.html#solution-9",
    "title": "Rosalind Armory 문제풀이",
    "section": "10.3 Solution",
    "text": "10.3 Solution\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom io import StringIO\n\ndef is_palindrome(seq):\n    \"\"\"Check if a sequence is equal to its reverse complement.\"\"\"\n    my_seq = Seq(seq)\n    reverse_seq = my_seq.reverse_complement()\n    return my_seq == reverse_seq\n\ndef count_palindromic_sequences(seqs):\n    \"\"\"Count the number of palindromic sequences in the list.\"\"\"\n    return sum(is_palindrome(seq) for seq in seqs)\n\ndef parse_fasta_input(fasta_input):\n    \"\"\"Parse FASTA formatted input and return a list of sequences.\"\"\"\n    fasta_io = StringIO(fasta_input)\n    return [str(record.seq) for record in SeqIO.parse(fasta_io, \"fasta\")]\n\n# Sample input\nsample_input = \"\"\"\n&gt;Rosalind_64\nATAT\n&gt;Rosalind_48\nGCATA\n\"\"\"\n\n# Parse the sequences\nsequences = parse_fasta_input(sample_input)\n\n# Count palindromic sequences\npalindrome_count = count_palindromic_sequences(sequences)\n\n# Print the result\nprint(palindrome_count)"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-10",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-10",
    "title": "Rosalind Armory 문제풀이",
    "section": "11.1 Sample Dataset",
    "text": "11.1 Sample Dataset\n26\n@Rosalind_0029\nGCCCCAGGGAACCCTCCGACCGAGGATCGT\n+\n&gt;?F?@6&lt;C&lt;HF?&lt;85486B;85:8488/2/\n@Rosalind_0029\nTGTGATGGCTCTCTGAATGGTTCAGGCAGT\n+\n@J@H@&gt;B9:B;&lt;D==:&lt;;:,&lt;::?463-,,\n@Rosalind_0029\nCACTCTTACTCCCTAGCCGAACTCCTTTTT\n+\n=88;99637@5,4664-65)/?4-2+)$)$\n@Rosalind_0029\nGATTATGATATCAGTTGGCTCCGAGAGCGT\n+\n&lt;@BGE@8C9=B9:B&lt;&gt;&gt;&gt;7?B&gt;7:02+33."
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-10",
    "href": "posts/md/Rosalind_armory.html#sample-output-10",
    "title": "Rosalind Armory 문제풀이",
    "section": "11.2 Sample Output",
    "text": "11.2 Sample Output\n17"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-10",
    "href": "posts/md/Rosalind_armory.html#solution-10",
    "title": "Rosalind Armory 문제풀이",
    "section": "11.3 Solution",
    "text": "11.3 Solution\nfrom Bio import SeqIO\nfrom io import StringIO\n\ndef parse_threshold_and_fastq(data):\n    \"\"\"Parse the threshold and FASTQ data from the input string.\"\"\"\n    lines = data.strip().split('\\n')\n    threshold = int(lines[0])\n    fastq_data = '\\n'.join(lines[1:])\n    return threshold, fastq_data\n\ndef extract_quality_scores(fastq_data):\n    \"\"\"Extract quality scores from FASTQ data.\"\"\"\n    fastq_io = StringIO(fastq_data)\n    return [record.letter_annotations[\"phred_quality\"] \n            for record in SeqIO.parse(fastq_io, \"fastq\")]\n\ndef count_below_threshold(qualities, threshold):\n    \"\"\"Count positions where the average quality score is below the threshold.\"\"\"\n    num_sequences = len(qualities)\n    num_positions = len(qualities[0])\n    count = 0\n    \n    for i in range(num_positions):\n        average_quality = sum(q[i] for q in qualities) / num_sequences\n        if average_quality &lt; threshold:\n            count += 1\n    \n    return count\n\ndef bphr(data):\n    \"\"\"Calculate the number of positions with average quality below the threshold.\"\"\"\n    threshold, fastq_data = parse_threshold_and_fastq(data)\n    qualities = extract_quality_scores(fastq_data)\n    return count_below_threshold(qualities, threshold)\n\n# Sample input\nsample_input = \"\"\"\n26\n@Rosalind_0029\nGCCCCAGGGAACCCTCCGACCGAGGATCGT\n+\n&gt;?F?@6&lt;C&lt;HF?&lt;85486B;85:8488/2/\n@Rosalind_0029\nTGTGATGGCTCTCTGAATGGTTCAGGCAGT\n+\n@J@H@&gt;B9:B;&lt;D==:&lt;;:,&lt;::?463-,,\n@Rosalind_0029\nCACTCTTACTCCCTAGCCGAACTCCTTTTT\n+\n=88;99637@5,4664-65)/?4-2+)$)$\n@Rosalind_0029\nGATTATGATATCAGTTGGCTCCGAGAGCGT\n+\n&lt;@BGE@8C9=B9:B&lt;&gt;&gt;&gt;7?B&gt;7:02+33.\n\"\"\"\n\ncount = bphr(sample_input)\nprint(count)"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-11",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-11",
    "title": "Rosalind Armory 문제풀이",
    "section": "12.1 Sample Dataset",
    "text": "12.1 Sample Dataset\nAGCCATGTAGCTAACTCAGGTTACATGGGGATGACCCCGCGACTTGGATTAGAGTCTCTTTTGGAATAAGCCTGAATGATCCGAGTAGCATCTCAG"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-11",
    "href": "posts/md/Rosalind_armory.html#sample-output-11",
    "title": "Rosalind Armory 문제풀이",
    "section": "12.2 Sample Output",
    "text": "12.2 Sample Output\nMLLGSFRLIPKETLIQVAGSSPCNLS"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-11",
    "href": "posts/md/Rosalind_armory.html#solution-11",
    "title": "Rosalind Armory 문제풀이",
    "section": "12.3 Solution",
    "text": "12.3 Solution\nfrom Bio.Seq import Seq\n\ndef gene_ORFs(dna_sequence):\n    \"\"\"Finds and prints the longest open reading frame (ORF) from a DNA sequence.\"\"\"\n    def translate_sequence(seq):\n        \"\"\"Translate a sequence in all three reading frames and split by stop codons.\"\"\"\n        return [str(seq[i:].translate(to_stop=False)).split(\"*\") for i in range(3)]\n\n    # Create a Seq object from the DNA sequence\n    seq = Seq(dna_sequence)\n    seq_reverse = seq.reverse_complement()\n\n    # Translate the sequence and its reverse complement in all reading frames\n    proteins = []\n    proteins.extend(translate_sequence(seq))\n    proteins.extend(translate_sequence(seq_reverse))\n\n    # Flatten the list of lists\n    proteins = [protein for sublist in proteins for protein in sublist]\n\n    # Find ORFs starting with 'M' and sort by length\n    orfs = sorted([protein[protein.find(\"M\"):] for protein in proteins if \"M\" in protein], key=len, reverse=True)\n    return orfs\n\n# Sample input\nsample_input = \"\"\"\nAGCCATGTAGCTAACTCAGGTTACATGGGGATGACCCCGCGACTTGGATTAGAGTCTCTTTTGGAATAAGCCTGAATGATCCGAGTAGCATCTCAG\n\"\"\"\n\n# Strip any leading/trailing whitespace\ndna = sample_input.strip()\n\n# Find and print the longest ORF\norfs = gene_ORFs(dna)\nprint(orfs[0])"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-12",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-12",
    "title": "Rosalind Armory 문제풀이",
    "section": "13.1 Sample Dataset",
    "text": "13.1 Sample Dataset\n20\n@Rosalind_0049\nGCAGAGACCAGTAGATGTGTTTGCGGACGGTCGGGCTCCATGTGACACAG\n+\nFD@@;C&lt;AI?4BA:=&gt;C&lt;G=:AE=&gt;&lt;A??&gt;764A8B797@A:58:527+,\n@Rosalind_0049\nAATGGGGGGGGGAGACAAAATACGGCTAAGGCAGGGGTCCTTGATGTCAT\n+\n1&lt;&lt;65:793967&lt;4:92568-34:.&gt;1;2752)24')*15;1,*3+*!\n@Rosalind_0049\nACCCCATACGGCGAGCGTCAGCATCTGATATCCTCTTTCAATCCTAGCTA\n+\nB:EI&gt;JDB5=&gt;DA?E6B@@CA?C;=;@@C:6D:3=@49;@87;::;;?8+"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-12",
    "href": "posts/md/Rosalind_armory.html#sample-output-12",
    "title": "Rosalind Armory 문제풀이",
    "section": "13.2 Sample Output",
    "text": "13.2 Sample Output\n@Rosalind_0049\nGCAGAGACCAGTAGATGTGTTTGCGGACGGTCGGGCTCCATGTGACAC\n+\nFD@@;C&lt;AI?4BA:=&gt;C&lt;G=:AE=&gt;&lt;A??&gt;764A8B797@A:58:527\n@Rosalind_0049\nATGGGGGGGGGAGACAAAATACGGCTAAGGCAGGGGTCCT\n+\n&lt;&lt;65:793967&lt;4:92568-34:.&gt;1;2752)24')*15;\n@Rosalind_0049\nACCCCATACGGCGAGCGTCAGCATCTGATATCCTCTTTCAATCCTAGCT\n+\nB:EI&gt;JDB5=&gt;DA?E6B@@CA?C;=;@@C:6D:3=@49;@87;::;;?8"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-12",
    "href": "posts/md/Rosalind_armory.html#solution-12",
    "title": "Rosalind Armory 문제풀이",
    "section": "13.3 Solution",
    "text": "13.3 Solution\ndef convert_to_phred(quality_string):\n    \"\"\"Convert a quality string to Phred scores.\"\"\"\n    return [ord(char) - 33 for char in quality_string]\n\ndef find_quality_bounds(phred_quality, threshold):\n    \"\"\"Find the start and end indices where quality is &gt;= threshold.\"\"\"\n    start = 0\n    while start &lt; len(phred_quality) and phred_quality[start] &lt; threshold:\n        start += 1\n    \n    end = len(phred_quality)\n    while end &gt; start and phred_quality[end-1] &lt; threshold:\n        end -= 1\n    \n    return start, end\n\ndef format_fastq_record(identifier, sequence, quality_string, start, end):\n    \"\"\"Format a FASTQ record with the given bounds.\"\"\"\n    return f\"@{identifier}\\n{sequence[start:end]}\\n+\\n{quality_string[start:end]}\"\n\ndef Base_Filtration_Quality(data):\n    \"\"\"Process FASTQ data and filter based on quality threshold.\"\"\"\n    threshold = int(data[0])\n    filtered_records = []\n\n    for i in range(1, len(data), 4):\n        identifier = data[i].strip()\n        sequence = data[i+1].strip()\n        quality_string = data[i+3].strip()\n        \n        phred_quality = convert_to_phred(quality_string)\n        start, end = find_quality_bounds(phred_quality, threshold)\n        \n        if start &lt; end:\n            filtered_record = format_fastq_record(identifier, sequence, quality_string, start, end)\n            filtered_records.append(filtered_record)\n    \n    return filtered_records\n\n# Sample input\nsample_input = \"\"\"\n20\n@Rosalind_0049\nGCAGAGACCAGTAGATGTGTTTGCGGACGGTCGGGCTCCATGTGACACAG\n+\nFD@@;C&lt;AI?4BA:=&gt;C&lt;G=:AE=&gt;&lt;A??&gt;764A8B797@A:58:527+,\n@Rosalind_0050\nAATGGGGGGGGGAGACAAAATACGGCTAAGGCAGGGGTCCTTGATGTCAT\n+\n1&lt;&lt;65:793967&lt;4:92568-34:.&gt;1;2752)24')*15;1,*3+*!\n@Rosalind_0051\nACCCCATACGGCGAGCGTCAGCATCTGATATCCTCTTTCAATCCTAGCTA\n+\nB:EI&gt;JDB5=&gt;DA?E6B@@CA?C;=;@@C:6D:3=@49;@87;::;;?8+\n\"\"\".strip().split(\"\\n\")\n\n# Process the input and print the results\nfiltered_records = Base_Filtration_Quality(sample_input)\nfor record in filtered_records:\n    print(record)"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-13",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-13",
    "title": "Rosalind Armory 문제풀이",
    "section": "14.1 Sample Dataset",
    "text": "14.1 Sample Dataset\n&gt;Rosalind_12\nGACTCCTTTGTTTGCCTTAAATAGATACATATTTACTCTTGACTCTTTTGTTGGCCTTAAATAGATACATATTTGTGCGACTCCACGAGTGATTCGTA\n&gt;Rosalind_37\nATGGACTCCTTTGTTTGCCTTAAATAGATACATATTCAACAAGTGTGCACTTAGCCTTGCCGACTCCTTTGTTTGCCTTAAATAGATACATATTTG"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-13",
    "href": "posts/md/Rosalind_armory.html#sample-output-13",
    "title": "Rosalind Armory 문제풀이",
    "section": "14.2 Sample Output",
    "text": "14.2 Sample Output\n2 2"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-13",
    "href": "posts/md/Rosalind_armory.html#solution-13",
    "title": "Rosalind Armory 문제풀이",
    "section": "14.3 Solution",
    "text": "14.3 Solution\nfrom Bio import SeqIO\nfrom io import StringIO\n\ndef hamming_distance(s1, s2):\n    return sum(c1 != c2 for c1, c2 in zip(s1, s2))\n\ndef count_inexact_repeats(pattern, seq, max_mismatches=3):\n    count = 0\n    pattern_length = len(pattern)\n    seq_length = len(seq)\n    \n    for i in range(seq_length - pattern_length + 1):\n        if hamming_distance(seq[i:i+pattern_length], pattern) &lt;= max_mismatches:\n            count += 1\n    \n    return count\n\n# Your sample input\nsample_input = \"\"\"\n&gt;Rosalind_12\nGACTCCTTTGTTTGCCTTAAATAGATACATATTTACTCTTGACTCTTTTGTTGGCCTTAAATAGATACATATTTGTGCGACTCCACGAGTGATTCGTA\n&gt;Rosalind_37\nATGGACTCCTTTGTTTGCCTTAAATAGATACATATTCAACAAGTGTGCACTTAGCCTTGCCGACTCCTTTGTTTGCCTTAAATAGATACATATTTG\n\"\"\"\n\n# Parse the FASTA data\nfasta_io = StringIO(sample_input.strip())\nsequences = list(SeqIO.parse(fasta_io, \"fasta\"))\n\n# Extract the sequences as strings\ns = str(sequences[0].seq)\nt = str(sequences[1].seq)\n\n# Find the shared inexact repeat\n# We'll use a simple approach: try all substrings of length 32-40 from s as potential repeats\nbest_repeat = \"\"\nbest_count = 0\n\nfor length in range(32, 41):\n    for i in range(len(s) - length + 1):\n        potential_repeat = s[i:i+length]\n        count_s = count_inexact_repeats(potential_repeat, s)\n        count_t = count_inexact_repeats(potential_repeat, t)\n        total_count = count_s + count_t\n        \n        if total_count &gt; best_count:\n            best_count = total_count\n            best_repeat = potential_repeat\n\n# Count the occurrences of the best repeat in both sequences\ncount_s = count_inexact_repeats(best_repeat, s)\ncount_t = count_inexact_repeats(best_repeat, t)\n\nprint(f\"{count_s} {count_t}\")"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-dataset-14",
    "href": "posts/md/Rosalind_armory.html#sample-dataset-14",
    "title": "Rosalind Armory 문제풀이",
    "section": "15.1 Sample Dataset",
    "text": "15.1 Sample Dataset\n&gt;Rosalind_18\nGACATGTTTGTTTGCCTTAAACTCGTGGCGGCCTAGCCGTAAGTTAAG\n&gt;Rosalind_23\nACTCATGTTTGTTTGCCTTAAACTCTTGGCGGCTTAGCCGTAACTTAAG\n&gt;Rosalind_51\nTCCTATGTTTGTTTGCCTCAAACTCTTGGCGGCCTAGCCGTAAGGTAAG\n&gt;Rosalind_7\nCACGTCTGTTCGCCTAAAACTTTGATTGCCGGCCTACGCTAGTTAGTTA\n&gt;Rosalind_28\nGGGGTCATGGCTGTTTGCCTTAAACCCTTGGCGGCCTAGCCGTAATGTTT"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#sample-output-14",
    "href": "posts/md/Rosalind_armory.html#sample-output-14",
    "title": "Rosalind Armory 문제풀이",
    "section": "15.2 Sample Output",
    "text": "15.2 Sample Output\nRosalind_7"
  },
  {
    "objectID": "posts/md/Rosalind_armory.html#solution-14",
    "href": "posts/md/Rosalind_armory.html#solution-14",
    "title": "Rosalind Armory 문제풀이",
    "section": "15.3 Solution",
    "text": "15.3 Solution\nfrom Bio import SeqIO\nfrom io import StringIO\n\ndef simple_msa(sequences):\n    # Find the length of the longest sequence\n    max_length = max(len(seq) for seq in sequences)\n    \n    # Pad shorter sequences with gaps\n    aligned_sequences = [seq.ljust(max_length, '-') for seq in sequences]\n    \n    return aligned_sequences\n\ndef calculate_differences(seq1, seq2):\n    return sum(a != b for a, b in zip(seq1, seq2))\n\ndef find_most_different_sequence(fasta_string):\n    # Parse the FASTA string\n    fasta_io = StringIO(fasta_string)\n    sequences = list(SeqIO.parse(fasta_io, \"fasta\"))\n    \n    # Perform a simple multiple sequence alignment\n    aligned_seqs = simple_msa([str(seq.seq) for seq in sequences])\n    \n    n_seqs = len(aligned_seqs)\n    avg_distances = []\n\n    for i in range(n_seqs):\n        total_distance = sum(calculate_differences(aligned_seqs[i], aligned_seqs[j]) \n                             for j in range(n_seqs) if i != j)\n        avg_distances.append(total_distance / (n_seqs - 1))\n\n    most_different_index = avg_distances.index(max(avg_distances))\n    return sequences[most_different_index].id\n\n# Your sample dataset\nsample_data = \"\"\"\n&gt;Rosalind_18\nGACATGTTTGTTTGCCTTAAACTCGTGGCGGCCTAGCCGTAAGTTAAG\n&gt;Rosalind_23\nACTCATGTTTGTTTGCCTTAAACTCTTGGCGGCTTAGCCGTAACTTAAG\n&gt;Rosalind_51\nTCCTATGTTTGTTTGCCTCAAACTCTTGGCGGCCTAGCCGTAAGGTAAG\n&gt;Rosalind_7\nCACGTCTGTTCGCCTAAAACTTTGATTGCCGGCCTACGCTAGTTAGTTA\n&gt;Rosalind_28\nGGGGTCATGGCTGTTTGCCTTAAACCCTTGGCGGCCTAGCCGTAATGTTT\n\"\"\"\n\nresult = find_most_different_sequence(sample_data)\nprint(result)"
  },
  {
    "objectID": "posts/md/Install_pixi.html#기존의-의존성-관리-도구",
    "href": "posts/md/Install_pixi.html#기존의-의존성-관리-도구",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "1.1 기존의 의존성 관리 도구",
    "text": "1.1 기존의 의존성 관리 도구\n\npip: 파이썬의 기본 패키지 관리자로, 패키지 설치 및 관리에 사용됩니다.\nvenv: Python 3.3 이후부터 기본적으로 제공되는 가상 환경 생성 도구로 프로젝트별로 독립된 가상 환경을 만들어주는 도구로, 각 프로젝트의 의존성을 분리하여 관리할 수 있습니다.\npipenv: Pipfile과 Pipfile.lock 파일을 사용하여 의존성을 관리하는 도구로, 가상 환경과 패키지 설치를 함께 처리합니다.\npoetry: 프로젝트의 의존성 및 버전 관리, 패키지 빌드 및 배포 등을 지원하는 도구로, 편리한 프로젝트 관리를 제공합니다.\nconda: 데이터 과학 및 기계 학습을 위한 패키지 관리 및 환경 관리 도구로, 파이썬 패키지 외에도 다양한 라이브러리 및 도구를 관리할 수 있습니다."
  },
  {
    "objectID": "posts/md/Install_pixi.html#새로운-의존성-관리-도구",
    "href": "posts/md/Install_pixi.html#새로운-의존성-관리-도구",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "1.2 새로운 의존성 관리 도구",
    "text": "1.2 새로운 의존성 관리 도구\nPixi는 기존의 conda 생태계 위에 구축된 빠른 소프트웨어 패키지 매니저로 Windows, macOS 및 Linux에서 신속하게 개발 환경을 구축하는데 적합합니다. 심지어 Docker 없이도 운영 체제 간 재현 가능한 환경을 생성하며 Python, R, C/C++, Rust, Ruby 및 기타 많은 언어를 지원합니다.\n\n무엇보다도 빠르다는 것이 큰 장점입니다.\n\n\n\n\n\n\n\nWarning\n\n\n\nPixi는 활발하게 개발되고 있기 때문에 아래의 명령어가 변경될 가능성이 있습니다."
  },
  {
    "objectID": "posts/md/Install_pixi.html#가상환경-만들기",
    "href": "posts/md/Install_pixi.html#가상환경-만들기",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "4.1 가상환경 만들기",
    "text": "4.1 가상환경 만들기\n새로운 프로젝트를 시작할 때는 다음 명령어를 사용합니다.\n❯ pixi init hello-world\n❯ cd hello-word\n그러면 아래와 같은 3개의 파일이 생성됩니다.\n.\n├── .gitattributes\n├── .gitignore\n└── pixi.toml\n\n0 directories, 3 **files******\n\npixi.toml에 의존성 패키지 정보가 저장되게 됩니다."
  },
  {
    "objectID": "posts/md/Install_pixi.html#의존성-추가",
    "href": "posts/md/Install_pixi.html#의존성-추가",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "4.2 의존성 추가",
    "text": "4.2 의존성 추가\n의존성을 추가할 때 가장 먼저 해야 할 작업은 파이썬 버전을 지정하는 것입니다. pixi는 자체적으로 Python 버전을 설치하는 것을 지원합니다. 아래와 같이 pixi add 명령어를 사용하세요.\n❯ pixi add python==3.10\n✔ Added python 3.10\n위 명령어를 실행하면 1개의 폴더(.pixi)와 pixi.lock이 생성되는 것을 알수 있습니다. .pixi폴더는 설치한 패키지들이 저장되는 곳이고 pixi.lock은 현재 프로젝트의 종속성 및 버전을 정확하게 기록한 파일입니다.\n.\n├── .gitattributes\n├── .gitignore\n├── .pixi\n├── pixi.lock\n└── pixi.toml\n\n1 directory, 4 files\n\n\n\n\n\n\nNote\n\n\n\npixi.lock파일은 pixi add를 실행하거나 수동으로 pixi.toml 파일을 변경한 뒤 pixi install을 실행할 때 생성됩니다.\n\n\n다른 원하는 패키지는 설치할 때도 pixi add 명령어로 추가 할 수 있습니다. 항상 Prefix.dev에서 접속해서 지원되는 패키지를 검색하고 명령어를 확인해 보도록 하세요.\n\n4.2.1 채널 추가하기\npixi는 기본적으로 conda-forge를 기본 패키지 채널로 씁니다. 가끔 conda-forge에 해당 패키지가 없다면 아래와 같이 다른 채널을 추가하면 됩니다.\n➜ pixi project channel add bioconda\n✔ Added bioconda (https://conda.anaconda.org/bioconda/)\n\n❯ pixi add anndata2ri\n✔ Added anndata2ri\n\n\n\n\n\n\nTip\n\n\n\n아직 패키지 별로 채널을 선택해서 패키지를 설치하는 명령어는 없는 것으로 보입니다. 그러니 당분간은 위의 명령어를 사용할 수 밖에 없습니다.\n\n\n\n\n4.2.2 PyPI 의존성 추가\nPrefix.dev에서 찾을 수 없는 파이썬 패키지의 경우 pixi add --pypi [패키지이름]으로 설치 할 수 있습니다.\n\n\n\n\n\n\nWarning\n\n\n\n현재 이 기능은 베타 버전입니다."
  },
  {
    "objectID": "posts/md/Install_pixi.html#패키지-설치하기",
    "href": "posts/md/Install_pixi.html#패키지-설치하기",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "4.3 패키지 설치하기",
    "text": "4.3 패키지 설치하기\npixi add 명령어는 기본적으로 pixi.toml 파일에 패키지 이름을 추가하는 역할을 합니다. 보통은 동시에 pixi.lock파일도 업데이트가 됩니다. 그렇지 않은 경우에는 pixi install 명령어를 사용하면 업데이트와 설치가 동시에 됩니다. 그냥 항상 습관적으로 입력하세요.\n❯ pixi install"
  },
  {
    "objectID": "posts/md/Install_pixi.html#패키지-삭제하기",
    "href": "posts/md/Install_pixi.html#패키지-삭제하기",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "4.4 패키지 삭제하기",
    "text": "4.4 패키지 삭제하기\n설치한 패키지를 삭제하는 것은 2가지 방법이 있습니다. 두 번째 방법이 안되면 첫 번째방법으로 해보세요.\n\npixi.toml파일을 수정하고 pixi install명령어 입력하기\npixi remove [패키지] 명령어 사용하기\n\n\n\n\n\n\n\nNote\n\n\n\nPyPI로 설치한 패키지는 pixi remove --pypi [패키지]로 삭제할 수 있습니다."
  },
  {
    "objectID": "posts/md/Install_pixi.html#실행하기",
    "href": "posts/md/Install_pixi.html#실행하기",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "4.5 실행하기",
    "text": "4.5 실행하기\n\n4.5.1 shell 실행\n❯ pixi shell\n\n\n4.5.2 명령어 실행\npixi run [명령어] 형식으로 실행하면 됩니다.\n❯ pixi run python --version\nPython 3.10.0"
  },
  {
    "objectID": "posts/md/Install_pixi.html#파일로-의존성-저장하기",
    "href": "posts/md/Install_pixi.html#파일로-의존성-저장하기",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "4.6 파일로 의존성 저장하기",
    "text": "4.6 파일로 의존성 저장하기\n아직 명령어로 지원하고 있지 않습니다. 하지만 아래와 같이 conda 명령어로 우회할 수 있습니다.\n❯ pixi run conda env export -f env.yml"
  },
  {
    "objectID": "posts/md/Install_pixi.html#yml-파일에서-의존성-불러오기",
    "href": "posts/md/Install_pixi.html#yml-파일에서-의존성-불러오기",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "4.7 .yml 파일에서 의존성 불러오기",
    "text": "4.7 .yml 파일에서 의존성 불러오기\n❯ pixi init --import environment.yml\n환경을 가져올 때, 환경 파일에서 종속성이 있는 pixi.toml이 생성됩니다. 다만 pip 패키지의 종속성으로 git+URL을 지원하지 않습니다."
  },
  {
    "objectID": "posts/md/Install_pixi.html#가상환경-삭제",
    "href": "posts/md/Install_pixi.html#가상환경-삭제",
    "title": "의존성 지옥에 빠진 당신을 구하러 온 Pixi",
    "section": "4.8 가상환경 삭제",
    "text": "4.8 가상환경 삭제\n`\n아직 명령어로 지원하고 있지 않습니다. 프로젝트 폴더 전체를 삭제하거나 pixi와 연관된 파일을 삭제하는 방법으로 우회합니다.\n❯ rm -rf .pixi\n❯ rm pixi.toml \n❯ rm pixi.lock"
  },
  {
    "objectID": "posts/md/Lab_protocols.html",
    "href": "posts/md/Lab_protocols.html",
    "title": "실험실 프로토콜 모음",
    "section": "",
    "text": "이곳에는 실험 방법에 대한 간략한 정보가 있습니다. 계속해서 자주 사용하는 프로토콜들을 정리할 예정입니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#왜-이름이-miniprep-인가요",
    "href": "posts/md/Lab_protocols.html#왜-이름이-miniprep-인가요",
    "title": "실험실 프로토콜 모음",
    "section": "2.1 왜 이름이 miniprep 인가요?",
    "text": "2.1 왜 이름이 miniprep 인가요?\n다양한 회사에서 plasmid DNSA 를 정제하는 하는 키트를 생산하고 있는데, 시작하는 E.coli 배양액의 양에 따라서 다음과 같이 miniprep, midiprep, maxiprep 라고 공통적으로 부르고 있습니다. 이름에서 알 수 있듯 양이 적으면 mini 많으면 maxi 입니다.\nminiprep 은 (상대적으로) 빠르고 작은 규모로 plasmid DNA 를 뽑아 낼 수 있기에 많이 사용되고 있습니다. 원리는 alkaline lysis method 를 사용하고 있는데 간단히 이야기 하면 염기성 용액으로 셀을 깨주고, 빠르게 중화시켜 Plasmid DNA 만 선택해 내는 방법 입니다. 보통의 경우 miniprep 으로 약 50µg 의 plasmid DNA 를 얻을 수 있습니다.\n\nminiprep 에 사용되는 다양한 키트가 있지만, 여기에서는 Qiagen Spin Miniprep 키트 기준으로 설명합니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#준비물",
    "href": "posts/md/Lab_protocols.html#준비물",
    "title": "실험실 프로토콜 모음",
    "section": "2.2 준비물",
    "text": "2.2 준비물\n\n하룻밤 키운 E.coli 배양액 (1-5ml)\n차가운 P1 버퍼 (50 mM Tris-HCl pH 8.0, 10 mM EDTA, 100 µg/ml RNaseA) RNaseA 가 들어있습니다, 항상 냉장 보관하세요.\nP2 버퍼 (200 mM NaOH, 1% SDS)\nN3 버퍼 (4.2 M Gu-HCl, 0.9 M potassium acetate, pH 4.8)\nPB 버퍼 (5 M Gu-HCl, 30% ethanol)\nPE 버퍼 (10 mM Tris-HCl pH 7.5, 80% ethanol)\nEB 버퍼 (10 mM Tris·Cl, pH 8.5; DW 로 대체하셔도 됩니다.)\nQIAprep spin column (키트에 포함되어 있음)\nCentrifuge\n멸균된 1.5-ml microcentrifuge 튜브"
  },
  {
    "objectID": "posts/md/Lab_protocols.html#방법",
    "href": "posts/md/Lab_protocols.html#방법",
    "title": "실험실 프로토콜 모음",
    "section": "2.3 방법",
    "text": "2.3 방법\n\nmicrocentrifuge 튜브에 1.5 ml 의 배양액을 넣어 줍니다.\n최대 속도로 Centrifuge for 1 min at room temperature, gently aspirate out the supernatant and discard it.\n충분한 크기의 cell pellet 이 생길때 까지 반복합니다.\n250 µl 의 차가운 Buffer P1 를 넣어주고, Resuspend pelleted bacterial cells\nAdd 250 μl Buffer P2 and 조심스럽게 invert the tube 4–6 times to mix.\nAdd 300 µl of Buffer N3.\n\n\n\n\n\n\n\nNote\n\n\n\nProceed to the next step within immediately !\n\n\n\nClose the tube tightly and invert the tube 4 - 6 times . The solution should become cloudy.\nCentrifuge for 10 min at 13,000 rpm (~17,900 x g) in a table-top microcentrifuge. A compact white pellet will form. Apply the supernatants from step 4 to the QIAprep spin column by decanting or pipetting.\nCentrifuge for 30–60 s. Discard the flow-through. Spinning for 60 seconds produces good results.\nWash QIAprep spin column by adding 0.75 ml Buffer PE and centrifuging for 30–60 s.\nDiscard the flow-through, and centrifuge for an additional 1 min to remove residual wash buffer.\n\n\n\n\n\n\n\nImportant\n\n\n\nIMPORTANT: Residual wash buffer will inhibit subsequent enzymatic reactions.\n\n\n\nPlace the QIAprep column in a clean 1.5 ml microcentrifuge tube. To elute DNA, add 50 μl Buffer EB (10 mM Tris·Cl, pH 8.5) or water to the center of each QIAprep spin column, let stand for 1 min, and centrifuge for 1 min.\n\n\n\n\n\n\n\nNote\n\n\n\n\n만약 높은 농도로 elution 받고 싶다면 add 30 μL 의 DW 를 컬럼 가운데에 넣고, incubate at room temperature on the bench for 5 mins and then centrifuge for 1 min.\n한번에 10 개 이상의 miniprep 을 한다면 vacuum manifold 방법을 사용하는게 빠릅니다.\ncell lysate 를 column 에 두번 거치면 수율이 약 20% 증가됩니다.\n시퀀싱 경과가 안나오는 이유는 대체로 염에 의한 오염으로, Washing 하는 과정을 충분하게 해야합니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#원리",
    "href": "posts/md/Lab_protocols.html#원리",
    "title": "실험실 프로토콜 모음",
    "section": "3.1 원리",
    "text": "3.1 원리\n이름에서 알 수 있듯이 효소가 사용됩니다. 항원의 농도는 기질 전환 정도에 달라집니다 항체나 항원이 고체상에 흡착되어있으며 이렇게 해야지만 결합하지 않은 free 항원들을 세척으로 없앨 수 있습니다. 실험동물을 immunization 한 후 얻은 serum 이나 fusion 을 통해 얻은 하이브리도마의 culture supernatant 안에 원하는 항체가 생성되어있는지 확인할 수 있습니다. 방사능을 사용하지 않으면서 검사할 수 있고 샌드위치와 경쟁적 ELISA 방법이 가장 많이 사용됩니다. Dirent ELISA 도 사용되는데 항원이 고체상에 고정됩니다. 이 방법은 항원특이적 항체 검출시에 용이합니다. 효소로는 간단한 기질을 넣어주었을때 색이 나는 반응을 이용합니다. 대표적으로는 alkaline phosphatase 와 HRP(horseradish peroxidase) 가 사용됩니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#종류",
    "href": "posts/md/Lab_protocols.html#종류",
    "title": "실험실 프로토콜 모음",
    "section": "3.2 종류",
    "text": "3.2 종류\n\nDirect ELISA: 항원과 반응하는 항체에 바로 효소를 결합시킨다.\nIndirect ELISA: 항원과 결합하는 항체 (1 차항체) 에는 효소가 없고, 그 항체와 결합하는 항체 (2 차항체) 에는 효소가 결합되어있다. 일반적으로 isotype 에 대한 항체에 효소가 결합된 형태로 판매되고 있다. 자신이 이용하는 일차항체의 isotype 에 맞는 효소결합항체를 구입하여 사용하면된다.항체를 정량 및 정성적으로 분석 할때 사용한다.\nSandwich ELISA: 항원에 대한 항체를 먼저 well 에 결합시키고 그 항체에 대한 항원 (시료) 을 결합시킨다. 그 후 직접적이나 간접법으로 조사한다. 항원을 정성 및 정량적으로 분석한다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#ab1-파일을-열기-위한-프로그램",
    "href": "posts/md/Lab_protocols.html#ab1-파일을-열기-위한-프로그램",
    "title": "실험실 프로토콜 모음",
    "section": "4.1 ab1 파일을 열기 위한 프로그램",
    "text": "4.1 ab1 파일을 열기 위한 프로그램\n저는 개인적으로 ApE 를 선호하지만, 다음과 같이 다양한 프로그램이 있습니다. 마음에 드시는 것을 선택하시면 됩니다.\n\n4Peaks (Mac)\nSnapGene Viewer (Mac/Windows)\nFinchTV(Mac/PC)\nSequence Scanner (Windows)\nChromas (Windows)\nApE(Mac/Windows)"
  },
  {
    "objectID": "posts/md/Lab_protocols.html#참고-사항",
    "href": "posts/md/Lab_protocols.html#참고-사항",
    "title": "실험실 프로토콜 모음",
    "section": "4.2 참고 사항",
    "text": "4.2 참고 사항\n\n믿을 수 있는 DNA 서열은 trace 크로마토그램이 서로 겹쳐있지 않아야 합니다\nDNA 서열은 500-700 개 까지는 믿을 만합니다: 생거 시퀀싱에는 한계로 인해서 한 번에 약 600 개 정도 까지의 결과값만 신뢰할 수 있습니다. 최근에는 약 1000 개까지도 신뢰할 만합니다.\nDNA 처음 20-30 개의 서열은 신뢰도가 낮습니다. 그래서 Primer 가 붙는 위치를 원하는 서열의 앞쪽 50 개 정도로 선택하는게 좋습니다.\nDNA 시퀀싱을 보낼 샘플은 깨끗해야 합니다. 항상 깨끗한 튜브에 깔끔하게 준비한 DNA 샘플을 보내세요. 시퀀싱 반응이 되지 않더라도 업체는 돈을 받습니다. 그리고 생각보다 시퀀싱 반응은 오염에 민감합니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#sds-page-gel-만들기",
    "href": "posts/md/Lab_protocols.html#sds-page-gel-만들기",
    "title": "실험실 프로토콜 모음",
    "section": "5.1 SDS-PAGE Gel 만들기",
    "text": "5.1 SDS-PAGE Gel 만들기\n준비물 : - 2X running gel buffer : 750 mM Tris-HCl, pH8.8 상온보관 - 1x stacking gel solution : 62.5mM Tris-HCl, pH6.8, 4% acrylamide 냉장보관 - 30% acrylamide stock solution (29:1): 냉장보관 - 10% Ammonium persulfate : 냉장보관\n\n\n\n\n\n\nNote\n\n\n\nAcrylamide 의 quality 는 해상도에 영향을 줍니다. 따라서 high quality 를 사용하시는 편이 좋고 가급적 빠른 시간내 소비하는 것이 좋습니다.\n\n\n\n2x running gel buffer 5ml + 20% acrylamide stock 5ml + 10% APS 100ul 섞어 줍니다.\nTEMED 10 ul 넣고 천천히 섞어줍니다.\nGel cast 에 기포가 생기지 않게 조심히 부어 줍니다.\n100% Ethanol 을 위에 조심스럽게 넣습니다. 양은 500ul 정도 입니다.\n30 분후 Gel 이 굳은 걸 확인하고 DW 로 gel 윗 부분을 한번 씻어줍니다.\n1x stacking gel solution 1ml + 10% APS 10ul 를 섞어 줍니다.\nTEMED 1ul 를 마지막으로 넣어 천천히 섞어 줍니다.\n5 번 과정을 끝낸 running gel 위에 붇고 comb 을 꼽아 줍니다.\n20 분후 comb 을 뽑고 실험에 사용합니다.\n\n\n\n\n\n\n\nNote\n\n\n\n\nGel 이 굳는 시간은 전적으로 APS 의 첨가량에 따라 달라지며 시간이 촉박한 경우 APS 의 양을 늘려 줍니다. Gel 이 완전히 굳지 않았을 경우에 해상도는 떨어집니다. 이러한 점에서 오랜 시간 굳히는 방법보다는 APS 의 양을 늘려 빨리 굳히는 편이 좋습니다.\nSDS 는 sample buffer 와 running buffer 에 들어있으므로 gel 에 넣지 않아도 해상도에 문제가 없습니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#standard-protocol-insert-vector-dna-ligation",
    "href": "posts/md/Lab_protocols.html#standard-protocol-insert-vector-dna-ligation",
    "title": "실험실 프로토콜 모음",
    "section": "6.1 Standard Protocol: Insert + Vector DNA Ligation",
    "text": "6.1 Standard Protocol: Insert + Vector DNA Ligation\n대부분의 경우 3 insert : 1 vector 비율을 추천한다. 또한 ligation 반응 시 총 DNA 의 양은 100ng 정도가 권장 된다.\n\nCombine the following in a PCR or Eppendorf tube:\n\n25ng Vector DNA\n75ng Insert DNA\nLigase Buffer (1μL/10μL reaction for 10X buffer, and 2μL/10μL reaction for 5X buffer) 0.5-1μL T4 DNA Ligase\nDW 를 넣어 total 10μL\n만약 DNA 농도가 너무 낮다면 total volume 을 증가 시켜서 진행한다.\n항상 Vector 만 넣은 control 실험을 하고 다양한 vector : insert 비율을 시도 한다.\n\nRT 에서 2 시간 혹은 16°C 에서 overnight 반응 시킨다.\n\n“high concentration” ligase 를 사용하는 경우 RT 5 분이면 충분하다.\n\ntransformation 을 진행한다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#vectorinsert-비율에-대하여",
    "href": "posts/md/Lab_protocols.html#vectorinsert-비율에-대하여",
    "title": "실험실 프로토콜 모음",
    "section": "6.2 Vector:Insert 비율에 대하여",
    "text": "6.2 Vector:Insert 비율에 대하여\n보통 3:1 비율로 넣어줄 경우 충분하지만 안될 경우 vector: insert 비율을 조절 해 줄 필요가 있다. 자동으로 계산해주는 도구 이 있으니 참고한다.\n\n\n\n\n\n\nTip\n\n\n\nligation 단계는 실험의 성공 여부를 알기 힘들기 때문에 항상 아래와 같은 컨트롤 넣어서 실험을 하는 것을 추천합니다.\n\n\n\n\n\n\n\n\n\n\nControl\nLigase\nInterpretation\n\n\n\n\nUncut vector\n-\nChecks viability of competent cells and verifies the antibiotic resistance of the plasmid\n\n\nCut vector\n-\nBackground due to uncut vector\n\n\nCut vector\n+\nBackground due to vector re-circularization - most useful for phosphatase treated vector\n\n\nInsert or water\n+\nAny colonies indicate contamination of intact plasmid in ligation or transformation reagents"
  },
  {
    "objectID": "posts/md/Lab_protocols.html#목적",
    "href": "posts/md/Lab_protocols.html#목적",
    "title": "실험실 프로토콜 모음",
    "section": "7.1 목적",
    "text": "7.1 목적\n복잡한 전체 genome 중에 연구하고자 하는 유전자가 희귀유전자를 분석하고 연구하는데 가장 큰 문제점이였다. PCR 은 특정 DNA sequence 의 copy 수를 기하급수적으로 증폭시킴으로써 증폭된 DNA 를 여러 가지 실험에 이용할 수 있고, 실험 결과를 토대로 분자생물학, 의학, 이학, 농학, 수의학, 식품과학, 환경과학 연구에 응용할 수 있음"
  },
  {
    "objectID": "posts/md/Lab_protocols.html#pcr-구성-요소",
    "href": "posts/md/Lab_protocols.html#pcr-구성-요소",
    "title": "실험실 프로토콜 모음",
    "section": "7.2 PCR 구성 요소",
    "text": "7.2 PCR 구성 요소\n\nDNA, RNA template: 증폭 대상이 되는 DNA, RNA\nPCR Primers: 증폭할 부분을 잡는 짧은 염기서열.\nTaq polymerase: 열에 특별히 강한 유전자 합성효소 (Taq polymerase: Thermus aquaticus 라는 온천에 사는 세균의 DNA polymerase, 72℃가 최적온도, 94℃에서도 안정함)\ndNTP (dATP, dCTP, dGTP, dTTP): 유전자를 합성하는 재료가 되는 각 nucleotide\nMgCl+2: dNTP 와 복합체를 형성하여 효소활성, primer annealing 등에 관여"
  },
  {
    "objectID": "posts/md/Lab_protocols.html#pcr-의-단계",
    "href": "posts/md/Lab_protocols.html#pcr-의-단계",
    "title": "실험실 프로토콜 모음",
    "section": "7.3 PCR 의 단계",
    "text": "7.3 PCR 의 단계\n\nDNA 의 변성 (denaturation)\n\n90℃∼96℃로 가열하여 두가닥 DNA 를 단일가닥 DNA 로 분리.\n일반적으로 94℃사용: 높은 온도일수록 단일가닥 DNA 로 잘 이행되지만 온도가 너무 높으면 Taq DNA polymerase 역시 activity(활성) 가 낮아짐.\n첫 Cycle 에서는 확실한 변성을 위하여 약 5 분간 지속시킴.\n이 후의 cycle 에서는 약 1 분간 변성시킴.\n\nPrimer 의 결합 (annealing)\n\n50℃∼65℃에서 진행.\n30sec~1min.\n염기 간의 결합은 G, C 의경우 세군데 에서 수소결합이 일어나고 A, T 는 두군데에서 결합이 일어나므로 G+C 비율에 따라 결합 온도 결정.\nPrimer design 시에 Annealing temperature 를 고려해야 함.\n일반적으로 GC content 가 50% 가 되는 primer 쌍을 이용하는 것이 바람직.\n\nDNA 의 합성 (polymerization, extension)\n\n70℃∼74℃에서 시행.\n1min ~ 1min 30sec.\nTaq DNA polymerase 의 합성 속도: 2,000∼4,000 nucleotides/min, 1 kb 마다 1 분 정도의 시간 필요. 원하는 PCR 산물의 크기가 크 거나 반응요소의 농도가 낮을 때에는 시간을 연장할 수 있음.\nCycle 이 계속되면서 효소 활성이 감소할 수 있고 DNA 산물은 점점 많이 존재하게 되므로 cycle 후반부에는 반응시간을 조금씩 늘려가는 것도 좋은 방법의 하나이며 마지막 cycle 에는 약 10 분 정도 시간을 충분히 주어서 효소의 활성이 충분히 발휘되도록 함."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#pcr-유의-사항",
    "href": "posts/md/Lab_protocols.html#pcr-유의-사항",
    "title": "실험실 프로토콜 모음",
    "section": "7.4 PCR 유의 사항",
    "text": "7.4 PCR 유의 사항\n\n여러 component 를 혼합할 때에는 시료간에 오염이 되지 않도록 주의하여야 하며 가능하면 공기를 통한 오염을 방지할 수 있는 tip 을 사용하는 것이 좋습니다. 반응물 혼합 시에는 tube 를 ice 상에 두고서 혼합하여야 상온에서의 잘못 primer annealing 에 의한 extension 을 방지할 수 있습니다. 이론적으로 Taq DNA polymerase 는 최적 온도 이하에서도 반응이 어느 정도 진행됨 으로 상온 등에서 정확하게 annealing 되지 않은 primer 에 의한 임의의 반응이 진행됨으로써 원하는 size 의 product 이외의 non-specific product 가 만들어질 수 있습니다.\nPCR 은 민감도가 뛰어난 실험이기 때문에 아주 적은 양의 DNA 가 오염되더라도 실험에 큰 영향을 미칠 수 있습니다. 그러므로 PCR 을 위한 template 를 준비하는 곳과 PCR 반응을 하는 곳, 그리고 PCR 후 전기영동 및 분석을 하는 곳은 격리시키는 것이 좋으며, DNase 와 RNase free PCR tube 를 사용하는 것이 좋습니다. 모든 시약류는 반드시 autoclave 와 filteration 을 거친 후 사용하여야 합니다.\nPCR cycling 조건은 PCR 의 종류와 주형 DNA, primer 그리고 PCR 기기등에 따라 달라 져야 합니다\ntemplate DNA 의 완전한 denaturation 이 중요한데 94℃∼95℃에서 2∼3 min 정도로 충분하지만 대부분 5 min 정도 초기 변성 시간을 주는 것이 좋습니다. denaturation 이 충분하지 않으면 primer 의 annealing 과 extension 이 방해받아 정확한 반응물이 생기지 않을 수 도 있습니다.\n보통 94℃∼95℃에서 20∼30 sec 정도이지만 PCR 기기와 tube 등에 따라 시간을 늘리기도 합니다. Template 의 GC 함량이 높으면 높은 온도와 긴 시간을 사용하기도 하지만 필요 이상으로 변성 온도가 높거나 길면 Taq DNA polymerase 의 활성 이 감소됩니다.\n대개의 경우 annealing 온도는 primer 의 Tm 값에 따라 결정됩니다. 온도가 너무 높으면 primer 가 annealing 되지 않아 PCR product 가 생기지 않게 되고, 온도가 너무 낮으면 non-specific annealing 이 일어나 정확한 PCR product 가 생기지 않습니다.\nTaq polymerase 의 경우 72℃에서 1 초당 약 60 개의 염기를 중합시키기 때문에 1 kb 까지는 45 sec 정도면 충분합니다. 하지만 대부분의 경우 1kb 당 1 분정도의 시간이 필요합니다.\n대부분의 경우 25∼35 cycles 을 진행하고, Template 분자가 10 개 이하인 경우에는 40 cycles 정도 진행하면 product 을 관찰할 수 있습니다. 그러나 cycle 의 수를 무작 정 늘린다고 해서 product 의양이 급격히 늘지는 않으며 오히려 비특이적 밴드가 늘어날 수 있습니다.\nPCR 기기는 기본적으로 PCR 반응을 구성하는 세 가지 온도를 최소한의 시간에 정확하 고 재현성있게 유지할 수 있어야 합니다. 또한 반응 tube 에 따라 열전도율의 차이가 있기 때문에 가능하면 thin-walled reaction tube 를 사용하는 것이 좋으며 thermal cycler 의 block 에 꼭 맞는 크기를 사용하여야 합니다.\nTemplate 양과 질은 PCR 에 절대적인 영향을 미칩니다. template 가 적을수록 product 의 양 역시 비례적으로 감소하게 되며, RNA 의 오염은 Mg2+ 이온을 잡아먹어 yield 를 낮추게 되고 오염된 template 에는 반응저해제들을 많이 포함 하고 있어 반응의 효율을 떨어뜨립니다.\nPCR 의 많은 요소들 중에서도 primer 의 염기서열과 농도는 전체 반응의 성패에 가장 큰 영향을 미치는 요인 중 하나로 다음과 같은 사항들을 고려하여 설계하는 것이 좋습니다. 길이는 18∼24mer 가 적당하며 두 primer 의 Tm 값의 차이는 5℃ 이내로 하고 가급적 2 차 구조가 형성되지 않도록하며 G+C 값은 40∼60% 로하여 두 primer 의 3′ 사이에 상보결합이 없어야 합니다.\nPCR 반응에 사용하는 Taq DNA polymerase 는 0.5∼2.5U/20∼50ul volume 정도가 적당합니다. 비율적으로 너무 많은 효소가 들어가게 되면 높은 glycerol 농도로 인하여 product 가 끌리는 현상이나 특이성이 떨어져 불균형적인 결과를 초래하게 되며, 너무 적은 양의 효소를 사용하면 생성물의 양이 부족하게 됩니다.\n항상 dNTP 의 4 가지 요소들은 동일 농도로 사용하여야 합니다. dNTP mixture 의 불균 형은 Taq polymerase 의 fidelity 를 감소시켜 error rate 가 증가될 수 있습니다. 또한 dNTP stock 은 thawing/freezing 에 민감하여 3∼5 차례만 반복하여도 활성이 감소하여 올바른 결과를 기대할 수 없습니다. 그러므로 stock 은 사용량에 맞게끔 적절하게 배분해놓는 것이 좋습니다. 만일 dNTP 의 농도를 증가시키려면 반드시 Mg2+ 의 농도 역시 증가시켜 주어야 합니다. 높은 dNTP 농도는 free Mg2+ 을 감소시켜 효소의 반응을 방해하고 primer 의 annealing 을 감소시키게 됩니다. 일반적으로 사용되는 dNTP 의 최종 농도는 각 200∼250uM 입니다.\nMg2+ 은 dNTP 와 복합체를 형성하여 효소의 실질적인 substrate 로 이용됩니다. free Mg2+ 의 농도는 dNTP, free pyrophosphate 그리 고 EDTA 같은 ion 결합 물질의 농도에 영향을 받게 됩니다. 최적의 실험결과를 위해선 적절한 MgCl2 의 농도를 사용하여야 하는데 가장 일반적인 농도는 1.5mM (dNTP 각 200uM 일 때) 입니다. Mg2+ 은 효소 활성에 영향을 미치고 double-strand DNA 의 Tm 값 을 증가시키는 효과가 있습니다. 과다한 Mg2+ 은 primer 의 비 특이적인 결합과 background 를 증가시키게 됩니다.\nPCR 반응을 하는 동안 mixture 가 증발되는 것을 방지하기 위하여 mineral oil 을 넣어주어야 합니다. 하지만 PCR 기기의 두껑에 히터가 달려있다면 mineral oil 을 넣어 줄 필요 없습니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#annealing-온도",
    "href": "posts/md/Lab_protocols.html#annealing-온도",
    "title": "실험실 프로토콜 모음",
    "section": "7.5 Annealing 온도",
    "text": "7.5 Annealing 온도\nTm 은 DNA 두 가닥이 반쯤 풀어졌을때 또는 두 가닥 ssDNA 가 반쯤 annealing 됬을떄의 온도를 말합니다. ” 반 쯤 ” 이라는 표현은 ” 반쯤 풀어졌을때 ” 와 ” 반쯤 붙었을때 ” 가 같은 의미로 사용됩니다. 긴 template 와 고농도의 짧은 primer 들이 같이 있을때 프라이머가 적당한 온도가 되면 서서히 유사한 서열에 붙기 시작합니다. 하지만 Tm 에서는 반 밖에 붙지 못하므로, 특히 프라이머의 3’ 쪽이 붙지 못한 상태라면 PCR 이 일어나지 않습니다. 그래서 온도를 좀 더 내려서 프라이머가 template 에 더 붙도록 합니다. DNA 의 Tm 값은 다음과 같이 계산할 수 있습니다.\n\\[ Tm = 4*(G+C)+2*(A+T) \\]\nPrimer 를 디자인할때 가능하면 같은 Tm 값의 forward,reverse primer 를 주문하는 하는것이 좋습니다. 그렇지 못한 경우 낮은 Tm 값의 primer 를 기준으로 약 5 도 정도 낮은 온도에서 PCR 을 수행합니다. 만약 원하는 사이즈의 PCR 밴드와 잡밴드가 같이 뜨면 annealing 온도를 2 도씩 올려 가면서 PCR 실행합니다. 만약 실험실에 gradient PCR 기가 있으면 Tm 값에서 위아래 5 도정도로 설정해 한번에 하는 방법도 있습니다. 보통 20~30 mer 사이는 보통 55 도 근처에서 하면 잘 나오는경우가 많습니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#pcr-잘-안되는-경우",
    "href": "posts/md/Lab_protocols.html#pcr-잘-안되는-경우",
    "title": "실험실 프로토콜 모음",
    "section": "7.6 PCR 잘 안되는 경우",
    "text": "7.6 PCR 잘 안되는 경우\nprimer 의 길이가 50~60mer 를 넘어간다던가 GC 비율이 현저하게 높거나 낮을때에는 annealing temperature 를 찾아야 하는 경우가 많습니다. 그럴때는 바로 gradient PCR 을 최저 온도 (보통 48~50 도) 에서 윗쪽으로 해보는것이 좋습니다. 잡밴드를 감수하고 원하는 밴드를 PCR 하기 위함입니다."
  },
  {
    "objectID": "posts/md/Lab_protocols.html#footnotes",
    "href": "posts/md/Lab_protocols.html#footnotes",
    "title": "실험실 프로토콜 모음",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://bitesizebio.com/articles/how-to-analyze-dna-sequencing-results-properly/↩︎"
  },
  {
    "objectID": "posts/md/Install_pymol.html",
    "href": "posts/md/Install_pymol.html",
    "title": "오픈 소스 PyMOL 설치하기",
    "section": "",
    "text": "PyMOL 은 생화학과 분자 생물학에서 자주 사용되는 분자 시각화 소프트웨어입니다. 단백질, 핵산, 소형 화합물 등의 3D 구조를 시각화하고 분석하고 그래픽 렌더링 기능을 통해 고품질의 이미지를 생성할 수 있습니다. PyMOL 에는 오픈 소스 버전과 PyMOL by Schrödinger(상업 라이센스) 가 존재하기 때문에 영리기관 (회사) 에서 일을 하시거나 라이센스 문제를 겪고 싶지 않다면 오픈 소스 버전을 컴파일해서 설치하는 것을 추천합니다."
  },
  {
    "objectID": "posts/md/Install_pymol.html#미니콘다-설치",
    "href": "posts/md/Install_pymol.html#미니콘다-설치",
    "title": "오픈 소스 PyMOL 설치하기",
    "section": "1.1 미니콘다 설치",
    "text": "1.1 미니콘다 설치\n먼저 Windows 용 Miniconda 를 설치합니다. 자세한 설치 방법은 공식 홈페이지를 참고하세요. Anaconda 를 사용하시거나 다른 유사한 conda 배포판을 사용하셔도 상관은 없습니다."
  },
  {
    "objectID": "posts/md/Install_pymol.html#pymol-환경-생성",
    "href": "posts/md/Install_pymol.html#pymol-환경-생성",
    "title": "오픈 소스 PyMOL 설치하기",
    "section": "1.2 PyMOL 환경 생성",
    "text": "1.2 PyMOL 환경 생성\nPyMOL 3.0.x 용 가상 환경을 생성합니다.\n$conda create -n pymol python=3.12\n그런 다음 가상 환경을 활성화하고 필요한 패키지를 설치합니다.\n$conda activate pymol\n\n# 필요한 패키지 설치\n(pymol-3.0)$conda install numpy pmw pyqt pip"
  },
  {
    "objectID": "posts/md/Install_pymol.html#파일-다운로드-및-pymol-설치",
    "href": "posts/md/Install_pymol.html#파일-다운로드-및-pymol-설치",
    "title": "오픈 소스 PyMOL 설치하기",
    "section": "1.3 파일 다운로드 및 PyMOL 설치",
    "text": "1.3 파일 다운로드 및 PyMOL 설치\n파이썬의 wheel 파일은 파이썬 패키지를 배포하기 위한 표준 형식 중 하나입니다. .whl 확장자를 가지며, 설치 속도 빠르고 빌드가 불필요하다는 장점이 있습니다. 윈도우용 PyMOL wheel 파일은 GitHub 리포지토리 의 릴리즈 페이지 에서 다운로드할 수 있습니다.\nWindows 11 x64 의 경우, pymol-3.0.0-cp312-cp312-win_amd64.whl 을 다운로드합니다. 그런 다음 아래 명령어로 설치합니다. 아래 사용한 파일명은 예시로 다운로드한 파일의 이름으로 변경하셔야 합니다.\n(pymol)$python -m pip install pymol-3.0.0-cp312-cp312-win_amd64.whl\n\n\n\n\n\n\nNote\n\n\n\n예전 버전과는 달리 더 이상 pymol_launcher 가 필요하지 않습니다.\n\n\n만약 오류가 발생했다면 wheel 파일의 경로가 올바른지 확인하세요. 오류 메시지가 발생하지 않았다면 다음의 명령어로 PyMOL 을 실행할 수 있습니다:\n(pymol)$pymol"
  },
  {
    "objectID": "posts/md/Install_pymol.html#기본-패키지-매니저-사용",
    "href": "posts/md/Install_pymol.html#기본-패키지-매니저-사용",
    "title": "오픈 소스 PyMOL 설치하기",
    "section": "2.1 기본 패키지 매니저 사용",
    "text": "2.1 기본 패키지 매니저 사용\nLinux 에서는 최신 버전을 제공하지 않는 경우가 많지만 오픈 소스 PyMOL을 위한 바이너리 패키지를 기본 제공하기 때문에 아주 편리하게 설치 할 수 있습니다. 아래는 일부 인기 있는 배포판에 대한 설치 명령어입니다. 참고로 모든 명령은 superuser 로 실행해야 합니다.\n# Debian/Ubuntu/Mint\napt-get install pymol\n\n# Fedora\ndnf install pymol\n\n# Arch/Manjaro\npacman -S pymol\n\n# CentOS with EPEL\nrpm -i http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-5.noarch.rpm\nyum --enablerepo=epel install pymol\n우분투를 NVIDIA 그래픽 카드 및 일반 드라이버와 함께 사용하는 경우 렌더링 불량, 검은색 픽셀화 및 기타 그래픽 이상이 발생할 수 있습니다. NVIDIA 전용 드라이버 설치 가이드는 Ubuntu 커뮤니티 Nvidia 드라이버 설치 방법 에서 확인할 수 있습니다."
  },
  {
    "objectID": "posts/md/Install_pymol.html#homebrew-사용",
    "href": "posts/md/Install_pymol.html#homebrew-사용",
    "title": "오픈 소스 PyMOL 설치하기",
    "section": "2.2 Homebrew 사용",
    "text": "2.2 Homebrew 사용\nHomebrew는 macOS 와 Linux 시스템을 위한 패키지 관리자입니다. Homebrew는 전용 디렉토리에 패키지를 설치하기 때문에 시스템의 다른 부분에 영향을 주지 않는 장점이 있습니다. 저는 가능하다면 이 방법을 가장 추천합니다.\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nHomebrew 를 설치가 완료되면 다음 명령어로 간단하게 설치가 가능합니다.\nbrew install pymol"
  },
  {
    "objectID": "posts/md/Install_pymol.html#footnotes",
    "href": "posts/md/Install_pymol.html#footnotes",
    "title": "오픈 소스 PyMOL 설치하기",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://omicx.cc/posts/2024-04-13-install-open-source-pymol-3.0.x-in-windows-11/↩︎"
  },
  {
    "objectID": "posts/md/Install_miniforge.html#맘바mamba는-또-뭐죠",
    "href": "posts/md/Install_miniforge.html#맘바mamba는-또-뭐죠",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "3.1 맘바(mamba)는 또 뭐죠?",
    "text": "3.1 맘바(mamba)는 또 뭐죠?\n콘다의 다른 단점에 패키지 설치 속도가 느리다는 점이 었습니다. 그래서 C++으로 작성된 맘바라는 도구가 새로 나오게 되었죠.\n\n파이썬 생태계에는 항상 새로운 도구가 우후죽순 나오죠. 그것이 장점이자 단점 입니다."
  },
  {
    "objectID": "posts/md/Install_miniforge.html#설치하기",
    "href": "posts/md/Install_miniforge.html#설치하기",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "3.2 설치하기",
    "text": "3.2 설치하기\n공식문서에서 각각의 OS에 맞는 설치 방법을 찾아 볼 수 있습니다. 저는 리눅스를 사용하기에 아래 명령어로 설치 하였습니다."
  },
  {
    "objectID": "posts/md/Install_miniforge.html#리눅스와-맥",
    "href": "posts/md/Install_miniforge.html#리눅스와-맥",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "3.3 리눅스와 맥",
    "text": "3.3 리눅스와 맥\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\nbash Mambaforge-$(uname)-$(uname -m).sh"
  },
  {
    "objectID": "posts/md/Install_miniforge.html#윈도우즈",
    "href": "posts/md/Install_miniforge.html#윈도우즈",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "3.4 윈도우즈",
    "text": "3.4 윈도우즈\nGithub repo를 참고하세요."
  },
  {
    "objectID": "posts/md/Install_miniforge.html#가상환경-만들기",
    "href": "posts/md/Install_miniforge.html#가상환경-만들기",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.1 가상환경 만들기",
    "text": "4.1 가상환경 만들기\n맘바포지의 (base) 환경에 왠만하면 아무런 패키지를 설치하지 않을 것을 권장합니다. 따라서 새로운 가상환경인 ipynb를 만들어 주피터랩을 사용해 보겠습니다.\nmamba create -n ipynb python=3.11 pip r-base r-essentials\n기존 conda 명령어에서 mamba로 바꾸기만 하면 됩니다. python=3.11을 통해 파이썬 버전을 지정할 수 있고, PYPI 패키지를 설치하기 위해 pip도 같이 설치해줍니다. 저는 R을 사용하기 위해 r-base, r-essential 도 설치해주었습니다."
  },
  {
    "objectID": "posts/md/Install_miniforge.html#가상환경-활성화-하기",
    "href": "posts/md/Install_miniforge.html#가상환경-활성화-하기",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.2 가상환경 활성화 하기",
    "text": "4.2 가상환경 활성화 하기\n생성된 가상환경은 mamba env list명령어를 통해 리스트로 볼 수 있습니다. 위에서 만든 가상환경을 활성화 하려면 다음 명령어를 입력합니다.\nmamba activate ipynb"
  },
  {
    "objectID": "posts/md/Install_miniforge.html#anaconda에서-제공하는-패키지",
    "href": "posts/md/Install_miniforge.html#anaconda에서-제공하는-패키지",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.3 Anaconda에서 제공하는 패키지",
    "text": "4.3 Anaconda에서 제공하는 패키지\nAnaconda.org에서 제공하는 파이썬과 R 패키지는 다음 명령어를 사용해 쉽게 설치할 수 있습니다. miniforge에서 사용하는 기본 채널은 conda-forge입니다. 그래서 해당 채널을 원하지 않는 경우에는 검색을 해보고 conda install명령어를 복사해서 사용합니다.\n\n\n\n\n\n\nNote\n\n\n\nAnaconda.org를 살펴보면 동일한 이름의 패키지도 다양한 채널을 가지고 있습니다. 서로 다른 채널을 섞어서 사용하면 의존성 문제가 발생할 수 있기 때문에 가능하다면 동일한 채널의 패키지들로 구성하세요.\n\n\n\n4.3.1 설치하기\nmamba install [패키지]\n\n\n4.3.2 제거하기\n설치한 패키지를 제거하고 싶다면 아래 명령어를 사용합니다.\nmamba remove [패키지]"
  },
  {
    "objectID": "posts/md/Install_miniforge.html#pypi에서-제공하는-파이썬-패키지",
    "href": "posts/md/Install_miniforge.html#pypi에서-제공하는-파이썬-패키지",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.4 PyPI에서 제공하는 파이썬 패키지",
    "text": "4.4 PyPI에서 제공하는 파이썬 패키지\n대부분의 경우 conda 명령어로 해결할 수 있지만 가끔 PyPI에서만 제공하는 패키지를 써야할 경우도 있습니다. 그럴 때는 먼저 PYPI에서 패키지 검색을 해보시고 아래 명령어를 사용하는 것을 권장합니다.\n\n\n\n\n\n\nNote\n\n\n\n이 부분이 약간이 까다로운 부분입니다. 파이썬의 내장 패키지인 pip을 사용하다보면 실수로 global(프로젝트 폴더가 아닌 전체를 의미)에 설치를 하는 경우가 생기기 때문이죠.\n\n\n\n\n\n\n\n\nWarning\n\n\n\npip 명령어는 가장 마지막 수단이 되어야 합니다. 항상 mamba install을 최우선으로 고려하세요."
  },
  {
    "objectID": "posts/md/Install_miniforge.html#cran에서-제공하는-r-패키지",
    "href": "posts/md/Install_miniforge.html#cran에서-제공하는-r-패키지",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.5 CRAN에서 제공하는 R 패키지",
    "text": "4.5 CRAN에서 제공하는 R 패키지\n다시 한번 Anaconda.org에서 제공하는 방법을 사용하는 것이 최선입니다만, 만약 그게 어렵다면 R 콘솔에 들어가서 install.packages(\"[패키지 이름]\") 으로 설치할 수 있습니다.\n\n4.5.1 설치\n\n먼저 conda 가상 환경에서 pip 설치\n\nmamba install pip -y\n\npip install대신 다음 명령어를 사용하기\n\npython -m pip install [패키지]\n\n\n4.5.2 제거하기\n설치한 패키지를 제거하고 싶다면 아래 명령어를 사용합니다.\npython -m pip uninstall [패키지]\n\n## 설치된 패키지 제거하기\n\n설치한 패키지를 제거하고 싶다면 아래 명령어를 사용합니다.\n\n```bash\nmamba remove r-tidyverse"
  },
  {
    "objectID": "posts/md/Install_miniforge.html#가상환경-비활성화하기",
    "href": "posts/md/Install_miniforge.html#가상환경-비활성화하기",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.6 가상환경 비활성화하기",
    "text": "4.6 가상환경 비활성화하기\n일반적으로는 그냥 터미널을 꺼버리고는 합니다만, 실수를 방지하기 위해 다음 명령어를 습관적으로 써주는 것이 좋습니다.\nmamba deactivate"
  },
  {
    "objectID": "posts/md/Install_miniforge.html#가상환경-제거하기",
    "href": "posts/md/Install_miniforge.html#가상환경-제거하기",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.7 가상환경 제거하기",
    "text": "4.7 가상환경 제거하기\n필요없는 가상 환경은 다음과 같이 제거 합니다.\nmamba env remove -n ipynb"
  },
  {
    "objectID": "posts/md/Install_miniforge.html#의존성-정보를-파일로-저장하기",
    "href": "posts/md/Install_miniforge.html#의존성-정보를-파일로-저장하기",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.8 의존성 정보를 파일로 저장하기",
    "text": "4.8 의존성 정보를 파일로 저장하기\n배포 혹은 프로젝트간의 전환을 위해 의존성을 파일로 저장해야하는 경우가 생깁니다. 그럴 때에는 아래와 같이 env.yaml파일을 생성하면 됩니다.\nmamba env export &gt; env.yaml"
  },
  {
    "objectID": "posts/md/Install_miniforge.html#의존성-파일로-부터-가상환경-만들기",
    "href": "posts/md/Install_miniforge.html#의존성-파일로-부터-가상환경-만들기",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "4.9 의존성 파일로 부터 가상환경 만들기",
    "text": "4.9 의존성 파일로 부터 가상환경 만들기\n위 명령어로 생성된 파일을 가지고 다음 명령어를 사용해 새로운 가상환경을 만들 수 있습니다. 생성된 가상환경의 이름은 env.yaml 파일 속에 적혀있는 것과 동일합니다.\nmamba env create -f env.yaml"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html",
    "href": "posts/md/Rosalind_stronghold.html",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "",
    "text": "생물정보학의 다양한 주제인 질량 분석, 서열 정렬, 동적 프로그래밍, 게놈 어셈블리, 계통 발생, 확률, 문자열 알고리즘 등의 기초가 되는 알고리즘에 대해 알아봅니다.\nRosalind 는 프로젝트 오일러, 구글 코드 잼에서 영감을 얻었습니다. 이 프로젝트의 이름은 DNA 이중나선을 발견하는 데 기여한 로잘린드 프랭클린 에서 따왔습니다. Rosalind 는 프로그래밍 실력을 키우고자 하는 생물학자와 분자생물학의 계산 문제를 접해본 적이 없는 프로그래머들에게 도움이 될 것입니다."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "1.1 Sample Dataset",
    "text": "1.1 Sample Dataset\nAGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output",
    "href": "posts/md/Rosalind_stronghold.html#sample-output",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "1.2 Sample Output",
    "text": "1.2 Sample Output\n20 12 17 21"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution",
    "href": "posts/md/Rosalind_stronghold.html#solution",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "1.3 Solution",
    "text": "1.3 Solution\n주어진 DNA 문자열 ‘s’ 에서 각 뉴클레오티드 (‘A’, ‘C’, ‘G’, ‘T’) 의 발생 횟수를 세는 문제를 해결하려면 다음 단계를 따르세요.\n\n카운터를 초기화합니다: ‘A’, ‘C’, ‘G’, ‘T’ 에 대한 카운터를 설정합니다.\n문자열을 반복합니다: 문자열의 각 문자를 순회하며 해당 카운터를 증가시킵니다.\n결과를 출력합니다: ‘A’, ‘C’, ‘G’, ‘T’ 의 개수를 공백으로 구분하여 인쇄합니다.\n\ndef count_nucleotides(dna_string):\n    count_A = 0\n    count_C = 0\n    count_G = 0\n    count_T = 0\n    \n    for nucleotide in dna_string:\n        if nucleotide == 'A':\n            count_A += 1\n        elif nucleotide == 'C':\n            count_C += 1\n        elif nucleotide == 'G':\n            count_G += 1\n        elif nucleotide == 'T':\n            count_T += 1\n    \n    return count_A, count_C, count_G, count_T\n\n# Sample Dataset\ns = \"AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGC\"\n\nresult = count_nucleotides(s)\nprint(\" \".join(map(str, result)))\n# Output should be \"20 12 17 21\""
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#설명",
    "href": "posts/md/Rosalind_stronghold.html#설명",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "1.4 설명",
    "text": "1.4 설명\n\n초기화: ‘A’, ‘C’, ‘G’, ‘T’ 의 카운터가 0 으로 초기화됩니다.\n각 문자를 반복합니다: 루프는 DNA 문자열의 각 문자를 검사하고 발견된 문자에 따라 해당 카운터를 증가시킵니다.\n결과를 반환하고 인쇄합니다: 이 함수는 카운트를 반환한 다음 필요한 형식으로 출력합니다.\n\n이 접근 방식은 각 뉴클레오타이드가 문자열을 한 번 통과할 때 효율적으로 카운트되도록 보장하며, 시간 복잡도는 \\(O(n)\\) 입니다."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-1",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "2.1 Sample Dataset",
    "text": "2.1 Sample Dataset\nGATGGAACTTGACTACGTAAATT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-1",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "2.2 Sample Output",
    "text": "2.2 Sample Output\nGAUGGAACUUGACUACGUAAAUU"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-1",
    "href": "posts/md/Rosalind_stronghold.html#solution-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "2.3 Solution",
    "text": "2.3 Solution\nTo transcribe a DNA string to an RNA string, we need to replace every occurrence of the nucleotide ‘T’ in the DNA string with ‘U’ to form the RNA string. This is because RNA uses uracil (U) instead of thymine (T).\ndef transcribe_dna_to_rna(dna_string):\n    # Replace all occurrences of 'T' with 'U'\n    rna_string = dna_string.replace('T', 'U')\n    return rna_string\n\n# Sample Dataset\ndna_string = \"GATGGAACTTGACTACGTAAATT\"\nprint(transcribe_dna_to_rna(dna_string))  # Output should be \"GAUGGAACUUGACUACGUAAAUU\""
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation",
    "href": "posts/md/Rosalind_stronghold.html#explanation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "2.4 Explanation",
    "text": "2.4 Explanation\n\nFunction Definition: transcribe_dna_to_rna(dna_string) takes a DNA string as input.\nString Replacement: dna_string.replace('T', 'U') creates a new string where all Ts are replaced with Us.\nReturn Statement: The resulting RNA string is returned."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-2",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "3.1 Sample Dataset",
    "text": "3.1 Sample Dataset\nAAAACCCGGT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-2",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "3.2 Sample Output",
    "text": "3.2 Sample Output\nACCGGGTTTT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#soultion",
    "href": "posts/md/Rosalind_stronghold.html#soultion",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "3.3 soultion",
    "text": "3.3 soultion\nTo find the reverse complement of a DNA string, follow these steps:\n\nReverse the string: First, reverse the input DNA string.\nComplement the string: Replace each nucleotide with its complement: ‘A’ with ‘T’, ‘T’ with ‘A’, ‘C’ with ‘G’, and ‘G’ with ‘C’.\n\ndef reverse_complement(dna_string):\n    # Dictionary to map each nucleotide to its complement\n    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    \n    # Reverse the DNA string\n    reversed_dna = dna_string[::-1]\n    \n    # Replace each nucleotide with its complement\n    reverse_complement_dna = ''.join(complement[base] for base in reversed_dna)\n    \n    return reverse_complement_dna\n\n# Sample Dataset\ndna_string = \"AAAACCCGGT\"\nprint(reverse_complement(dna_string))  \n# Output should be \"ACCGGGTTTT\""
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-1",
    "href": "posts/md/Rosalind_stronghold.html#explanation-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "3.4 Explanation",
    "text": "3.4 Explanation\n\nComplement Mapping:\n\nA dictionary complement is used to map each nucleotide to its complementary nucleotide.\n\nReversing the String:\n\nThe slicing operation dna_string[::-1] reverses the string.\n\nGenerating the Complement:\n\nA list comprehension is used to replace each nucleotide in the reversed string with its complement.\n''.join() combines the list of complemented nucleotides into a single string."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-3",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "4.1 Sample Dataset",
    "text": "4.1 Sample Dataset\n5 3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-3",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "4.2 Sample Output",
    "text": "4.2 Sample Output\n19"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-2",
    "href": "posts/md/Rosalind_stronghold.html#solution-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "4.3 Solution",
    "text": "4.3 Solution\nTo solve the problem of computing the total number of rabbit pairs after a given number of months \\(n\\) when each pair of reproduction-age rabbits produces \\(k\\) rabbit pairs each month, we can modify the classic Fibonacci sequence. Instead of each rabbit pair producing just one new pair, they produce \\(k\\) new pairs.\nLet’s break down the steps to create the solution:\n\nDefine the recurrence relation: The problem can be modeled with a recurrence relation. Let \\(F(n)\\) represent the number of rabbit pairs after \\(n\\) months. The recurrence relation can be expressed as: \\[ F(n) = F(n-1) + k \\times F(n-2) \\] Here, \\(F(n-1)\\) represents the number of rabbit pairs from the previous month, and \\(k\\) times \\(F(n-2)\\) represents the new rabbit pairs produced by the pairs from two months ago.\nInitial conditions:\n\n\\(F(1) = 1\\) (initially, there is one pair of rabbits)\n\\(F(2) = 1\\) (in the second month, there is still only one pair, as they have not yet reproduced)\n\nIterative computation: Using a loop, compute the number of rabbit pairs for each month up to \\(n\\) based on the recurrence relation.\n\nHere is the Python function to implement this approach:\ndef total_rabbit_pairs(n, k):\n    if n == 1 or n == 2:\n        return 1\n    \n    # Initialize the first two months\n    F1 = 1\n    F2 = 1\n    \n    # Compute the number of rabbit pairs for each subsequent month\n    for month in range(3, n + 1):\n        F_current = F2 + k * F1\n        F1 = F2\n        F2 = F_current\n    \n    return F2\n\n# Sample Dataset\nn = 5\nk = 3\nprint(total_rabbit_pairs(n, k))  # Output should be 19"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "4.4 Explanation of the Code",
    "text": "4.4 Explanation of the Code\n\nBase Cases:\n\nIf \\(n\\) is 1 or 2, the function returns 1 because the first two terms are both 1.\n\nInitialization:\n\nVariables F1 and F2 are initialized to 1, representing the number of rabbit pairs in the first and second months, respectively.\n\nLoop Through Months:\n\nFor each month from 3 to \\(n\\) , the number of rabbit pairs is calculated using the recurrence relation. F_current is calculated as the sum of the number of rabbit pairs from the previous month (F2) and the number of new rabbit pairs produced by the pairs from two months ago (k * F1).\n\nUpdate Variables:\n\nAfter computing F_current, update F1 and F2 to the values of the last two computed terms to prepare for the next iteration.\n\nReturn the Result:\n\nFinally, return F2, which holds the number of rabbit pairs after \\(n\\) months."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-4",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "5.1 Sample Dataset",
    "text": "5.1 Sample Dataset\n&gt;Rosalind_6404\nCCTGCGGAAGATCGGCACTAGAATAGCCAGAACCGTTTCTCTGAGGCTTCCGGCCTTCCC\nTCCCACTAATAATTCTGAGG\n&gt;Rosalind_5959\nCCATCGGTAGCGCATCCTTAGTCCAATTAAGTCCCTATCCAGGCGCTCCGCCGAAGGTCT\nATATCCATTTGTCAGCAGACACGC\n&gt;Rosalind_0808\nCCACCCTCGTGGTATGGCTAGGCATTCAGGAACCGGAGAACGCTTCAGACCAGCCCGGAC\nTGGGAACCTGCGGGCAGTAGGTGGAAT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-4",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "5.2 Sample Output",
    "text": "5.2 Sample Output\nRosalind_0808\n60.919540"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-3",
    "href": "posts/md/Rosalind_stronghold.html#solution-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "5.3 Solution",
    "text": "5.3 Solution\nTo solve this problem, we need to compute the GC-content of multiple DNA strings provided in FASTA format and identify the string with the highest GC-content."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "5.4 Steps to Solve the Problem",
    "text": "5.4 Steps to Solve the Problem\n\nParse the FASTA formatted input: Extract the DNA strings and their corresponding IDs.\nCompute GC-content: For each DNA string, calculate the percentage of nucleotides that are ‘C’ or ‘G’.\nDetermine the highest GC-content: Identify the DNA string with the highest GC-content and return its ID along with the computed GC-content.\n\nHere’s the Python code to achieve this:\ndef parse_fasta(fasta_strings):\n    sequences = {}\n    label = None\n    \n    for line in fasta_strings.splitlines():\n        if line.startswith('&gt;'):\n            label = line[1:].strip()\n            sequences[label] = \"\"\n        else:\n            sequences[label] += line.strip()\n    \n    return sequences\n\ndef gc_content(dna_string):\n    gc_count = dna_string.count('G') + dna_string.count('C')\n    return (gc_count / len(dna_string)) * 100\n\ndef highest_gc_content(fasta_strings):\n    sequences = parse_fasta(fasta_strings)\n    max_gc_id = None\n    max_gc_content = 0\n    \n    for label, dna_string in sequences.items():\n        gc = gc_content(dna_string)\n        if gc &gt; max_gc_content:\n            max_gc_content = gc\n            max_gc_id = label\n            \n    return max_gc_id, max_gc_content\n\n# Sample Dataset\nfasta_strings = \"\"\"&gt;Rosalind_6404\nCCTGCGGAAGATCGGCACTAGAATAGCCAGAACCGTTTCTCTGAGGCTTCCGGCCTTCCC\nTCCCACTAATAATTCTGAGG\n&gt;Rosalind_5959\nCCATCGGTAGCGCATCCTTAGTCCAATTAAGTCCCTATCCAGGCGCTCCGCCGAAGGTCT\nATATCCATTTGTCAGCAGACACGC\n&gt;Rosalind_0808\nCCACCCTCGTGGTATGGCTAGGCATTCAGGAACCGGAGAACGCTTCAGACCAGCCCGGAC\nTGGGAACCTGCGGGCAGTAGGTGGAAT\"\"\"\n\n# Compute and print the result\nresult_id, result_gc_content = highest_gc_content(fasta_strings)\nprint(result_id)\nprint(f\"{result_gc_content:f}\")"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-2",
    "href": "posts/md/Rosalind_stronghold.html#explanation-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "5.5 Explanation",
    "text": "5.5 Explanation\n\nParsing FASTA Format:\n\nThe parse_fasta function reads the FASTA formatted string and extracts the sequences.\nIt uses a dictionary to store the DNA sequences with their labels as keys.\n\nComputing GC-content:\n\nThe gc_content function calculates the GC-content by counting ‘G’ and ‘C’ nucleotides and dividing by the total length of the DNA string.\n\nFinding the Highest GC-content:\n\nThe highest_gc_content function iterates through the parsed sequences, calculates the GC-content for each, and keeps track of the highest value and its corresponding label."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-5",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-5",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "6.1 Sample Dataset",
    "text": "6.1 Sample Dataset\nGAGCCTACTAACGGGAT\nCATCGTAATGACGGCCT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-5",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-5",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "6.2 Sample Output",
    "text": "6.2 Sample Output\n7"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-4",
    "href": "posts/md/Rosalind_stronghold.html#solution-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "6.3 Solution",
    "text": "6.3 Solution\nThe Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols differ. Given two DNA strings, we can compute the Hamming distance by comparing each position in the strings and counting the differences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-1",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "6.4 Steps to Solve the Problem",
    "text": "6.4 Steps to Solve the Problem\n\nEnsure Strings are of Equal Length: The problem guarantees that the strings are of equal length, so we don’t need to check for this.\nCompare Corresponding Symbols: Traverse both strings and compare corresponding characters.\nCount Differences: Increment a counter whenever the characters at the same position are different.\n\ndef hamming_distance(s, t):\n    # Initialize the counter for differences\n    count = 0\n    \n    # Traverse both strings and compare characters\n    for char1, char2 in zip(s, t):\n        if char1 != char2:\n            count += 1\n    \n    return count\n\n# Sample Dataset\ns = \"GAGCCTACTAACGGGAT\"\nt = \"CATCGTAATGACGGCCT\"\nprint(hamming_distance(s, t))  # Output should be 7"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-3",
    "href": "posts/md/Rosalind_stronghold.html#explanation-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "6.5 Explanation",
    "text": "6.5 Explanation\n\nInitialize Counter:\n\ncount is initialized to zero. This will keep track of the number of differing positions.\n\nTraverse Strings:\n\nzip(s, t) pairs up characters from both strings at each position.\nFor each pair of characters (char1, char2), compare them.\n\nCount Differences:\n\nIf char1 is not equal to char2, increment the count."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-6",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-6",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "7.1 Sample Dataset",
    "text": "7.1 Sample Dataset\n2 2 2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-6",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-6",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "7.2 Sample Output",
    "text": "7.2 Sample Output\n0.78333"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-5",
    "href": "posts/md/Rosalind_stronghold.html#solution-5",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "7.3 Solution",
    "text": "7.3 Solution\nTo solve this problem, we need to calculate the probability that two randomly selected organisms from a population will produce an offspring with at least one dominant allele. The population is divided into three groups: - \\(k\\): Homozygous dominant organisms (AA) - \\(m\\): Heterozygous organisms (Aa) - \\(n\\): Homozygous recessive organisms (aa)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-2",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "7.4 Steps to Solve the Problem",
    "text": "7.4 Steps to Solve the Problem\n\nCalculate Total Population Size:\n\nTotal number of organisms: \\((T = k + m + n)\\)\n\nCalculate the Probability of Each Possible Pairing:\n\nThere are several pairings to consider:\n\n\\(AA \\times AA\\)\n\\(AA \\times Aa\\)\n\\(AA \\times aa\\)\n\\(Aa \\times Aa\\)\n\\(Aa \\times aa\\)\n\\(aa \\times aa\\)\n\n\nCalculate the Probability of Producing a Dominant Phenotype from Each Pairing:\n\n\\(AA \\times AA\\): 100% dominant phenotype.\n\\(AA \\times Aa\\): 100% dominant phenotype.\n\\(AA \\times aa\\): 100% dominant phenotype.\n\\(Aa \\times Aa\\): 75% dominant phenotype (since the combinations are AA, Aa, Aa, aa).\n\\(Aa \\times aa\\): 50% dominant phenotype (since the combinations are Aa, Aa, aa, aa).\n\\(aa \\times aa\\): 0% dominant phenotype.\n\nCalculate the Probability of Selecting Each Pairing:\n\nThe probability of selecting two organisms is determined by the number of ways to choose them from the total population.\n\nCombine Probabilities to Get the Overall Probability of Dominant Phenotype:\n\nSum the probabilities of all pairings that produce a dominant phenotype, weighted by their probability of selection.\n\n\nHere is the Python code that implements the above steps:\ndef probability_dominant_phenotype(k, m, n):\n    # Total population\n    total = k + m + n\n    \n    # Total number of possible pairings\n    total_pairings = total * (total - 1)\n    \n    # Probabilities of each pairing type\n    prob_AA_AA = k * (k - 1) / total_pairings\n    prob_AA_Aa = 2 * k * m / total_pairings\n    prob_AA_aa = 2 * k * n / total_pairings\n    prob_Aa_Aa = m * (m - 1) / total_pairings\n    prob_Aa_aa = 2 * m * n / total_pairings\n    prob_aa_aa = n * (n - 1) / total_pairings\n    \n    # Probabilities of dominant phenotype from each pairing\n    prob_dom_AA_AA = 1.0  # 100%\n    prob_dom_AA_Aa = 1.0  # 100%\n    prob_dom_AA_aa = 1.0  # 100%\n    prob_dom_Aa_Aa = 0.75 # 75%\n    prob_dom_Aa_aa = 0.5  # 50%\n    prob_dom_aa_aa = 0.0  # 0%\n    \n    # Total probability of dominant phenotype\n    prob_dom = (prob_AA_AA * prob_dom_AA_AA +\n                prob_AA_Aa * prob_dom_AA_Aa +\n                prob_AA_aa * prob_dom_AA_aa +\n                prob_Aa_Aa * prob_dom_Aa_Aa +\n                prob_Aa_aa * prob_dom_Aa_aa +\n                prob_aa_aa * prob_dom_aa_aa)\n    \n    return prob_dom\n\n# Sample Dataset\nk, m, n = 2, 2, 2\n\n# Calculate and print the result\nresult = probability_dominant_phenotype(k, m, n)\nprint(f\"{result:f}\")"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-4",
    "href": "posts/md/Rosalind_stronghold.html#explanation-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "7.5 Explanation",
    "text": "7.5 Explanation\n\nTotal Population:\n\nWe calculate the total number of organisms, \\(total = k + m + n\\).\n\nPairing Probabilities:\n\nEach pairing probability is calculated based on the number of ways to select pairs from the total population.\n\nDominant Phenotype Probabilities:\n\nEach pairing type has a different probability of producing a dominant phenotype based on Mendelian inheritance.\n\nOverall Probability:\n\nThe overall probability is a weighted sum of the probabilities of each pairing type producing a dominant phenotype.\n\n\nThis code computes the required probability efficiently and accurately, matching the example output provided in the problem description."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-7",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-7",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "8.1 Sample Dataset",
    "text": "8.1 Sample Dataset\nAUGGCCAUGGCGCCCAGAACUGAGAUCAAUAGUACCCGUAUUAACGGGUGA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-7",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-7",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "8.2 Sample Output",
    "text": "8.2 Sample Output\nMAMAPRTEINSTRING"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-6",
    "href": "posts/md/Rosalind_stronghold.html#solution-6",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "8.3 Solution",
    "text": "8.3 Solution\nTo convert an RNA string into a protein string, you need to translate the RNA codons into their corresponding amino acids based on the RNA codon table. Each RNA codon (a sequence of three nucleotides) corresponds to a specific amino acid or a stop signal, which terminates translation.\nHere’s the step-by-step approach to solving the problem:\n\nCreate an RNA Codon Table: The RNA codon table maps each of the 64 possible codons to their corresponding amino acid or stop signal. For example, the codon “AUG” codes for Methionine (M), and “UGA” is a stop codon.\nRead the RNA String: The RNA string will be read in chunks of three nucleotides (codons).\nTranslate Each Codon: Using the codon table, translate each codon into the corresponding amino acid. If a stop codon is encountered, terminate the translation.\nConstruct the Protein String: Concatenate the translated amino acids to form the final protein string.\n\nHere is the Python implementation of this approach:\ndef translate_rna_to_protein(rna_sequence):\n    codon_table = {\n        \"UUU\": \"F\", \"UUC\": \"F\", \"UUA\": \"L\", \"UUG\": \"L\",\n        \"UCU\": \"S\", \"UCC\": \"S\", \"UCA\": \"S\", \"UCG\": \"S\",\n        \"UAU\": \"Y\", \"UAC\": \"Y\", \"UAA\": \"Stop\", \"UAG\": \"Stop\",\n        \"UGU\": \"C\", \"UGC\": \"C\", \"UGA\": \"Stop\", \"UGG\": \"W\",\n        \"CUU\": \"L\", \"CUC\": \"L\", \"CUA\": \"L\", \"CUG\": \"L\",\n        \"CCU\": \"P\", \"CCC\": \"P\", \"CCA\": \"P\", \"CCG\": \"P\",\n        \"CAU\": \"H\", \"CAC\": \"H\", \"CAA\": \"Q\", \"CAG\": \"Q\",\n        \"CGU\": \"R\", \"CGC\": \"R\", \"CGA\": \"R\", \"CGG\": \"R\",\n        \"AUU\": \"I\", \"AUC\": \"I\", \"AUA\": \"I\", \"AUG\": \"M\",\n        \"ACU\": \"T\", \"ACC\": \"T\", \"ACA\": \"T\", \"ACG\": \"T\",\n        \"AAU\": \"N\", \"AAC\": \"N\", \"AAA\": \"K\", \"AAG\": \"K\",\n        \"AGU\": \"S\", \"AGC\": \"S\", \"AGA\": \"R\", \"AGG\": \"R\",\n        \"GUU\": \"V\", \"GUC\": \"V\", \"GUA\": \"V\", \"GUG\": \"V\",\n        \"GCU\": \"A\", \"GCC\": \"A\", \"GCA\": \"A\", \"GCG\": \"A\",\n        \"GAU\": \"D\", \"GAC\": \"D\", \"GAA\": \"E\", \"GAG\": \"E\",\n        \"GGU\": \"G\", \"GGC\": \"G\", \"GGA\": \"G\", \"GGG\": \"G\"\n    }\n\n    protein_string = []\n    \n    # Proces the RNA sequence in chunks of three nucleotides (codons)\n    for i in range(0, len(rna_sequence), 3):\n        codon = rna_sequence[i:i+3]\n        if codon in codon_table:\n            amino_acid = codon_table[codon]\n            if amino_acid == \"Stop\":\n                break\n            protein_string.append(amino_acid)\n    \n    return ''.join(protein_string)\n\n# Sample Dataset\nrna_sequence = \"AUGGCCAUGGCGCCCAGAACUGAGAUCAAUAGUACCCGUAUUAACGGGUGA\"\nprint(translate_rna_to_protein(rna_sequence))\n# Output should be \"MAMAPRTEINSTRING\""
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-5",
    "href": "posts/md/Rosalind_stronghold.html#explanation-5",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "8.4 Explanation",
    "text": "8.4 Explanation\n\nCodon Table: The dictionary codon_table maps RNA codons to their corresponding amino acids or stop signals.\nProcessing the RNA Sequence:\n\nThe loop iterates over the RNA sequence in steps of three nucleotides.\nFor each codon, the corresponding amino acid is retrieved from the codon_table.\nIf the amino acid is “Stop”, the loop terminates, indicating the end of the protein sequence.\nOtherwise, the amino acid is appended to the protein_string list.\n\nConstructing the Protein String:\n\nThe list of amino acids is joined into a single string and returned as the final protein string.\n\n\nThis method ensures that the RNA sequence is translated efficiently and correctly into the corresponding protein string."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-8",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-8",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "9.1 Sample Dataset",
    "text": "9.1 Sample Dataset\nGATATATGCATATACTT\nATAT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-8",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-8",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "9.2 Sample Output",
    "text": "9.2 Sample Output\n2 4 10"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-7",
    "href": "posts/md/Rosalind_stronghold.html#solution-7",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "9.3 Solution",
    "text": "9.3 Solution\nTo solve the problem of finding all locations of a substring t in a string s, we need to identify each position in s where t starts. This can be achieved using simple string matching techniques."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-3",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "9.4 Steps to Solve the Problem",
    "text": "9.4 Steps to Solve the Problem\n\nRead the Input Strings:\n\nWe have two DNA strings, s and t.\n\nIterate Through the Main String s:\n\nCheck for occurrences of the substring t starting at each position in s.\n\nCollect All Starting Positions:\n\nWhenever t is found in s, record the starting position. Note that the positions should be 1-based as per the problem statement.\n\nOutput the Results:\n\nPrint all recorded positions separated by spaces."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation",
    "href": "posts/md/Rosalind_stronghold.html#implementation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "9.5 Implementation",
    "text": "9.5 Implementation\nHere is the Python code that implements the above logic:\ndef find_motif_locations(s, t):\n    positions = []\n    len_s = len(s)\n    len_t = len(t)\n    \n    # Iterate through the main string `s`\n    for i in range(len_s - len_t + 1):\n        # Check if the substring `t` matches the segment in `s` starting at position `i`\n        if s[i:i+len_t] == t:\n            # If it matches, record the 1-based position\n            positions.append(i + 1)\n    \n    return positions\n\n# Sample Dataset\ns = \"GATATATGCATATACTT\"\nt = \"ATAT\"\n\n# Find and print the locations\nlocations = find_motif_locations(s, t)\nprint(\" \".join(map(str, locations)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-6",
    "href": "posts/md/Rosalind_stronghold.html#explanation-6",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "9.6 Explanation",
    "text": "9.6 Explanation\n\nIterate Through the Main String s:\n\nWe use a for loop to go through each possible starting position for t in s. The loop runs from 0 to len(s) - len(t) to ensure we don’t go out of bounds.\n\nCheck for Substring Match:\n\nFor each position i, we check if the substring s[i:i+len(t)] matches t.\n\nRecord the Position:\n\nIf a match is found, we append the 1-based position (i.e., i + 1) to our list of positions.\n\nOutput the Results:\n\nWe convert the list of positions to a space-separated string and print it.\n\n\nThis approach ensures that all occurrences of t in s are found and correctly reported. The solution efficiently handles the constraints of the problem, making it suitable for DNA strings up to 1 kbp in length."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-9",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-9",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "10.1 Sample Dataset",
    "text": "10.1 Sample Dataset\n&gt;Rosalind_1\nATCCAGCT\n&gt;Rosalind_2\nGGGCAACT\n&gt;Rosalind_3\nATGGATCT\n&gt;Rosalind_4\nAAGCAACC\n&gt;Rosalind_5\nTTGGAACT\n&gt;Rosalind_6\nATGCCATT\n&gt;Rosalind_7\nATGGCACT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-9",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-9",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "10.2 Sample Output",
    "text": "10.2 Sample Output\nATGCAACT\nA: 5 1 0 0 5 5 0 0\nC: 0 0 1 4 2 0 6 1\nG: 1 1 6 3 0 1 0 0\nT: 1 5 0 0 0 1 1 6"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-8",
    "href": "posts/md/Rosalind_stronghold.html#solution-8",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "10.3 Solution",
    "text": "10.3 Solution\nTo solve the problem of finding the consensus string and profile matrix from a collection of DNA strings, we need to follow these steps:\n\nParse the FASTA format input to extract the DNA strings.\nInitialize the profile matrix to keep track of the frequency of each nucleotide (A, C, G, T) at each position in the strings.\nPopulate the profile matrix by iterating through each DNA string and counting the occurrences of each nucleotide at each position.\nConstruct the consensus string by selecting the most frequent nucleotide at each position based on the profile matrix.\nOutput the consensus string and the profile matrix.\n\nHere’s the Python code to solve the problem:\ndef parse_fasta(fasta_string):\n    sequences = {}\n    current_label = \"\"\n    \n    lines = fasta_string.strip().split('\\n')\n    for line in lines:\n        line = line.strip()\n        if line.startswith(\"&gt;\"):\n            current_label = line[1:]\n            sequences[current_label] = \"\"\n        else:\n            sequences[current_label] += line\n            \n    return list(sequences.values())\n\ndef calculate_profile_matrix(dna_strings):\n    n = len(dna_strings[0])\n    profile_matrix = {\n        'A': [0] * n,\n        'C': [0] * n,\n        'G': [0] * n,\n        'T': [0] * n\n    }\n    \n    for dna in dna_strings:\n        for i, nucleotide in enumerate(dna):\n            profile_matrix[nucleotide][i] += 1\n    \n    return profile_matrix\n\ndef calculate_consensus_string(profile_matrix, length):\n    consensus = []\n    for i in range(length):\n        max_count = 0\n        max_nucleotide = ''\n        for nucleotide in \"ACGT\":\n            if profile_matrix[nucleotide][i] &gt; max_count:\n                max_count = profile_matrix[nucleotide][i]\n                max_nucleotide = nucleotide\n        consensus.append(max_nucleotide)\n    return ''.join(consensus)\n\ndef consensus_and_profile(fasta_string):\n    dna_strings = parse_fasta(fasta_string)\n    profile_matrix = calculate_profile_matrix(dna_strings)\n    consensus_string = calculate_consensus_string(profile_matrix, len(dna_strings[0]))\n    \n    return consensus_string, profile_matrix\n\n# Sample Dataset as a single string\nfasta_string = \"\"\"\n&gt;Rosalind_1\nATCCAGCT\n&gt;Rosalind_2\nGGGCAACT\n&gt;Rosalind_3\nATGGATCT\n&gt;Rosalind_4\nAAGCAACC\n&gt;Rosalind_5\nTTGGAACT\n&gt;Rosalind_6\nATGCCATT\n&gt;Rosalind_7\nATGGCACT\n\"\"\"\n\n# Compute the result\nconsensus_string, profile_matrix = consensus_and_profile(fasta_string)\n\n# Print the result\nprint(consensus_string)\nfor nucleotide in \"ACGT\":\n    print(f\"{nucleotide}: {' '.join(map(str, profile_matrix[nucleotide]))}\")"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-7",
    "href": "posts/md/Rosalind_stronghold.html#explanation-7",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "10.4 Explanation",
    "text": "10.4 Explanation\n\nParsing FASTA Input:\n\nThe parse_fasta function now processes a single string input and splits it into lines. It reads through each line, detecting labels (lines starting with ‘&gt;’) and corresponding DNA sequences, which are stored in a dictionary and then converted into a list of sequences.\n\nProfile Matrix Calculation:\n\nThe calculate_profile_matrix function initializes a dictionary with keys ‘A’, ‘C’, ‘G’, and ‘T’ and lists as values to store nucleotide counts at each position.\n\nConsensus String Calculation:\n\nThe calculate_consensus_string function builds the consensus string by selecting the nucleotide with the highest count at each position.\n\nMain Function:\n\nThe consensus_and_profile function orchestrates the entire process, returning the consensus string and the profile matrix."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-10",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-10",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "10.5 Sample Output",
    "text": "10.5 Sample Output\nRunning the provided dataset through the code will produce the following output:\nATGCAACT\nA: 5 1 0 0 5 5 0 0\nC: 0 0 1 4 2 0 6 1\nG: 1 1 6 3 0 1 0 0\nT: 1 5 0 0 0 1 1 6\nThis output shows the consensus string and the profile matrix with counts of each nucleotide at each position, formatted as required by the problem statement."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-10",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-10",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "11.1 Sample Dataset",
    "text": "11.1 Sample Dataset\n6 3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-11",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-11",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "11.2 Sample Output",
    "text": "11.2 Sample Output\n4"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-9",
    "href": "posts/md/Rosalind_stronghold.html#solution-9",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "11.3 Solution",
    "text": "11.3 Solution\nTo solve the problem of computing the number of rabbit pairs after a given number of months with a lifespan constraint, we need to adjust the classic Fibonacci sequence to consider the mortality of rabbits. Here’s how we can approach this using dynamic programming:\n\nInitialize the state:\n\nWe keep track of the number of rabbits of different ages using an array.\nrabbits[i] will represent the number of rabbit pairs of age i.\n\nSimulate each month:\n\nIn each month, rabbits of age 0 produce new rabbits.\nAll rabbits get older by one month.\nRabbits older than m months die.\n\nUpdate the state:\n\nShift all elements in the rabbits array to the right.\nUpdate the number of new-born rabbits based on the rabbits of age 1 to m-1.\n\n\ndef mortal_fibonacci_rabbits(n, m):\n    rabbits = [0] * m\n    rabbits[0] = 1  # Initial pair of rabbits\n    \n    for month in range(1, n):\n        new_born = sum(rabbits[1:])  # All rabbits that are not in their first month\n        # Shift all rabbits to the next month\n        for i in range(m-1, 0, -1):\n            rabbits[i] = rabbits[i-1]\n        rabbits[0] = new_born  # Update the new-born rabbits\n    \n    return sum(rabbits)\n\n# Test the function with the sample dataset\nprint(mortal_fibonacci_rabbits(6, 3))  # Output should be 4"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-8",
    "href": "posts/md/Rosalind_stronghold.html#explanation-8",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "11.4 Explanation",
    "text": "11.4 Explanation\n\nInitialization:\n\nWe start with rabbits = [1, 0, 0], which represents 1 pair of newborn rabbits and no other rabbits of other ages.\n\nMonthly updates:\n\nFor each month, compute the number of new-born rabbits.\nShift the ages of rabbits, which involves moving each count in the rabbits array to the next index.\nRabbits older than m-1 months (last index) die off automatically as they are not carried forward.\n\nResult:\n\nThe total number of rabbits is the sum of all entries in the rabbits array after the loop ends.\n\n\nThis approach ensures that we accurately track the age of each rabbit pair and account for their mortality, providing the correct number of rabbit pairs after n months."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-11",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-11",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "12.1 Sample Dataset",
    "text": "12.1 Sample Dataset\nMA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-12",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-12",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "12.2 Sample Output",
    "text": "12.2 Sample Output\n12"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-10",
    "href": "posts/md/Rosalind_stronghold.html#solution-10",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "12.3 Solution",
    "text": "12.3 Solution\nTo solve the problem of finding the total number of different RNA strings from which a given protein string could have been translated, we need to consider the redundancy in the genetic code. Each amino acid can be encoded by one or more codons, and this redundancy will influence the number of possible RNA sequences that can result in the same protein."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-4",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "12.4 Steps to Solve the Problem",
    "text": "12.4 Steps to Solve the Problem\n\nUnderstand the Genetic Code:\n\nCreate a mapping of each amino acid to the number of possible codons that encode it.\nDon’t forget to include the stop codons, which signal the end of translation.\n\nCalculate Possible RNA Strings for Each Amino Acid:\n\nFor each amino acid in the given protein string, multiply the number of possible codons for that amino acid.\nInclude a factor for the stop codon at the end.\n\nUse Modular Arithmetic:\n\nSince the resulting number can be very large, use modulo \\(1,000,000\\) to avoid overflow and ensure the result fits within standard integer sizes."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#genetic-code-table",
    "href": "posts/md/Rosalind_stronghold.html#genetic-code-table",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "12.5 Genetic Code Table",
    "text": "12.5 Genetic Code Table\nHere is the mapping of amino acids to their respective number of codons: - ‘A’: 4, ‘C’: 2, ‘D’: 2, ‘E’: 2, ‘F’: 2, ‘G’: 4, ‘H’: 2, ‘I’: 3, ‘K’: 2, ‘L’: 6, - ‘M’: 1, ‘N’: 2, ‘P’: 4, ‘Q’: 2, ‘R’: 6, ‘S’: 6, ‘T’: 4, ‘V’: 4, ‘W’: 1, ‘Y’: 2, - Stop codon: 3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation-1",
    "href": "posts/md/Rosalind_stronghold.html#implementation-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "12.6 Implementation",
    "text": "12.6 Implementation\nBelow is the Python code that implements the solution:\ndef infer_mrna_from_protein(protein):\n    # Codon counts for each amino acid and stop codon\n    codon_count = {\n        'A': 4, 'C': 2, 'D': 2, 'E': 2, 'F': 2, 'G': 4,\n        'H': 2, 'I': 3, 'K': 2, 'L': 6, 'M': 1, 'N': 2,\n        'P': 4, 'Q': 2, 'R': 6, 'S': 6, 'T': 4, 'V': 4,\n        'W': 1, 'Y': 2, 'Stop': 3\n    }\n    \n    # Initialize the number of possible RNA strings\n    possible_rna_strings = 1\n    \n    # Calculate the product of possible codons for each amino acid\n    for aa in protein:\n        possible_rna_strings *= codon_count[aa]\n        possible_rna_strings %= 1000000  # Take modulo 1,000,000 to keep the number manageable\n    \n    # Multiply by the number of stop codons\n    possible_rna_strings *= codon_count['Stop']\n    possible_rna_strings %= 1000000  # Take modulo 1,000,000 again\n    \n    return possible_rna_strings\n\n# Sample Dataset\nprotein_string = \"MA\"\n\n# Compute the result\nresult = infer_mrna_from_protein(protein_string)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-9",
    "href": "posts/md/Rosalind_stronghold.html#explanation-9",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "12.7 Explanation",
    "text": "12.7 Explanation\n\nCodon Count Mapping:\n\nThe codon_count dictionary stores the number of codons that can encode each amino acid, along with the number of stop codons.\n\nProduct Calculation:\n\nWe initialize possible_rna_strings to 1.\nFor each amino acid in the protein string, multiply possible_rna_strings by the number of codons that can encode that amino acid.\nUse modulo 1,000,000 after each multiplication to keep the number within manageable limits.\n\nStop Codon Factor:\n\nFinally, multiply by the number of stop codons and take modulo 1,000,000 again.\n\n\nThis approach ensures that we efficiently compute the total number of possible RNA sequences modulo 1,000,000."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-12",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-12",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "13.1 Sample Dataset",
    "text": "13.1 Sample Dataset\n&gt;Rosalind_0498\nAAATAAA\n&gt;Rosalind_2391\nAAATTTT\n&gt;Rosalind_2323\nTTTTCCC\n&gt;Rosalind_0442\nAAATCCC\n&gt;Rosalind_5013\nGGGTGGG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-13",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-13",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "13.2 Sample Output",
    "text": "13.2 Sample Output\nRosalind_0498 Rosalind_2391\nRosalind_0498 Rosalind_0442\nRosalind_2391 Rosalind_2323"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-11",
    "href": "posts/md/Rosalind_stronghold.html#solution-11",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "13.3 Solution",
    "text": "13.3 Solution\nTo solve the problem of constructing an overlap graph from a collection of DNA strings, we need to follow these steps:\n\nParse the input data: Read the DNA strings in FASTA format.\nConstruct the graph: Identify edges between nodes based on the overlap condition.\nOutput the adjacency list: List all directed edges that satisfy the overlap condition."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-implement-the-solution",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-implement-the-solution",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "13.4 Steps to Implement the Solution",
    "text": "13.4 Steps to Implement the Solution\n\nRead the Input Data:\n\nUse a parser to read the input DNA strings in FASTA format.\nStore the strings in a dictionary with their labels as keys.\n\nCheck for Overlaps:\n\nFor each pair of strings, check if the suffix of length k of one string matches the prefix of length k of the other string.\nIf they match and the strings are different, record the directed edge from the first string to the second.\n\nOutput the Results:\n\nPrint each directed edge in the format “label1 label2”."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#example-implementation",
    "href": "posts/md/Rosalind_stronghold.html#example-implementation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "13.5 Example Implementation",
    "text": "13.5 Example Implementation\nBelow is the Python code that performs these steps:\ndef parse_fasta(data):\n    sequences = {}\n    label = None\n    for line in data.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            label = line[1:]\n            sequences[label] = \"\"\n        else:\n            sequences[label] += line\n    return sequences\n\ndef overlap_graph(sequences, k):\n    adjacency_list = []\n    for s1 in sequences:\n        for s2 in sequences:\n            if s1 != s2:\n                if sequences[s1][-k:] == sequences[s2][:k]:\n                    adjacency_list.append((s1, s2))\n    return adjacency_list\n\ndef print_adjacency_list(adjacency_list):\n    for edge in adjacency_list:\n        print(f\"{edge[0]} {edge[1]}\")\n\n# Sample dataset\ndata = \"\"\"\n&gt;Rosalind_0498\nAAATAAA\n&gt;Rosalind_2391\nAAATTTT\n&gt;Rosalind_2323\nTTTTCCC\n&gt;Rosalind_0442\nAAATCCC\n&gt;Rosalind_5013\nGGGTGGG\n\"\"\"\n\nsequences = parse_fasta(data)\nadjacency_list = overlap_graph(sequences, 3)\nprint_adjacency_list(adjacency_list)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-1",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "13.6 Explanation of the Code",
    "text": "13.6 Explanation of the Code\n\nparse_fasta function:\n\nReads the FASTA formatted input.\nStores sequences in a dictionary where keys are the labels and values are the sequences.\n\noverlap_graph function:\n\nTakes the sequences dictionary and the overlap length k.\nChecks each pair of sequences to see if the suffix of length k of the first sequence matches the prefix of length k of the second sequence.\nRecords the directed edge if the condition is met.\n\nprint_adjacency_list function:\n\nPrints each edge in the required format."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-13",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-13",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "14.1 Sample Dataset",
    "text": "14.1 Sample Dataset\n1 0 0 1 0 1"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-14",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-14",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "14.2 Sample Output",
    "text": "14.2 Sample Output\n3.5"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-12",
    "href": "posts/md/Rosalind_stronghold.html#solution-12",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "14.3 Solution",
    "text": "14.3 Solution\nTo solve this problem, we need to calculate the expected number of offspring displaying the dominant phenotype given six nonnegative integers representing the number of couples with specific genotype pairings. Each couple has exactly two offspring."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#genotype-pairings-and-dominance",
    "href": "posts/md/Rosalind_stronghold.html#genotype-pairings-and-dominance",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "14.4 Genotype Pairings and Dominance",
    "text": "14.4 Genotype Pairings and Dominance\nThe six genotype pairings are: 1. AA-AA: 100% dominant phenotype 2. AA-Aa: 100% dominant phenotype 3. AA-aa: 100% dominant phenotype 4. Aa-Aa: 75% dominant phenotype 5. Aa-aa: 50% dominant phenotype 6. aa-aa: 0% dominant phenotype\nWe can represent the probability of offspring having the dominant phenotype for each genotype pairing as follows: 1. AA-AA: \\(1.0\\) 2. AA-Aa: \\(1.0\\) 3. AA-aa: \\(1.0\\) 4. Aa-Aa: \\(0.75\\) 5. Aa-aa: \\(0.5\\) 6. aa-aa: \\(0.0\\)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#expected-number-of-dominant-offspring",
    "href": "posts/md/Rosalind_stronghold.html#expected-number-of-dominant-offspring",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "14.5 Expected Number of Dominant Offspring",
    "text": "14.5 Expected Number of Dominant Offspring\nFor each couple, since they produce exactly two offspring, we can multiply the number of couples by 2 and then by the probability of having a dominant phenotype to get the expected number of dominant offspring per genotype pairing."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation-2",
    "href": "posts/md/Rosalind_stronghold.html#implementation-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "14.6 Implementation",
    "text": "14.6 Implementation\nHere’s the Python code to compute the expected number of dominant offspring:\ndef expected_dominant_offspring(couples):\n    # Probabilities of offspring having dominant phenotype for each genotype pairing\n    probabilities = [1.0, 1.0, 1.0, 0.75, 0.5, 0.0]\n    \n    # Calculate the expected number of dominant offspring\n    expected_value = 0\n    for i in range(6):\n        expected_value += couples[i] * probabilities[i] * 2\n    \n    return expected_value\n\n# Sample dataset\nsample_input = \"1 0 0 1 0 1\"\ninput_ = [int(x) for x in \"1 0 0 1 0 1\".split()]\nprint(expected_dominant_offspring(input_))  # Output: 3.5"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-10",
    "href": "posts/md/Rosalind_stronghold.html#explanation-10",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "14.7 Explanation",
    "text": "14.7 Explanation\n\nInput: We take a list of six integers representing the number of each genotype pairing.\nProbabilities: We define the probabilities for each pairing’s offspring to display the dominant phenotype.\nCalculation: We iterate through each pairing, multiply the number of couples by the corresponding probability and by 2 (since each couple has 2 offspring), and sum these values to get the total expected number of dominant offspring.\nOutput: The result is the expected number of dominant phenotype offspring.\n\nThis code will compute the expected number of offspring displaying the dominant phenotype for any valid input as specified by the problem statement."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-14",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-14",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "15.1 Sample Dataset",
    "text": "15.1 Sample Dataset\n&gt;Rosalind_1\nGATTACA\n&gt;Rosalind_2\nTAGACCA\n&gt;Rosalind_3\nATACA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-15",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-15",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "15.2 Sample Output",
    "text": "15.2 Sample Output\nAC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-13",
    "href": "posts/md/Rosalind_stronghold.html#solution-13",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "15.3 Solution",
    "text": "15.3 Solution\nTo find the longest common substring (LCS) among a collection of DNA strings given in FASTA format, we can use the following approach:\n\nRead and Parse Input: Parse the input FASTA format to get a list of DNA strings.\nIdentify Potential Substrings: Generate all possible substrings of the shortest DNA string since the longest common substring can’t be longer than the shortest string.\nCheck Commonality: Check each substring from longest to shortest to see if it appears in all DNA strings.\nReturn the LCS: Return the first longest common substring found."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-steps",
    "href": "posts/md/Rosalind_stronghold.html#detailed-steps",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "15.4 Detailed Steps",
    "text": "15.4 Detailed Steps\n\nParse the Input:\n\nRead the input strings and remove the FASTA headers.\nCollect the DNA strings into a list.\n\nGenerate All Substrings:\n\nGenerate all substrings of the shortest string in the list. Start with the longest substrings and move to shorter ones.\n\nCheck Substrings:\n\nFor each substring generated, check if it exists in all other DNA strings.\nReturn the first substring that is found in all strings since we are generating substrings from longest to shortest."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation-3",
    "href": "posts/md/Rosalind_stronghold.html#implementation-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "15.5 Implementation",
    "text": "15.5 Implementation\nHere’s a Python function that implements the above plan:\ndef parse_fasta(data):\n    sequences = []\n    seq = \"\"\n    for line in data.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if seq:\n                sequences.append(seq)\n                seq = \"\"\n        else:\n            seq += line.strip()\n    if seq:\n        sequences.append(seq)\n    return sequences\n\ndef find_longest_common_substring(dna_strings):\n    # Find the shortest string in the list\n    shortest_str = min(dna_strings, key=len)\n    len_shortest = len(shortest_str)\n    \n    # Function to check if a substring is common in all strings\n    def is_common(sub):\n        return all(sub in dna for dna in dna_strings)\n    \n    # Iterate over all substrings of the shortest string\n    for length in range(len_shortest, 0, -1):\n        for start in range(len_shortest - length + 1):\n            candidate = shortest_str[start:start + length]\n            if is_common(candidate):\n                return candidate\n    return \"\"\n\n# Sample dataset\ndata = \"\"\"&gt;Rosalind_1\nGATTACA\n&gt;Rosalind_2\nTAGACCA\n&gt;Rosalind_3\nATACA\"\"\"\n\n# Parse the FASTA format data\ndna_strings = parse_fasta(data)\n\n# Find and print the longest common substring\nlcs = find_longest_common_substring(dna_strings)\nprint(lcs)  # Output: AC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-11",
    "href": "posts/md/Rosalind_stronghold.html#explanation-11",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "15.6 Explanation",
    "text": "15.6 Explanation\n\nParsing the Input:\n\nThe parse_fasta function reads the FASTA data, splits it into individual sequences, and returns a list of DNA strings.\n\nFinding the Longest Common Substring:\n\nThe find_longest_common_substring function first identifies the shortest string among the DNA strings.\nIt then iterates over all possible substrings of the shortest string, starting from the longest possible substrings.\nFor each candidate substring, it checks if this substring is present in all other DNA strings using the is_common function.\nThe first substring found that is common to all DNA strings is returned as the result.\n\n\nThis approach ensures that we find the longest common substring efficiently by leveraging the properties of substrings and the fact that the longest common substring cannot be longer than the shortest string in the list."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-15",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-15",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "16.1 Sample Dataset",
    "text": "16.1 Sample Dataset\n2 1"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-16",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-16",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "16.2 Sample Output",
    "text": "16.2 Sample Output\n0.684"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-16",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-16",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "17.1 Sample Dataset",
    "text": "17.1 Sample Dataset\nA2Z669\nB5ZC00\nP07204_TRBM_HUMAN\nP20840_SAG1_YEAST"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-17",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-17",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "17.2 Sample Output",
    "text": "17.2 Sample Output\nB5ZC00\n85 118 142 306 395\nP07204_TRBM_HUMAN\n47 115 116 382 409\nP20840_SAG1_YEAST\n79 109 135 248 306 348 364 402 485 501 614"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-14",
    "href": "posts/md/Rosalind_stronghold.html#solution-14",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "17.3 Solution",
    "text": "17.3 Solution\nTo solve the problem of identifying N-glycosylation motifs in protein sequences from the UniProt database, we need to follow these steps:\n\nFetch Protein Sequences: Retrieve the protein sequences in FASTA format from UniProt using the given acces IDs.\nIdentify Motif Locations: Search for the motif N{P}[ST]{P} in each protein sequence and record the positions where it occurs.\nOutput Results: For each protein containing the motif, output its ID followed by the positions where the motif is found.\n\nHere is a complete Python script to perform these tasks:\nimport requests\nimport re\n\ndef fetch_fasta(uniprot_id):\n    url = f\"http://www.uniprot.org/uniprot/{uniprot_id}.fasta\"\n    response = requests.get(url)\n    response.raise_for_status()  # Ensure we notice bad responses\n    fasta_data = response.text\n    return ''.join(fasta_data.split('\\n')[1:])  # Remove the first line and join the rest\n\ndef find_motif_locations(sequence, motif_regex):\n    matches = re.finditer(motif_regex, sequence)\n    return [match.start() + 1 for match in matches]  # Convert to 1-based index\n    \ndef fetch_input(data):\n    ids = []\n    for line in data.strip().split('\\n'):\n        ids.append(line)\n    return ids\n   \ndata = \"\"\"\nA2Z669\nB5ZC00\nP07204_TRBM_HUMAN\nP20840_SAG1_YEAST\n\"\"\"\n\nids = fetch_input(data)\nmotif_regex = re.compile(r'N[^P][ST][^P]')\nresults = {}\n\nfor uniprot_id in ids:\n    sequence = fetch_fasta(uniprot_id)\n    locations = find_motif_locations(sequence, motif_regex)\n    if locations:\n        results[uniprot_id] = locations\n\nfor uniprot_id, locations in results.items():\n    print(uniprot_id)\n    print(' '.join(map(str, locations)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-12",
    "href": "posts/md/Rosalind_stronghold.html#explanation-12",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "17.4 Explanation",
    "text": "17.4 Explanation\n\nfetch_fasta(uniprot_id):\n\nTakes a UniProt ID and fetches the corresponding protein sequence in FASTA format.\nStrips out the header line and joins the remaining lines to form the complete sequence.\n\nfind_motif_locations(sequence, motif_regex):\n\nUses the regex N[^P][ST][^P] to find all positions of the motif in the sequence.\nReturns a list of start positions in 1-based index format.\n\nmain():\n\nDefines the list of UniProt IDs.\nCompiles the regex for the motif.\nFetches each protein sequence, finds motif locations, and stores the results.\nOutputs the protein IDs followed by the locations of the motif.\n\n\nThis script fetches protein sequences from UniProt, searches for the N-glycosylation motif, and prints the locations where the motif occurs for each protein that contains it."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-17",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-17",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "18.1 Sample Dataset",
    "text": "18.1 Sample Dataset\n&gt;Rosalind_99\nAGCCATGTAGCTAACTCAGGTTACATGGGGATGACCCCGCGACTTGGATTAGAGTCTCTTTTGGAATAAGCCTGAATGATCCGAGTAGCATCTCAG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-18",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-18",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "18.2 Sample Output",
    "text": "18.2 Sample Output\nMLLGSFRLIPKETLIQVAGSSPCNLS\nM\nMGMTPRLGLESLLE\nMTPRLGLESLLE"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-15",
    "href": "posts/md/Rosalind_stronghold.html#solution-15",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "18.3 Solution",
    "text": "18.3 Solution\nTo find all distinct candidate protein strings from open reading frames (ORFs) in the given DNA sequence, the following approach is used:\n\nParse the Input DNA Sequence: Read the input in FASTA format and obtain the DNA sequence.\nGenerate Reading Frames: Generate six reading frames: three from the original DNA strand and three from its reverse complement.\nIdentify ORFs: For each reading frame, identify sequences that start with a start codon (ATG) and end with a stop codon (TAA, TAG, TGA).\nTranslate to Proteins: Translate the identified ORFs to protein sequences.\nCollect and Print Distinct Proteins: Collect all distinct protein sequences.\n\nHere’s the complete implementation in Python:\nCODON_TABLE = {\n    'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M',\n    'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n    'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K',\n    'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',\n    'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n    'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n    'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q',\n    'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n    'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',\n    'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n    'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E',\n    'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n    'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n    'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',\n    'TAC':'Y', 'TAT':'Y', 'TAA':'*', 'TAG':'*',\n    'TGC':'C', 'TGT':'C', 'TGA':'*', 'TGG':'W',\n}\n\ndef translate_dna_to_protein(dna_seq):\n    protein = []\n    has_stop_codon = False\n    for i in range(0, len(dna_seq) - 2, 3):\n        codon = dna_seq[i:i + 3]\n        amino_acid = CODON_TABLE.get(codon, '')\n        if amino_acid == '*':\n            has_stop_codon = True\n            break\n        protein.append(amino_acid)\n    return ''.join(protein), has_stop_codon\n\ndef find_orfs(dna_seq):\n    orfs = set()\n    \n    # Generate 3 reading frames for the DNA sequence\n    for frame in range(3):\n        for i in range(frame, len(dna_seq) - 2, 3):\n            if dna_seq[i:i + 3] == 'ATG':\n                protein, has_stop_codon = translate_dna_to_protein(dna_seq[i:])\n                if protein and has_stop_codon:\n                    orfs.add(protein)\n    \n    return orfs\n\ndef reverse_complement(dna_seq):\n    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    return ''.join(complement[base] for base in reversed(dna_seq))\n\ndef main():\n    fasta_input = \"\"\"&gt;Rosalind_99\nAGCCATGTAGCTAACTCAGGTTACATGGGGATGACCCCGCGACTTGGATTAGAGTCTCTTTTGGAATAAGCCTGAATGATCCGAGTAGCATCTCAG\"\"\"\n\n    dna_seq = ''.join(line.strip() for line in fasta_input.split('\\n') if not line.startswith('&gt;'))\n    \n    # Get reverse complement of the DNA sequence\n    reverse_complement_seq = reverse_complement(dna_seq)\n\n    # Find ORFs in the original and reverse complement sequences\n    original_orfs = find_orfs(dna_seq)\n    reverse_orfs = find_orfs(reverse_complement_seq)\n\n    # Combine results and remove duplicates\n    all_orfs = original_orfs.union(reverse_orfs)\n\n    # Print all distinct protein sequences\n    for protein in all_orfs:\n        print(protein)\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanations",
    "href": "posts/md/Rosalind_stronghold.html#explanations",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "18.4 Explanations",
    "text": "18.4 Explanations\n\ntranslate_dna_to_protein: Now returns a tuple containing the translated protein and a boolean indicating if a stop codon was found.\nfind_orfs: Checks for the presence of a stop codon in the translated protein before adding it to the set of ORFs."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-18",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-18",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "19.1 Sample Dataset",
    "text": "19.1 Sample Dataset\n3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-19",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-19",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "19.2 Sample Output",
    "text": "19.2 Sample Output\n6\n1 2 3\n1 3 2\n2 1 3\n2 3 1\n3 1 2\n3 2 1"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-16",
    "href": "posts/md/Rosalind_stronghold.html#solution-16",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "19.3 Solution",
    "text": "19.3 Solution\nTo solve the problem of enumerating all permutations of a given integer \\(n\\), we can use Python’s itertools.permutations to generate all possible permutations. Here’s a simple code that accomplishes this:\nfrom itertools import permutations\n\ndef enumerate_gene_orders(n):\n    # Generate permutations\n    perm = permutations(range(1, n + 1))\n    \n    # Convert permutations to a list\n    perm_list = list(perm)\n    \n    # Print the number of permutations\n    print(len(perm_list))\n    \n    # Print each permutation\n    for p in perm_list:\n        print(' '.join(map(str, p)))\n\n# Example usage\nn = 3\nenumerate_gene_orders(n)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-13",
    "href": "posts/md/Rosalind_stronghold.html#explanation-13",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "19.4 Explanation",
    "text": "19.4 Explanation\n\nImporting permutations: We import the permutations function from Python’s itertools module, which is perfect for generating permutations of a sequence.\nGenerating permutations: Using permutations(range(1, n + 1)), we generate all permutations of the list [1, 2, ..., n].\nConverting to a list: We convert the permutations object to a list to easily count and iterate over the permutations.\nPrinting the count: We print the total number of permutations.\nPrinting each permutation: We iterate through each permutation and print it in the required format."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-19",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-19",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "20.1 Sample Dataset",
    "text": "20.1 Sample Dataset\nSKADYEK"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-20",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-20",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "20.2 Sample Output",
    "text": "20.2 Sample Output\n821.392"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-17",
    "href": "posts/md/Rosalind_stronghold.html#solution-17",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "20.3 Solution",
    "text": "20.3 Solution\n\n먼저, 각 아미노산의 질량을 딕셔너리로 정의합니다.\n주어진 단백질 문자열의 각 아미노산의 질량을 더해서 총 질량을 계산합니다.\n\n# 아미노산 질량 테이블\nmass_table = {\n    'A': 71.03711, 'C': 103.00919, 'D': 115.02694, 'E': 129.04259,\n    'F': 147.06841, 'G': 57.02146, 'H': 137.05891, 'I': 113.08406,\n    'K': 128.09496, 'L': 113.08406, 'M': 131.04049, 'N': 114.04293,\n    'P': 97.05276, 'Q': 128.05858, 'R': 156.10111, 'S': 87.03203,\n    'T': 101.04768, 'V': 99.06841, 'W': 186.07931, 'Y': 163.06333\n}\n\ndef calculate_protein_mass(protein):\n    total_mas = 0.0\n    for amino_acid in protein:\n        if amino_acid in mass_table:\n            total_mas += mass_table[amino_acid]\n        else:\n            print(f\"Unknown amino acid: {amino_acid}\")\n    return total_mass\n\n# 샘플 데이터셋\nsequence = \"SKADYEK\"\n\n# 총 질량 계산\ntotal_mas = calculate_protein_mass(sequence)\nprint(f\"{total_mass:.3f}\")\n이 코드는 단백질 문자열 SKADYEK 의 총 질량을 계산하여 821.392 라는 결과를 출력합니다. mass_table 에 정의된 각 아미노산의 질량을 이용하여 문자열을 순회하면서 질량을 더해 총 질량을 계산합니다."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-20",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-20",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "21.1 Sample Dataset",
    "text": "21.1 Sample Dataset\n&gt;Rosalind_24\nTCAATGCATGCGGGTCTATATGCAT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-21",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-21",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "21.2 Sample Output",
    "text": "21.2 Sample Output\n4 6\n5 4\n6 6\n7 4\n17 4\n18 4\n20 6\n21 4"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-18",
    "href": "posts/md/Rosalind_stronghold.html#solution-18",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "21.3 Solution",
    "text": "21.3 Solution\n주어진 DNA 문자열에서 역방향 팔린드롬을 찾는 코드를 작성하겠습니다. 역방향 팔린드롬은 해당 문자열이 그 역상 보완 문자열과 동일한 문자열을 말합니다. 역방향 팔린드롬의 위치와 길이를 반환하도록 하겠습니다.\ndef reverse_complement(dna):\n    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    return ''.join(complement[base] for base in reversed(dna))\n\ndef find_reverse_palindromes(dna):\n    results = []\n    length = len(dna)\n    \n    for i in range(length):\n        for j in range(4, 13):  # 길이가 4에서 12까지인 모든 경우를 확인\n            if i + j &gt; length:\n                break\n            substring = dna[i:i+j]\n            if substring == reverse_complement(substring):\n                results.append((i+1, j))\n    \n    return results\n\n# 샘플 데이터셋\nsample_fasta = \"\"\"&gt;Rosalind_24\nTCAATGCATGCGGGTCTATATGCAT\"\"\"\n\n# FASTA 포맷에서 DNA 문자열 추출\ndna_string = ''.join(sample_fasta.split('\\n')[1:])\n\n# 역방향 팔린드롬 찾기\npalindromes = find_reverse_palindromes(dna_string)\n\n# 결과 출력\nfor pos, length in palindromes:\n    print(pos, length)\n위 코드는 다음과 같이 작동합니다:\n\nreverse_complement 함수는 DNA 문자열의 역상 보완 문자열을 생성합니다.\nfind_reverse_palindromes 함수는 DNA 문자열을 순회하면서 길이가 4 에서 12 사이인 모든 부분 문자열에 대해 역방향 팔린드롬인지 확인합니다.\n샘플 데이터를 입력으로 사용하여 역방향 팔린드롬의 위치와 길이를 출력합니다."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-21",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-21",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "22.1 Sample Dataset",
    "text": "22.1 Sample Dataset\n&gt;Rosalind_10\nATGGTCTACATAGCTGACAAACAGCACGTAGCAATCGGTCGAATCTCGAGAGGCATATGGTCACATGATCGGTCGAGCGTGTTTCAAAGTTTGCGCCTAG\n&gt;Rosalind_12\nATCGGTCGAA\n&gt;Rosalind_15\nATCGGTCGAGCGTGT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-22",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-22",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "22.2 Sample Output",
    "text": "22.2 Sample Output\nMVYIADKQHVASREAYGHMFKVCA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-19",
    "href": "posts/md/Rosalind_stronghold.html#solution-19",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "22.3 Solution",
    "text": "22.3 Solution\n먼저, 주어진 데이터를 파싱하고 인트론을 제거한 후 RNA 로 전사하고, 이를 단백질로 번역합니다.\n# DNA 문자열에서 RNA로 전사\ndef transcribe(dna):\n    return dna.replace('T', 'U')\n\n# RNA를 단백질로 번역하기 위한 코돈 테이블\ncodon_table = {\n    'AUG': 'M', 'UGU': 'C', 'UGC': 'C', 'UGA': '', 'UAA': '', 'UAG': '',\n    'UUU': 'F', 'UUC': 'F', 'UUA': 'L', 'UUG': 'L', 'UAU': 'Y', 'UAC': 'Y',\n    'UGG': 'W', 'CUU': 'L', 'CUC': 'L', 'CUA': 'L', 'CUG': 'L', 'CCU': 'P',\n    'CCC': 'P', 'CCA': 'P', 'CCG': 'P', 'CAU': 'H', 'CAC': 'H', 'CAA': 'Q',\n    'CAG': 'Q', 'CGU': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'AUU': 'I',\n    'AUC': 'I', 'AUA': 'I', 'ACU': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n    'AAU': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K', 'AGU': 'S', 'AGC': 'S',\n    'AGA': 'R', 'AGG': 'R', 'GUU': 'V', 'GUC': 'V', 'GUA': 'V', 'GUG': 'V',\n    'GCU': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A', 'GAU': 'D', 'GAC': 'D',\n    'GAA': 'E', 'GAG': 'E', 'GGU': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G',\n    'UCU': 'S', 'UCC': 'S', 'UCA': 'S', 'UCG': 'S'\n}\n\n# RNA 문자열을 단백질로 번역\ndef translate(rna):\n    protein = []\n    for i in range(0, len(rna) - 2, 3):\n        codon = rna[i:i+3]\n        if codon in codon_table:\n            if codon_table[codon] == '':\n                break\n            protein.append(codon_table[codon])\n    return ''.join(protein)\n\n# FASTA 형식의 데이터를 파싱하여 DNA 문자열과 인트론을 추출\ndef parse_fasta(fasta_data):\n    sequences = []\n    label = None\n    for line in fasta_data.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            label = line[1:]\n            sequences.append([])\n        else:\n            sequences[-1].append(line)\n    return [''.join(seq) for seq in sequences]\n\n# 샘플 데이터셋\nsample_data = \"\"\"&gt;Rosalind_10\nATGGTCTACATAGCTGACAAACAGCACGTAGCAATCGGTCGAATCTCGAGAGGCATATGGTCACATGATCGGTCGAGCGTGTTTCAAAGTTTGCGCCTAG\n&gt;Rosalind_12\nATCGGTCGAA\n&gt;Rosalind_15\nATCGGTCGAGCGTGT\"\"\"\n\n# FASTA 데이터를 파싱하여 주어진 DNA와 인트론 추출\nsequences = parse_fasta(sample_data)\nmain_dna = sequences[0]\nintrons = sequences[1:]\n\n# 인트론을 제거하여 엑손 결합\nfor intron in introns:\n    main_dna = main_dna.replace(intron, '')\n\n# 엑손을 RNA로 전사\nrna = transcribe(main_dna)\n\n# RNA를 단백질로 번역\nprotein = translate(rna)\n\n# 결과 출력\nprint(protein)\n이 코드는 다음과 같이 동작합니다:\n\nparse_fasta 함수는 FASTA 형식의 데이터를 파싱하여 DNA 문자열과 인트론을 추출합니다.\nmain_dna 에서 모든 인트론을 제거하여 엑손을 결합합니다.\ntranscribe 함수는 DNA 를 RNA 로 전사합니다.\ntranslate 함수는 RNA 를 단백질로 번역합니다.\n최종 결과를 출력합니다.\n\n샘플 데이터를 사용하여 실행하면, 결과는 MVYIADKQHVASREAYGHMFKVCA 가 됩니다."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-22",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-22",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "23.1 Sample Dataset",
    "text": "23.1 Sample Dataset\nA C G T\n2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-23",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-23",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "23.2 Sample Output",
    "text": "23.2 Sample Output\nAA\nAC\nAG\nAT\nCA\nCC\nCG\nCT\nGA\nGC\nGG\nGT\nTA\nTC\nTG\nTT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-20",
    "href": "posts/md/Rosalind_stronghold.html#solution-20",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "23.3 Solution",
    "text": "23.3 Solution\n주어진 알파벳과 길이 \\(n\\) 에 대해 가능한 모든 \\(n\\) 길이의 문자열을 사전순으로 나열하는 파이썬 코드를 작성하겠습니다. 이를 위해 itertools 모듈의 product 함수를 사용할 수 있습니다.\n다음은 이를 구현한 파이썬 코드입니다:\nimport itertools\n\ndef enumerate_kmers(alphabet, n):\n    return [''.join(p) for p in itertools.product(alphabet, repeat=n)]\n\n# 샘플 데이터셋\nalphabet = \"A C G T\".split()\nn = 2\n\n# k-mer를 나열하고 사전순으로 정렬\nkmers = enumerate_kmers(alphabet, n)\nfor kmer in kmers:\n    print(kmer)\n이 코드는 다음과 같은 절차로 동작합니다:\n\nitertools.product 를 사용하여 주어진 알파벳의 모든 가능한 길이 \\(n\\) 의 조합을 생성합니다.\n각 조합을 문자열로 변환하여 리스트에 저장합니다.\n결과 리스트를 출력합니다.\n\n샘플 데이터를 사용하여 실행하면, 결과는 다음과 같습니다:\nAA\nAC\nAG\nAT\nCA\nCC\nCG\nCT\nGA\nGC\nGG\nGT\nTA\nTC\nTG\nTT\n이 코드는 주어진 알파벳과 길이 n 에 대해 가능한 모든 문자열을 사전순으로 올바르게 나열합니다."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-23",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-23",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "24.1 Sample Dataset",
    "text": "24.1 Sample Dataset\n5\n5 1 4 2 3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-24",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-24",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "24.2 Sample Output",
    "text": "24.2 Sample Output\n1 2 3\n5 4 2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-21",
    "href": "posts/md/Rosalind_stronghold.html#solution-21",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "24.3 Solution",
    "text": "24.3 Solution\n주어진 시퀀스에서 ” 가장 긴 증가하는 부분 수열 ” 과 ” 가장 긴 감소하는 부분 수열 ” 을 찾아서 출력하는 기능을 합니다.\ndef input_processor(seq_str):\n    \"\"\"\n    입력 문자열을 처리하여 정수 리스트로 변환하는 함수\n    \"\"\"\n    return list(map(int, seq_str.split()))\n\ndef longest_subsequence(long_type, seq):\n    \"\"\"\n    가장 긴 증가 또는 감소하는 부분 수열을 찾는 함수\n\n    Args:\n    - long_type (str): 부분 수열의 종류 (\"inc\" 또는 \"dec\")\n    - seq (list of int): 입력 시퀀스\n\n    Returns:\n    - prev_idxes (list of int): 이전 인덱스를 기록한 리스트\n    - seq (list of int): 변환된 입력 시퀀스\n    \"\"\"\n    # 입력 시퀀스를 내림차순으로 변경할지 여부에 따라 결정\n    seq = list(reversed(seq)) if long_type != \"inc\" else seq\n    \n    # 부분 수열의 길이를 저장할 리스트 초기화\n    L = [1] * len(seq)\n    # 이전 인덱스를 저장할 리스트 초기화\n    prev_idxes = []\n    \n    # 각 위치마다 최장 부분 수열의 길이 계산\n    for i in range(len(L)):\n        subproblems = [L[k] for k in range(i) if seq[k] &lt; seq[i]]\n        L[i] = 1 + max(subproblems, default=0)\n        \n        if len(subproblems) == 0:\n            prev_idxes.append(-1)\n        else:\n            # 최장 부분 수열의 마지막 인덱스 계산\n            last_idx = len(L[:i]) - L[:i][::-1].index(max(subproblems)) - 1\n            prev_idxes.append(last_idx)\n    \n    return prev_idxes, seq\n\ndef decode_prev_idx(data_package):\n    \"\"\"\n    이전 인덱스를 기반으로 최장 부분 수열을 복원하는 함수\n\n    Args:\n    - data_package (tuple): (prev_idxes, seq), 이전 인덱스 리스트와 시퀀스\n\n    Returns:\n    - vals (list of list): 복원된 최장 부분 수열들의 리스트\n    \"\"\"\n    prev_idxes, seq = data_package\n    vals = []\n    \n    # 각 최장 부분 수열을 복원\n    for cur_idx in range(len(prev_idxes) - 1, -1, -1):\n        val = []\n        while cur_idx != -1:\n            cur_val = seq[cur_idx]\n            val.append(cur_val)\n            prev_idx = prev_idxes[cur_idx]\n            cur_idx = prev_idx\n        vals.append(val)\n    \n    return vals\n\ndef print_result(long_type, seq):\n    \"\"\"\n    결과를 출력하는 함수\n\n    Args:\n    - long_type (str): 부분 수열의 종류 (\"inc\" 또는 \"dec\")\n    - seq (list of int): 출력할 시퀀스\n    \"\"\"\n    if long_type == \"inc\":\n        print(*seq[::-1])\n    else:\n        print(*seq)\n\n# 입력 데이터\ndata = \"5 1 4 2 3\"\n\n# \"inc\"와 \"dec\" 각각에 대해 최장 부분 수열을 찾고 출력\nfor long_type in [\"inc\", \"dec\"]:\n    seq = input_processor(data)\n    prev_idxes, seq = longest_subsequence(long_type, seq)\n    vals = decode_prev_idx((prev_idxes, seq))\n    longest_subseq = max(vals, key=len)\n    print_result(long_type, longest_subseq)\n\nlongest_subsequence(long_type, seq):\n\nlong_type 이 “inc” 인 경우에는 시퀀스를 그대로 사용하고, “dec” 인 경우에는 시퀀스를 뒤집어서 사용합니다.\nL 리스트를 초기화하고 각 위치에서 최장 증가 부분 수열의 길이를 계산합니다.\nprev_idxes 리스트에는 각 위치에서의 이전 인덱스를 기록하여 후에 부분 수열을 복원하는 데 사용됩니다.\n\ndecode_prev_idx(data_package):\n\nlongest_subsequence 함수에서 반환된 (prev_idxes, seq) 를 받아서 이전 인덱스를 기반으로 최장 부분 수열을 복원합니다.\n각 부분 수열을 vals 리스트에 저장하고 반환합니다.\n\nprint_result(long_type, seq):\n\nlong_type 이 “inc” 인 경우에는 시퀀스를 역순으로 출력하고, “dec” 인 경우에는 그대로 출력합니다.\n\nMain Loop:\n\n입력 데이터인 “5 1 4 2 3” 에 대해 “inc” 와 “dec” 각각에 대해 최장 부분 수열을 찾고 출력합니다.\n예를 들어, “inc” 인 경우 [1, 2, 3] 이 출력되며, “dec” 인 경우 [5, 4, 2] 가 출력됩니다."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-24",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-24",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "25.1 Sample Dataset",
    "text": "25.1 Sample Dataset\n&gt;Rosalind_56\nATTAGACCTG\n&gt;Rosalind_57\nCCTGCCGGAA\n&gt;Rosalind_58\nAGACCTGCCG\n&gt;Rosalind_59\nGCCGGAATAC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-25",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-25",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "25.2 Sample Output",
    "text": "25.2 Sample Output\nATTAGACCTGCCGGAATAC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-22",
    "href": "posts/md/Rosalind_stronghold.html#solution-22",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "25.3 Solution",
    "text": "25.3 Solution\n아래는 주어진 FASTA 형식 텍스트를 입력으로 받아 최단 슈퍼스트링을 계산하는 파이썬 코드입니다.\ndef parse_fasta(fasta_text):\n    sequences = {}\n    current_header = None\n    current_sequence = []\n    \n    lines = fasta_text.splitlines()\n    for line in lines:\n        line = line.strip()\n        if line.startswith('&gt;'):\n            if current_header:\n                sequences[current_header] = ''.join(current_sequence)\n                current_sequence = []\n            current_header = line[1:]\n        else:\n            current_sequence.append(line)\n    \n    # 마지막 시퀀스 처리\n    if current_header:\n        sequences[current_header] = ''.join(current_sequence)\n    \n    return sequences\n\ndef overlap(s1, s2):\n    max_len = min(len(s1), len(s2))\n    for i in range(max_len, 0, -1):\n        if s1[-i:] == s2[:i]:\n            return i\n    return 0\n\ndef shortest_superstring(dna_sequences):\n    strings = list(dna_sequences.values())\n    n = len(strings)\n    \n    while n &gt; 1:\n        max_overlap = -1\n        best_i, best_j = -1, -1\n        \n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    overlap_len = overlap(strings[i], strings[j])\n                    if overlap_len &gt; max_overlap:\n                        max_overlap = overlap_len\n                        best_i, best_j = i, j\n        \n        if max_overlap &gt; 0:\n            strings[best_i] += strings[best_j][max_overlap:]\n            strings.pop(best_j)\n            n -= 1\n        else:\n            break\n    \n    return strings[0]\n\n# 예시로 주어진 FASTA 형식 텍스트\nfasta_text = '''&gt;Rosalind_56\nATTAGACCTG\n&gt;Rosalind_57\nCCTGCCGGAA\n&gt;Rosalind_58\nAGACCTGCCG\n&gt;Rosalind_59\nGCCGGAATAC'''\n\n# FASTA 형식 텍스트를 파싱하여 DNA 시퀀스 딕셔너리를 얻음\ndna_sequences = parse_fasta(fasta_text)\n\n# 최단 슈퍼스트링 계산\nresult = shortest_superstring(dna_sequences)\nprint(\"Shortest superstring:\", result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#코드-설명",
    "href": "posts/md/Rosalind_stronghold.html#코드-설명",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "25.4 코드 설명",
    "text": "25.4 코드 설명\n\nparse_fasta 함수: 입력으로 받은 FASTA 형식 텍스트를 파싱하여 시퀀스 헤더를 키로, 시퀀스를 값으로 갖는 딕셔너리를 반환합니다.\noverlap 함수: 두 문자열 사이의 최대 겹치는 길이를 계산합니다.\nshortest_superstring 함수: DNA 시퀀스들을 최단 슈퍼스트링으로 합치는 과정을 반복하여 수행합니다. 각 반복에서 가장 많이 겹치는 두 시퀀스를 찾아 이어붙이고, 필요 없는 시퀀스는 제거합니다.\n예시 입력 (fasta_text): 문제에서 제공된 예시 FASTA 형식의 텍스트입니다. 이를 통해 각 DNA 시퀀스를 추출하여 최단 슈퍼스트링을 계산합니다.\n결과 출력: 계산된 최단 슈퍼스트링을 출력합니다.\n\n이 코드를 실행하면 주어진 FASTA 형식 텍스트에서 DNA 시퀀스들을 추출하고, 이를 이용하여 최단 슈퍼스트링을 계산하여 출력합니다."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-25",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-25",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "26.1 Sample Dataset",
    "text": "26.1 Sample Dataset\n&gt;Rosalind_23\nAGCUAGUCAU"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-26",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-26",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "26.2 Sample Output",
    "text": "26.2 Sample Output\n12"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-23",
    "href": "posts/md/Rosalind_stronghold.html#solution-23",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "26.3 Solution",
    "text": "26.3 Solution\nTo solve the problem of calculating the total number of perfect matchings in the bonding graph of an RNA string s, we can break down the solution into clear steps:"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-5",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-5",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "26.4 Steps to Solve the Problem",
    "text": "26.4 Steps to Solve the Problem\n\nParse the Input: Extract the RNA string from the input, ignoring the header line (if any).\nCount Nucleotide Pairs: Count the occurrences of each nucleotide (A, U, C, G) in the RNA string s.\nCalculate Perfect Matchings:\n\nThe number of perfect matchings in the bonding graph of s is determined by pairing each A with a U and each C with a G.\nCompute the factorial of half the count of each nucleotide pair (A with U and C with G). This gives the number of ways to form perfect matchings for each pair.\n\nOutput the Result: Print the computed number of perfect matchings."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#python-implementation",
    "href": "posts/md/Rosalind_stronghold.html#python-implementation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "26.5 Python Implementation",
    "text": "26.5 Python Implementation\nHere’s the Python code that implements the above approach:\nimport math\n\ndef count_nucleotides(s):\n    count_A = s.count('A')\n    count_U = s.count('U')\n    count_C = s.count('C')\n    count_G = s.count('G')\n    return count_A, count_U, count_C, count_G\n\ndef calculate_perfect_matchings(s):\n    count_A, count_U, count_C, count_G = count_nucleotides(s)\n    \n    # Check if counts of A == U and C == G\n    if count_A != count_U or count_C != count_G:\n        return 0\n    \n    # Calculate number of perfect matchings\n    perfect_matchings = math.factorial(count_A) * math.factorial(count_C)\n    \n    return perfect_matchings\n\n# Example usage with sample dataset\nrna_string = \"AGCUAGUCAU\"\n\nresult = calculate_perfect_matchings(rna_string)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-2",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "26.6 Explanation of the Code",
    "text": "26.6 Explanation of the Code\n\ncount_nucleotides: This function counts the occurrences of A, U, C, and G in the RNA string s.\ncalculate_perfect_matchings:\n\nIt first calls count_nucleotides to get the counts of each nucleotide.\nChecks if the counts of A equal U and C equal G. If not, it returns 0 since perfect matchings are not possible.\nCalculates the number of perfect matchings using factorials of half the counts of A and C (since each A pairs with a U and each C pairs with a G).\n\nExample Usage:\n\nIt demonstrates how to use the calculate_perfect_matchings function with the RNA string \"AGCUAGUCAU\", which is given in the sample dataset.\nThe result is printed, which in this case would be 12, indicating the total number of perfect matchings in the bonding graph of \"AGCUAGUCAU\".\n\n\nThis code efficiently calculates the required number of perfect matchings based on the properties of RNA and the bonding rules specified. Adjustments can be made to handle different inputs as needed, ensuring accurate computation of perfect matchings."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-26",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-26",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "27.1 Sample Dataset",
    "text": "27.1 Sample Dataset\n21 7"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-27",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-27",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "27.2 Sample Output",
    "text": "27.2 Sample Output\n51200"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-24",
    "href": "posts/md/Rosalind_stronghold.html#solution-24",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "27.3 Solution",
    "text": "27.3 Solution\nTo solve the problem of calculating the number of partial permutations \\(P(n, k)\\), where \\(P(n, k) = \\frac{n!}{(n-k)!}\\), we need to consider the constraints provided and handle computations under a modulo operation.\nHere’s a step-by-step approach to implement the solution:\n\nRead Input: Extract integers \\(n\\) and \\(k\\) from the input.\nCompute Factorials: Calculate \\(n!\\) and \\((n-k)!\\) using factorial computations. Since \\(n\\) can be up to 100, Python’s built-in math.factorial function is suitable for this task.\nCompute Partial Permutations: Compute \\(P(n, k)\\) using the formula \\(P(n, k) = \\frac{n!}{(n-k)!}\\).\nApply Modulo Operation: Since the result needs to be modulo \\(1,000,000\\), compute the result using % 1,000,000 to prevent overflow and adhere to the problem’s requirement.\nOutput the Result: Print the computed result.\n\nHere’s the Python code that implements the above plan:\nimport math\n\ndef partial_permutations(n, k):\n    # Calculate n!\n    n_fact = math.factorial(n)\n    \n    # Calculate (n-k)!\n    nk_fact = math.factorial(n - k)\n    \n    # Calculate P(n, k) = n! / (n-k)!\n    P_n_k = n_fact // nk_fact\n    \n    # Return P(n, k) % 1,000,000\n    return P_n_k % 1000000\n\n# Example usage with sample dataset\nn, k = 21, 7\nresult = partial_permutations(n, k)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-14",
    "href": "posts/md/Rosalind_stronghold.html#explanation-14",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "27.4 Explanation",
    "text": "27.4 Explanation\n\nmath.factorial: This function from the math module efficiently computes factorials, which is crucial given the constraints \\(n \\leq 100\\).\npartial_permutations function:\n\nComputes \\(n!\\) and \\((n-k)!\\).\nComputes \\(P(n, k)\\) using integer division // to ensure the result is an integer.\nApplies the modulo operation % 1,000,000 to handle large numbers and ensure the result fits within the specified range.\n\nExample Usage:\n\nThe code snippet demonstrates how to compute \\(P(21, 7)\\) using the partial_permutations function and prints the result.\n\n\nThis approach efficiently computes the required number of partial permutations while adhering to the constraints and output requirements specified in the problem statement."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#hint",
    "href": "posts/md/Rosalind_stronghold.html#hint",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "28.1 Hint",
    "text": "28.1 Hint\nOne property of the logarithm function is that for any positive numbers \\(x\\) and \\(y\\), \\(\\log_{10}(x⋅y)= \\log_{10}(x)+ \\log_{10}(y)\\)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-27",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-27",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "28.2 Sample Dataset",
    "text": "28.2 Sample Dataset\nACGATACAA\n0.129 0.287 0.423 0.476 0.641 0.742 0.783"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-28",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-28",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "28.3 Sample Output",
    "text": "28.3 Sample Output\n-5.737 -5.217 -5.263 -5.360 -5.958 -6.628 -7.009"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-25",
    "href": "posts/md/Rosalind_stronghold.html#solution-25",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "28.4 Solution",
    "text": "28.4 Solution\nimport math\n\ndef calculate_log_probabilities(s, gc_contents):\n    \"\"\"\n    Calculate the logarithm of the probabilities of the DNA string s\n    matching random strings generated with different GC-contents.\n\n    Parameters:\n    s (str): DNA string\n    gc_contents (list of float): List of GC-content values\n\n    Returns:\n    list of float: List of log probabilities for each GC-content\n    \"\"\"\n    log_probs = [calculate_log_prob_for_gc_content(s, gc_content) for gc_content in gc_contents]\n    return log_probs\n\ndef calculate_log_prob_for_gc_content(s, gc_content):\n    \"\"\"\n    Calculate the logarithm of the probability of the DNA string s\n    given a specific GC-content.\n\n    Parameters:\n    s (str): DNA string\n    gc_content (float): GC-content value\n\n    Returns:\n    float: Logarithm of the probability\n    \"\"\"\n    p_gc = gc_content / 2\n    p_at = (1 - gc_content) / 2\n    log_prob = sum(math.log10(p_gc if nucleotide in 'GC' else p_at) for nucleotide in s)\n    return log_prob\n\ndef parse_input(input_string):\n    \"\"\"\n    Parse the input string to extract the DNA string and GC-content values.\n\n    Parameters:\n    input_string (str): Input string containing the DNA string and GC-content values\n\n    Returns:\n    tuple: DNA string and list of GC-content values\n    \"\"\"\n    lines = input_string.strip().split('\\n')\n    s = lines[0]\n    gc_contents = list(map(float, lines[1].split()))\n    return s, gc_contents\n\ndef format_output(log_probs):\n    \"\"\"\n    Format the output to match the required format.\n\n    Parameters:\n    log_probs (list of float): List of log probabilities\n\n    Returns:\n    str: Formatted output string\n    \"\"\"\n    return ' '.join(f'{x:f}' for x in log_probs)\n\n# Example usage:\ninput_string = \"\"\"ACGATACAA\n0.129 0.287 0.423 0.476 0.641 0.742 0.783\"\"\"\n\ns, gc_contents = parse_input(input_string)\nlog_probs = calculate_log_probabilities(s, gc_contents)\noutput = format_output(log_probs)\nprint(output)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-3",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "28.5 Explanation of the Code",
    "text": "28.5 Explanation of the Code\n\nFunction calculate_log_probabilities:\n\nThis is the main function that calculates the log probabilities for each GC-content in the input list.\nIt uses a list comprehension to call calculate_log_prob_for_gc_content for each GC-content value.\n\nFunction calculate_log_prob_for_gc_content:\n\nThis function calculates the log probability for a specific GC-content value.\nIt computes the probabilities of G/C and A/T based on the GC-content.\nIt sums the logarithms of the probabilities of each nucleotide in the DNA string s.\n\nFunction parse_input:\n\nThis function parses the input string to extract the DNA string and the list of GC-content values.\nIt splits the input string into lines and processes them accordingly.\n\nFunction format_output:\n\nThis function formats the list of log probabilities to the required output format.\nIt uses a list comprehension to format each log probability to three decimal places."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-28",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-28",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "29.1 Sample Dataset",
    "text": "29.1 Sample Dataset\n2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-29",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-29",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "29.2 Sample Output",
    "text": "29.2 Sample Output\n8\n-1 -2\n-1 2\n1 -2\n1 2\n-2 -1\n-2 1\n2 -1\n2 1"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-26",
    "href": "posts/md/Rosalind_stronghold.html#solution-26",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "29.3 solution",
    "text": "29.3 solution\nTo generate the correct total number of signed permutations and their combinations, we need to combine each permutation of the numbers with all possible sign variations correctly. Here’s the revised approach:\n\nGenerate Permutations: First, generate all possible permutations of the integers from 1 to \\(n\\).\nGenerate Signed Permutations: For each permutation, generate all possible signed versions of that permutation. Each integer in the permutation can be either positive or negative.\nCombine and Output: Combine all the signed permutations and output the total count followed by all the signed permutations.\n\nHere’s the corrected implementation:\nfrom itertools import permutations, product\n\ndef signed_permutations(n):\n    # Generate all permutations of length n\n    perms = list(permutations(range(1, n + 1)))\n    \n    # Generate all possible signed permutations\n    signed_perms = []\n    for perm in perms:\n        for signs in product([-1, 1], repeat=n):\n            signed_perm = [a * sign for a, sign in zip(perm, signs)]\n            signed_perms.append(signed_perm)\n    \n    return signed_perms\n\n# Read the input\nn = 2  # Example input, you can change this value\n\n# Get all signed permutations\nresult = signed_permutations(n)\n\n# Output the total number of signed permutations\nprint(len(result))\n\n# Output each signed permutation\nfor perm in result:\n    print(' '.join(map(str, perm)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-15",
    "href": "posts/md/Rosalind_stronghold.html#explanation-15",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "29.4 Explanation",
    "text": "29.4 Explanation\n\nGenerating Permutations:\n\nUse itertools.permutations to generate all permutations of the integers from 1 to \\(n\\).\n\nGenerating Signed Permutations:\n\nFor each permutation, we use itertools.product to generate all possible sign combinations (each element can be either -1 or 1).\nFor each sign combination, multiply each element of the permutation with the respective sign to create a signed permutation.\n\nCombining and Output:\n\nWe store all the signed permutations in a list.\nFirst, print the total number of signed permutations.\nThen, print each signed permutation.\n\n\nThis should correctly generate all signed permutations and output them in the desired format. The total number of signed permutations for a given \\(n\\) is \\(n! \\times 2^n\\), ensuring all combinations of signs and orderings are included."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-29",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-29",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "30.1 Sample Dataset",
    "text": "30.1 Sample Dataset\n&gt;Rosalind_14\nACGTACGTGACG\n&gt;Rosalind_18\nGTA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-30",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-30",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "30.2 Sample Output",
    "text": "30.2 Sample Output\n3 4 5"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-27",
    "href": "posts/md/Rosalind_stronghold.html#solution-27",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "30.3 Solution",
    "text": "30.3 Solution\nThank you for the clarification. Let’s refine the approach to ensure it correctly finds the indices of the subsequence.\nHere’s the corrected version of the code without any hardcoding:\ndef parse_fasta(fasta_str):\n    sequences = []\n    current_seq = []\n    for line in fasta_str.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if current_seq:\n                sequences.append(''.join(current_seq))\n                current_seq = []\n        else:\n            current_seq.append(line)\n    if current_seq:\n        sequences.append(''.join(current_seq))\n    return sequences\n\ndef find_spliced_motif(s, t):\n    indices = []\n    t_index = 0\n    \n    for s_index in range(len(s)):\n        if t_index &lt; len(t) and s[s_index] == t[t_index]:\n            indices.append(s_index + 1)\n            t_index += 1\n            if t_index == len(t):\n                break\n    \n    return indices\n\n# Sample input\nfasta_input = \"\"\"&gt;Rosalind_14\nACGTACGTGACG\n&gt;Rosalind_18\nGTA\"\"\"\n\nsequences = parse_fasta(fasta_input)\ns = sequences[0]\nt = sequences[1]\n\nresult = find_spliced_motif(s, t)\nprint(' '.join(map(str, result)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-16",
    "href": "posts/md/Rosalind_stronghold.html#explanation-16",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "30.4 Explanation",
    "text": "30.4 Explanation\n\nparse_fasta Function:\n\nThis function parses the input FASTA string into sequences. It collects lines of sequences until it encounters a new sequence identifier (a line starting with &gt;). This function does not hardcode sequence identifiers and can handle any number of sequences.\n\nfind_spliced_motif Function:\n\nThis function searches for the subsequence t within the sequence s and returns the 1-based indices of s where the characters of t appear in order.\nIt uses a single loop over s to find matches for the characters in t.\nIt stops searching as soon as it finds all characters of t within s.\n\n\nThe sample dataset should now correctly produce the output 3 8 10."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-30",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-30",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "31.1 Sample Dataset",
    "text": "31.1 Sample Dataset\n&gt;Rosalind_0209\nGCAACGCACAACGAAAACCCTTAGGGACTGGATTATTTCGTGATCGTTGTAGTTATTGGA\nAGTACGGGCATCAACCCAGTT\n&gt;Rosalind_2200\nTTATCTGACAAAGAAAGCCGTCAACGGCTGGATAATTTCGCGATCGTGCTGGTTACTGGC\nGGTACGAGTGTTCCTTTGGGT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-31",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-31",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "31.2 Sample Output",
    "text": "31.2 Sample Output\n1.21428571429"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-28",
    "href": "posts/md/Rosalind_stronghold.html#solution-28",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "31.3 Solution",
    "text": "31.3 Solution\nTo solve the problem of calculating the transition/transversion ratio, we need to perform the following steps:\n\nParse the input data to extract the two DNA sequences.\nDefine transition and transversion mutations:\n\nTransitions are interchanges of two purines (A &lt;-&gt; G) or two pyrimidines (C &lt;-&gt; T).\nTransversions are interchanges between a purine and a pyrimidine (A &lt;-&gt; C, A &lt;-&gt; T, G &lt;-&gt; C, G &lt;-&gt; T).\n\nCount the number of transitions and transversions between the two sequences.\nCalculate the ratio of transitions to transversions."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#step-by-step-implementation",
    "href": "posts/md/Rosalind_stronghold.html#step-by-step-implementation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "31.4 Step-by-step Implementation",
    "text": "31.4 Step-by-step Implementation\n\nParsing the Input:\n\nRead the input in FASTA format.\nExtract the sequences associated with each identifier.\n\nCounting Mutations:\n\nCompare each base of the two sequences.\nIncrement the transition count if a transition mutation is found.\nIncrement the transversion count if a transversion mutation is found.\n\nCalculating the Ratio:\n\nCompute the ratio of the number of transitions to the number of transversions.\n\n\nHere’s the Python implementation of the solution:\ndef parse_fasta(fasta_str):\n    sequences = []\n    current_seq = []\n    for line in fasta_str.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if current_seq:\n                sequences.append(''.join(current_seq))\n                current_seq = []\n        else:\n            current_seq.append(line)\n    if current_seq:\n        sequences.append(''.join(current_seq))\n    return sequences\n\ndef count_transitions_transversions(s1, s2):\n    transitions = 0\n    transversions = 0\n    \n    transitions_set = {('A', 'G'), ('G', 'A'), ('C', 'T'), ('T', 'C')}\n    \n    for base1, base2 in zip(s1, s2):\n        if base1 != base2:\n            if (base1, base2) in transitions_set:\n                transitions += 1\n            else:\n                transversions += 1\n                \n    return transitions, transversions\n\ndef transition_transversion_ratio(s1, s2):\n    transitions, transversions = count_transitions_transversions(s1, s2)\n    if transversions == 0:\n        return float('inf')  # or some other large number or special case\n    return transitions / transversions\n\n# Sample input\nfasta_input = \"\"\"&gt;Rosalind_0209\nGCAACGCACAACGAAAACCCTTAGGGACTGGATTATTTCGTGATCGTTGTAGTTATTGGA\nAGTACGGGCATCAACCCAGTT\n&gt;Rosalind_2200\nTTATCTGACAAAGAAAGCCGTCAACGGCTGGATAATTTCGCGATCGTGCTGGTTACTGGC\nGGTACGAGTGTTCCTTTGGGT\"\"\"\n\nsequences = parse_fasta(fasta_input)\ns1 = sequences[0]\ns2 = sequences[1]\n\nresult = transition_transversion_ratio(s1, s2)\nprint(f\"{result:.11f})"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-17",
    "href": "posts/md/Rosalind_stronghold.html#explanation-17",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "31.5 Explanation",
    "text": "31.5 Explanation\n\nparse_fasta Function:\n\nThis function parses the FASTA formatted input to extract sequences. It handles multiple sequences and collects lines until a new identifier is found.\n\ncount_transitions_transversions Function:\n\nThis function takes two DNA sequences and counts the transitions and transversions by comparing each nucleotide in the two sequences.\nIt uses a set of tuples to check if a given mutation is a transition.\n\ntransition_transversion_ratio Function:\n\nThis function calculates the ratio of transitions to transversions. If there are no transversions, it handles this by returning infinity or some other large number."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-31",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-31",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "32.1 Sample Dataset",
    "text": "32.1 Sample Dataset\n10\n1 2\n2 8\n4 10\n5 9\n6 10\n7 9"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-32",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-32",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "32.2 Sample Output",
    "text": "32.2 Sample Output\n3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-29",
    "href": "posts/md/Rosalind_stronghold.html#solution-29",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "32.3 Solution",
    "text": "32.3 Solution\nTo solve the problem of determining the minimum number of edges needed to make a given graph a tree, we can follow these steps:\n\nUnderstand the Input and Output:\n\nThe input consists of an integer n, which is the number of nodes, followed by a list of edges given as pairs of integers.\nThe output should be the minimum number of edges required to make the graph a tree.\n\nConcepts:\n\nA tree is a connected graph with no cycles.\nFor a graph with n nodes to be a tree, it must have exactly n-1 edges.\nIf the graph has fewer than n-1 edges, it must be connected. If it is not connected, it will have multiple connected components.\n\nAlgorithm:\n\nUse a graph traversal algorithm (e.g., Depth-First Search (DFS) or Breadth-First Search (BFS)) to find all the connected components of the graph.\nCount the number of connected components, c.\nThe minimum number of edges needed to connect all components to form a single connected component (tree) is c-1.\n\n\nHere is the implementation in Python:\ndef find_connected_components(n, edges):\n    from collections import defaultdict, deque\n\n    def bfs(start):\n        queue = deque([start])\n        visited.add(start)\n        while queue:\n            node = queue.popleft()\n            for neighbor in graph[node]:\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    queue.append(neighbor)\n\n    graph = defaultdict(list)\n    for u, v in edges:\n        graph[u].append(v)\n        graph[v].append(u)\n\n    visited = set()\n    num_components = 0\n\n    for node in range(1, n + 1):\n        if node not in visited:\n            bfs(node)\n            num_components += 1\n\n    return num_components\n\ndef min_edges_to_tree(n, edges):\n    num_components = find_connected_components(n, edges)\n    return num_components - 1\n\ndef parse_input(input_text):\n    lines = input_text.strip().split('\\n')\n    n = int(lines[0])\n    edges = [tuple(map(int, line.split())) for line in lines[1:]]\n    return n, edges\n\n# Sample input\ninput_text = \"\"\"\n10\n1 2\n2 8\n4 10\n5 9\n6 10\n7 9\n\"\"\"\n\nn, edges = parse_input(input_text)\nresult = min_edges_to_tree(n, edges)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-18",
    "href": "posts/md/Rosalind_stronghold.html#explanation-18",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "32.4 Explanation",
    "text": "32.4 Explanation\n\nfind_connected_components Function:\n\nThis function remains the same, using BFS to find the number of connected components in the graph.\n\nmin_edges_to_tree Function:\n\nThis function remains the same, calculating the minimum number of edges required to connect all components into a single tree.\n\nparse_input Function:\n\nThis function takes the input as a string, splits it into lines, and processes the first line to get the number of nodes n.\nThe remaining lines are processed to extract the edges as tuples of integers.\n\nSample Input and Running the Code:\n\nThe sample input is provided as a multi-line string.\nThe parse_input function parses this string to extract n and the list of edges.\nThe min_edges_to_tree function calculates the result and prints it.\n\n\nThe expected output for the provided sample input is 3, which is the minimum number of edges required to make the graph a tree."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-32",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-32",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "33.1 Sample Dataset",
    "text": "33.1 Sample Dataset\n&gt;Rosalind_57\nAUAU"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-33",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-33",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "33.2 Sample Output",
    "text": "33.2 Sample Output\n2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-30",
    "href": "posts/md/Rosalind_stronghold.html#solution-30",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "33.3 Solution",
    "text": "33.3 Solution\ndef solve(rna):\n    \"\"\"\n    Given an RNA string consisting of {A, U, C, G},\n    calculates the number of non-overlapping perfect matchings.\n\n    Parameters:\n    rna (str): The RNA string.\n\n    Returns:\n    int: The number of non-overlapping perfect matchings modulo 1,000,000.\n    \"\"\"\n    return count_non_crossing_matchings(rna) % 1000000\n\ndef count_non_crossing_matchings(rna):\n    \"\"\"\n    Helper function that recursively calculates the number of non-crossing perfect matchings\n    of base pairs in the RNA string.\n\n    Parameters:\n    rna (str): The RNA string.\n    \n    Returns:\n    int: The number of non-crossing perfect matchings modulo 1,000,000.\n    \"\"\"\n    # Define complementary nucleotides\n    mapping = {\n        \"A\": \"U\",\n        \"U\": \"A\",\n        \"G\": \"C\",\n        \"C\": \"G\"\n    }\n    \n    n = len(rna)\n    \n    # If the length of the RNA string is odd, return 0\n    if n % 2 != 0:\n        return 0\n    \n    # Memoization dictionary\n    dp = {}\n    \n    def helper(lo, hi):\n        \"\"\"\n        Recursive helper function that computes the number of non-crossing perfect matchings\n        between indices lo and hi in the RNA string.\n\n        Parameters:\n        lo (int): Start index of the substring.\n        hi (int): End index of the substring.\n\n        Returns:\n        int: Number of non-crossing perfect matchings between indices lo and hi.\n        \"\"\"\n        # Base cases\n        if lo &gt;= hi:\n            return 1\n        if (lo, hi) in dp:\n            return dp[(lo, hi)]\n        \n        curr = rna[lo]\n        target = mapping[curr]\n        acc = 0\n        \n        # Iterate through the possible pairs\n        for i in range(lo + 1, hi + 1, 2):\n            if rna[i] == target:\n                left = helper(lo + 1, i - 1)\n                right = helper(i + 1, hi)\n                acc += (left * right) % 1000000\n        \n        dp[(lo, hi)] = acc % 1000000\n        return dp[(lo, hi)]\n    \n    # Call the helper function starting from index 0 to n-1\n    return helper(0, n - 1)\n\n# Parsing the input\ndef parse_fasta(fasta_str):\n    sequences = {}\n    current_label = None\n    \n    for line in fasta_str.strip().split(\"\\n\"):\n        if line.startswith(\"&gt;\"):\n            current_label = line[1:].strip()\n            sequences[current_label] = \"\"\n        else:\n            sequences[current_label] += line.strip()\n    \n    return sequences\n\n# Sample Input in FASTA format\nfasta_input = \"\"\"\n&gt;Rosalind_9378\nAUAU\n\"\"\"\n\n# Parse the input to get the RNA string\nsequences = parse_fasta(fasta_input)\nrna_string = list(sequences.values())[0]\n\n# Output the result\nprint(solve(rna_string))  # Output should be 2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-19",
    "href": "posts/md/Rosalind_stronghold.html#explanation-19",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "33.4 Explanation",
    "text": "33.4 Explanation\n\nsolve(rna) function:\n\nThis function is the entry point that computes and returns the number of non-overlapping perfect matchings of the RNA string modulo 1,000,000.\nIt calls count_non_crossing_matchings(rna) and returns its result modulo 1,000,000.\n\ncount_non_crossing_matchings(rna) function:\n\nThis is the core function that recursively computes the number of non-crossing perfect matchings.\nIt uses a helper function helper(lo, hi) which performs the recursive computation.\nThe function checks for edge cases such as odd length of RNA string and uses memoization (dp dictionary) to store already computed results to avoid redundant computations.\nIt iterates through possible pairs of nucleotides and calculates the number of matchings recursively using the defined base cases and recurrence relations.\n\nparse_fasta(fasta_str) function:\n\nThis function parses the given FASTA formatted input string and extracts the RNA sequence from it.\nIt returns a dictionary where the key is the label (e.g., Rosalind_9378) and the value is the RNA sequence.\n\nUsage:\n\nThe sample input in FASTA format is parsed to obtain the RNA sequence.\nThe solve function is called with the RNA sequence as input, and the result is printed out.\n\n\nThis approach efficiently computes the desired number of non-crossing perfect matchings using recursion with memoization, ensuring that the computation remains feasible even for longer RNA sequences up to 300 base pairs."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-33",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-33",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "34.1 Sample Dataset",
    "text": "34.1 Sample Dataset\n&gt;Rosalind_52\nTCATC\n&gt;Rosalind_44\nTTCAT\n&gt;Rosalind_68\nTCATC\n&gt;Rosalind_28\nTGAAA\n&gt;Rosalind_95\nGAGGA\n&gt;Rosalind_66\nTTTCA\n&gt;Rosalind_33\nATCAA\n&gt;Rosalind_21\nTTGAT\n&gt;Rosalind_18\nTTTCC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-34",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-34",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "34.2 Sample Output",
    "text": "34.2 Sample Output\nTTCAT-&gt;TTGAT\nGAGGA-&gt;GATGA\nTTTCC-&gt;TTTCA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-31",
    "href": "posts/md/Rosalind_stronghold.html#solution-31",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "34.3 Solution",
    "text": "34.3 Solution\n\nParse the FASTA format input to extract reads.\nIdentify correct reads:\n\nReads that appear at least twice or appear once but their reverse complement also appears at least once.\n\nIdentify incorrect reads:\n\nReads that appear exactly once and do not have their reverse complement in the list of correct reads.\n\nCorrect the errors:\n\nFor each incorrect read, find the correct read that has a Hamming distance of 1 with the incorrect read or its reverse complement.\n\nOutput the corrections."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#step-by-step-implementation-1",
    "href": "posts/md/Rosalind_stronghold.html#step-by-step-implementation-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "34.4 Step-by-step Implementation",
    "text": "34.4 Step-by-step Implementation\nHere’s the revised Python code to achieve the above steps:\ndef parse_fasta(fasta_str):\n    \"\"\"\n    Parses a FASTA formatted string.\n    \"\"\"\n    sequences = {}\n    current_label = None\n    for line in fasta_str.strip().split(\"\\n\"):\n        if line.startswith(\"&gt;\"):\n            current_label = line[1:].strip()\n            sequences[current_label] = \"\"\n        else:\n            sequences[current_label] += line.strip()\n    return sequences\n\ndef reverse_complement(dna):\n    \"\"\"\n    Returns the reverse complement of a DNA string.\n    \"\"\"\n    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    return \"\".join(complement[base] for base in reversed(dna))\n\ndef hamming_distance(s1, s2):\n    \"\"\"\n    Calculates the Hamming distance between two strings.\n    \"\"\"\n    return sum(1 for a, b in zip(s1, s2) if a != b)\n\ndef find_correct_reads(reads):\n    \"\"\"\n    Identifies the correct reads in the dataset.\n    \"\"\"\n    from collections import defaultdict\n    \n    read_counts = defaultdict(int)\n    for read in reads:\n        read_counts[read] += 1\n        read_counts[reverse_complement(read)] += 1\n    \n    correct_reads = {read for read, count in read_counts.items() if count &gt; 1}\n    return correct_reads\n\ndef find_corrections(reads, correct_reads):\n    \"\"\"\n    Identifies corrections needed for the erroneous reads.\n    \"\"\"\n    corrections = []\n    for read in reads:\n        if read not in correct_reads:\n            for correct_read in correct_reads:\n                if hamming_distance(read, correct_read) == 1:\n                    corrections.append(f\"{read}-&gt;{correct_read}\")\n                    break\n                elif hamming_distance(read, reverse_complement(correct_read)) == 1:\n                    corrections.append(f\"{read}-&gt;{reverse_complement(correct_read)}\")\n                    break\n    return corrections\n\n# Sample Input in FASTA format\nfasta_input = \"\"\"\n&gt;Rosalind_52\nTCATC\n&gt;Rosalind_44\nTTCAT\n&gt;Rosalind_68\nTCATC\n&gt;Rosalind_28\nTGAAA\n&gt;Rosalind_95\nGAGGA\n&gt;Rosalind_66\nTTTCA\n&gt;Rosalind_33\nATCAA\n&gt;Rosalind_21\nTTGAT\n&gt;Rosalind_18\nTTTCC\n\"\"\"\n\n# Parsing the input\nsequences = parse_fasta(fasta_input)\nreads = list(sequences.values())\n\n# Find correct reads\ncorrect_reads = find_correct_reads(reads)\n\n# Find necessary corrections\ncorrections = find_corrections(reads, correct_reads)\n\n# Output the corrections\nfor correction in corrections:\n    print(correction)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-20",
    "href": "posts/md/Rosalind_stronghold.html#explanation-20",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "34.5 Explanation",
    "text": "34.5 Explanation\n\nparse_fasta(fasta_str): Parses the input FASTA formatted string to extract the reads.\nreverse_complement(dna): Returns the reverse complement of a given DNA string.\nhamming_distance(s1, s2): Computes the Hamming distance between two strings.\nfind_correct_reads(reads): Identifies reads that are correct (appear at least twice considering both original and reverse complement).\nfind_corrections(reads, correct_reads): Identifies the necessary corrections for erroneous reads by checking each read against the set of correct reads and their reverse complements."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-34",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-34",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "35.1 Sample Dataset",
    "text": "35.1 Sample Dataset\n4"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-35",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-35",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "35.2 Sample Output",
    "text": "35.2 Sample Output\n2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-32",
    "href": "posts/md/Rosalind_stronghold.html#solution-32",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "35.3 Solution",
    "text": "35.3 Solution\nTo solve the problem of finding the number of internal nodes in an unrooted binary tree given \\(n\\) leaves, let’s delve into some tree properties and the characteristics of unrooted binary trees."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#key-concepts-and-approach",
    "href": "posts/md/Rosalind_stronghold.html#key-concepts-and-approach",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "35.4 Key Concepts and Approach",
    "text": "35.4 Key Concepts and Approach\n\nTree Properties:\n\nAn unrooted binary tree is a tree where every internal node has exactly three connections (degree 3), and each leaf node has one connection (degree 1).\n\nLeaves and Internal Nodes Relationship:\n\nIn any tree, if we let \\(L\\) be the number of leaves and \\(I\\) be the number of internal nodes, for an unrooted binary tree, there is a specific relationship:\nFor every new leaf added to maintain the tree as binary, you essentially add a new internal node to accommodate the structure.\n\nMathematical Relationship:\n\nIt is known that for an unrooted binary tree with \\(n\\) leaves, the number of internal nodes \\(I\\) is given by: [ I = n - 2 ]\nThis is derived from the fact that the total number of nodes in an unrooted binary tree with \\(n\\) leaves is \\(2n - 2\\). Out of these, \\(n\\) are leaves, and the remaining \\(n - 2\\) are internal nodes."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation-4",
    "href": "posts/md/Rosalind_stronghold.html#implementation-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "35.5 Implementation",
    "text": "35.5 Implementation\nGiven this understanding, the implementation to find the number of internal nodes in an unrooted binary tree with \\(n\\) leaves is straightforward. Here’s the Python code to accomplish this:\ndef count_internal_nodes(n):\n    return n - 2\n\n# Sample Input\nn = 4\nprint(count_internal_nodes(n))  # Output should be 2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-35",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-35",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "36.1 Sample Dataset",
    "text": "36.1 Sample Dataset\n&gt;Rosalind_6431\nCTTCGAAAGTTTGGGCCGAGTCTTACAGTCGGTCTTGAAGCAAAGTAACGAACTCCACGG\nCCCTGACTACCGAACCAGTTGTGAGTACTCAACTGGGTGAGAGTGCAGTCCCTATTGAGT\nTTCCGAGACTCACCGGGATTTTCGATCCAGCCTCAGTCCAGTCTTGTGGCCAACTCACCA\nAATGACGTTGGAATATCCCTGTCTAGCTCACGCAGTACTTAGTAAGAGGTCGCTGCAGCG\nGGGCAAGGAGATCGGAAAATGTGCTCTATATGCGACTAAAGCTCCTAACTTACACGTAGA\nCTTGCCCGTGTTAAAAACTCGGCTCACATGCTGTCTGCGGCTGGCTGTATACAGTATCTA\nCCTAATACCCTTCAGTTCGCCGCACAAAAGCTGGGAGTTACCGCGGAAATCACAG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-36",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-36",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "36.2 Sample Output",
    "text": "36.2 Sample Output\n4 1 4 3 0 1 1 5 1 3 1 2 2 1 2 0 1 1 3 1 2 1 3 1 1 1 1 2 2 5 1 3 0 2 2 1 1 1 1 3 1 0 0 1 5 5 1 5 0 2 0 2 1 2 1 1 1 2 0 1 0 0 1 1 3 2 1 0 3 2 3 0 0 2 0 8 0 0 1 0 2 1 3 0 0 0 1 4 3 2 1 1 3 1 2 1 3 1 2 1 2 1 1 1 2 3 2 1 1 0 1 1 3 2 1 2 6 2 1 1 1 2 3 3 3 2 3 0 3 2 1 1 0 0 1 4 3 0 1 5 0 2 0 1 2 1 3 0 1 2 2 1 1 0 3 0 0 4 5 0 3 0 2 1 1 3 0 3 2 2 1 1 0 2 1 0 2 2 1 2 0 2 2 5 2 2 1 1 2 1 2 2 2 2 1 1 3 4 0 2 1 1 0 1 2 2 1 1 1 5 2 0 3 2 1 1 2 2 3 0 3 0 1 3 1 2 3 0 2 1 2 2 1 2 3 0 1 2 3 1 1 3 1 0 1 1 3 0 2 1 2 2 0 2 1 1"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-33",
    "href": "posts/md/Rosalind_stronghold.html#solution-33",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "36.3 Solution",
    "text": "36.3 Solution\nTo solve the problem of finding the 4-mer composition of a given DNA string, we need to follow these steps:\n\nParse the input FASTA format to extract the DNA sequence.\nGenerate all possible 4-mers from the given DNA alphabet.\nCount the occurrences of each 4-mer in the DNA sequence.\nOutput the counts in lexicographical order of the 4-mers."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-explanation",
    "href": "posts/md/Rosalind_stronghold.html#detailed-explanation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "36.4 Detailed Explanation",
    "text": "36.4 Detailed Explanation\n\nParsing the FASTA format: The input DNA sequence is provided in FASTA format. We need to extract the actual DNA string from this format.\nGenerating all possible 4-mers: A 4-mer is a sequence of 4 nucleotides. Since the DNA alphabet consists of {A, C, G, T}, there are $4^4 = 256) possible 4-mers. We can generate these 4-mers lexicographically (sorted order).\nCounting occurrences of each 4-mer: We will slide a window of length 4 acros the DNA sequence and count how many times each 4-mer appears.\nOutput the results: We output the counts of each 4-mer in the lexicographical order."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#python-implementation-1",
    "href": "posts/md/Rosalind_stronghold.html#python-implementation-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "36.5 Python Implementation",
    "text": "36.5 Python Implementation\nHere is the complete Python code that implements the above steps:\nfrom itertools import product\n\ndef parse_fasta(fasta_str):\n    sequences = {}\n    current_label = None\n    \n    for line in fasta_str.strip().split(\"\\n\"):\n        if line.startswith(\"&gt;\"):\n            current_label = line[1:].strip()\n            sequences[current_label] = \"\"\n        else:\n            sequences[current_label] += line.strip()\n    \n    return sequences\n\ndef generate_kmers(k, alphabet='ACGT'):\n    return [''.join(p) for p in product(alphabet, repeat=k)]\n\ndef count_kmers(dna, k):\n    kmer_counts = {}\n    for i in range(len(dna) - k + 1):\n        kmer = dna[i:i+k]\n        if kmer in kmer_counts:\n            kmer_counts[kmer] += 1\n        else:\n            kmer_counts[kmer] = 1\n    return kmer_counts\n\ndef kmer_composition(dna, k=4):\n    kmers = generate_kmers(k)\n    kmer_counts = count_kmers(dna, k)\n    \n    return [kmer_counts.get(kmer, 0) for kmer in kmers]\n\n# Sample Input in FASTA format\nfasta_input = \"\"\"\n&gt;Rosalind_6431\nCTTCGAAAGTTTGGGCCGAGTCTTACAGTCGGTCTTGAAGCAAAGTAACGAACTCCACGG\nCCCTGACTACCGAACCAGTTGTGAGTACTCAACTGGGTGAGAGTGCAGTCCCTATTGAGT\nTTCCGAGACTCACCGGGATTTTCGATCCAGCCTCAGTCCAGTCTTGTGGCCAACTCACCA\nAATGACGTTGGAATATCCCTGTCTAGCTCACGCAGTACTTAGTAAGAGGTCGCTGCAGCG\nGGGCAAGGAGATCGGAAAATGTGCTCTATATGCGACTAAAGCTCCTAACTTACACGTAGA\nCTTGCCCGTGTTAAAAACTCGGCTCACATGCTGTCTGCGGCTGGCTGTATACAGTATCTA\nCCTAATACCCTTCAGTTCGCCGCACAAAAGCTGGGAGTTACCGCGGAAATCACAG\n\"\"\"\n\n# Parsing the input\nsequences = parse_fasta(fasta_input)\n\n# There should be only one sequence in the given input\ndna_string = list(sequences.values())[0]\n\n# Getting the 4-mer composition\ncomposition = kmer_composition(dna_string, k=4)\n\n# Printing the result\nprint(\" \".join(map(str, composition)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-4",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "36.6 Explanation of the Code",
    "text": "36.6 Explanation of the Code\n\nparse_fasta(fasta_str): This function parses the input FASTA format string and returns a dictionary of sequences.\ngenerate_kmers(k, alphabet=‘ACGT’): This function generates all possible k-mers of length k using the given alphabet.\ncount_kmers(dna, k): This function counts the occurrences of each k-mer in the DNA sequence.\nkmer_composition(dna, k=4): This function calculates the k-mer composition by using the previous two functions. It returns a list of counts of each k-mer in lexicographical order.\nThe main block: Parses the input, extracts the DNA sequence, computes the 4-mer composition, and prints the results."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-36",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-36",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "37.1 Sample Dataset",
    "text": "37.1 Sample Dataset\n&gt;Rosalind_87\nCAGCATGGTATCACAGCAGAG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-37",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-37",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "37.2 Sample Output",
    "text": "37.2 Sample Output\n0 0 0 1 2 0 0 0 0 0 0 1 2 1 2 3 4 5 3 0 0"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-34",
    "href": "posts/md/Rosalind_stronghold.html#solution-34",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "37.3 Solution",
    "text": "37.3 Solution\nTo solve the problem of computing the failure array of a given DNA string in FASTA format, we need to follow these steps:\n\nParse the input FASTA format to extract the DNA sequence.\nCompute the failure array using the Knuth-Morris-Prat (KMP) algorithm.\nOutput the failure array."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-explanation-1",
    "href": "posts/md/Rosalind_stronghold.html#detailed-explanation-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "37.4 Detailed Explanation",
    "text": "37.4 Detailed Explanation\n\nParsing the FASTA format: We need to extract the actual DNA string from the provided FASTA format.\nComputing the Failure Array: The failure array is computed using the KMP preprocessing algorithm. The failure array P at position k represents the length of the longest prefix of the substring s[1:k] that is also a suffix of this substring."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#python-implementation-2",
    "href": "posts/md/Rosalind_stronghold.html#python-implementation-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "37.5 Python Implementation",
    "text": "37.5 Python Implementation\nHere is the complete Python code that implements the above steps:\ndef parse_fasta(fasta_str):\n    \"\"\"\n    Parses a FASTA format string and returns the DNA sequence.\n    \"\"\"\n    sequences = []\n    for line in fasta_str.strip().split(\"\\n\"):\n        if not line.startswith(\"&gt;\"):\n            sequences.append(line.strip())\n    return \"\".join(sequences)\n\ndef compute_failure_array(s):\n    \"\"\"\n    Computes the failure array for a given string s using the KMP algorithm.\n    \"\"\"\n    n = len(s)\n    P = [0] * n\n    k = 0\n    \n    for i in range(1, n):\n        while k &gt; 0 and s[k] != s[i]:\n            k = P[k - 1]\n        \n        if s[k] == s[i]:\n            k += 1\n        \n        P[i] = k\n    \n    return P\n\n# Sample Input in FASTA format\nfasta_input = \"\"\"\n&gt;Rosalind_87\nCAGCATGGTATCACAGCAGAG\n\"\"\"\n\n# Parsing the input\ndna_string = parse_fasta(fasta_input)\n\n# Computing the failure array\nfailure_array = compute_failure_array(dna_string)\n\n# Printing the result\nprint(\" \".join(map(str, failure_array)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-5",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-5",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "37.6 Explanation of the Code",
    "text": "37.6 Explanation of the Code\n\nparse_fasta(fasta_str): This function parses the input FASTA format string and returns the concatenated DNA sequence.\ncompute_failure_array(s): This function computes the failure array for the string s using the KMP algorithm.\n\nInitialize an array P of length n with zeros.\nIterate through the string s from the second character to the end.\nFor each character, update the value of k to the length of the longest prefix which is also a suffix for the substring s[1:i+1].\nStore the value of k in P[i].\n\nThe main block:\n\nParse the input FASTA format string to extract the DNA sequence.\nCompute the failure array for the DNA sequence.\nPrint the failure array as a space-separated string."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-37",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-37",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "38.1 Sample Dataset",
    "text": "38.1 Sample Dataset\n&gt;Rosalind_23\nAACCTTGG\n&gt;Rosalind_64\nACACTGTGA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-38",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-38",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "38.2 Sample Output",
    "text": "38.2 Sample Output\nAACTGG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-35",
    "href": "posts/md/Rosalind_stronghold.html#solution-35",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "38.3 Solution",
    "text": "38.3 Solution\nTo solve the problem of finding the longest common subsequence (LCS) between two DNA strings provided in FASTA format, you can use dynamic programming. Here’s a step-by-step explanation and implementation:"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-6",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-6",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "38.4 Steps to Solve the Problem",
    "text": "38.4 Steps to Solve the Problem\n\nParse the FASTA Input: Extract the DNA sequences from the FASTA format input.\nCompute the Longest Common Subsequence: Use a dynamic programming approach to find the LCS. The idea is to use a 2D table where dp[i][j] represents the length of the longest common subsequence of the substrings s[0:i] and t[0:j].\nReconstruct the LCS: Once the table is filled, backtrack to reconstruct the longest common subsequence from the table."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-explanation-2",
    "href": "posts/md/Rosalind_stronghold.html#detailed-explanation-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "38.5 Detailed Explanation",
    "text": "38.5 Detailed Explanation\n\nDynamic Programming Table Initialization:\n\nCreate a 2D list dp where dp[i][j] contains the length of the LCS of substrings s[0:i] and t[0:j].\nInitialize the first row and first column of the table to 0 because an LCS with an empty string is 0.\n\nFilling the DP Table:\n\nIterate through each character of s and t. If the characters match, update dp[i][j] based on dp[i-1][j-1] + 1.\nIf they do not match, set dp[i][j] to the maximum of dp[i-1][j] and dp[i][j-1].\n\nBacktracking to Find LCS:\n\nStart from dp[len(s)][len(t)] and trace back to build the LCS string by comparing characters and using the DP table to decide whether to include a character or move in a specific direction."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#python-implementation-3",
    "href": "posts/md/Rosalind_stronghold.html#python-implementation-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "38.6 Python Implementation",
    "text": "38.6 Python Implementation\nHere’s the Python code to achieve the above steps:\ndef parse_fasta(fasta_str):\n    \"\"\"\n    Parses a FASTA format string and returns the DNA sequences.\n    \"\"\"\n    sequences = []\n    current_sequence = []\n    for line in fasta_str.strip().split(\"\\n\"):\n        if line.startswith(\"&gt;\"):\n            if current_sequence:\n                sequences.append(\"\".join(current_sequence))\n                current_sequence = []\n        else:\n            current_sequence.append(line.strip())\n    if current_sequence:\n        sequences.append(\"\".join(current_sequence))\n    return sequences\n\ndef longest_common_subsequence(s, t):\n    \"\"\"\n    Finds the longest common subsequence between strings s and t.\n    \"\"\"\n    m, n = len(s), len(t)\n    # Create a 2D table to store lengths of longest common subsequences.\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Fill the table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if s[i - 1] == t[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n    \n    # Backtrack to find the LCS\n    lcs = []\n    i, j = m, n\n    while i &gt; 0 and j &gt; 0:\n        if s[i - 1] == t[j - 1]:\n            lcs.append(s[i - 1])\n            i -= 1\n            j -= 1\n        elif dp[i - 1][j] &gt; dp[i][j - 1]:\n            i -= 1\n        else:\n            j -= 1\n    \n    return ''.join(reversed(lcs))\n\n# Sample Input in FASTA format\nfasta_input = \"\"\"\n&gt;Rosalind_23\nAACCTTGG\n&gt;Rosalind_64\nACACTGTGA\n\"\"\"\n\n# Parsing the input\nsequences = parse_fasta(fasta_input)\ns = sequences[0]\nt = sequences[1]\n\n# Finding the longest common subsequence\nlcs = longest_common_subsequence(s, t)\n\n# Printing the result\nprint(lcs)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-6",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-6",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "38.7 Explanation of the Code",
    "text": "38.7 Explanation of the Code\n\nparse_fasta(fasta_str): This function parses the FASTA format string and returns a list of DNA sequences.\nlongest_common_subsequence(s, t): This function calculates the longest common subsequence using a dynamic programming table and then backtracks to reconstruct the LCS.\nMain Execution:\n\nParse the input FASTA string to get the DNA sequences.\nCompute the LCS using the longest_common_subsequence function.\nPrint the result.\n\n\nThis code will correctly find and output the longest common subsequence of the given DNA sequences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-38",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-38",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "39.1 Sample Dataset",
    "text": "39.1 Sample Dataset\nD N A\n3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-39",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-39",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "39.2 Sample Output",
    "text": "39.2 Sample Output\nD\nDD\nDDD\nDDN\nDDA\nDN\nDND\nDNN\nDNA\nDA\nDAD\nDAN\nDAA\nN\nND\nNDD\nNDN\nNDA\nNN\nNND\nNNN\nNNA\nNA\nNAD\nNAN\nNAA\nA\nAD\nADD\nADN\nADA\nAN\nAND\nANN\nANA\nAA\nAAD\nAAN\nAAA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-36",
    "href": "posts/md/Rosalind_stronghold.html#solution-36",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "39.3 Solution",
    "text": "39.3 Solution\nTo solve the problem of generating all strings of length up to \\(n\\) formed from an ordered alphabet \\(A\\), and then ordering them lexicographically based on the given alphabet order, we can use a recursive approach or itertools to generate the permutations. Here is a detailed step-by-step explanation and implementation:"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-7",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-7",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "39.4 Steps to Solve the Problem",
    "text": "39.4 Steps to Solve the Problem\n\nInput Parsing:\n\nParse the given ordered alphabet \\(A\\) and the integer \\(n\\).\n\nGenerate All Possible Strings:\n\nUse recursion or itertools to generate all strings of length from 1 to \\(n\\) using the symbols in \\(A\\).\n\nSorting:\n\nSort the generated strings based on the custom order provided by \\(A\\)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-explanation-3",
    "href": "posts/md/Rosalind_stronghold.html#detailed-explanation-3",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "39.5 Detailed Explanation",
    "text": "39.5 Detailed Explanation\n\nGenerating Combinations:\n\nFor each length \\(k\\) from 1 to \\(n\\), generate all possible strings of that length using the symbols in \\(A\\).\n\nCustom Sorting:\n\nUse the order of symbols in \\(A\\) to sort the generated strings lexicographically."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#python-implementation-4",
    "href": "posts/md/Rosalind_stronghold.html#python-implementation-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "39.6 Python Implementation",
    "text": "39.6 Python Implementation\nHere is the Python code to achieve the above steps:\nimport itertools\n\ndef parse_input(input_str):\n    lines = input_str.strip().split(\"\\n\")\n    alphabet = lines[0].split()\n    n = int(lines[1])\n    return alphabet, n\n\ndef generate_strings(alphabet, n):\n    all_strings = []\n    for length in range(1, n + 1):\n        for combo in itertools.product(alphabet, repeat=length):\n            all_strings.append(\"\".join(combo))\n    return all_strings\n\ndef custom_sort(strings, alphabet):\n    order_map = {char: idx for idx, char in enumerate(alphabet)}\n    return sorted(strings, key=lambda word: [order_map[char] for char in word])\n\n# Sample Input\ninput_data = \"\"\"\nD N A\n3\n\"\"\"\n\n# Parse input\nalphabet, n = parse_input(input_data)\n\n# Generate all strings of length up to n\nall_strings = generate_strings(alphabet, n)\n\n# Sort the strings based on the custom lexicographical order\nsorted_strings = custom_sort(all_strings, alphabet)\n\n# Print the result\nfor s in sorted_strings:\n    print(s)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-7",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-7",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "39.7 Explanation of the Code",
    "text": "39.7 Explanation of the Code\n\nparse_input(input_str): This function parses the input string to extract the alphabet and the integer \\(n\\).\ngenerate_strings(alphabet, n): This function generates all possible strings of lengths from 1 to \\(n\\) using itertools.product.\ncustom_sort(strings, alphabet): This function sorts the generated strings based on the custom order defined by the alphabet. It uses a mapping of characters to their indices in the given order for sorting.\nMain Execution:\n\nParse the input data.\nGenerate all possible strings.\nSort the strings using the custom lexicographical order.\nPrint each string in the sorted list."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-39",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-39",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "40.1 Sample Dataset",
    "text": "40.1 Sample Dataset\n&gt;Rosalind_92\nAUGCUUC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-40",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-40",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "40.2 Sample Output",
    "text": "40.2 Sample Output\n6"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-37",
    "href": "posts/md/Rosalind_stronghold.html#solution-37",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "40.3 Solution",
    "text": "40.3 Solution\n\nCounting Nucleotides: Count occurrences of each nucleotide.\nCalculating Pairings:\n\nThe number of possible AU pairs is determined by the minimum of A and U.\nSimilarly, the number of possible GC pairs is determined by the minimum of G and C.\n\nUsing Factorials: Calculate how many ways to pair these nucleotides.\n\nLet’s correct the implementation:\nfrom math import factorial\n\ndef count_nucleotides(sequence):\n    \"\"\"\n    Count the occurrences of each nucleotide in the RNA sequence.\n    \n    Args:\n    sequence (str): The RNA sequence as a string.\n    \n    Returns:\n    dict: A dictionary with counts for 'A', 'U', 'G', and 'C'.\n    \"\"\"\n    counts = {'A': 0, 'U': 0, 'G': 0, 'C': 0}\n    for nucleotide in sequence:\n        if nucleotide in counts:\n            counts[nucleotide] += 1\n    return counts\n\ndef max_matching_pairs(a, b):\n    \"\"\"\n    Calculate the number of ways to pair 'a' items with 'b' items.\n    \n    Args:\n    a (int): Number of items of type A.\n    b (int): Number of items of type B.\n    \n    Returns:\n    int: The number of maximum matching pairs.\n    \"\"\"\n    return factorial(a) // (factorial(b) * factorial(a - b))\n\ndef maximum_matchings(sequence):\n    \"\"\"\n    Calculate the number of maximum matchings in an RNA sequence.\n    \n    Args:\n    sequence (str): The RNA sequence.\n    \n    Returns:\n    int: The number of maximum matchings.\n    \"\"\"\n    # Count nucleotides\n    counts = count_nucleotides(sequence)\n    \n    # Get counts for each nucleotide\n    A = counts['A']\n    U = counts['U']\n    G = counts['G']\n    C = counts['C']\n    \n    # Calculate the number of possible AU and GC pairings\n    min_au_pairs = min(A, U)\n    min_gc_pairs = min(G, C)\n    \n    # Compute number of ways to form these pairs\n    au_pairings = max_matching_pairs(A, min_au_pairs) * max_matching_pairs(U, min_au_pairs)\n    gc_pairings = max_matching_pairs(G, min_gc_pairs) * max_matching_pairs(C, min_gc_pairs)\n    \n    # Total number of maximum matchings\n    total_matchings = au_pairings * gc_pairings\n    return total_matchings\n\n# Example RNA sequence\nrna_sequence = \"AUGCUUC\"\n\n# Compute and print the number of maximum matchings\nprint(maximum_matchings(rna_sequence))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-changes",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-changes",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "40.4 Explanation of Changes",
    "text": "40.4 Explanation of Changes\n\ncount_nucleotides(sequence): Counts the number of each nucleotide.\nmax_matching_pairs(a, b): Calculates the number of ways to form b pairs from a items using factorials. This function helps in calculating possible pairings for nucleotides.\nmaximum_matchings(sequence): Computes the number of ways to match A with U and G with C, and then multiplies these to get the total number of matchings.\n\n\n\n\n\n\n\nNote\n\n\n\nRosalind 의 서버는 python2.7 로 구현되어 있어 python3 에서 실행한 결과는 옳지 않다고 판단합니다. 따라서 상대적으로 정확도가 떨어지는 python2.7 로 실행하세요."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-40",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-40",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "41.1 Sample Dataset",
    "text": "41.1 Sample Dataset\n&gt;Rosalind_9499\nTTTCCATTTA\n&gt;Rosalind_0942\nGATTCATTTC\n&gt;Rosalind_6568\nTTTCCATTTT\n&gt;Rosalind_1833\nGTTCCATTTA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-41",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-41",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "41.2 Sample Output",
    "text": "41.2 Sample Output\n0.00000 0.40000 0.10000 0.10000\n0.40000 0.00000 0.40000 0.30000\n0.10000 0.40000 0.00000 0.20000\n0.10000 0.30000 0.20000 0.00000"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-38",
    "href": "posts/md/Rosalind_stronghold.html#solution-38",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "41.3 Solution",
    "text": "41.3 Solution\nTo create a distance matrix based on the p-distance for a given set of DNA strings, we will follow these steps:\n\nRead the Input: Parse the FASTA formatted input to extract DNA strings.\nCalculate p-distance: Compute the p-distance between each pair of DNA strings. The p-distance is defined as the proportion of differing symbols between two strings of equal length.\nConstruct the Distance Matrix: Populate the distance matrix with the computed p-distances."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-implementation",
    "href": "posts/md/Rosalind_stronghold.html#detailed-implementation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "41.4 Detailed Implementation",
    "text": "41.4 Detailed Implementation\n\nReading the Input: We’ll parse the input to extract the DNA sequences.\nComputing p-distance: For each pair of sequences, we’ll count the differing positions and divide by the total length of the sequences.\nOutput the Distance Matrix: Format the matrix with each entry having a precision up to five decimal places.\n\nHere’s the Python code to achieve this:\ndef read_fasta(fasta_string):\n    \"\"\"\n    Parses a FASTA formatted string and returns a list of sequences.\n    \"\"\"\n    sequences = []\n    current_sequence = []\n    for line in fasta_string.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if current_sequence:\n                sequences.append(''.join(current_sequence))\n                current_sequence = []\n        else:\n            current_sequence.append(line.strip())\n    if current_sequence:\n        sequences.append(''.join(current_sequence))\n    return sequences\n\ndef p_distance(s1, s2):\n    \"\"\"\n    Computes the p-distance between two DNA sequences of equal length.\n    \"\"\"\n    assert len(s1) == len(s2), \"Sequences must be of equal length.\"\n    differences = sum(1 for a, b in zip(s1, s2) if a != b)\n    return differences / len(s1)\n\ndef distance_matrix(sequences):\n    \"\"\"\n    Computes the distance matrix for a list of sequences based on p-distance.\n    \"\"\"\n    n = len(sequences)\n    matrix = [[0] * n for _ in range(n)]\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                matrix[i][j] = p_distance(sequences[i], sequences[j])\n    return matrix\n\ndef format_matrix(matrix):\n    \"\"\"\n    Formats the matrix for output with each entry having five decimal places.\n    \"\"\"\n    return '\\n'.join(' '.join(f\"{cell:f}\" for cell in row) for row in matrix)\n\n# Sample dataset\nfasta_string = \"\"\"&gt;Rosalind_9499\nTTTCCATTTA\n&gt;Rosalind_0942\nGATTCATTTC\n&gt;Rosalind_6568\nTTTCCATTTT\n&gt;Rosalind_1833\nGTTCCATTTA\"\"\"\n\n# Reading sequences from the sample dataset\nsequences = read_fasta(fasta_string)\n\n# Calculating the distance matrix\ndist_matrix = distance_matrix(sequences)\n\n# Formatting and printing the distance matrix\nformatted_matrix = format_matrix(dist_matrix)\nprint(formatted_matrix)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-21",
    "href": "posts/md/Rosalind_stronghold.html#explanation-21",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "41.5 Explanation",
    "text": "41.5 Explanation\n\nReading FASTA Data: The read_fasta function processes the input FASTA data and extracts the sequences.\nCalculating p-distance: The p_distance function computes the proportion of differing symbols between two sequences.\nConstructing Distance Matrix: The distance_matrix function creates a matrix where each entry $(i, j)) contains the p-distance between sequences $i) and $j).\nFormatting the Output: The format_matrix function ensures that each entry in the matrix is printed with five decimal places for precision."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-41",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-41",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "42.1 Sample Dataset",
    "text": "42.1 Sample Dataset\n1 2 3 4 5 6 7 8 9 10\n3 1 5 2 7 4 9 6 10 8\n\n3 10 8 2 5 4 7 1 6 9\n5 2 3 1 7 4 10 8 6 9\n\n8 6 7 9 4 1 3 10 2 5\n8 2 7 6 9 1 5 3 10 4\n\n3 9 10 4 1 8 6 7 5 2\n2 9 8 5 1 7 3 4 6 10\n\n1 2 3 4 5 6 7 8 9 10\n1 2 3 4 5 6 7 8 9 10"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-42",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-42",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "42.2 Sample Output",
    "text": "42.2 Sample Output\n9 4 5 7 0"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-39",
    "href": "posts/md/Rosalind_stronghold.html#solution-39",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "42.3 Solution",
    "text": "42.3 Solution\nimport collections\n\ndef get_all_permutations(s):\n    for i in range(len(s)):\n        for j in range(i + 2, len(s) + 1):\n            yield s[:i] + s[i:j][::-1] + s[j:]\n\ndef get_reversal_distance(p1, p2):\n    if p1 == p2:\n        return 0\n    \n    target = tuple(p2)\n    fromfirst = {tuple(p1): 0}\n    q = collections.deque([p1])\n    \n    while q:\n        s = q.popleft()\n        c = fromfirst[s]\n        \n        for j in get_all_permutations(s):\n            if j == target:\n                return c + 1\n            \n            if j not in fromfirst:\n                fromfirst[j] = c + 1\n                \n                if c != 4:\n                    q.append(j)\n                    \n    fromsecond = {tuple(p2): 0}\n    target = tuple(p1)\n    q = collections.deque([p2])\n    answer = 100000\n    \n    while q:\n        s = q.popleft()\n        c = fromsecond[s]\n        \n        if c == 4:\n            break\n        \n        for j in get_all_permutations(s):\n            if j == target:\n                return c + 1\n            \n            if j not in fromsecond:\n                fromsecond[j] = c + 1\n                \n                if c != 3:\n                    q.append(j)\n                    \n            if j in fromfirst:\n                answer = min(answer, fromfirst[j] + fromsecond[j])\n                \n    return answer\n\n\ninput_data = \"\"\"\n1 2 3 4 5 6 7 8 9 10\n3 1 5 2 7 4 9 6 10 8\n\n3 10 8 2 5 4 7 1 6 9\n5 2 3 1 7 4 10 8 6 9\n\n8 6 7 9 4 1 3 10 2 5\n8 2 7 6 9 1 5 3 10 4\n\n3 9 10 4 1 8 6 7 5 2\n2 9 8 5 1 7 3 4 6 10\n\n1 2 3 4 5 6 7 8 9 10\n1 2 3 4 5 6 7 8 9 10\n\"\"\"\n\n# Proces input data\ndataset = list(map(str.strip, input_data.strip().split('\\n')))\ndistances = []\n\nfor i in range(0, len(dataset), 3):\n    s = tuple(map(int, dataset[i].split()))\n    t = tuple(map(int, dataset[i + 1].split()))\n    distances.append(get_reversal_distance(t, s))\n\nprint(' '.join(map(str, distances)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#key-changes-and-additions",
    "href": "posts/md/Rosalind_stronghold.html#key-changes-and-additions",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "42.4 Key Changes and Additions",
    "text": "42.4 Key Changes and Additions\n\nConversion to Python 3:\n\nChanged xrange to range.\nChanged print statement to print() function.\n\nInput Handling:\n\nReplaced file reading with a direct input_data string for simplicity and demonstration purposes.\nProcessed the input_data string to split it into individual lines and then handled them similarly to how they would be read from a file."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-42",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-42",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "43.1 Sample Dataset",
    "text": "43.1 Sample Dataset\n3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-43",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-43",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "43.2 Sample Output",
    "text": "43.2 Sample Output\n8"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-40",
    "href": "posts/md/Rosalind_stronghold.html#solution-40",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "43.3 Solution",
    "text": "43.3 Solution\nTo solve the problem of counting the total number of subsets of the set \\({1, 2, \\ldots, n}\\) modulo 1,000,000, we need to understand a few key points:\n\nSubsets of a Set:\n\nFor any set of size \\(n\\), the number of possible subsets is \\(2^n\\). This includes the empty set and the set itself.\n\nModulo Operation:\n\nSince \\(n\\) can be as large as 1000, \\(2^n\\) can be a very large number. To manage this, we will compute the result modulo 1,000,000."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solution",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solution",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "43.4 Steps to Solution",
    "text": "43.4 Steps to Solution\n\nCalculate \\(2^n \\mod 1,000,000\\):\n\nWe will use modular exponentiation to compute this efficiently. Direct computation of \\(2^n\\) for large \\(n\\) is impractical due to the size of the number.\n\nModular Exponentiation:\n\nThis technique allows us to compute \\((base^{exp}) \\mod mod\\) efficiently using an iterative or recursive approach that reduces the number of multiplications required.\n\n\nHere’s the Python code to solve the problem:\ndef modular_exponentiation(base, exp, mod):\n    result = 1\n    base = base % mod\n    while exp &gt; 0:\n        if (exp % 2) == 1:  # If exp is odd, multiply base with result\n            result = (result * base) % mod\n        exp = exp &gt;&gt; 1  # exp = exp // 2\n        base = (base * base) % mod  # Change base to base^2\n    return result\n\ndef count_subsets(n):\n    mod = 1000000\n    return modular_exponentiation(2, n, mod)\n\n# Sample input\nn = 3\nprint(count_subsets(n))  # Output should be 8"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-8",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-8",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "43.5 Explanation of the Code",
    "text": "43.5 Explanation of the Code\n\nFunction modular_exponentiation:\n\nInputs: base (2), exp (n), and mod (1,000,000).\nProcess: This function uses an efficient loop to compute the exponentiation modulo mod. By squaring the base and halving the exponent iteratively, it ensures that we keep the numbers manageable and perform fewer multiplications.\n\nFunction count_subsets:\n\nThis function simply calls modular_exponentiation with base 2, exp n, and mod 1,000,000.\n\nMain Execution:\n\nThe sample input n = 3 is used to demonstrate the function, which should output 8 as expected.\nThe script can also read from standard input for actual use cases."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-43",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-43",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "44.1 Sample Dataset",
    "text": "44.1 Sample Dataset\n90000 0.6\nATAGCCGA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-44",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-44",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "44.2 Sample Output",
    "text": "44.2 Sample Output\n0.689"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-41",
    "href": "posts/md/Rosalind_stronghold.html#solution-41",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "44.3 Solution",
    "text": "44.3 Solution\nTo solve the problem of calculating the probability that at least one out of \\(N\\)random DNA strings matches a given DNA string \\(s\\), we can follow these steps:\n\nCalculate the Probability of Matching a Single String:\n\nThe probability of a specific base in \\(s\\)matching a random base depends on the GC-content \\(x\\).\nFor GC-content \\(x\\), the probabilities are:\n\nProbability of ‘G’ or ‘C’: \\(\\frac{x}{2}\\)\nProbability of ‘A’ or ‘T’: \\(\\frac{1 - x}{2}\\)\n\n\nCompute the Probability of the Entire String Matching:\n\nThe probability that a random DNA string of the same length as \\(s\\)matches \\(s\\)exactly is the product of the probabilities for each individual base.\n\nCalculate the Complementary Probability:\n\nCompute the probability that a single random DNA string does NOT match \\(s\\).\nUsing this, compute the probability that all \\(N\\)random strings do NOT match \\(s\\).\n\nCompute the Final Probability:\n\nThe probability that at least one out of \\(N\\)random DNA strings matches \\(s\\)is the complement of the probability that none of them matches \\(s\\).\n\n\nLet’s go through the implementation of this step-by-step:"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation-5",
    "href": "posts/md/Rosalind_stronghold.html#implementation-5",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "44.4 Implementation",
    "text": "44.4 Implementation\ndef calculate_probability(N, x, s):\n    # Step 1: Calculate the probability of matching a single base\n    prob_match = 1.0\n    for base in s:\n        if base in 'GC':\n            prob_match *= x / 2\n        else:  # base in 'AT'\n            prob_match *= (1 - x) / 2\n    \n    # Step 2: Compute the probability of the entire string matching\n    # This is already computed as prob_match\n    \n    # Step 3: Calculate the complementary probability\n    prob_not_match = 1 - prob_match\n    \n    # Step 4: Compute the final probability\n    prob_all_not_match = prob_not_match ** N\n    prob_at_least_one_match = 1 - prob_all_not_match\n    \n    return prob_at_least_one_match\n\n# Sample input\nN = 90000\nx = 0.6\ns = \"ATAGCCGA\"\n\n# Calculate and print the probability\nresult = calculate_probability(N, x, s)\nprint(f\"{result:f}\")"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-9",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-9",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "44.5 Explanation of the Code",
    "text": "44.5 Explanation of the Code\n\nProbability Calculation for Each Base:\n\nFor each base in the string \\(s\\), the probability of it being either ‘G’ or ‘C’ is \\(\\frac{x}{2}\\) and for ‘A’ or ‘T’ is \\(\\frac{1 - x}{2}\\).\n\nComputing Probability for the Entire String:\n\nMultiply the probabilities of each base matching to get the probability of the entire string matching a random string of the same length.\n\nComplementary Probability:\n\nCompute the probability that a single random DNA string does NOT match \\(s\\).\nRaise this probability to the power \\(N\\) to get the probability that none of the \\(N\\) strings match \\(s\\).\n\nFinal Probability:\n\nSubtract the complementary probability from 1 to get the probability that at least one out of \\(N\\) random strings matches \\(s\\)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-44",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-44",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "45.1 Sample Dataset",
    "text": "45.1 Sample Dataset\n6 3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-45",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-45",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "45.2 Sample Output",
    "text": "45.2 Sample Output\n42"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-42",
    "href": "posts/md/Rosalind_stronghold.html#solution-42",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "45.3 Solution",
    "text": "45.3 Solution\nHere’s the refactored code that performs the required calculation using Python’s built-in math module while maintaining readability and simplicity.\nimport math\n\ndef sum_of_combinations(n, m, MOD):\n    total = 0\n    for k in range(m, n + 1):\n        # Calculate C(n, k) using math.factorial\n        comb = math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n        total = (total + comb) % MOD\n    return total\n\n# Sample input\nsample_input = \"6 3\"\nn, m = map(int, sample_input.strip().split())\n\n# Define the modulo\nMOD = 1000000\n\n# Calculate the sum of combinations\nresult = sum_of_combinations(n, m, MOD)\n\n# Print the results\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-22",
    "href": "posts/md/Rosalind_stronghold.html#explanation-22",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "45.4 Explanation",
    "text": "45.4 Explanation\n\nFunction Definition:\nMain Logic:\n\nIterate through all \\(k\\)from \\(m\\)to \\(n\\).\nCalculate \\(C(n, k)\\) using the factorial function.\nSum the results, taking modulo \\(1000000\\)at each step to handle large numbers efficiently.\n\nSample Input Handling:\n\nParse the sample input to get the values of \\(n\\)and \\(m\\).\nDefine the modulo value \\(MOD = 1000000\\).\nCalculate the result using the sum_of_combinations function and print the results."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-45",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-45",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "46.1 Sample Dataset",
    "text": "46.1 Sample Dataset\n&gt;Rosalind_39\nPLEASANTLY\n&gt;Rosalind_11\nMEANLY"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-46",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-46",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "46.2 Sample Output",
    "text": "46.2 Sample Output\n5"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-46",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-46",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "47.1 Sample Dataset",
    "text": "47.1 Sample Dataset\n10\nAG\n0.25 0.5 0.75"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-47",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-47",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "47.2 Sample Output",
    "text": "47.2 Sample Output\n0.422 0.563 0.422"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-43",
    "href": "posts/md/Rosalind_stronghold.html#solution-43",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "47.3 Solution",
    "text": "47.3 Solution\nTo solve this problem, we need to compute the expected number of times a given DNA string \\(s\\)will appear as a substring in a random DNA string \\(t\\)of length \\(n\\). The DNA string \\(t\\)is generated with varying GC-content values given in an array \\(A\\)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#approach",
    "href": "posts/md/Rosalind_stronghold.html#approach",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "47.4 Approach",
    "text": "47.4 Approach\n\nCalculate Probabilities:\n\nThe probability of each nucleotide (A, T, C, G) depends on the GC-content.\nFor a given GC-content \\(gc\\):\n\nProbability of G or C: \\(\\frac{gc}{2}\\)\nProbability of A or T: \\(\\frac{1 - gc}{2}\\)\n\n\nCompute the Probability of \\(s\\)as a Substring:\n\nFor each GC-content value \\(A[i]\\), compute the probability that \\(s\\)appears at any specific position in \\(t\\).\nSum the probabilities for all possible starting positions of \\(s\\)in \\(t\\)(from 0 to \\(n - \\text{len}(s)\\)).\n\nExpected Value:\n\nMultiply the single-position probability by the number of possible starting positions to get the expected number of times \\(s\\)appears in \\(t\\)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-steps-and-code",
    "href": "posts/md/Rosalind_stronghold.html#detailed-steps-and-code",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "47.5 Detailed Steps and Code",
    "text": "47.5 Detailed Steps and Code\n\nReading Input:\n\nRead \\(n\\), the DNA string \\(s\\), and the array \\(A\\).\n\nProbability Calculation:\n\nFor each GC-content value in \\(A\\), compute the probability of \\(s\\).\n\nOutput:\n\nPrint the expected values for each GC-content in \\(A\\).\n\n\nHere’s the Python code to achieve this:\ndef expected_restriction_sites(n, s, A):\n    len_s = len(s)\n    B = []\n    \n    for gc_content in A:\n        p_gc = gc_content / 2\n        p_at = (1 - gc_content) / 2\n        \n        prob_s = 1.0\n        for nucleotide in s:\n            if nucleotide in 'GC':\n                prob_s *= p_gc\n            else:\n                prob_s *= p_at\n        \n        expected_count = prob_s * (n - len_s + 1)\n        B.append(expected_count)\n    \n    return B\n\n# Sample input\ntext = \"\"\"\n10\nAG\n0.25 0.5 0.75\"\"\"\n\nn = int(text.strip().split(\"\\n\")[0])\ns = text.strip().split(\"\\n\")[1]\nA = [float(x) for x in text.strip().split(\"\\n\")[2].split()]\n\nresult = expected_restriction_sites(n, s, A)\nprint(\" \".join(f\"{x:f}\" for x in result))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-23",
    "href": "posts/md/Rosalind_stronghold.html#explanation-23",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "47.6 Explanation",
    "text": "47.6 Explanation\n\nReading Input:\n\nn is the length of the random DNA string.\ns is the DNA substring we are looking for.\nA is an array of GC-content values.\n\nProbability Calculation:\n\nFor each GC-content \\(gc\\), compute the probability prob_s that the substring \\(s\\) will match exactly at a given position.\nUse the formula: \\[\n\\text{prob\\_s} = \\prod_{i=1}^{\\text{len}(s)} \\text{probability of } s[i]\n\\]\n\nExpected Value:\n\nMultiply prob_s by the number of possible starting positions in the string \\(t\\)(which is \\(n - \\text{len}(s) + 1\\)).\n\nOutput:\n\nPrint the expected counts, formatted to three decimal places."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-47",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-47",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "48.1 Sample Dataset",
    "text": "48.1 Sample Dataset\n&gt;Rosalind_57\nAUAU"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-48",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-48",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "48.2 Sample Output",
    "text": "48.2 Sample Output\n7"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-44",
    "href": "posts/md/Rosalind_stronghold.html#solution-44",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "48.3 Solution",
    "text": "48.3 Solution\nWe want to count the number of ways to form noncrossing matchings of basepair edges in an RNA string. RNA strings can form base pairs between A and U or C and G.\n\nInitialization:\n\nInput: An RNA string rna of length n.\nDP Table: We create a 2D list dp of size (n+1) x (n+1) initialized to zero. dp[i][j] will store the number of noncrossing matchings in the substring from index i to j-1 of the RNA string.\nBase Case:\n\ndp[i][i] = 1 because a single nucleotide can only be matched with itself.\ndp[i][i+1] = 1 because a pair of adjacent nucleotides can either be unmatched or form one valid base pair.\n\n\nFilling the DP Table:\n\nWe iterate over all possible substring lengths starting from 2 up to n.\nFor each substring of length length starting at index i and ending at index j-1:\n\nWe start by assuming the first nucleotide rna[i] is not paired, so the count of valid matchings is initially dp[i+1][j].\nThen we check all possible positions k where rna[i] can form a valid base pair (i.e., rna[i] with rna[k]). If they form a valid base pair:\n\nWe add the number of ways to match the left part (dp[i+1][k]) and the right part (dp[k+1][j]).\n\nSum the results and take modulo $10^6) to avoid large numbers.\n\n\nResult:\n\nThe result for the entire RNA string is stored in dp[0][n]."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#code-explanation",
    "href": "posts/md/Rosalind_stronghold.html#code-explanation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "48.4 Code Explanation",
    "text": "48.4 Code Explanation\nHere’s the code again with comments to explain each part:\ndef count_noncrossing_matchings(rna):\n    n = len(rna)  # Length of the RNA string\n    MOD = 1000000  # Modulo value to avoid large numbers\n\n    # Initialize a dp table with all zeros\n    dp = [[0] * (n + 1) for _ in range(n + 1)]\n\n    # Base case: Single nucleotides and empty string\n    for i in range(n + 1):\n        dp[i][i] = 1  # A single nucleotide matches with itself\n        if i &lt; n:\n            dp[i][i + 1] = 1  # Two adjacent nucleotides can be unmatched or paired\n\n    # Fill the dp table for all substring lengths\n    for length in range(2, n + 1):  # Length of the substring\n        for i in range(n - length + 1):\n            j = i + length\n            dp[i][j] = dp[i + 1][j]  # Case when the first nucleotide is unpaired\n            for k in range(i + 1, j):\n                # Check if rna[i] and rna[k] can form a valid base pair\n                if (rna[i] == 'A' and rna[k] == 'U') or (rna[i] == 'U' and rna[k] == 'A') or \\\n                   (rna[i] == 'C' and rna[k] == 'G') or (rna[i] == 'G' and rna[k] == 'C'):\n                    dp[i][j] += dp[i + 1][k] * dp[k + 1][j]\n                    dp[i][j] %= MOD  # Take modulo to avoid large numbers\n\n    # The result for the entire string\n    return dp[0][n]\n\n# Sample input\nrna_string = \"AUAU\"\nresult = count_noncrossing_matchings(rna_string)\nprint(result)  # Output: 7"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#key-points",
    "href": "posts/md/Rosalind_stronghold.html#key-points",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "48.5 Key Points",
    "text": "48.5 Key Points\n\nBase Cases: Handle single and adjacent nucleotides.\nDynamic Programming: Use a table to store results of subproblems to build up the solution for the entire string.\nModulo Operation: Keep results manageable by taking modulo $10^6).\n\nThis approach efficiently calculates the number of noncrossing matchings for the given RNA string."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-48",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-48",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "49.1 Sample Dataset",
    "text": "49.1 Sample Dataset\n(cat)dog;\ndog cat\n\n(dog,cat);\ndog cat"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-49",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-49",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "49.2 Sample Output",
    "text": "49.2 Sample Output\n1 2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-45",
    "href": "posts/md/Rosalind_stronghold.html#solution-45",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "49.3 Solution",
    "text": "49.3 Solution\ndef dis_tree(T, x, y):\n    # Find the indices of x and y in the Newick string T\n    x_index = T.find(x)\n    y_index = T.find(y)\n    \n    # Extract the relevant substring between the indices of x and y\n    sub_tree = [i for i in T[min(x_index, y_index):max(x_index, y_index)] if i in [')', '(', ',']]\n    \n    # Convert the list of characters to a string\n    bracket = ''.join(sub_tree)\n    \n    # Remove empty pairs of parentheses\n    while '(,)' in bracket:\n        bracket = bracket.replace('(,)', '')\n    \n    # Determine the number of steps based on the type of brackets remaining\n    if bracket.count('(') == len(bracket) or bracket.count(')') == len(bracket):\n        return len(bracket)\n    elif bracket.count(',') == len(bracket):\n        return 2\n    else:\n        return bracket.count(')') + bracket.count('(') + 2\n\ndef process_input(input_data):\n    # Parse the input data into a list of tree and node pairs\n    tree_data = [line.strip().replace(\";\", \"\") for line in input_data.strip().split(\"\\n\") if line.strip()]\n    results = []\n    \n    # Iterate through the parsed data to proces each tree and node pair\n    for i in range(0, len(tree_data), 2):\n        T = tree_data[i]\n        x, y = tree_data[i+1].split(' ')\n        results.append(dis_tree(T, x, y))\n    \n    return results\n\n# Sample input\ninput_data = \"\"\"\n(cat)dog;\ndog cat\n\n(dog,cat);\ndog cat\n\"\"\"\n\n# Proces the input and print the results\noutput_data = process_input(input_data)\nprint(\" \".join(map(str, output_data)))  # Output should be: 1 2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-24",
    "href": "posts/md/Rosalind_stronghold.html#explanation-24",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "49.4 Explanation",
    "text": "49.4 Explanation\n\nThe dis_tree function computes the distance between nodes x and y in the given Newick string T.\nThe process_input function processes the input string, extracts the tree and node pairs, and computes the distances using the dis_tree function.\nFinally, the results are printed in the required format."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-49",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-49",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "50.1 Sample Dataset",
    "text": "50.1 Sample Dataset\nATCTGAT\nTGCATA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-50",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-50",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "50.2 Sample Output",
    "text": "50.2 Sample Output\nATGCATGAT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-46",
    "href": "posts/md/Rosalind_stronghold.html#solution-46",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "50.3 Solution",
    "text": "50.3 Solution\nTo solve the problem of finding the shortest common supersequence (SCS) of two DNA strings $ s$ and $ t$, we can use a dynamic programming approach. The idea is similar to finding the longest common subsequence (LCS), but with a few modifications to ensure that we construct the SCS."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-8",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve-the-problem-8",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "50.4 Steps to Solve the Problem",
    "text": "50.4 Steps to Solve the Problem\n\nDefine the Dynamic Programming Table:\n\nLet \\(dp[i][j]\\) represent the length of the SCS of the substrings \\(s[0:i]\\) and \\(t[0:j]\\).\n\nInitialize the Table:\n\nFor \\(dp[i][0]\\), the SCS is simply the prefix of \\(s\\) of length \\(i\\), so \\(dp[i][0] = i\\).\nFor \\(dp[0][j]\\), the SCS is simply the prefix of \\(t\\) of length \\(j\\), so \\(dp[0][j] = j\\).\n\nFill the DP Table:\n\nIf \\(s[i-1] == t[j-1]\\), then \\(dp[i][j] = dp[i-1][j-1] + 1\\) because the characters match and they contribute once to the SCS.\nOtherwise, \\(dp[i][j] = \\min(dp[i-1][j], dp[i][j-1]) + 1\\), meaning we take the shorter SCS by either adding the current character of \\(s\\) or \\(t\\).\n\nConstruct the SCS:\n\nUse the DP table to backtrack and construct the SCS by starting from \\(dp[len(s)][len(t)]\\)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#code-implementation",
    "href": "posts/md/Rosalind_stronghold.html#code-implementation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "50.5 Code Implementation",
    "text": "50.5 Code Implementation\nHere’s the code to solve the problem:\ndef shortest_common_supersequence(s, t):\n    m, n = len(s), len(t)\n    \n    # Initialize the DP table\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Fill the base cases\n    for i in range(1, m + 1):\n        dp[i][0] = i\n    for j in range(1, n + 1):\n        dp[0][j] = j\n    \n    # Fill the DP table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if s[i - 1] == t[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = min(dp[i - 1][j], dp[i][j - 1]) + 1\n    \n    # Backtrack to find the SCS\n    i, j = m, n\n    scs = []\n    \n    while i &gt; 0 and j &gt; 0:\n        if s[i - 1] == t[j - 1]:\n            scs.append(s[i - 1])\n            i -= 1\n            j -= 1\n        elif dp[i - 1][j] &lt; dp[i][j - 1]:\n            scs.append(s[i - 1])\n            i -= 1\n        else:\n            scs.append(t[j - 1])\n            j -= 1\n    \n    while i &gt; 0:\n        scs.append(s[i - 1])\n        i -= 1\n    while j &gt; 0:\n        scs.append(t[j - 1])\n        j -= 1\n    \n    return ''.join(reversed(scs))\n\n# Sample input\ns = \"ATCTGAT\"\nt = \"TGCATA\"\n\n# Calculate and print the shortest common supersequence\nprint(shortest_common_supersequence(s, t))  # Output should be a valid SCS like \"ATGCATGAT\""
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-50",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-50",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "51.1 Sample Dataset",
    "text": "51.1 Sample Dataset\n10\n{1, 2, 3, 4, 5}\n{2, 8, 5, 10}"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-51",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-51",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "51.2 Sample Output",
    "text": "51.2 Sample Output\n{1, 2, 3, 4, 5, 8, 10}\n{2, 5}\n{1, 3, 4}\n{8, 10}\n{8, 9, 10, 6, 7}\n{1, 3, 4, 6, 7, 9}"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-47",
    "href": "posts/md/Rosalind_stronghold.html#solution-47",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "51.3 Solution",
    "text": "51.3 Solution\nTo solve this problem involving set operations, we need to perform union, intersection, set difference, and complement operations on two given sets \\(A\\) and \\(B\\), with respect to a universal set \\(U\\) which contains all integers from 1 to \\(n\\). Here’s how we can approach this step-by-step:"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps",
    "href": "posts/md/Rosalind_stronghold.html#steps",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "51.4 Steps",
    "text": "51.4 Steps\n\nRead Input:\n\nThe first line contains the integer \\(n\\), the size of the universal set.\nThe second line contains set \\(A\\).\nThe third line contains set \\(B\\).\n\nParse Sets:\n\nExtract the elements of sets \\(A\\) and \\(B\\) from the input strings.\n\nDefine Universal Set \\(U\\):\n\nThis is simply the set of all integers from 1 to \\(n\\).\n\nPerform Set Operations:\n\nUnion \\(A \\cup B\\): Elements in either \\(A\\) or \\(B\\).\nIntersection \\(A \\cap B\\): Elements common to both \\(A\\) and \\(B\\).\nDifference \\(A - B\\): Elements in \\(A\\) but not in \\(B\\).\nDifference \\(B - A\\): Elements in \\(B\\) but not in \\(A\\).\nComplement \\(A^c\\): Elements in \\(U\\) but not in \\(A\\).\nComplement \\(B^c\\): Elements in \\(U\\) but not in \\(B\\).\n\n\nHere’s the Python code to implement the above logic:\ndef set_operations(n, A, B):\n    U = set(range(1, n+1))\n    A = set(A)\n    B = set(B)\n\n    union = A | B\n    intersection = A & B\n    difference_A_B = A - B\n    difference_B_A = B - A\n    complement_A = U - A\n    complement_B = U - B\n\n    return union, intersection, difference_A_B, difference_B_A, complement_A, complement_B\n\n# Sample input\nn = 10\nA = {1, 2, 3, 4, 5}\nB = {2, 8, 5, 10}\n\n# Get the results\nresults = set_operations(n, A, B)\n\n# Print the results\nfor result in results:\n    print(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-25",
    "href": "posts/md/Rosalind_stronghold.html#explanation-25",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "51.5 Explanation",
    "text": "51.5 Explanation\n\nUnion (\\(A \\cup B\\)): Combines all unique elements from both sets.\nIntersection (\\(A \\cap B\\)): Selects only the elements that are present in both sets.\nDifference (\\(A - B\\)): Selects elements that are in \\(A\\) but not in \\(B\\).\nDifference (\\(B - A\\)): Selects elements that are in \\(B\\) but not in \\(A\\).\nComplement (\\(A^c\\)): Selects elements that are in the universal set \\(U\\) but not in \\(A\\).\nComplement (\\(B^c\\)): Selects elements that are in the universal set \\(U\\) but not in \\(B\\)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-51",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-51",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "52.1 Sample Dataset",
    "text": "52.1 Sample Dataset\n1 2 3 4 5 6 7 8 9 10\n1 8 9 3 2 7 6 5 4 10"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-52",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-52",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "52.2 Sample Output",
    "text": "52.2 Sample Output\n2\n4 9\n2 5"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-48",
    "href": "posts/md/Rosalind_stronghold.html#solution-48",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "52.3 Solution",
    "text": "52.3 Solution\nTo solve the problem of sorting by reversals, we need to find the minimum number of reversals needed to transform one permutation \\(\\pi\\) into another permutation \\(\\gamma\\), as well as the specific reversals that accomplish this transformation."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#key-concepts",
    "href": "posts/md/Rosalind_stronghold.html#key-concepts",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "52.4 Key Concepts",
    "text": "52.4 Key Concepts\n\nReversal: A reversal is an operation that takes a segment of the permutation and reverses the order of the elements within that segment.\nReversal Distance: The reversal distance between two permutations is the minimum number of reversals needed to transform one permutation into another."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#approach-1",
    "href": "posts/md/Rosalind_stronghold.html#approach-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "52.5 Approach",
    "text": "52.5 Approach\n\nGraph Representation:\n\nRepresent the permutations as nodes in a graph, where each node is a permutation and edges represent a single reversal operation transforming one permutation into another.\n\nBreadth-First Search (BFS):\n\nUse BFS to find the shortest path from the starting permutation \\(\\pi\\) to the target permutation \\(\\gamma\\). Each level of BFS corresponds to one reversal.\nTrack the reversals used to transform \\(\\pi\\) to \\(\\gamma\\).\n\nImplementing BFS:\n\nUse a queue to manage the permutations to explore.\nUse a set to keep track of visited permutations to avoid redundant work.\nFor each permutation, generate all possible permutations resulting from a single reversal and enqueue them if they haven’t been visited.\n\nReconstruct Path:\n\nOnce the target permutation \\(\\gamma\\) is reached, backtrack to reconstruct the sequence of reversals."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#example-solution-in-python",
    "href": "posts/md/Rosalind_stronghold.html#example-solution-in-python",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "52.6 Example Solution in Python",
    "text": "52.6 Example Solution in Python\nHere’s the implementation of the approach:\nfrom collections import deque\n\ndef reverse_sublist(lst, start, end):\n    lst[start:end+1] = lst[start:end+1][::-1]\n\ndef bfs(start, target):\n    queue = deque([(start, [])])\n    visited = set()\n    visited.add(tuple(start))\n\n    while queue:\n        current, path = queue.popleft()\n\n        if current == target:\n            return len(path), path\n\n        for i in range(len(current)):\n            for j in range(i+1, len(current)):\n                new_perm = current[:]\n                reverse_sublist(new_perm, i, j)\n                new_tuple = tuple(new_perm)\n\n                if new_tuple not in visited:\n                    visited.add(new_tuple)\n                    queue.append((new_perm, path + [(i+1, j+1)]))\n                    \n    return -1, []\n\ndef sorting_by_reversals(pi, gamma):\n    distance, reversals = bfs(pi, gamma)\n    return distance, reversals\n    \ndef parse_input(input_string):\n    lines = input_string.strip().split(\"\\n\")\n    pi = [int(x) for x in lines[0].split()]\n    gamma = [int(x) for x in lines[1].split()]\n    return pi, gamma\n    \n# Sample input\nsample_input = \"\"\"\n6 5 4 7 2 3 9 8 10 1\n4 6 2 9 7 1 3 8 5 10\n\"\"\"\n\npi, gamma = parse_input(sample_input)\n# Get the results\ndistance, reversals = sorting_by_reversals(pi, gamma)\n\n# Print the results\nprint(distance)\nfor r in reversals:\n    print(r[0], r[1])"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-26",
    "href": "posts/md/Rosalind_stronghold.html#explanation-26",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "52.7 Explanation",
    "text": "52.7 Explanation\n\nreverse_sublist: A helper function to reverse a sublist within a list.\nbfs: The BFS function to explore all possible permutations resulting from single reversals, tracking the path taken.\nsorting_by_reversals: Main function to find the reversal distance and the specific reversals."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-52",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-52",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "53.1 Sample Dataset",
    "text": "53.1 Sample Dataset\n3524.8542\n3710.9335\n3841.974\n3970.0326\n4057.0646"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-53",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-53",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "53.2 Sample Output",
    "text": "53.2 Sample Output\nWMQS"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-49",
    "href": "posts/md/Rosalind_stronghold.html#solution-49",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "53.3 Solution",
    "text": "53.3 Solution\nTo solve this problem, we need to infer a protein string from its prefix spectrum. The prefix spectrum is a list of cumulative masses of prefixes of the protein. Given a list of masses, our goal is to determine which amino acids correspond to the differences between successive masses in this list.\nHere’s a step-by-step approach to solve the problem:\n\nParse the input list of masses.\nCompute the differences between successive masses. These differences should correspond to the masses of amino acids.\nMatch the computed differences to the known monoisotopic masses of amino acids.\nConstruct the protein string from the matched amino acids.\n\nWe’ll use the monoisotopic mas table for amino acids, which provides the exact masses of each amino acid.\nHere’s the Python code to implement this:\n# Monoisotopic mas table for amino acids\nmonoisotopic_mass_table = {\n    'A': 71.03711, 'C': 103.00919, 'D': 115.02694, 'E': 129.04259, 'F': 147.06841,\n    'G': 57.02146, 'H': 137.05891, 'I': 113.08406, 'K': 128.09496, 'L': 113.08406,\n    'M': 131.04049, 'N': 114.04293, 'P': 97.05276, 'Q': 128.05858, 'R': 156.10111,\n    'S': 87.03203, 'T': 101.04768, 'V': 99.06841, 'W': 186.07931, 'Y': 163.06333\n}\n\ndef parse_input(input_string):\n    return list(map(float, input_string.strip().split()))\n\ndef find_amino_acid(delta_mass):\n    for amino_acid, mas in monoisotopic_mass_table.items():\n        if abs(mas - delta_mass) &lt; 0.01:  # Allowing a small tolerance\n            return amino_acid\n    return None\n\ndef infer_protein_from_spectrum(spectrum):\n    protein = \"\"\n    for i in range(1, len(spectrum)):\n        delta_mas = spectrum[i] - spectrum[i-1]\n        amino_acid = find_amino_acid(delta_mass)\n        if amino_acid:\n            protein += amino_acid\n        else:\n            raise ValueError(f\"No matching amino acid found for mas difference {delta_mass}\")\n    return protein\n\ndef main():\n    # Sample input\n    sample_input = \"\"\"\n    3524.8542\n    3710.9335\n    3841.974\n    3970.0326\n    4057.0646\n    \"\"\"\n\n    spectrum = parse_input(sample_input)\n    protein = infer_protein_from_spectrum(spectrum)\n    print(protein)\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-27",
    "href": "posts/md/Rosalind_stronghold.html#explanation-27",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "53.4 Explanation",
    "text": "53.4 Explanation\n\nparse_input: This function parses the input string into a list of floats representing the masses.\nfind_amino_acid: This function takes a mas difference and finds the corresponding amino acid by comparing it against the monoisotopic mas table, allowing for a small tolerance due to floating-point precision issues.\ninfer_protein_from_spectrum: This function computes the differences between successive masses in the spectrum and uses find_amino_acid to map these differences to amino acids, constructing the protein string."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-53",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-53",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "54.1 Sample Dataset",
    "text": "54.1 Sample Dataset\nATAGA\nATC\nGAT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-54",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-54",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "54.2 Sample Output",
    "text": "54.2 Sample Output\n1 2 A\n2 3 T\n3 4 A\n4 5 G\n5 6 A\n3 7 C\n1 8 G\n8 9 A\n9 10 T"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-50",
    "href": "posts/md/Rosalind_stronghold.html#solution-50",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "54.3 Solution",
    "text": "54.3 Solution\nTo construct a trie from a given collection of DNA strings, we need to follow these steps:\n\nInitialization: Start with a root node.\nInsertion: For each string in the collection, insert it into the trie by creating new nodes and edges as necessary.\nOutput: Generate the adjacency list representation of the trie.\n\nHere is the Python code to accomplish this task:\ndef build_trie(patterns):\n    trie = {1: {}}\n    next_node = 2\n    for pattern in patterns:\n        current_node = 1\n        for char in pattern:\n            if char in trie[current_node]:\n                current_node = trie[current_node][char]\n            else:\n                trie[current_node][char] = next_node\n                trie[next_node] = {}\n                current_node = next_node\n                next_node += 1\n    return trie\n\ndef trie_to_adjacency_list(trie):\n    adjacency_list = []\n    for parent in trie:\n        for char, child in trie[parent].items():\n            adjacency_list.append((parent, child, char))\n    return adjacency_list\n\ndef main():\n    # Sample input\n    input_data = \"\"\"\n    ATAGA\n    ATC\n    GAT\n    \"\"\"\n    patterns = input_data.strip().split()\n    \n    # Build the trie\n    trie = build_trie(patterns)\n    \n    # Convert trie to adjacency list\n    adjacency_list = trie_to_adjacency_list(trie)\n    \n    # Print the adjacency list\n    for parent, child, char in adjacency_list:\n        print(f\"{parent} {child} {char}\")\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-28",
    "href": "posts/md/Rosalind_stronghold.html#explanation-28",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "54.4 Explanation",
    "text": "54.4 Explanation\n\nFunction build_trie:\n\nInitializes the trie with a root node labeled 1.\nFor each pattern, it iterates through its characters, and either moves to an existing node if the character is already in the current node’s dictionary or creates a new node if the character is not present.\nIt maintains a counter next_node to assign new labels to nodes.\n\nFunction trie_to_adjacency_list:\n\nConverts the trie structure into an adjacency list format, which is a list of tuples where each tuple represents an edge from a parent node to a child node labeled by a character.\n\nFunction main:\n\nHandles input and output operations.\nReads the input strings, constructs the trie, converts it to an adjacency list, and then prints the adjacency list."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-54",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-54",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "55.1 Sample Dataset",
    "text": "55.1 Sample Dataset\n186.07931 287.12699 548.20532 580.18077 681.22845 706.27446 782.27613 968.35544 968.35544\n101.04768 158.06914 202.09536 318.09979 419.14747 463.17369"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-55",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-55",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "55.2 Sample Output",
    "text": "55.2 Sample Output\n3\n85.03163"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-51",
    "href": "posts/md/Rosalind_stronghold.html#solution-51",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "55.3 Solution",
    "text": "55.3 Solution\nTo solve the problem of comparing spectra using the spectral convolution, we need to follow these steps:\n\nParse the input: Read the two multisets \\(S1\\) and \\(S2\\).\nCompute the spectral convolution: For each pair of elements \\((s1, s2)\\) where \\(s1 \\in S1\\) and \\(s2 \\in S2\\), compute the difference \\(s1 - s2\\).\nCount the occurrences of each difference: Track how many times each difference appears.\nFind the most frequent difference: Identify the difference that appears most frequently and its multiplicity.\n\nHere’s the Python code to implement the above steps:\nfrom collections import Counter\n\ndef parse_input(input_data):\n    lines = input_data.strip().split(\"\\n\")\n    S1 = list(map(float, lines[0].split()))\n    S2 = list(map(float, lines[1].split()))\n    return S1, S2\n\ndef spectral_convolution(S1, S2):\n    convolution = []\n    for s1 in S1:\n        for s2 in S2:\n            convolution.append(round(s1 - s2, 5))\n    return convolution\n\ndef find_max_multiplicity(convolution):\n    count = Counter(convolution)\n    max_value, max_count = max(count.items(), key=lambda x: x[1])\n    return max_count, abs(max_value)\n\ndef main(input_data):\n    S1, S2 = parse_input(input_data)\n    convolution = spectral_convolution(S1, S2)\n    max_count, max_value = find_max_multiplicity(convolution)\n    return max_count, max_value\n\n# Sample input\nsample_input = \"\"\"\n186.07931 287.12699 548.20532 580.18077 681.22845 706.27446 782.27613 968.35544 968.35544\n101.04768 158.06914 202.09536 318.09979 419.14747 463.17369\n\"\"\"\n\n# Proces the input and get the result\nresult = main(sample_input)\nprint(result[0])\nprint(f\"{result[1]:f}\")"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-29",
    "href": "posts/md/Rosalind_stronghold.html#explanation-29",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "55.4 Explanation",
    "text": "55.4 Explanation\n\nParsing Input:\n\nThe parse_input function reads the input data, splits it into lines, and then converts each line into a list of floats representing \\(S1\\) and \\(S2\\).\n\nComputing Spectral Convolution:\n\nThe spectral_convolution function takes all pairs \\((s1, s2)\\) from \\(S1\\) and \\(S2\\), computes the difference \\(s1 - s2\\), and stores these differences in a list. The round function ensures precision to 5 decimal places, as floating-point arithmetic can introduce small errors.\n\nCounting Occurrences:\n\nThe find_max_multiplicity function uses Python’s Counter from the collections module to count how often each difference appears in the convolution list. It then finds the difference with the maximum count (multiplicity) and its corresponding value.\n\nMain Function:\n\nThe main function orchestrates the proces by calling the helper functions and printing the result."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-55",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-55",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "56.1 Sample Dataset",
    "text": "56.1 Sample Dataset\n(dog,((elephant,mouse),robot),cat);"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-56",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-56",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "56.2 Sample Output",
    "text": "56.2 Sample Output\n00110\n00111"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-52",
    "href": "posts/md/Rosalind_stronghold.html#solution-52",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "56.3 Solution",
    "text": "56.3 Solution\nThe code parses a Newick string representing a phylogenetic tree and converts it into a character table, where each row of the table represents a partition of taxa.\nfrom collections import defaultdict\n\n# Node clas to represent a node in the tree\nclas Node:\n    def __init__(self, name=\"\"):\n        self.name = name         # Name of the taxon or internal node\n        self.children = []       # List to store child nodes\n\n# Function to parse a Newick string into a tree structure\ndef parse_newick(newick):\n    def parse_node():\n        nonlocal i\n        node = Node()\n        if newick[i] == '(':\n            i += 1\n            while newick[i] != ')':\n                node.children.append(parse_node())\n                if newick[i] == ',':\n                    i += 1\n            i += 1\n        # Extract node name\n        if newick[i] not in ',)':\n            name_start = i\n            while newick[i] not in ',)':\n                i += 1\n            node.name = newick[name_start:i]\n        return node\n\n    i = 0\n    return parse_node()\n\n# Recursive function to find all splits (partitions) of the taxa\ndef find_splits(node, taxa, splits):\n    # If it's a leaf node, return the set containing the taxon name\n    if not node.children:\n        return {node.name}\n    \n    # Recursively find splits in left and right children\n    left = find_splits(node.children[0], taxa, splits)\n    right = find_splits(node.children[1], taxa, splits)\n    \n    # Combine left and right splits\n    split = left | right\n    \n    # Check if this split is non-trivial and add it to the splits list\n    if 1 &lt; len(split) &lt; len(taxa) - 1:\n        splits.append(split)\n    \n    return split\n\n# Function to create a character table from the splits\ndef create_character_table(tree, taxa):\n    splits = []\n    find_splits(tree, set(taxa), splits)\n    \n    table = []\n    # Convert each split into a binary row\n    for split in splits:\n        row = ['1' if taxon in split else '0' for taxon in taxa]\n        table.append(''.join(row))\n    \n    return table\n\n# Sample Newick string\nsample_input = \"\"\"\n(dog,((elephant,mouse),robot),cat);\n\"\"\"\n\n# Strip leading/trailing whitespace and parse the Newick string\nnewick = sample_input.strip()\ntree = parse_newick(newick)\n\ntaxa = []\n\n# Function to collect all taxa names from the tree\ndef collect_taxa(node):\n    if node.name:\n        taxa.append(node.name)\n    for child in node.children:\n        collect_taxa(child)\n\n# Collect and sort taxa names\ncollect_taxa(tree)\ntaxa.sort()\n\n# Create the character table based on the collected taxa\ncharacter_table = create_character_table(tree, taxa)\n\n# Print each row of the character table\nfor row in character_table:\n    print(row)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explain",
    "href": "posts/md/Rosalind_stronghold.html#explain",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "56.4 Explain",
    "text": "56.4 Explain\n\nNode Class:\n\nPurpose: Represents a node in the tree.\nAttributes:\n\nname: Name of the taxon or internal node.\nchildren: List of child nodes.\n\n\nparse_newick Function:\n\nPurpose: Parses a Newick string into a tree structure.\nInner Function parse_node:\n\nHandles parentheses: It processes nested parentheses to build the tree structure.\nExtracts names: It extracts the name of each node by looking for characters until it hits a delimiter (comma or closing parenthesis).\n\n\nfind_splits Function:\n\nPurpose: Finds and collects all non-trivial splits of the taxa.\nParameters:\n\nnode: Current node in the tree.\ntaxa: Set of all taxa names.\nsplits: List to collect all non-trivial splits.\n\nLogic:\n\nRecursively computes splits for left and right subtrees.\nCombines splits and checks if they are non-trivial.\nAdds valid splits to the splits list.\n\n\ncreate_character_table Function:\n\nPurpose: Converts splits into a character table.\nParameters:\n\ntree: Root node of the tree.\ntaxa: List of sorted taxa names.\n\nLogic:\n\nUses find_splits to get the splits.\nConverts each split into a binary representation.\nConstructs and returns the character table as a list of strings.\n\n\nMain Execution:\n\nInput Handling: Reads and strips the Newick string, then parses it into a tree.\nTaxa Collection: Collects and sorts all taxa names from the tree.\nCharacter Table Creation: Generates and prints the character table based on the tree structure and taxa."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-56",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-56",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "57.1 Sample Dataset",
    "text": "57.1 Sample Dataset\nTGAT\nCATG\nTCAT\nATGC\nCATC\nCATC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-57",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-57",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "57.2 Sample Output",
    "text": "57.2 Sample Output\n(ATC, TCA)\n(ATG, TGA)\n(ATG, TGC)\n(CAT, ATC)\n(CAT, ATG)\n(GAT, ATG)\n(GCA, CAT)\n(TCA, CAT)\n(TGA, GAT)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-53",
    "href": "posts/md/Rosalind_stronghold.html#solution-53",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "57.3 Solution",
    "text": "57.3 Solution\nWe’ll compute the reverse complements and the updated set separately and then proceed with constructing the De Bruijn graph.\nfrom collections import defaultdict\n\ndef reverse_complement(dna):\n    \"\"\"Computes the reverse complement of a DNA string.\"\"\"\n    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    return ''.join(complement[base] for base in reversed(dna))\n\ndef construct_de_bruijn_graph(kmers):\n    \"\"\"Constructs the De Bruijn graph and returns the adjacency list.\"\"\"\n    # Create a new set to include reverse complements\n    kmers_with_rc = set(kmers)\n    for kmer in kmers:\n        rc_kmer = reverse_complement(kmer)\n        kmers_with_rc.add(rc_kmer)\n    \n    adjacency_list = defaultdict(set)\n    k = len(next(iter(kmers))) - 1  # Length of the k-mer\n    \n    for kmer in kmers_with_rc:\n        for i in range(len(kmer) - k):\n            prefix = kmer[i:i+k]\n            suffix = kmer[i+1:i+k+1]\n            adjacency_list[prefix].add(suffix)\n    \n    return adjacency_list\n\ndef format_adjacency_list(adj_list):\n    \"\"\"Formats the adjacency list into the required output format.\"\"\"\n    result = []\n    for start_node, end_nodes in adj_list.items():\n        for end_node in end_nodes:\n            result.append(f\"({start_node}, {end_node})\")\n    return sorted(result)\n\ndef main(input_data):\n    \"\"\"Main function to proces the input data and generate the De Bruijn graph.\"\"\"\n    lines = input_data.strip().split('\\n')\n    kplus1_mers = set(lines)\n    \n    # Determine k from the length of the (k+1)-mers\n    k = len(next(iter(kplus1_mers))) - 1\n    \n    # Construct the De Bruijn graph\n    adj_list = construct_de_bruijn_graph(kplus1_mers)\n    \n    # Format and print the adjacency list\n    formatted_output = format_adjacency_list(adj_list)\n    for line in formatted_output:\n        print(line)\n\n# Sample input\nsample_input = \"\"\"\nTGAT\nCATG\nTCAT\nATGC\nCATC\nCATC\n\"\"\"\n\n# Run the main function with the sample input\nmain(sample_input)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#changes-made",
    "href": "posts/md/Rosalind_stronghold.html#changes-made",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "57.4 Changes Made",
    "text": "57.4 Changes Made\n\nAvoided Modifying Set During Iteration:\n\nInstead of modifying kmers while iterating over it, we create a new set kmers_with_rc that initially contains all the original kmers and then add reverse complements to it.\n\nFixed the Extraction of Prefix and Suffix:\n\nAdjusted the slicing in the De Bruijn graph construction to ensure we correctly extract the prefix and suffix (k)-mers from each (k+1)-mer.\n\nEnsured Proper Handling of Adjacency List:\n\nCorrectly formatted the adjacency list to meet the output requirements."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-57",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-57",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "58.1 Sample Dataset",
    "text": "58.1 Sample Dataset\n&gt;Rosalind_43\nPRETTY\n&gt;Rosalind_97\nPRTTEIN"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-58",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-58",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "58.2 Sample Output",
    "text": "58.2 Sample Output\n4\nPRETTY--\nPR-TTEIN"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-54",
    "href": "posts/md/Rosalind_stronghold.html#solution-54",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "58.3 Solution",
    "text": "58.3 Solution\ndef parse_fasta(fasta_str):\n    \"\"\"\n    Parse a FASTA format string into a list of sequences.\n    \"\"\"\n    sequences = []\n    current_sequence = []\n    for line in fasta_str.strip().split(\"\\n\"):\n        if line.startswith(\"&gt;\"):\n            if current_sequence:\n                sequences.append(\"\".join(current_sequence))\n                current_sequence = []\n        else:\n            current_sequence.append(line.strip())\n    if current_sequence:\n        sequences.append(\"\".join(current_sequence))\n    return sequences\n\ndef edit_distance_alignment(s, t):\n    \"\"\"\n    Compute the edit distance and optimal alignment of two strings.\n    \"\"\"\n    m, n = len(s), len(t)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n    # Initialize the dp table for base cases\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n\n    # Fill the dp table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if s[i-1] == t[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1\n\n    # Traceback to construct the aligned strings\n    s_aligned, t_aligned = \"\", \"\"\n    i, j = m, n\n    while i &gt; 0 and j &gt; 0:\n        if s[i-1] == t[j-1]:\n            s_aligned = s[i-1] + s_aligned\n            t_aligned = t[j-1] + t_aligned\n            i -= 1\n            j -= 1\n        elif dp[i][j] == dp[i-1][j] + 1:\n            s_aligned = s[i-1] + s_aligned\n            t_aligned = \"-\" + t_aligned\n            i -= 1\n        elif dp[i][j] == dp[i][j-1] + 1:\n            s_aligned = \"-\" + s_aligned\n            t_aligned = t[j-1] + t_aligned\n            j -= 1\n        else:\n            s_aligned = s[i-1] + s_aligned\n            t_aligned = t[j-1] + t_aligned\n            i -= 1\n            j -= 1\n\n    # Handle any remaining characters\n    while i &gt; 0:\n        s_aligned = s[i-1] + s_aligned\n        t_aligned = \"-\" + t_aligned\n        i -= 1\n    while j &gt; 0:\n        s_aligned = \"-\" + s_aligned\n        t_aligned = t[j-1] + t_aligned\n        j -= 1\n\n    return dp[m][n], s_aligned, t_aligned\n\n# Sample input\nsample_input = \"\"\"\n&gt;Rosalind_43\nPRETTY\n&gt;Rosalind_97\nPRTTEIN\n\"\"\"\n\n# Proces the input\nsequences = parse_fasta(sample_input)\ns, t = sequences[0], sequences[1]\n\n# Compute edit distance and alignment\nedit_distance, s_aligned, t_aligned = edit_distance_alignment(s, t)\n\n# Print the results\nprint(edit_distance)\nprint(s_aligned)\nprint(t_aligned)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-10",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-10",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "58.4 Explanation of the Code",
    "text": "58.4 Explanation of the Code\n\nparse_fasta(fasta_str): Convert a FASTA format string into a list of sequences.\n\nSplit the input string into lines.\nCollect sequence lines into current_sequence until a new header line is encountered.\nAppend the complete sequence to sequences when a new header is found.\nReturn the list of sequences.\n\nedit_distance_alignment(s, t): Compute the edit distance and provide an optimal alignment of two sequences.\n\nInitialize DP Table: Set up a 2D table dp where dp[i][j] holds the minimum edit distance between the first i characters of s and the first j characters of t.\nFill DP Table: Use dynamic programming to calculate the edit distance considering substitutions, insertions, and deletions.\nTraceback: Build the aligned sequences by following the dp table from the bottom-right to the top-left, handling matches, insertions, and deletions.\nHandle Remaining Characters: If there are remaining characters in either string after the traceback, append them with gaps."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-58",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-58",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "59.1 Sample Dataset",
    "text": "59.1 Sample Dataset\n1988.21104821\n610.391039105\n738.485999105\n766.492149105\n863.544909105\n867.528589105\n992.587499105\n995.623549105\n1120.6824591\n1124.6661391\n1221.7188991\n1249.7250491\n1377.8200091"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-59",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-59",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "59.2 Sample Output",
    "text": "59.2 Sample Output\nKEKEP"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-55",
    "href": "posts/md/Rosalind_stronghold.html#solution-55",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "59.3 Solution",
    "text": "59.3 Solution\nimport random\nfrom typing import List, Tuple, Dict, Union\n\n# Amino acid mas mapping\namino_acid_masses: Dict[float, List[str]] = {\n    57.02146: [\"G\"], 71.03711: [\"A\"], 87.03203: [\"S\"], 97.05276: [\"P\"], 99.06841: [\"V\"],\n    101.04768: [\"T\"], 103.00919: [\"C\"], 113.08406: [\"I\", \"L\"], 114.04293: [\"N\"], 115.02694: [\"D\"],\n    128.05858: [\"Q\"], 128.09496: [\"K\"], 129.04259: [\"E\"], 131.04049: [\"M\"], 137.05891: [\"H\"],\n    147.06841: [\"F\"], 156.10111: [\"R\"], 163.06333: [\"Y\"], 186.07931: [\"W\"],\n}\n\ndef infer_peptide(n: int, parent_mass: float, ion_masses: List[float], peptides: List[str]) -&gt; List[str]:\n    \"\"\"\n    Infers peptide sequences based on given ion masses and the target peptide length.\n\n    :param n: Length of the peptide to be inferred.\n    :param parent_mass: Mas of the parent peptide.\n    :param ion_masses: List of ion masses representing b-ions and y-ions.\n    :param peptides: List of current peptide candidates.\n    :return: List of inferred peptide sequences.\n    \"\"\"\n    if len(peptides[0]) == n:\n        return peptides\n\n    possible_ions = []  # List to store possible amino acids between ion pairs\n\n    # Find possible amino acids between ion pairs\n    for i in range(len(ion_masses) - 1):\n        for j in range(i + 1, len(ion_masses)):\n            delta_mas = round(ion_masses[j] - ion_masses[i], 5)\n            amino_acids = amino_acid_masses.get(delta_mass, [])\n            if amino_acids:\n                possible_ions.append((i, j, amino_acids))\n    \n    if possible_ions:\n        # Update ion masses and peptide candidates\n        new_ion_masses = ion_masses[possible_ions[0][1]:]\n        new_amino_acids = possible_ions[0][2]\n        new_peptides = [peptide + aa for peptide in peptides for aa in new_amino_acids]\n        \n        # Recursively infer peptide sequences\n        return infer_peptide(n, parent_mass, new_ion_masses, new_peptides)\n\n    return peptides\n\n# Sample input\nsample_input = \"\"\"\n1988.21104821\n610.391039105\n738.485999105\n766.492149105\n863.544909105\n867.528589105\n992.587499105\n995.623549105\n1120.6824591\n1124.6661391\n1221.7188991\n1249.7250491\n1377.8200091\n\"\"\"\n\n# Parse input data\ninput_lines = [float(line) for line in sample_input.strip().split(\"\\n\")]\nparent_mass, ion_masses = input_lines[0], input_lines[1:]\n\n# Determine the length of the peptide\npeptide_length = (len(ion_masses) - 2) // 2\n\n# Infer peptide sequences\npossible_peptides = infer_peptide(peptide_length, parent_mass, ion_masses, [\"\"])\n\n# Print a random peptide sequence\nprint(random.choice(possible_peptides))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-59",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-59",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "60.1 Sample Dataset",
    "text": "60.1 Sample Dataset\n5"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-60",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-60",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "60.2 Sample Output",
    "text": "60.2 Sample Output\n0.000 -0.005 -0.024 -0.082 -0.206 -0.424 -0.765 -1.262 -1.969 -3.010"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-56",
    "href": "posts/md/Rosalind_stronghold.html#solution-56",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "60.3 Solution",
    "text": "60.3 Solution\nimport math\n\ndef calculate_shared_chromosome_probabilities(sample_input: str):\n    \"\"\"\n    Calculate the common logarithm of the probability that two diploid siblings\n    share at least k of their 2n chromosomes, given n.\n    \n    Args:\n    - sample_input (str): The input string representing the value of n.\n    \n    Returns:\n    - List of float: Logarithm base 10 of the cumulative probabilities.\n    \"\"\"\n    # Parse the sample input to an integer\n    n = int(sample_input.strip())\n\n    # Probability of sharing each chromosome (independent coin flip)\n    p = 0.5\n\n    # Initialize the cumulative probability and the result array\n    Pr = 0\n    A = []\n\n    # Loop from 2*n down to 1 (inclusive) to calculate cumulative probabilities\n    for k in range(2 * n, 0, -1):\n        # Calculate the binomial coefficient: C(2n, k)\n        binom_coeff = math.factorial(2 * n) / (math.factorial(k) * math.factorial(2 * n - k))\n\n        # Calculate the probability of exactly k shared chromosomes\n        Pr += binom_coeff * math.pow(p, k) * math.pow(1 - p, 2 * n - k)\n\n        # Append the common logarithm (base 10) of the cumulative probability to the result array\n        A.append(math.log10(Pr))\n\n    # Return the result array in reverse order\n    return [round(value, 3) for value in A[::-1]]\n\n# Example usage\nsample_input = \"5\"\nresult = calculate_shared_chromosome_probabilities(sample_input)\nprint(\" \".join(f\"{value:3f}\" for value in result))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-function",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-function",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "60.4 Explanation of the Function",
    "text": "60.4 Explanation of the Function\n\nFunction Definition:\n\nThe function calculate_shared_chromosome_probabilities takes a string sample_input.\n\nParse Input:\n\nsample_input is stripped of any surrounding whitespace and converted to an integer n.\n\nInitialize Variables:\n\np is set to 0.5, representing the probability of sharing each chromosome.\nPr is initialized to store the cumulative probability.\nA is an empty list to store the logarithms of cumulative probabilities.\n\nCalculate Cumulative Probabilities:\n\nLoop from 2*n down to 1 to calculate the cumulative probability for at least k shared chromosomes.\nFor each k, compute the binomial coefficient \\(C(2n, k)\\).\nCalculate the probability of exactly k shared chromosomes and add it to Pr.\nAppend the logarithm (base 10) of Pr to the list A.\n\nReturn the Result:\n\nReturn the values in A in reverse order, rounded to 3 decimal places.\n\nExample Usage:\n\nThe function is called with a sample input \"5\", and the results are printed in the specified format."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-60",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-60",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "61.1 Sample Dataset",
    "text": "61.1 Sample Dataset\nGACCACGGTT\nACAG\nGT\nCCG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-61",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-61",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "61.2 Sample Output",
    "text": "61.2 Sample Output\n0 0 1\n0 1 0\n1 0 0"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-57",
    "href": "posts/md/Rosalind_stronghold.html#solution-57",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "61.3 Solution",
    "text": "61.3 Solution\nimport numpy as np\n\ndef is_interwoven(dna1, dna2, superstr):\n    \"\"\"\n    Recursive function to check if dna1 and dna2 can be interwoven to form superstr.\n    \"\"\"\n    if len(superstr) == 0:\n        return True\n    elif dna1 and dna2 and dna1[0] == dna2[0] == superstr[0]:\n        return is_interwoven(dna1[1:], dna2, superstr[1:]) or is_interwoven(dna1, dna2[1:], superstr[1:])\n    elif dna1 and dna1[0] == superstr[0]:\n        return is_interwoven(dna1[1:], dna2, superstr[1:])\n    elif dna2 and dna2[0] == superstr[0]:\n        return is_interwoven(dna1, dna2[1:], superstr[1:])\n    else:\n        return False\n\ndef find_disjoint_motifs(super_string, patterns):\n    \"\"\"\n    Function to find the disjoint motifs matrix for the given super_string and patterns.\n    \"\"\"\n    n = len(patterns)\n    M = np.zeros((n, n), dtype=int)\n\n    for i in range(n):\n        for j in range(i, n):\n            pattern1 = patterns[i]\n            pattern2 = patterns[j]\n            combined_length = len(pattern1) + len(pattern2)\n            combined_profile = [pattern1.count(nuc) + pattern2.count(nuc) for nuc in \"ACGT\"]\n\n            for index in range(len(super_string) - combined_length + 1):\n                superstr_segment = super_string[index:index + combined_length]\n                superstr_profile = [superstr_segment.count(nuc) for nuc in \"ACGT\"]\n\n                if combined_profile == superstr_profile:\n                    if is_interwoven(pattern1 + '$', pattern2 + '$', superstr_segment):\n                        M[i][j] = 1\n                        break\n\n            if i != j:\n                M[j][i] = M[i][j]\n\n    return M\n\n# Sample dataset\nsample_input = \"\"\"\nGACCACGGTT\nACAG\nGT\nCCG\n\"\"\"\ndata = sample_input.strip().split()\nsuper_string = data[0]\npatterns = data[1:]\n\n# Find the disjoint motifs matrix\nresult_matrix = find_disjoint_motifs(super_string, patterns)\n\n# Print the result matrix\nfor row in result_matrix:\n    print(\" \".join(map(str, row)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-30",
    "href": "posts/md/Rosalind_stronghold.html#explanation-30",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "61.4 Explanation",
    "text": "61.4 Explanation\n\nis_interwoven Function:\n\nPurpose: To check if dna1 and dna2 can be interwoven to form superstr.\nParameters: dna1, dna2, and superstr.\nLogic:\n\nIf superstr is empty, return True because the interweaving is complete.\nIf both dna1 and dna2 are non-empty and their first characters match the first character of superstr, recursively check both possibilities (taking from dna1 or dna2).\nIf the first character of dna1 matches the first character of superstr, recursively check the remaining parts.\nIf the first character of dna2 matches the first character of superstr, recursively check the remaining parts.\nIf none of the above conditions are met, return False.\n\n\nfind_disjoint_motifs Function:\n\nPurpose: To find the disjoint motifs matrix for the given super_string and patterns.\nParameters: super_string and patterns.\nLogic:\n\nInitialize a zero matrix M of size n x n where n is the number of patterns.\nFor each pair of patterns pattern1 and pattern2, calculate their combined length and nucleotide profile.\nIterate over all possible substrings of super_string of the same length.\nCompare the nucleotide profile of the substring with the combined profile.\nIf they match, use is_interwoven to check if they can be interwoven to form the substring.\nUpdate the matrix M accordingly.\nSince the comparison is symmetric, update both M[i][j] and M[j][i].\n\n\nMain Execution:\n\nParse the input to extract the super_string and patterns.\nCall find_disjoint_motifs to get the result matrix.\nPrint the result matrix."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-61",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-61",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "62.1 Sample Dataset",
    "text": "62.1 Sample Dataset\nCATACATAC$\n2\nnode1 node2 1 1\nnode1 node7 2 1\nnode1 node14 3 3\nnode1 node17 10 1\nnode2 node3 2 4\nnode2 node6 10 1\nnode3 node4 6 5\nnode3 node5 10 1\nnode7 node8 3 3\nnode7 node11 5 1\nnode8 node9 6 5\nnode8 node10 10 1\nnode11 node12 6 5\nnode11 node13 10 1\nnode14 node15 6 5\nnode14 node16 10 1"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-62",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-62",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "62.2 Sample Output",
    "text": "62.2 Sample Output\nCATAC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-58",
    "href": "posts/md/Rosalind_stronghold.html#solution-58",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "62.3 Solution",
    "text": "62.3 Solution\nTo solve this problem, we need to find the longest substring that appears at least k times in a given string s, using its suffix tree. Here’s how we can approach the problem step-by-step:"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-approach-the-problem",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-approach-the-problem",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "62.4 Steps to Approach the Problem",
    "text": "62.4 Steps to Approach the Problem\n\nParse the Input:\n\nRead the DNA string s with $ appended.\nRead the integer k.\nParse the edges defining the suffix tree.\n\nSuffix Tree Representation:\n\nUse the given edges to construct the suffix tree.\nEach edge contains information about the parent node, child node, start position of the substring in s, and the length of the substring.\n\nTraverse the Suffix Tree:\n\nTraverse the tree to count the occurrences of substrings.\nUse a depth-first search (DFS) to explore all paths from the root to the leaves.\n\nIdentify the Longest Substring with at least k Occurrences:\n\nTrack the longest substring that meets the condition during the traversal."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation-6",
    "href": "posts/md/Rosalind_stronghold.html#implementation-6",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "62.5 Implementation",
    "text": "62.5 Implementation\nHere’s the Python code to implement the solution:\nclas SuffixTreeNode:\n    def __init__(self):\n        self.children = {}\n        self.start = -1\n        self.length = -1\n        self.parent = None\n\ndef build_suffix_tree(edges, s):\n    nodes = {}\n    for edge in edges:\n        parent, child, start, length = edge\n        if parent not in nodes:\n            nodes[parent] = SuffixTreeNode()\n        if child not in nodes:\n            nodes[child] = SuffixTreeNode()\n        nodes[child].parent = nodes[parent]\n        nodes[child].start = start - 1  # Convert to zero-based index\n        nodes[child].length = length\n        nodes[parent].children[child] = nodes[child]\n    return nodes\n\ndef dfs(node, s, k, path, results):\n    if len(node.children) == 0:\n        return 1  # Leaf node\n    \n    count = 0\n    for child in node.children.values():\n        count += dfs(child, s, k, path + s[child.start:child.start + child.length], results)\n    \n    if count &gt;= k:\n        results.append((path, len(path)))\n    \n    return count\n\ndef longest_k_fold_substring(s, k, edges):\n    nodes = build_suffix_tree(edges, s)\n    root = nodes['node1']\n    results = []\n    dfs(root, s, k, \"\", results)\n    \n    results.sort(key=lambda x: x[1], reverse=True)\n    \n    return results[0][0] if results else \"\"\n\n# Sample input\nsample_input = \"\"\"\nCATACATAC$\n2\nnode1 node2 1 1\nnode1 node7 2 1\nnode1 node14 3 3\nnode1 node17 10 1\nnode2 node3 2 4\nnode2 node6 10 1\nnode3 node4 6 5\nnode3 node5 10 1\nnode7 node8 3 3\nnode7 node11 5 1\nnode8 node9 6 5\nnode8 node10 10 1\nnode11 node12 6 5\nnode11 node13 10 1\nnode14 node15 6 5\nnode14 node16 10 1\n\"\"\"\n\ndata = sample_input.strip().split('\\n')\ns = data[0]\nk = int(data[1])\nedges = [tuple(line.split()) for line in data[2:]]\nedges = [(e[0], e[1], int(e[2]), int(e[3])) for e in edges]\n\nresult = longest_k_fold_substring(s, k, edges)\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-31",
    "href": "posts/md/Rosalind_stronghold.html#explanation-31",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "62.6 Explanation",
    "text": "62.6 Explanation\n\nSuffixTreeNode Class:\n\nA clas to represent each node in the suffix tree.\n\nbuild_suffix_tree Function:\n\nConstructs the suffix tree using the given edges.\n\ndfs Function:\n\nPerforms a depth-first search to count the occurrences of substrings and keep track of the valid ones.\n\nlongest_k_fold_substring Function:\n\nBuilds the suffix tree, performs DFS, and identifies the longest substring with at least k occurrences.\n\nSample Input:\n\nParses the input and invokes the longest_k_fold_substring function to find and print the result."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-62",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-62",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "63.1 Sample Dataset",
    "text": "63.1 Sample Dataset\n(dog:42,cat:33);\ncat dog\n\n((dog:4,cat:3):74,robot:98,elephant:58);\ndog elephant"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-63",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-63",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "63.2 Sample Output",
    "text": "63.2 Sample Output\n75 136"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-59",
    "href": "posts/md/Rosalind_stronghold.html#solution-59",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "63.3 Solution",
    "text": "63.3 Solution\nclas Node:\n    def __init__(self, number, parent, name=None):\n        self.number = number\n        self.parent = parent\n        self.name = \"Node_\" + str(self.number) if name is None else name\n\n    def __repr__(self):\n        tmp = \"\"\n        if self.name != \"Node_\" + str(self.number):\n            tmp = f\"({self.name})\"\n        return f\"Node_{self.number}{tmp}\"\n\n\nclas WeightedNewick:\n    def __init__(self, data):\n        self.nodes = []\n        self.edge_weight = {}\n        self.construct_tree(data)\n        self.name_index = {node.name: node.number for node in self.nodes}\n\n    def construct_tree(self, data):\n        \"\"\"Constructs the Newick Tree from the input data.\"\"\"\n        data = data.replace(',', ' ').replace('(', '( ').replace(')', ' )').strip(';').split()\n        current_parent = Node(-1, None)\n        for item in data:\n            if item[0] == '(':\n                # New internal node\n                current_parent = Node(len(self.nodes), current_parent.number)\n                self.nodes.append(current_parent)\n            elif item[0] == ')':\n                # End of a subtree, backtrack to parent\n                if len(item) &gt; 1:\n                    self.edge_weight[(current_parent.number, current_parent.parent)] = int(item[item.find(':') + 1:])\n                    if len(item) &gt; 2:\n                        current_parent.name = item[1:item.find(':')]\n                current_parent = self.nodes[current_parent.parent]\n            else:\n                # Leaf node\n                self.edge_weight[(len(self.nodes), current_parent.number)] = int(item[item.find(':') + 1:])\n                self.nodes.append(Node(len(self.nodes), current_parent.number, item[:item.find(':')]))\n\n    def distance(self, name1, name2):\n        \"\"\"Returns the distance between nodes with names name1 and name2.\"\"\"\n        if name1 == name2:\n            return 0\n\n        # Create the branches from the two desired nodes to the root\n        idx1 = self.name_index[name1]\n        branch1 = [(idx1, self.nodes[idx1].parent)]\n        idx2 = self.name_index[name2]\n        branch2 = [(idx2, self.nodes[idx2].parent)]\n\n        # Trace the path to the root for both nodes\n        while branch1[-1][1] != -1:\n            current_idx = branch1[-1][1]\n            branch1.append((current_idx, self.nodes[current_idx].parent))\n        while branch2[-1][1] != -1:\n            current_idx = branch2[-1][1]\n            branch2.append((current_idx, self.nodes[current_idx].parent))\n\n        # Calculate the distance as the sum of edge weights in the symmetric difference of paths\n        return sum([self.edge_weight[edge] for edge in set(branch1) ^ set(branch2)])\n\n\n# Sample input\nsample_input = \"\"\"\n(dog:42,cat:33);\ncat dog\n\n((dog:4,cat:3):74,robot:98,elephant:58);\ndog elephant\n\"\"\"\ninput_lines = sample_input.strip().split('\\n')\n\n# Compute distances between pairs of nodes in each tree\ndistance_list = []\nfor i in range(0, len(input_lines) - 1, 3):\n    tree = input_lines[i]\n    nodeA, nodeB = input_lines[i + 1].split()\n    distance_list.append(str(WeightedNewick(tree).distance(nodeA, nodeB)))\n\n# Print the computed distances\nprint(\" \".join(distance_list))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-32",
    "href": "posts/md/Rosalind_stronghold.html#explanation-32",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "63.4 Explanation",
    "text": "63.4 Explanation\n\nNode Class:\n\nThis clas represents a node in the tree.\nEach node has a number, a parent, and an optional name.\nThe __repr__ method provides a string representation of the node.\n\nWeightedNewick Class:\n\nThis clas constructs a tree from a Newick string and provides functionality to compute distances between nodes.\n__init__: Initializes the tree, constructs it from the input data, and creates a mapping from node names to their indices.\nconstruct_tree: Parses the Newick string to build the tree structure and store edge weights.\ndistance: Computes the distance between two nodes by tracing their paths to the root and summing the edge weights in the symmetric difference of these paths.\n\nMain Execution:\n\nThe sample input is split into lines, and the trees and node pairs are extracted.\nFor each tree and node pair, a WeightedNewick object is created, and the distance between the specified nodes is computed.\nThe distances are printed in the required format."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-63",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-63",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "64.1 Sample Dataset",
    "text": "64.1 Sample Dataset\nAUGCUAGUACGGAGCGAGUCUAGCGAGCGAUGUCGUGAGUACUAUAUAUGCGCAUAAGCCACGU"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-64",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-64",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "64.2 Sample Output",
    "text": "64.2 Sample Output\n284850219977421"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-60",
    "href": "posts/md/Rosalind_stronghold.html#solution-60",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "64.3 Solution",
    "text": "64.3 Solution\nTo solve the problem of counting distinct valid matchings of basepair edges in an RNA string considering wobble base pairing (G-U pairs) and noncrossing constraints, we can use a dynamic programming approach. Below is a detailed explanation and implementation in Python:"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#approach-2",
    "href": "posts/md/Rosalind_stronghold.html#approach-2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "64.4 Approach",
    "text": "64.4 Approach\n\nDynamic Programming (DP) Setup:\n\nDefine a 2D DP table dp where dp[i][j] represents the number of valid matchings for the substring of the RNA sequence from index i to index j.\nBase case: dp[i][i-1] = 1 for all i because an empty substring has one valid matching (the empty matching).\n\nRecursive Relation:\n\nFor each pair of indices i and j such that i &lt; j, we consider the possibility of the base at position i pairing with any valid base at position k where i &lt; k &lt;= j and k &gt;= i + 4 (to respect the distance constraint).\nThe RNA bases can pair if they form a valid pair: A-U, U-A, C-G, G-C, G-U, or U-G.\nThe number of matchings for substring s[i:j+1] is calculated by splitting it into the matchings between s[i] and s[k] and recursively solving for the substrings s[i+1:k-1] and s[k+1:j].\n\nIterative Calculation:\n\nFill in the DP table iteratively, starting from smaller substrings and building up to the entire string."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation-7",
    "href": "posts/md/Rosalind_stronghold.html#implementation-7",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "64.5 Implementation",
    "text": "64.5 Implementation\ndef count_valid_matchings(rna):\n    n = len(rna)\n    dp = [[0] * n for _ in range(n)]\n\n    def can_pair(b1, b2):\n        return (b1 == 'A' and b2 == 'U') or (b1 == 'U' and b2 == 'A') or \\\n               (b1 == 'C' and b2 == 'G') or (b1 == 'G' and b2 == 'C') or \\\n               (b1 == 'G' and b2 == 'U') or (b1 == 'U' and b2 == 'G')\n\n    for i in range(n):\n        dp[i][i] = 1  # A single base has one valid matching (itself)\n\n    for length in range(1, n + 1):  # length is the length of the substring\n        for i in range(n - length):\n            j = i + length\n            dp[i][j] = dp[i+1][j]  # Case where s[i] is not paired\n            for k in range(i + 4, j + 1):\n                if can_pair(rna[i], rna[k]):\n                    if k == j:\n                        dp[i][j] += dp[i+1][k-1]\n                    else:\n                        dp[i][j] += dp[i+1][k-1] * dp[k+1][j]\n\n    return dp[0][n-1]\n\n# Sample Dataset\nrna = \"AUGCUAGUACGGAGCGAGUCUAGCGAGCGAUGUCGUGAGUACUAUAUAUGCGCAUAAGCCACGU\"\n\n# Output the result\nprint(count_valid_matchings(rna))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-33",
    "href": "posts/md/Rosalind_stronghold.html#explanation-33",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "64.6 Explanation",
    "text": "64.6 Explanation\n\nBase Case:\n\nEach single base (or empty substring) has one valid matching (dp[i][i] = 1).\n\nFilling DP Table:\n\nFor each possible substring length, calculate the number of valid matchings by considering all possible pairs for the first base and ensuring the substrings formed by removing the matched bases also have valid matchings.\n\nHelper Function:\n\ncan_pair checks if two bases can pair according to the given rules including wobble base pairing."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-64",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-64",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "65.1 Sample Dataset",
    "text": "65.1 Sample Dataset\n0.1 0.25 0.5"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-65",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-65",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "65.2 Sample Output",
    "text": "65.2 Sample Output\n0.532 0.75 0.914"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-61",
    "href": "posts/md/Rosalind_stronghold.html#solution-61",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "65.3 Solution",
    "text": "65.3 Solution\nTo solve the problem of determining the probability that a randomly selected individual carries at least one copy of the recessive allele for each Mendelian factor, we need to work with the Hardy-Weinberg equilibrium principles."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#steps-to-solve",
    "href": "posts/md/Rosalind_stronghold.html#steps-to-solve",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "65.4 Steps to Solve",
    "text": "65.4 Steps to Solve\n\nGiven Data:\n\nArray \\(A\\) where \\(A[k]\\) is the proportion of homozygous recessive individuals for the \\(k\\)-th factor.\nWe need to find an array \\(B\\) where \\(B[k]\\) is the probability that a randomly selected individual carries at least one copy of the recessive allele for the \\(k\\)-th factor.\n\nHardy-Weinberg Principle:\n\nIn genetic equilibrium, the proportion of homozygous recessive individuals (denoted \\(q^2\\)) is \\(A[k]\\).\nThe recessive allele frequency \\(q\\) is the square root of \\(A[k]\\), i.e., \\(q = \\sqrt{A[k]}\\).\nThe dominant allele frequency \\(p\\) is \\(1 - q\\).\n\nCarrier Probability:\n\nThe probability that an individual carries at least one recessive allele (i.e., they are either heterozygous or homozygous recessive) is given by \\(1 - p^2\\).\nSince \\(p = 1 - q\\), the carrier probability becomes \\(1 - (1 - q)^2\\).\n\nCalculation:\n\nFor each \\(k\\): \\[ q = \\sqrt{A[k]} \\] \\[ B[k] = 1 - (1 - q)^2 \\]"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#implementation-8",
    "href": "posts/md/Rosalind_stronghold.html#implementation-8",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "65.5 Implementation",
    "text": "65.5 Implementation\nHere’s how you can implement this in Python:\nimport math\n\ndef calculate_carrier_probability(A):\n    B = []\n    for q_squared in A:\n        q = math.sqrt(q_squared)\n        p = 1 - q\n        carrier_probability = 1 - p**2\n        B.append(carrier_probability)\n    return B\n\n# Sample Dataset\nsample_input = \"\"\"\n0.1 0.25 0.5\"\"\"\nA = [float(x) for x in sample_input.strip().split()]\n\n# Calculate the carrier probabilities\nB = calculate_carrier_probability(A)\n\n# Print the results formatted to three decimal places\nprint(\" \".join(f\"{prob:f}\" for prob in B))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-34",
    "href": "posts/md/Rosalind_stronghold.html#explanation-34",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "65.6 Explanation",
    "text": "65.6 Explanation\n\nFunction Definition:\n\ncalculate_carrier_probability takes the array \\(A\\) as input and returns the array \\(B\\).\n\nLoop Through \\(A\\):\n\nFor each element in \\(A\\):\n\nCompute \\(q\\) as the square root of the element.\nCompute \\(p\\) as \\(1 - q\\).\nCompute the carrier probability using \\(1 - p^2\\).\nAppend the result to \\(B\\).\n\n\nOutput:\n\nFormat the output to three decimal places for better readability."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-65",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-65",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "66.1 Sample Dataset",
    "text": "66.1 Sample Dataset\nATGCTACC\nCGTTTACC\nATTCGACC\nAGTCTCCC\nCGTCTATC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-66",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-66",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "66.2 Sample Output",
    "text": "66.2 Sample Output\n10110\n10100"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-62",
    "href": "posts/md/Rosalind_stronghold.html#solution-62",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "66.3 Solution",
    "text": "66.3 Solution\ndef char_table_from_strings(dna_list):\n    \"\"\"\n    Builds a character table from a given list of DNA strings.\n    \n    Parameters:\n    dna_list (list of str): A list of DNA strings.\n    \n    Returns:\n    set of str: A set containing nontrivial character rows.\n    \"\"\"\n    character_table = set()\n    \n    # Iterate over each position in the DNA strings\n    for pos in range(len(dna_list[0])):\n        # Determine the reference character at the current position from the first DNA string\n        ref_char = dna_list[0][pos]\n        \n        # Create a binary array indicating the presence of the reference character at the current position\n        char_array = [int(dna[pos] == ref_char) for dna in dna_list]\n        \n        # Check if the character array is nontrivial (i.e., it has both 0s and 1s but not all 0s or all 1s)\n        if 1 &lt; sum(char_array) &lt; len(dna_list) - 1:\n            # Convert the binary array to a string and add it to the character table\n            character_table.add(''.join(map(str, char_array)))\n    \n    return character_table\n\n# Sample input\nsample_input = \"\"\"\nATGCTACC\nCGTTTACC\nATTCGACC\nAGTCTCCC\nCGTCTATC\n\"\"\"\n\n# Convert the input string to a list of DNA strings\ndna_list = sample_input.strip().split(\"\\n\")\n\n# Get the character table\ncharacter_table = char_table_from_strings(dna_list)\n\n# Print the character table\nfor row in character_table:\n    print(row)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-35",
    "href": "posts/md/Rosalind_stronghold.html#explanation-35",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "66.4 Explanation",
    "text": "66.4 Explanation\n\nFunction Definition: The function char_table_from_strings is defined to take a list of DNA strings and return a set of nontrivial character rows.\nInitialize Character Table: An empty set character_table is initialized to store the nontrivial character rows.\nIterate Over Positions: A loop iterates over each position in the DNA strings. The length of the first string is used to determine the number of positions.\nReference Character: For each position, the reference character ref_char is taken from the first DNA string.\nCreate Binary Array: A binary array char_array is created using a list comprehension. For each DNA string, it checks if the character at the current position matches the reference character and records 1 if it does and 0 if it doesn’t.\nCheck Nontrivial Condition: The array is considered nontrivial if it contains both 0s and 1s but is not all 0s or all 1s. This is checked using the condition 1 &lt; sum(char_array) &lt; len(dna_list) - 1.\nAdd to Character Table: If the binary array is nontrivial, it is converted to a string and added to the character_table set.\nReturn Character Table: The function returns the character_table set containing all nontrivial character rows.\nSample Input: The sample input is given as a multiline string, which is converted to a list of DNA strings by stripping and splitting by newline characters.\nGenerate and Print Character Table: The character table is generated by calling the function and printed row by row."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-66",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-66",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "67.1 Sample Dataset",
    "text": "67.1 Sample Dataset\n&gt;Rosalind_78\nPLEASANTLY\n&gt;Rosalind_33\nMEANLY"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-67",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-67",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "67.2 Sample Output",
    "text": "67.2 Sample Output\n4"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-63",
    "href": "posts/md/Rosalind_stronghold.html#solution-63",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "67.3 Solution",
    "text": "67.3 Solution\ndef parse_fasta(fasta_string):\n    '''Parses a FASTA format string and returns the sequences in a list.'''\n    sequences = []\n    sequence = []\n\n    for line in fasta_string.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if sequence:\n                sequences.append(''.join(sequence))\n                sequence = []\n        else:\n            sequence.append(line.strip())\n\n    if sequence:\n        sequences.append(''.join(sequence))\n    \n    return sequences\n\ndef count_optimal_alignments(s, t):\n    '''Counts the total number of optimal alignments of s and t with respect to edit alignment score.'''\n    MOD = 134217727  # Modulo value to prevent overflow\n    \n    m, n = len(s), len(t)\n    \n    # Initialize DP tables\n    dp = [[0] * (n + 1) for _ in range(m + 1)]  # Table for edit distances\n    count = [[0] * (n + 1) for _ in range(m + 1)]  # Table for counting optimal alignments\n    \n    # Base cases: edit distance and count for aligning to empty string\n    for i in range(m + 1):\n        dp[i][0] = i\n        count[i][0] = 1\n    \n    for j in range(n + 1):\n        dp[0][j] = j\n        count[0][j] = 1\n    \n    # Fill the DP tables\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # Calculate the cost of insert, delete, and replace operations\n            insert_cost = dp[i][j-1] + 1\n            delete_cost = dp[i-1][j] + 1\n            replace_cost = dp[i-1][j-1] + (0 if s[i-1] == t[j-1] else 1)\n            \n            # Find the minimum cost among the three operations\n            dp[i][j] = min(insert_cost, delete_cost, replace_cost)\n            \n            # Count the number of ways to achieve this minimum cost\n            if dp[i][j] == insert_cost:\n                count[i][j] += count[i][j-1]\n            if dp[i][j] == delete_cost:\n                count[i][j] += count[i-1][j]\n            if dp[i][j] == replace_cost:\n                count[i][j] += count[i-1][j-1]\n            \n            # Apply the modulo to keep the count manageable\n            count[i][j] %= MOD\n    \n    return count[m][n]\n\n# Sample dataset in FASTA format\nsample_input = \"\"\"\n&gt;Rosalind_78\nPLEASANTLY\n&gt;Rosalind_33\nMEANLY\n\"\"\"\n\n# Parse the FASTA input to get the sequences\nsequences = parse_fasta(sample_input)\ns, t = sequences[0], sequences[1]\n\n# Get the number of optimal alignments\nresult = count_optimal_alignments(s, t)\n\n# Print the result\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-36",
    "href": "posts/md/Rosalind_stronghold.html#explanation-36",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "67.4 Explanation",
    "text": "67.4 Explanation\n\nFASTA Parsing (parse_fasta):\n\nThis function reads a FASTA formatted string and extracts sequences.\nIt initializes an empty list sequences to store the parsed sequences and another list sequence to build each sequence.\nIt iterates over each line of the input string:\n\nIf a line starts with &gt;, it indicates a new sequence header. If sequence is not empty, it joins its elements into a single string and adds it to sequences.\nIf a line does not start with &gt;, it is part of a sequence, so it is added to sequence.\n\nAfter the loop, any remaining sequence is added to sequences.\nThe function returns the list of sequences.\n\nCounting Optimal Alignments (count_optimal_alignments):\n\nThis function calculates the number of optimal alignments between two strings s and t.\nMOD is a large prime number used to keep the counts within manageable limits.\ndp is a table where dp[i][j] stores the minimum edit distance between the first i characters of s and the first j characters of t.\ncount is a table where count[i][j] stores the number of optimal alignments that result in the minimum edit distance for the first i characters of s and the first j characters of t.\nThe base cases initialize the first row and first column of dp and count to represent alignments with an empty string.\nThe nested loops fill in the dp and count tables by considering insertion, deletion, and replacement operations.\nThe minimum cost operation is selected, and the number of ways to achieve this cost is counted.\nThe result is the number of optimal alignments for the entire strings s and t, stored in count[m][n].\n\nMain Execution:\n\nThe sample input is given in FASTA format.\nThe parse_fasta function is called to extract the sequences.\nThe count_optimal_alignments function is called with the parsed sequences to get the number of optimal alignments.\nThe result is printed."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-67",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-67",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "68.1 Sample Dataset",
    "text": "68.1 Sample Dataset\n5"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-68",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-68",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "68.2 Sample Output",
    "text": "68.2 Sample Output\n15"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-64",
    "href": "posts/md/Rosalind_stronghold.html#solution-64",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "68.3 Solution",
    "text": "68.3 Solution\nimport functools\n\ndef count_unrooted_binary_trees(num_leaves: int) -&gt; int:\n    \"\"\"\n    Returns the number of unrooted binary trees with num_leaves leaves, \n    modulo 1,000,000.\n    \"\"\"\n    MODULO = 10**6\n\n    def double_factorial(n: int) -&gt; int:\n        result = 1\n        for i in range(n, 1, -2):\n            result = (result * i) % MODULO\n        return result\n\n    return double_factorial(2 * num_leaves - 5)\n\n# Sample input\nsample_input = \"5\"\nnum_leaves = int(sample_input.strip())\n\n# Get the number of unrooted binary trees\ntree_count = count_unrooted_binary_trees(num_leaves)\n\n# Print the result\nprint(tree_count)\n##$ Explanation\n\nDouble Factorial Calculation without Lambda:\n\nThe double_factorial function is now implemented using a simple for loop.\nThe loop iterates over the range from n down to 1, stepping by -2 (to get only odd numbers).\nIn each iteration, the current result is multiplied by i and taken modulo (10^6).\nThis avoids the use of a lambda function and functools.reduce.\n\nRest of the Code:\n\nThe rest of the code remains unchanged.\nThe count_unrooted_binary_trees function calls double_factorial with 2 * num_leaves - 5 to compute the number of unrooted binary trees.\nThe result is printed after parsing the sample input."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-68",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-68",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "69.1 Sample Dataset",
    "text": "69.1 Sample Dataset\n&gt;Rosalind_67\nPLEASANTLY\n&gt;Rosalind_17\nMEANLY"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-69",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-69",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "69.2 Sample Output",
    "text": "69.2 Sample Output\n8"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-65",
    "href": "posts/md/Rosalind_stronghold.html#solution-65",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "69.3 Solution",
    "text": "69.3 Solution\nTo solve the problem of finding the maximum alignment score between two protein strings using the BLOSUM62 scoring matrix and a linear gap penalty of 5, we need to implement the Needleman-Wunsch algorithm for global sequence alignment. This involves dynamic programming to compute the optimal alignment score.\nHere is the step-by-step explanation of the solution along with the Python code implementation:\n\nParse the FASTA input to extract the two protein sequences.\nSet up the BLOSUM62 scoring matrix.\nImplement the Needleman-Wunsch algorithm to compute the alignment score using the given scoring matrix and gap penalty.\n\ndef parse_fasta(fasta_string):\n    '''Parses a FASTA format string and returns the sequences in a list.'''\n    sequences = []\n    sequence = []\n    for line in fasta_string.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if sequence:\n                sequences.append(''.join(sequence))\n                sequence = []\n        else:\n            sequence.append(line.strip())\n    if sequence:\n        sequences.append(''.join(sequence))\n    return sequences\n\n# BLOSUM62 matrix\nblosum62_str = \"\"\"\nA  C  D  E  F  G  H  I  K  L  M  N  P  Q  R  S  T  V  W  Y\nA  4  0 -2 -1 -2  0 -2 -1 -1 -1 -1 -2 -1 -1 -1  1  0  0 -3 -2\nC  0  9 -3 -4 -2 -3 -3 -1 -3 -1 -1 -3 -3 -3 -3 -1 -1 -1 -2 -2\nD -2 -3  6  2 -3 -1 -1 -3 -1 -4 -3  1 -1  0 -2  0 -1 -3 -4 -3\nE -1 -4  2  5 -3 -2  0 -3  1 -2 -2  0 -1  2  0  0 -1 -2 -3 -2\nF -2 -2 -3 -3  6 -3 -1  0 -3  0  0 -3 -4 -3 -3 -2 -2 -1  1  3\nG  0 -3 -1 -2 -3  6 -2 -4 -2 -4 -3  0 -2 -2 -3  0 -2 -3 -2 -3\nH -2 -3 -1  0 -1 -2  8 -3 -1 -3 -2  1 -2  0  0 -1 -2 -3 -2  2\nI -1 -1 -3 -3  0 -4 -3  4 -3  2  1 -3 -3 -3 -3 -2 -1  3 -3 -1\nK -1 -3 -1  1 -3 -2 -1 -3  5 -2 -1  0 -1  1  2  0 -1 -2 -3 -2\nL -1 -1 -4 -2  0 -4 -3  2 -2  4  2 -3 -3 -2 -2 -2 -1  1 -2 -1\nM -1 -1 -3 -2  0 -3 -2  1 -1  2  5 -2 -2  0 -1 -1 -1  1 -1 -1\nN -2 -3  1  0 -3  0  1 -3  0 -3 -2  6 -2  0  0  1  0 -3 -4 -2\nP -1 -3 -1 -1 -4 -2 -2 -3 -1 -3 -2 -2  7 -1 -2 -1 -1 -3 -4 -3\nQ -1 -3  0  2 -3 -2  0 -3  1 -2  0  0 -1  5  1  0 -1 -2 -2 -1\nR -1 -3 -2  0 -3 -3  0 -3  2 -2 -1  0 -2  1  5 -1 -1 -3 -3 -2\nS  1 -1  0  0 -2  0 -1 -2  0 -2 -1  1 -1  0 -1  4  1 -2 -3 -2\nT  0 -1 -1 -1 -2 -2 -2 -1 -1 -1 -1  0 -1 -1 -1  1  5  0 -2 -2\nV  0 -1 -3 -2 -1 -3 -3  3 -2  1  1 -3 -3 -2 -3 -2  0  4 -3 -1\nW -3 -2 -4 -3  1 -2 -2 -3 -3 -2 -1 -4 -4 -2 -3 -3 -2 -3 11  2\nY -2 -2 -3 -2  3 -3  2 -1 -2 -1 -1 -2 -3 -1 -2 -2 -2 -1  2  7\n\"\"\"\n\ndef parse_blosum62(matrix_str):\n    \"\"\"Parse the BLOSUM62 matrix from a string.\"\"\"\n    lines = matrix_str.strip().split('\\n')\n    headers = lines[0].split()\n    matrix = {}\n    for line in lines[1:]:\n        values = line.split()\n        row = values[0]\n        scores = list(map(int, values[1:]))\n        for col, score in zip(headers, scores):\n            matrix[(row, col)] = score\n    return matrix\n\ndef needleman_wunsch(s, t, blosum62, gap_penalty):\n    \"\"\"Perform the Needleman-Wunsch algorithm for global alignment.\"\"\"\n    m, n = len(s), len(t)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n    # Initialize dp table with gap penalties\n    for i in range(1, m + 1):\n        dp[i][0] = dp[i - 1][0] + gap_penalty\n    for j in range(1, n + 1):\n        dp[0][j] = dp[0][j - 1] + gap_penalty\n\n    # Fill the dp table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            match = dp[i - 1][j - 1] + blosum62[(s[i - 1], t[j - 1])]\n            delete = dp[i - 1][j] + gap_penalty\n            insert = dp[i][j - 1] + gap_penalty\n            dp[i][j] = max(match, delete, insert)\n\n    return dp[m][n]\n\n# Sample dataset in FASTA format\nsample_input = \"\"\"\n&gt;Rosalind_67\nPLEASANTLY\n&gt;Rosalind_17\nMEANLY\n\"\"\"\n\n# Parse the FASTA input to get the sequences\nsequences = parse_fasta(sample_input)\ns, t = sequences[0], sequences[1]\n\n# Parse the BLOSUM62 matrix\nblosum62 = parse_blosum62(blosum62_str)\n\n# Set the gap penalty\ngap_penalty = -5\n\n# Get the maximum alignment score using Needleman-Wunsch algorithm\nresult = needleman_wunsch(s, t, blosum62, gap_penalty)\n\n# Print the result\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-11",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-11",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "69.4 Explanation of the Code",
    "text": "69.4 Explanation of the Code\n\nParsing FASTA Input:\n\nparse_fasta function reads the input in FASTA format and returns the sequences in a list.\n\nBLOSUM62 Scoring Matrix:\n\nparse_blosum62 function parses the BLOSUM62 matrix string and stores the scores in a dictionary for easy lookup.\n\nNeedleman-Wunsch Algorithm:\n\nneedleman_wunsch function implements the dynamic programming algorithm to compute the global alignment score.\nThe dp table is initialized with gap penalties.\nThe table is filled based on the scores for matches, insertions, and deletions.\nThe final alignment score is found in dp[m][n].\n\nExecution:\n\nThe sequences are parsed, the scoring matrix is loaded, and the alignment score is computed using the Needleman-Wunsch algorithm.\nThe result is printed as the maximum alignment score.\n\n\nThis implementation ensures the alignment score is computed efficiently even for long protein sequences, utilizing the scoring matrix and gap penalties correctly."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-69",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-69",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "70.1 Sample Dataset",
    "text": "70.1 Sample Dataset\nATTAC\nTACAG\nGATTA\nACAGA\nCAGAT\nTTACA\nAGATT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-70",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-70",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "70.2 Sample Output",
    "text": "70.2 Sample Output\nATTACAG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-66",
    "href": "posts/md/Rosalind_stronghold.html#solution-66",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "70.3 Solution",
    "text": "70.3 Solution\ndef generate_coverings(current_string, edges, k):\n    \"\"\"\n    Generate all possible complete cycle coverings from the given edges.\n\n    Args:\n    - current_string: The current string being formed as part of the cycle.\n    - edges: Remaining edges in the De Bruijn graph to be used.\n    - k: Length of the k-mers.\n\n    Returns:\n    - A list of possible cycle coverings as strings.\n    \"\"\"\n    # Find the indices of edges that can be added next based on the current string.\n    next_edges_indices = [i for i, edge in enumerate(edges) if edge[0] == current_string[-k+1:]]\n\n    # If no more edges can be added:\n    if not next_edges_indices:\n        # Return the current string if all edges have been used (perfect covering).\n        return [current_string] if not edges else []\n\n    # Otherwise, recursively generate coverings with each possible next edge.\n    possible_coverings = []\n    for i in next_edges_indices:\n        next_string = current_string + edges[i][1][-1]\n        remaining_edges = edges[:i] + edges[i+1:]\n        possible_coverings.append(generate_coverings(next_string, remaining_edges, k))\n\n    return possible_coverings\n\ndef flatten(nested_list):\n    \"\"\"\n    Flattens a nested list into a single list.\n\n    Args:\n    - nested_list: A list that may contain other nested lists.\n\n    Yields:\n    - Individual elements from the nested list, flattened.\n    \"\"\"\n    for item in nested_list:\n        if isinstance(item, list):\n            yield from flatten(item)\n        else:\n            yield item\n\n# Sample input data\nsample_input = \"\"\"\nATTAC\nTACAG\nGATTA\nACAGA\nCAGAT\nTTACA\nAGATT\"\"\"\n\n# Split input into k-mers\nk_mers = sample_input.strip().split(\"\\n\")\n\n# Create edges of the De Bruijn graph from the k-mers\nk = len(k_mers[0])\ncreate_edge = lambda k_mer: [k_mer[:k-1], k_mer[1:]]\nde_bruijn_edges = [create_edge(k_mer) for k_mer in k_mers[1:]]\n\n# Generate all possible circular strings\ncircular_strings = set(flatten(generate_coverings(k_mers[0], de_bruijn_edges, k)))\n\n# Trim each circular string to the appropriate length (number of k-mers)\ncircular_strings = [cycle[:len(k_mers)] for cycle in circular_strings]\n\n# Print the resulting circular strings\nprint('\\n'.join(circular_strings))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-12",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-12",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "70.4 Explanation of the Code",
    "text": "70.4 Explanation of the Code\n\ngenerate_coverings Function:\n\nPurpose: This function recursively generates all possible cycle coverings (i.e., circular strings) by extending the current string with valid edges from the De Bruijn graph.\nHow it works:\n\nIt looks for edges that can be appended to the current string (based on the last \\(k-1\\) characters of the string).\nIf no valid edges are left, it checks if all edges have been used (indicating a perfect covering). If so, it returns the current string.\nIf there are valid edges, it recursively tries to extend the string with each possible edge and collects all possible coverings.\n\n\nflatten Function:\n\nPurpose: This helper function is used to flatten a nested list into a single-level list.\nHow it works: It recursively traverses the nested list and yields individual elements, effectively flattening the list.\n\nMain Execution:\n\nk_mers: The input strings are split into individual \\(k\\)-mers.\nde_bruijn_edges: This creates the edges of the De Bruijn graph. Each \\(k\\)-mer is split into its prefix and suffix of length \\(k-1\\).\ngenerate_coverings: This function is called with the first \\(k\\)-mer as the starting point, and it generates all possible circular strings by finding all Eulerian cycles in the graph.\ncircular_strings: The resulting strings are then trimmed to the length of the input (number of \\(k\\)-mers) to ensure that only the desired cyclic superstrings are printed."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-70",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-70",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "71.1 Sample Dataset",
    "text": "71.1 Sample Dataset\n4\nGSDMQS\nVWICN\nIASWMQS\nPVSMGAD\n445.17838\n115.02694\n186.07931\n314.13789\n317.1198\n215.09061"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-71",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-71",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "71.2 Sample Output",
    "text": "71.2 Sample Output\n3\nIASWMQS"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-67",
    "href": "posts/md/Rosalind_stronghold.html#solution-67",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "71.3 Solution",
    "text": "71.3 Solution\nfrom decimal import Decimal\nfrom collections import defaultdict\n\ndef get_protein_weights():\n    \"\"\"Returns a dictionary mapping amino acids to their weights.\"\"\"\n    return {\n        'G': Decimal('57.02146'), 'A': Decimal('71.03711'), 'S': Decimal('87.03203'), \n        'P': Decimal('97.05276'), 'V': Decimal('99.06841'), 'T': Decimal('101.04768'), \n        'C': Decimal('103.00919'), 'I': Decimal('113.08406'), 'L': Decimal('113.08406'), \n        'N': Decimal('114.04293'), 'D': Decimal('115.02694'), 'Q': Decimal('128.05858'), \n        'K': Decimal('128.09496'), 'E': Decimal('129.04259'), 'M': Decimal('131.04049'), \n        'H': Decimal('137.05891'), 'F': Decimal('147.06841'), 'R': Decimal('156.10111'), \n        'Y': Decimal('163.06333'), 'W': Decimal('186.07931')\n    }\n\ndef calculate_weight(protein_sequence):\n    \"\"\"Calculates the total weight of a given protein sequence based on amino acid weights.\"\"\"\n    weights = get_protein_weights()\n    total_weight = Decimal('0.0')\n    for amino_acid in protein_sequence:\n        total_weight += weights[amino_acid]\n    return total_weight\n\ndef calculate_multiplicity(proteins, spectrum_weights):\n    \"\"\"\n    Calculates the maximum multiplicity of spectrum weights for each protein\n    and identifies the protein with the highest multiplicity.\n    \"\"\"\n    max_multiplicity = -1\n    best_protein = None\n    \n    for protein in proteins:\n        # Compute the spectrum weights for all prefixes and suffixes of the protein\n        spectrum = []\n        for i in range(1, len(protein) + 1):\n            spectrum.append(calculate_weight(protein[:i]))\n        for i in range(len(protein)):\n            spectrum.append(calculate_weight(protein[i:]))\n        \n        spectrum_weights_count = defaultdict(int)\n        \n        # Count the differences between the spectrum weights and given weights\n        for protein_weight in spectrum:\n            for given_weight in spectrum_weights:\n                diff = round(protein_weight - given_weight, 3)\n                spectrum_weights_count[diff] += 1\n        \n        # Determine the maximum multiplicity for the current protein\n        current_multiplicity = max(spectrum_weights_count.values(), default=0)\n        \n        if current_multiplicity &gt; max_multiplicity:\n            max_multiplicity = current_multiplicity\n            best_protein = protein\n    \n    return max_multiplicity, best_protein\n\n# Sample input (for demonstration purposes)\nsample_input = \"\"\"\n4\nGSDMQS\nVWICN\nIASWMQS\nPVSMGAD\n445.17838\n115.02694\n186.07931\n314.13789\n317.1198\n215.09061\n\"\"\"\n\n# Parse the input\nlines = sample_input.strip().split(\"\\n\")\nnumber_of_proteins = int(lines[0])\nprotein_sequences = lines[1:number_of_proteins + 1]\nspectrum_weights = sorted(map(Decimal, lines[number_of_proteins + 1:]))\n\n# Calculate the maximum multiplicity and the corresponding protein\nmax_multiplicity, best_protein = calculate_multiplicity(protein_sequences, spectrum_weights)\n\n# Print the results\nprint(max_multiplicity)\nprint(best_protein)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-13",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-13",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "71.4 Explanation of the Code",
    "text": "71.4 Explanation of the Code\n\nget_protein_weights(): Returns a dictionary mapping each amino acid to its corresponding weight using the Decimal type for precision.\ncalculate_weight(protein_sequence): Computes the total weight of a protein sequence by summing the weights of its amino acids.\ncalculate_multiplicity(proteins, spectrum_weights):\n\nIterates through each protein sequence to compute the weights of all possible prefixes and suffixes.\nUses a defaultdict to count how often the difference between each protein weight and given spectrum weight appears.\nFinds and returns the protein with the highest multiplicity of such differences.\n\nInput Parsing:\n\nReads and parses the sample input to extract the number of proteins, the list of protein sequences, and the list of spectrum weights.\n\nResults:\n\nCalls calculate_multiplicity to get the protein with the maximum multiplicity and prints the result."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-71",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-71",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "72.1 Sample Dataset",
    "text": "72.1 Sample Dataset\ncat dog elephant ostrich mouse rabbit robot\n01xxx00\nx11xx00\n111x00x"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-72",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-72",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "72.2 Sample Output",
    "text": "72.2 Sample Output\n{cat, dog} {mouse, rabbit}\n{dog, elephant} {rabbit, robot}\n{cat, elephant} {mouse, rabbit}\n{dog, elephant} {mouse, rabbit}"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-68",
    "href": "posts/md/Rosalind_stronghold.html#solution-68",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "72.3 Solution",
    "text": "72.3 Solution\n# Sample input data\ndata = \"\"\"\ncat dog elephant ostrich mouse rabbit robot\n01xxx00\nx11xx00\n111x00x\n\"\"\"\n\n# Split the input data into lines\nlines = data.strip().split(\"\\n\")\n\n# Extract taxa (species) from the first line\ntaxa = lines[0].strip().split(' ')\n\n# Initialize a set to store unique quartets\nunique_quartets = set()\n\n# Proces each line of the partial character table\nfor line in lines[1:]:\n    # Initialize lists to hold taxa for two groups\n    group_C = []\n    group_D = []\n\n    # Classify taxa based on the partial character table\n    for i in range(len(line)):\n        if line[i] == '1':\n            group_C.append(taxa[i])\n        elif line[i] == '0':\n            group_D.append(taxa[i])\n    \n    # Ensure each group has at least two taxa to form a quartet\n    if len(group_C) &gt;= 2 and len(group_D) &gt;= 2:\n        # Generate all possible pairs for group C and group D\n        for i in range(len(group_C) - 1):\n            for j in range(i + 1, len(group_C)):\n                for k in range(len(group_D) - 1):\n                    for l in range(k + 1, len(group_D)):\n                        # Form pairs (A, B) from group_C and (C, D) from group_D\n                        pair_A = tuple(sorted([group_C[i], group_C[j]]))\n                        pair_B = tuple(sorted([group_D[k], group_D[l]]))\n                        \n                        # Add the sorted quartet to the set\n                        sorted_quartet = (pair_A, pair_B) if pair_A &lt; pair_B else (pair_B, pair_A)\n                        unique_quartets.add(sorted_quartet)\n\n# Print each unique quartet in the required format\nfor quartet in unique_quartets:\n    pair_A, pair_B = quartet\n    print('{{{}, {}}} {{{}, {}}}'.format(pair_A[0], pair_A[1], pair_B[0], pair_B[1]))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-14",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-14",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "72.4 Explanation of the Code",
    "text": "72.4 Explanation of the Code\n\nInput Data Handling:\n\ndata.strip().split(\"\\n\"): Split the input data into lines. The first line contains taxa names, and the subsequent lines contain the partial character table.\n\nExtracting Taxa:\n\ntaxa = lines[0].strip().split(' '): The first line is split into individual taxa names.\n\nInitialize Set for Quartets:\n\nunique_quartets = set(): This set will store unique quartets to avoid duplicates.\n\nProcessing Each Partial Character Table Line:\n\nFor each line after the first one, initialize group_C and group_D to store taxa based on the partial character table values (1 and 0 respectively).\nPopulate group_C and group_D based on whether the character is 1 or 0.\n\nForming Quartets:\n\nEnsure each group has at least two taxa to form pairs.\nGenerate all possible pairs from group_C and group_D.\nSort pairs and add them to the unique_quartets set, ensuring that each quartet is stored in a canonical (sorted) form to avoid duplicates.\n\nOutput Results:\n\nFor each unique quartet, format and print the result."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-72",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-72",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "73.1 Sample Dataset",
    "text": "73.1 Sample Dataset\n3524.8542\n3623.5245\n3710.9335\n3841.974\n3929.00603\n3970.0326\n4026.05879\n4057.0646\n4083.08025"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-73",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-73",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "73.2 Sample Output",
    "text": "73.2 Sample Output\nSPG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-69",
    "href": "posts/md/Rosalind_stronghold.html#solution-69",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "73.3 Solution",
    "text": "73.3 Solution\nfrom collections import defaultdict\n\n# Monoisotopic mas table for amino acids\nmass_table = {\n    'A': 71.03711, 'C': 103.00919, 'D': 115.02694, 'E': 129.04259,\n    'F': 147.06841, 'G': 57.02146, 'H': 137.05891, 'I': 113.08406,\n    'K': 128.09496, 'L': 113.08406, 'M': 131.04049, 'N': 114.04293,\n    'P': 97.05276, 'Q': 128.05858, 'R': 156.10111, 'S': 87.03203,\n    'T': 101.04768, 'V': 99.06841, 'W': 186.07931, 'Y': 163.06333\n}\n\ndef build_spectrum_graph(L, mass_table):\n    \"\"\"\n    Create a graph where each node represents a number in L.\n    Add a directed edge from u to v if v &gt; u and the weight difference\n    matches any amino acid's mas in the mass_table.\n    \"\"\"\n    graph = defaultdict(list)\n    \n    # Iterate over each pair of nodes (u, v) where v &gt; u\n    for i, u in enumerate(L):\n        for j, v in enumerate(L):\n            if v &gt; u:\n                # Calculate the weight difference between v and u\n                weight_diff = v - u\n                \n                # Check if this weight difference matches any amino acid mass\n                for symbol, mas in mass_table.items():\n                    if abs(weight_diff - mass) &lt; 1e-5:\n                        graph[u].append((v, symbol))\n    \n    return graph\n\ndef find_longest_path(graph, start):\n    \"\"\"\n    Use depth-first search to find the longest path in the graph starting from 'start'.\n    \"\"\"\n    stack = [(start, '')]  # Stack for DFS: (current_node, path_string)\n    longest_path = ''\n    \n    # Store the longest path ending at each node\n    path_map = defaultdict(str)\n    \n    while stack:\n        node, path = stack.pop()\n        \n        # Update the longest path for the current node\n        if len(path) &gt; len(path_map[node]):\n            path_map[node] = path\n        \n        # Traverse neighbors\n        for neighbor, symbol in graph[node]:\n            stack.append((neighbor, path + symbol))\n    \n    # Return the longest path found\n    longest_path = max(path_map.values(), key=len)\n    return longest_path\n\ndef find_longest_protein_string(L, mass_table):\n    \"\"\"\n    Build the spectrum graph and find the longest protein string.\n    \"\"\"\n    # Create the spectrum graph from the list L\n    graph = build_spectrum_graph(L, mass_table)\n    \n    # Find the longest path starting from each node in L\n    longest_protein = ''\n    for node in L:\n        current_protein = find_longest_path(graph, node)\n        if len(current_protein) &gt; len(longest_protein):\n            longest_protein = current_protein\n    \n    return longest_protein\n\n# Sample input\nsample_input = \"\"\"\n3524.8542\n3623.5245\n3710.9335\n3841.974\n3929.00603\n3970.0326\n4026.05879\n4057.0646\n4083.08025\"\"\"\n\n# Parse the sample input into a list of floats\nL = [float(x) for x in sample_input.strip().split(\"\\n\")]\n\n# Find and print the longest protein string\nlongest_protein = find_longest_protein_string(L, mass_table)\nprint(longest_protein)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-37",
    "href": "posts/md/Rosalind_stronghold.html#explanation-37",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "73.4 Explanation",
    "text": "73.4 Explanation\n\nMonoisotopic Mas Table:\n\nmass_table maps each amino acid to its mass.\n\nGraph Construction (build_spectrum_graph):\n\nPurpose: Create a directed graph where each node is a number from \\(L\\), and edges are added if the difference between nodes corresponds to the mas of an amino acid.\nProcess:\n\nIterate through each pair of numbers in \\(L\\) where the second number is greater than the first.\nCalculate the weight difference and check if it matches any mas in the mass_table.\nAdd a directed edge between these nodes labeled with the corresponding amino acid.\n\n\nFinding the Longest Path (find_longest_path):\n\nPurpose: Determine the longest path in the graph starting from a given node using depth-first search (DFS).\nProcess:\n\nUse a stack to explore nodes.\nTrack the longest path ending at each node.\nUpdate the longest path found during traversal.\n\n\nMain Function (find_longest_protein_string):\n\nPurpose: Integrates the graph construction and longest path finding to return the longest protein string.\nProcess:\n\nBuild the spectrum graph.\nFor each node, find the longest path starting from that node.\nReturn the longest path found.\n\n\nExecution:\n\nSample Input: Represents a list of mas values.\nProcessing: Converts the sample input into a list of floats, finds the longest protein string, and prints it."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-73",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-73",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "74.1 Sample Dataset",
    "text": "74.1 Sample Dataset\nATAAATG$"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-74",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-74",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "74.2 Sample Output",
    "text": "74.2 Sample Output\nA\nA\nATG$\nTG$\nT\nAAATG$\nG$\nT\nAAATG$\nG$\nG$\n$"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-70",
    "href": "posts/md/Rosalind_stronghold.html#solution-70",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "74.3 Solution",
    "text": "74.3 Solution\nfrom collections import defaultdict\n\nclas SuffixTree:\n    \"\"\"Creates a suffix tree for the provided word.\"\"\"\n    \n    def __init__(self, word):\n        \"\"\"Initializes the suffix tree.\"\"\"\n        self.nodes = [self.Node(None, 0)]  # Initialize with root node.\n        self.edges = dict()  # Dictionary to store edges.\n        self.descendants_count = dict()  # Cache for the number of descendants of nodes.\n        if isinstance(word, str):  # Check if the input is a string.\n            self._build_suffix_tree(word)\n\n    clas Node:\n        \"\"\"Represents a node in the suffix tree.\"\"\"\n        def __init__(self, parent, node_id):\n            self.parent = parent\n            self.node_id = node_id\n            self.children = []\n\n        def add_child(self, child_node):\n            self.children.append(child_node)\n\n        def remove_child(self, child_node):\n            self.children.remove(child_node)\n\n        def update_parent(self, new_parent):\n            self.parent = new_parent\n\n    def _build_suffix_tree(self, word):\n        \"\"\"Builds the suffix tree by adding each suffix of the word.\"\"\"\n        if word[-1] != '$':\n            word += '$'  # Ensure the word ends with the terminal symbol '$'.\n        self.word = word\n        self.length = len(self.word)\n\n        for i in range(self.length):\n            parent_node, edge_start, has_overlap = self._find_insertion_point(i, self.nodes[0])\n\n            if has_overlap:\n                existing_start, existing_end = self.edges[(parent_node.parent.node_id, parent_node.node_id)]\n\n                # Determine the length of the overlap.\n                overlap_length = 0\n                while self.word[edge_start:edge_start + overlap_length] == self.word[existing_start:existing_start + overlap_length]:\n                    overlap_length += 1\n\n                # Create a new internal node at the point of insertion.\n                new_internal_node = self.Node(parent_node.parent, len(self.nodes))\n                new_internal_node.add_child(parent_node)\n                self._add_edge_and_node(parent_node.parent, existing_start, existing_start + overlap_length - 1, new_internal_node)\n\n                # Update the edge and parent relationship for the original child node.\n                del self.edges[(parent_node.parent.node_id, parent_node.node_id)]\n                parent_node.parent.remove_child(parent_node)\n                parent_node.update_parent(new_internal_node)\n                self.edges[(new_internal_node.node_id, parent_node.node_id)] = [existing_start + overlap_length - 1, existing_end]\n\n                # Add the remaining suffix as a new child node.\n                self._add_edge_and_node(new_internal_node, edge_start + overlap_length - 1, self.length)\n\n            else:\n                # No overlap, simply add the entire suffix as a new edge.\n                self._add_edge_and_node(parent_node, edge_start, self.length)\n\n    def _find_insertion_point(self, start_index, parent_node):\n        \"\"\"Determines where to insert a suffix into the tree.\"\"\"\n        for child_node in parent_node.children:\n            edge_start, edge_end = self.edges[(parent_node.node_id, child_node.node_id)]\n            if self.word[start_index:start_index + edge_end - edge_start] == self.word[edge_start:edge_end]:\n                return self._find_insertion_point(start_index + edge_end - edge_start, child_node)\n            elif self.word[edge_start] == self.word[start_index]:\n                return child_node, start_index, True\n        return parent_node, start_index, False\n\n    def _add_edge_and_node(self, parent_node, edge_start, edge_end, child_node=None):\n        \"\"\"Adds a node and the corresponding edge to the suffix tree.\"\"\"\n        if child_node is None:\n            child_node = self.Node(parent_node, len(self.nodes))\n\n        self.nodes.append(child_node)\n        parent_node.add_child(child_node)\n        self.edges[(parent_node.node_id, child_node.node_id)] = [edge_start, edge_end]\n\n    def get_edge_labels(self):\n        \"\"\"Returns the substrings representing the edges of the suffix tree.\"\"\"\n        return [self.word[start:end] for start, end in self.edges.values()]\n\n    def count_total_descendants(self, node):\n        \"\"\"Calculates the total number of descendants of a given node.\"\"\"\n        if node not in self.descendants_count:\n            self.descendants_count[node] = len(node.children) + sum(self.count_total_descendants(child) for child in node.children)\n        return self.descendants_count[node]\n\n    def get_node_label(self, node):\n        \"\"\"Returns the string represented by the path from the root to a given node.\"\"\"\n        label = ''\n        while node.node_id != 0:\n            start, end = self.edges[(node.parent.node_id, node.node_id)]\n            label = self.word[start:end] + label\n            node = node.parent\n        return label.strip('$')\n\n# Example usage\nsample_input = \"ATAAATG$\"\nsuffix_tree = SuffixTree(sample_input)\nresult = '\\n'.join(suffix_tree.get_edge_labels())\nprint(result)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-15",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-15",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "74.4 Explanation of the Code",
    "text": "74.4 Explanation of the Code\n\nClas Structure:\n\nSuffixTree class: Manages the construction and representation of the suffix tree.\nNode clas (nested within SuffixTree): Represents each node in the suffix tree. Each node has a parent, a unique identifier (node_id), and a list of child nodes.\n\nInitialization:\n\nThe SuffixTree is initialized with a root node (with node_id = 0).\nThe _build_suffix_tree method ensures the input word ends with the terminal symbol $ and then processes each suffix of the word to build the tree.\n\nBuilding the Tree:\n\nFor each suffix, _find_insertion_point is called to determine where in the tree the suffix should be inserted.\nIf there’s an overlap with an existing edge, a new internal node is created, and the tree is split at the point of overlap.\nOtherwise, a new edge representing the suffix is added directly.\n\nEdge and Node Management:\n\nThe _add_edge_and_node method handles the actual insertion of edges and nodes into the tree structure.\nThe edges dictionary maps parent-child relationships to the start and end indices of the corresponding substring in the word.\n\nRetrieving Results:\n\nget_edge_labels: Returns the list of substrings corresponding to all edges in the tree.\ncount_total_descendants: Computes the number of descendants for any given node in the tree (cached to optimize repeated queries).\nget_node_label: Recovers the substring represented by a path from the root to a specific node."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-74",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-74",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "75.1 Sample Dataset",
    "text": "75.1 Sample Dataset\ncat dog elephant mouse rabbit rat\n011101\n001101\n001100"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-75",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-75",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "75.2 Sample Output",
    "text": "75.2 Sample Output\n(((cat,rabbit),dog),(elephant,mouse),rat);"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-71",
    "href": "posts/md/Rosalind_stronghold.html#solution-71",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "75.3 Solution",
    "text": "75.3 Solution\nfrom Bio import Phylo\nimport sys\n\ndef find_columns_to_unify(splits):\n    \"\"\"Finds two columns in the split matrix that should be unified into a single clade.\"\"\"\n    for split in splits:\n        sum_split = sum(split)\n        if sum_split == 2:\n            # Find the pair of columns where the sum is exactly 2.\n            return tuple(i for i, value in enumerate(split) if value == 1)\n        elif sum_split == len(split) - 2:\n            # Find the pair of columns where the sum is all but 2.\n            return tuple(i for i, value in enumerate(split) if value == 0)\n    raise ValueError('No columns to unify found!', splits)\n\ndef print_clade_trees(clades):\n    \"\"\"Prints the clades as Newick formatted trees.\"\"\"\n    for clade in clades:\n        tree = Phylo.BaseTree.Tree.from_clade(clade)\n        Phylo.write(tree, sys.stdout, 'newick', plain=True)\n\ndef build_phylogenetic_tree(input_lines):\n    \"\"\"Builds a phylogenetic tree based on the input split matrix.\"\"\"\n    clades = [Phylo.BaseTree.Clade(name=name) for name in input_lines[0].split()]\n\n    splits = []\n    for line in input_lines[1:]:\n        splits.append([int(x) for x in line])\n\n    while splits:\n        col1, col2 = find_columns_to_unify(splits)\n\n        # Remove the second of the unified columns from the splits\n        for split in splits:\n            split.pop(col2)\n\n        # Remove trivial splits where all entries are 1 or all but one are 1\n        splits = [split for split in splits if 1 &lt; sum(split) &lt; len(split) - 1]\n\n        # Unify the clades corresponding to the selected columns\n        clades[col1] = Phylo.BaseTree.Clade(clades=[clades[col1], clades[col2]])\n        clades.pop(col2)\n\n    # Final clade to represent the complete phylogenetic tree\n    final_clade = Phylo.BaseTree.Clade(clades=clades)\n    print_clade_trees([final_clade])\n\n# Sample Input\nsample_input = \"\"\"\ncat dog elephant mouse rabbit rat\n011101\n001101\n001100\n\"\"\"\n\ninput_lines = sample_input.strip().split(\"\\n\")\nbuild_phylogenetic_tree(input_lines)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-16",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-16",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "75.4 Explanation of the Code",
    "text": "75.4 Explanation of the Code\n\nPurpose:\n\nThe code is designed to build a phylogenetic tree based on a set of species (or objects) and their binary split representations. The input consists of species names followed by rows of binary digits representing splits between species.\n\nFunctions:\n\nfind_columns_to_unify(splits):\n\nThis function identifies two columns in the split matrix that can be unified into a single clade.\nIt looks for a pair of columns where the sum of the values in the columns equals 2 (indicating that exactly two species share a common clade) or equals the number of species minus 2 (indicating all but two species share a clade).\n\nprint_clade_trees(clades):\n\nThis function prints the clades in Newick format, which is a common format for representing phylogenetic trees.\n\nbuild_phylogenetic_tree(input_lines):\n\nThis is the main function that constructs the phylogenetic tree.\nIt first parses the input to create a list of clades, one for each species.\nThen it iterates through the split matrix, repeatedly unifying clades based on the identified columns until only one clade (the final tree) remains.\n\n\nProcess:\n\nInput Parsing:\n\nThe input string is split into lines, with the first line containing species names and the subsequent lines containing binary splits.\n\nTree Construction:\n\nThe code processes the splits by repeatedly identifying pairs of species to unify (using the find_columns_to_unify function).\nAfter unifying species into clades, it modifies the split matrix by removing the unified columns and trivial splits.\nThis continues until only one clade remains, representing the complete phylogenetic tree.\n\n\nOutput:\n\nThe resulting phylogenetic tree is printed in Newick format, which can be visualized or further analyzed using tools that support this format."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-75",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-75",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "76.1 Sample Dataset",
    "text": "76.1 Sample Dataset\n6\n(lobster,(cat,dog),(caterpillar,(elephant,mouse)));"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-76",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-76",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "76.2 Sample Output",
    "text": "76.2 Sample Output\n15"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-72",
    "href": "posts/md/Rosalind_stronghold.html#solution-72",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "76.3 Solution",
    "text": "76.3 Solution\n# Sample Input\nsample_input = \"\"\"\n6\n(lobster,(cat,dog),(caterpillar,(elephant,mouse)));\n\"\"\"\n\n# Parse input lines\ninput_lines = sample_input.strip().split(\"\\n\")\nn = int(input_lines[0])\n\n# Initialize a memoization list to store factorials\nfactorials = [1] * (n + 1)  # Initialize with 1 for factorial(0)\n\n# Calculate all factorials from 1 to n and store in the list\nfor i in range(1, n + 1):\n    factorials[i] = i * factorials[i - 1]\n\n# Calculate the number of possible quartets using the combination formula\n# C(n, 4) = n! / ((n-4)! * 4!)\nnum_quartets = (factorials[n] // (factorials[n - 4] * factorials[4])) % 1000000\n\n# Output the result\nprint(num_quartets)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-38",
    "href": "posts/md/Rosalind_stronghold.html#explanation-38",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "76.4 Explanation",
    "text": "76.4 Explanation\n\nInput Parsing:\n\nThe input is a string representing the number of taxa n and a Newick formatted tree. The first line is split to extract n.\n\nFactorial Calculation:\n\nWe need to calculate the number of quartets possible, which involves computing combinations. The combination formula C(n, 4) is used to calculate how many ways we can choose 4 taxa from n, and it requires calculating factorials.\nWe initialize a list factorials with n+1 elements, all set to 1. This list will store factorial values from 0! to n!.\nA loop is used to calculate each factorial iteratively and store it in the list.\n\nCombination Calculation:\n\nThe number of quartets is calculated using the formula C(n, 4) = n! / ((n-4)! * 4!).\nThe combination result is then taken modulo 1,000,000 to meet the problem’s requirement.\n\nOutput:\n\nThe final result is printed."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#key-concepts-1",
    "href": "posts/md/Rosalind_stronghold.html#key-concepts-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "76.5 Key Concepts",
    "text": "76.5 Key Concepts\n\nFactorials: Factorials are calculated iteratively and stored in a list to avoid recalculating the same value multiple times.\nCombinations: The formula for combinations C(n, 4) is used to find out how many quartets can be formed from n taxa.\nModulo Operation: Since the number of quartets can be large, the result is taken modulo 1,000,000."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-76",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-76",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "77.1 Sample Dataset",
    "text": "77.1 Sample Dataset\ndog cat mouse elephant"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-77",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-77",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "77.2 Sample Output",
    "text": "77.2 Sample Output\n((cat,(mouse,elephant)))dog\n((mouse,(cat,elephant)))dog\n((elephant,(cat,mouse)))dog"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-73",
    "href": "posts/md/Rosalind_stronghold.html#solution-73",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "77.3 Solution",
    "text": "77.3 Solution\nfrom itertools import combinations\n\ndef generate_combinations(n, items):\n    \"\"\"\n    Generates all combinations of `n` elements from the list `items`.\n    \"\"\"\n    if n == 0:\n        return [[]]\n    if not items:\n        return []\n    \n    head, *tail = items\n    with_head = [[head] + rest for rest in generate_combinations(n-1, tail)]\n    without_head = generate_combinations(n, tail)\n    \n    return with_head + without_head\n\ndef memoized_combinations(n, k):\n    \"\"\"\n    Returns all `k`-combinations of `n` items using memoization.\n    This prevents redundant computation of the same combination.\n    \"\"\"\n    memo = memoized_combinations.cache\n    if n not in memo:\n        memo[n] = {}\n    if k not in memo[n]:\n        memo[n][k] = generate_combinations(k, list(range(n)))\n    return memo[n][k]\n\nmemoized_combinations.cache = {}\n\ndef generate_unrooted_binary_trees(species):\n    \"\"\"\n    Generates all possible unrooted binary trees in Newick format for a given list of species.\n    \"\"\"\n    if len(species) == 1:\n        return species\n    elif len(species) == 2:\n        return [f\"({species[0]},{species[1]})\"]\n    elif len(species) &gt;= 3:\n        trees = []\n        for k in range(1, (len(species) // 2) + 1):\n            seen_combinations = set()\n            for selected_indices in memoized_combinations(len(species), k):\n                selected_species = [species[i] for i in selected_indices]\n                selected_key = ':'.join(sorted(selected_species))\n                if selected_key in seen_combinations:\n                    continue\n                seen_combinations.add(selected_key)\n                remaining_species = [sp for sp in species if sp not in selected_species]\n                remaining_key = ':'.join(sorted(remaining_species))\n                if remaining_key in seen_combinations:\n                    continue\n                seen_combinations.add(remaining_key)\n                \n                for left_tree in generate_unrooted_binary_trees(remaining_species):\n                    for right_tree in generate_unrooted_binary_trees(selected_species):\n                        trees.append(f\"({right_tree},{left_tree})\")\n        return trees\n    else:\n        raise Exception(\"Unexpected number of species\")\n\ninput_data = \"dog cat mouse elephant\"\nspecies_list = input_data.split()\nroot_species = species_list.pop(0)\n\nfor tree in generate_unrooted_binary_trees(species_list):\n    print(f\"({tree}){root_species}\")"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-17",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-the-code-17",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "77.4 Explanation of the Code",
    "text": "77.4 Explanation of the Code\n\ngenerate_combinations Function:\n\nThis function is a recursive implementation to generate all possible combinations of n elements from the given list items.\nIt works by considering each element (head) and recursively generating combinations with (with_head) and without (without_head) that element.\n\nmemoized_combinations Function:\n\nThis function leverages memoization to store previously computed combinations to avoid redundant calculations.\nIt checks if the combination of n and k has already been computed and stored in the cache (a dictionary attached to the function).\nIf not, it computes the combination using generate_combinations and stores it for future use.\n\ngenerate_unrooted_binary_trees Function:\n\nThis function generates all possible unrooted binary trees for the provided list of species in Newick format.\nIt uses a recursive approach:\n\nIf there’s only one species, it simply returns it.\nIf there are two species, it returns them in a pair.\nIf there are three or more species, it splits them into subgroups and recursively generates trees for each subgroup, ensuring that each possible tree structure is considered without duplicates."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-77",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-77",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "78.1 Sample Dataset",
    "text": "78.1 Sample Dataset\nAATCT\nTGTAA\nGATTA\nACAGA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-78",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-78",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "78.2 Sample Output",
    "text": "78.2 Sample Output\nTGTAATC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-74",
    "href": "posts/md/Rosalind_stronghold.html#solution-74",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "78.3 Solution",
    "text": "78.3 Solution\nfrom itertools import chain\n\ndef reverse_complement(dna):\n    \"\"\"Returns the reverse complement of a given DNA strand.\"\"\"\n    translation_table = str.maketrans('ATCG', 'TAGC')\n    return dna.translate(translation_table)[::-1]\n\ndef find_cyclic_superstring(dna_strings):\n    \"\"\"Finds the cyclic superstring from a list of DNA strings and their reverse complements.\"\"\"\n    def flatten_list_of_lists(list_of_lists):\n        \"\"\"Flattens one level of nesting in a list of lists.\"\"\"\n        return chain.from_iterable(list_of_lists)\n    \n    num_strings = len(dna_strings)\n    string_length = len(dna_strings[0])  # Assumes all strings have the same length\n    \n    for k in range(string_length - 1, 1, -1):\n        # Create adjacency list of k-mers\n        adj_list = dict(flatten_list_of_lists([\n            [(dna[i:i+k], dna[i+1:i+k+1]) for i in range(string_length - k)] \n            for dna in dna_strings\n        ]))\n        \n        # Start with an arbitrary k-mer and initialize the superstring\n        first_kmer = kmer = next(iter(adj_list))\n        superstring = ''\n        \n        while True:\n            if kmer in adj_list:\n                # Add the last character of the k-mer to the superstring\n                superstring += kmer[-1]\n                # Move to the next k-mer in the path\n                kmer = adj_list.pop(kmer)\n                # If we have returned to the start, we have completed a cycle\n                if kmer == first_kmer:\n                    return superstring\n            else:\n                # Exit if no continuation of the k-mer path is found\n                break\n\n# Read and preproces input\nsample_input = \"\"\"\nAATCT\nTGTAA\nGATTA\nACAGA\n\"\"\"\ndna_strings = sample_input.strip().split('\\n')\n# Add reverse complements of the DNA strings\ndna_strings = list(set(dna_strings + [reverse_complement(dna) for dna in dna_strings]))\n\n# Find and print the cyclic superstring\nprint(find_cyclic_superstring(dna_strings))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-39",
    "href": "posts/md/Rosalind_stronghold.html#explanation-39",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "78.4 Explanation",
    "text": "78.4 Explanation\n\nReverse Complement Function:\n\nreverse_complement(dna): Computes the reverse complement of a DNA sequence. It uses a translation table to map each nucleotide to its complement and then reverses the string.\n\nFinding Cyclic Superstring:\n\nfind_cyclic_superstring(dna_strings): Finds a cyclic superstring from the given list of DNA strings. It:\n\nUses flatten_list_of_lists to flatten a list of lists into a single list.\nIterates over possible lengths of k-mers from the longest (one les than the length of the DNA strings) to 2.\nConstructs a De Bruijn graph where edges are k-mers, and vertices are k-1-mers.\nTries to find a cyclic path in the graph that returns to the starting k-mer and constructs the superstring from this path.\n\n\nInput Processing:\n\nsample_input: Defines a sample input containing multiple DNA strings.\ndna_strings: Converts the input into a list of DNA strings and adds their reverse complements to handle both possible orientations.\n\nFinding and Printing the Superstring:\n\nCalls find_cyclic_superstring with the DNA strings (including their reverse complements) to find the cyclic superstring and prints the result."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-78",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-78",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "79.1 Sample Dataset",
    "text": "79.1 Sample Dataset\n&gt;Rosalind_79\nPLEASANTLY\n&gt;Rosalind_41\nMEANLY"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-79",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-79",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "79.2 Sample Output",
    "text": "79.2 Sample Output\n13"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-75",
    "href": "posts/md/Rosalind_stronghold.html#solution-75",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "79.3 Solution",
    "text": "79.3 Solution\nBLOSUM62 = {\n    ('W', 'F'): 1, ('L', 'R'): -2, ('S', 'P'): -1, ('V', 'T'): 0,\n    ('Q', 'Q'): 5, ('N', 'A'): -2, ('Z', 'Y'): -2, ('W', 'R'): -3,\n    ('Q', 'A'): -1, ('S', 'D'): 0, ('H', 'H'): 8, ('S', 'H'): -1,\n    ('H', 'D'): -1, ('L', 'N'): -3, ('W', 'A'): -3, ('Y', 'M'): -1,\n    ('G', 'R'): -2, ('Y', 'I'): -1, ('Y', 'E'): -2, ('B', 'Y'): -3,\n    ('Y', 'A'): -2, ('V', 'D'): -3, ('B', 'S'): 0, ('Y', 'Y'): 7,\n    ('G', 'N'): 0, ('E', 'C'): -4, ('Y', 'Q'): -1, ('Z', 'Z'): 4,\n    ('V', 'A'): 0, ('C', 'C'): 9, ('M', 'R'): -1, ('V', 'E'): -2,\n    ('T', 'N'): 0, ('P', 'P'): 7, ('V', 'I'): 3, ('V', 'S'): -2,\n    ('Z', 'P'): -1, ('V', 'M'): 1, ('T', 'F'): -2, ('V', 'Q'): -2,\n    ('K', 'K'): 5, ('P', 'D'): -1, ('I', 'H'): -3, ('I', 'D'): -3,\n    ('T', 'R'): -1, ('P', 'L'): -3, ('K', 'G'): -2, ('M', 'N'): -2,\n    ('P', 'H'): -2, ('F', 'Q'): -3, ('Z', 'G'): -2, ('X', 'L'): -1,\n    ('T', 'M'): -1, ('Z', 'C'): -3, ('X', 'H'): -1, ('D', 'R'): -2,\n    ('B', 'W'): -4, ('X', 'D'): -1, ('Z', 'K'): 1, ('F', 'A'): -2,\n    ('Z', 'W'): -3, ('F', 'E'): -3, ('D', 'N'): 1, ('B', 'K'): 0,\n    ('X', 'X'): -1, ('F', 'I'): 0, ('B', 'G'): -1, ('X', 'T'): 0,\n    ('F', 'M'): 0, ('B', 'C'): -3, ('Z', 'I'): -3, ('Z', 'V'): -2,\n    ('S', 'S'): 4, ('L', 'Q'): -2, ('W', 'E'): -3, ('Q', 'R'): 1,\n    ('N', 'N'): 6, ('W', 'M'): -1, ('Q', 'C'): -3, ('W', 'I'): -3,\n    ('S', 'C'): -1, ('L', 'A'): -1, ('S', 'G'): 0, ('L', 'E'): -3,\n    ('W', 'Q'): -2, ('H', 'G'): -2, ('S', 'K'): 0, ('Q', 'N'): 0,\n    ('N', 'R'): 0, ('H', 'C'): -3, ('Y', 'N'): -2, ('G', 'Q'): -2,\n    ('Y', 'F'): 3, ('C', 'A'): 0, ('V', 'L'): 1, ('G', 'E'): -2,\n    ('G', 'A'): 0, ('K', 'R'): 2, ('E', 'D'): 2, ('Y', 'R'): -2,\n    ('M', 'Q'): 0, ('T', 'I'): -1, ('C', 'D'): -3, ('V', 'F'): -1,\n    ('T', 'A'): 0, ('T', 'P'): -1, ('B', 'P'): -2, ('T', 'E'): -1,\n    ('V', 'N'): -3, ('P', 'G'): -2, ('M', 'A'): -1, ('K', 'H'): -1,\n    ('V', 'R'): -3, ('P', 'C'): -3, ('M', 'E'): -2, ('K', 'L'): -2,\n    ('V', 'V'): 4, ('M', 'I'): 1, ('T', 'Q'): -1, ('I', 'G'): -4,\n    ('P', 'K'): -1, ('M', 'M'): 5, ('K', 'D'): -1, ('I', 'C'): -1,\n    ('Z', 'D'): 1, ('F', 'R'): -3, ('X', 'K'): -1, ('Q', 'D'): 0,\n    ('X', 'G'): -1, ('Z', 'L'): -3, ('X', 'C'): -2, ('Z', 'H'): 0,\n    ('B', 'L'): -4, ('B', 'H'): 0, ('F', 'F'): 6, ('X', 'W'): -2,\n    ('B', 'D'): 4, ('D', 'A'): -2, ('S', 'L'): -2, ('X', 'S'): 0,\n    ('F', 'N'): -3, ('S', 'R'): -1, ('W', 'D'): -4, ('V', 'Y'): -1,\n    ('W', 'L'): -2, ('H', 'R'): 0, ('W', 'H'): -2, ('H', 'N'): 1,\n    ('W', 'T'): -2, ('T', 'T'): 5, ('S', 'F'): -2, ('W', 'P'): -4,\n    ('L', 'D'): -4, ('B', 'I'): -3, ('L', 'H'): -3, ('S', 'N'): 1,\n    ('B', 'T'): -1, ('L', 'L'): 4, ('Y', 'K'): -2, ('E', 'Q'): 2,\n    ('Y', 'G'): -3, ('Z', 'S'): 0, ('Y', 'C'): -2, ('G', 'D'): -1,\n    ('B', 'V'): -3, ('E', 'A'): -1, ('Y', 'W'): 2, ('E', 'E'): 5,\n    ('Y', 'S'): -2, ('C', 'N'): -3, ('V', 'C'): -1, ('T', 'H'): -2,\n    ('P', 'R'): -2, ('V', 'G'): -3, ('T', 'L'): -1, ('V', 'K'): -2,\n    ('K', 'Q'): 1, ('R', 'A'): -1, ('I', 'R'): -3, ('T', 'D'): -1,\n    ('P', 'F'): -4, ('I', 'N'): -3, ('K', 'I'): -3, ('M', 'D'): -3,\n    ('V', 'W'): -3, ('W', 'W'): 11, ('M', 'H'): -2, ('P', 'N'): -2,\n    ('K', 'A'): -1, ('M', 'L'): 2, ('K', 'E'): 1, ('Z', 'E'): 4,\n    ('X', 'N'): -1, ('Z', 'A'): -1, ('Z', 'M'): -1, ('X', 'F'): -1,\n    ('K', 'C'): -3, ('B', 'Q'): 0, ('X', 'B'): -1, ('B', 'M'): -3,\n    ('F', 'C'): -2, ('Z', 'Q'): 3, ('X', 'Z'): -1, ('F', 'G'): -3,\n    ('B', 'E'): 1, ('X', 'V'): -1, ('F', 'K'): -3, ('B', 'A'): -2,\n    ('X', 'R'): -1, ('D', 'D'): 6, ('W', 'G'): -2, ('Z', 'F'): -3,\n    ('S', 'Q'): 0, ('W', 'C'): -2, ('W', 'K'): -3, ('H', 'Q'): 0,\n    ('L', 'C'): -1, ('W', 'N'): -4, ('S', 'A'): 1, ('L', 'G'): -4,\n    ('W', 'S'): -3, ('S', 'E'): 0, ('H', 'E'): 0, ('S', 'I'): -2,\n    ('H', 'A'): -2, ('S', 'M'): -1, ('Y', 'L'): -1, ('Y', 'H'): 2,\n    ('Y', 'D'): -3, ('E', 'R'): 0, ('X', 'P'): -2, ('G', 'G'): 6,\n    ('G', 'C'): -3, ('E', 'N'): 0, ('Y', 'T'): -2, ('Y', 'P'): -3,\n    ('T', 'K'): -1, ('A', 'A'): 4, ('P', 'Q'): -1, ('T', 'C'): -1,\n    ('V', 'H'): -3, ('T', 'G'): -2, ('I', 'Q'): -3, ('Z', 'T'): -1,\n    ('C', 'R'): -3, ('V', 'P'): -2, ('P', 'E'): -1, ('M', 'C'): -1,\n    ('K', 'N'): 0, ('I', 'I'): 4, ('P', 'A'): -1, ('M', 'G'): -3,\n    ('T', 'S'): 1, ('I', 'E'): -3, ('P', 'M'): -2, ('M', 'K'): -1,\n    ('I', 'A'): -1, ('P', 'I'): -3, ('R', 'R'): 5, ('X', 'M'): -1,\n    ('L', 'I'): 2, ('X', 'I'): -1, ('Z', 'B'): 1, ('X', 'E'): -1,\n    ('Z', 'N'): 0, ('X', 'A'): 0, ('B', 'R'): -1, ('B', 'N'): 3,\n    ('F', 'D'): -3, ('X', 'Y'): -1, ('Z', 'R'): 0, ('F', 'H'): -1,\n    ('B', 'F'): -3, ('F', 'L'): 0, ('X', 'Q'): -1, ('B', 'B'): 4\n}\n\ndef parse_fasta(fasta_string):\n    \"\"\"Parse the input FASTA format string into individual sequences.\"\"\"\n    sequences = []\n    current_sequence = \"\"\n    for line in fasta_string.strip().split('\\n'):\n        if line.startswith(\"&gt;\"):\n            if current_sequence:\n                sequences.append(current_sequence)\n            current_sequence = \"\"\n        else:\n            current_sequence += line.strip()\n    sequences.append(current_sequence)  # Append the last sequence\n    return sequences\n\ndef global_alignment_with_gap_penalty(seq1, seq2):\n    \"\"\"Compute the global alignment score between two sequences with gap penalties.\"\"\"\n    seq1 = \"-\" + seq1  # Add a leading gap for alignment\n    seq2 = \"-\" + seq2  # Add a leading gap for alignment\n\n    # Initialize scoring matrices\n    lower_matrix = [[0 for _ in range(len(seq2))] for _ in range(len(seq1))]\n    middle_matrix = [[0 for _ in range(len(seq2))] for _ in range(len(seq1))]\n    upper_matrix = [[0 for _ in range(len(seq2))] for _ in range(len(seq1))]\n\n    # Set gap penalties for the first row and column\n    for col in range(1, len(seq2)):\n        lower_matrix[0][col] = -5\n        middle_matrix[0][col] = -5\n        upper_matrix[0][col] = -50\n\n    for row in range(1, len(seq1)):\n        lower_matrix[row][0] = -5\n        middle_matrix[row][0] = -5\n        upper_matrix[row][0] = -50\n\n    # Fill the scoring matrices\n    for col in range(1, len(seq2)):\n        for row in range(1, len(seq1)):\n            lower_matrix[row][col] = max(lower_matrix[row - 1][col], middle_matrix[row - 1][col] - 5)\n            upper_matrix[row][col] = max(upper_matrix[row][col - 1], middle_matrix[row][col - 1] - 5)\n\n            pair = (seq1[row], seq2[col])\n            reverse_pair = (seq2[col], seq1[row])\n            score = BLOSUM62.get(pair, BLOSUM62.get(reverse_pair, 0))\n            middle_matrix[row][col] = max(\n                lower_matrix[row][col],\n                middle_matrix[row - 1][col - 1] + score,\n                upper_matrix[row][col]\n            )\n\n    return middle_matrix[len(seq1) - 1][len(seq2) - 1]\n\n# Sample Input\nsample_input = \"\"\"&gt;Rosalind_79\nPLEASANTLY\n&gt;Rosalind_41\nMEANLY\n\"\"\"\n\n# Parse input FASTA format data\nsequences = parse_fasta(sample_input)\nsequence1, sequence2 = sequences[0], sequences[1]\n\n# Compute the global alignment score\nalignment_score = global_alignment_with_gap_penalty(sequence1, sequence2)\nprint(alignment_score)\nThe code computes the global alignment score between two sequences using a scoring matrix (BLOSUM62) and gap penalties. This involves parsing FASTA formatted sequence data, initializing scoring matrices for dynamic programming, and then filling these matrices to calculate the alignment score."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#parse_fastafasta_string",
    "href": "posts/md/Rosalind_stronghold.html#parse_fastafasta_string",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "79.4 1. parse_fasta(fasta_string)",
    "text": "79.4 1. parse_fasta(fasta_string)\nPurpose: Parses a FASTA format string into individual sequences.\nHow it works: - Input: A FASTA format string where sequences are prefixed with a &gt; character. - Output: A list of sequences.\nSteps: 1. Initialize an empty list sequences to store parsed sequences. 2. Initialize an empty string current_sequence to build sequences as you read the input. 3. Split the input string into lines and iterate through them: - If a line starts with &gt;, it indicates the start of a new sequence. Append the current sequence to sequences if it’s not empty, then reset current_sequence. - Otherwise, append the line (after stripping whitespace) to current_sequence. 4. Append the last sequence after exiting the loop. 5. Return the list of sequences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#global_alignment_with_gap_penaltyseq1-seq2",
    "href": "posts/md/Rosalind_stronghold.html#global_alignment_with_gap_penaltyseq1-seq2",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "79.5 2. global_alignment_with_gap_penalty(seq1, seq2)",
    "text": "79.5 2. global_alignment_with_gap_penalty(seq1, seq2)\nPurpose: Computes the global alignment score between two sequences using a dynamic programming approach with gap penalties.\nHow it works: - Input: Two sequences, seq1 and seq2. - Output: The alignment score for the sequences.\nSteps: 1. Initialize Sequences: - Prepend a gap (-) to both sequences to handle gaps at the start of alignments.\n\nInitialize Matrices:\n\nlower_matrix, middle_matrix, upper_matrix are 2D lists used to store scores during alignment:\n\nlower_matrix tracks scores when aligning with gaps in seq2.\nupper_matrix tracks scores when aligning with gaps in seq1.\nmiddle_matrix stores the scores for the current alignment considering both sequences.\n\n\nSet Initial Gap Penalties:\n\nFor the first row (aligning gaps in seq1 with the second sequence), initialize with gap penalties of -5 for lower_matrix and middle_matrix, and a larger penalty -50 for upper_matrix.\nFor the first column (aligning gaps in seq2 with the first sequence), use similar penalties.\n\nFill Matrices:\n\nUse nested loops to fill the matrices:\n\nLower Matrix: Computes scores for gaps in seq2.\nUpper Matrix: Computes scores for gaps in seq1.\nMiddle Matrix: Computes scores for aligning characters in seq1 and seq2.\n\nRetrieve the score from BLOSUM62 for the pair of characters.\nUse the maximum of the scores from lower_matrix, middle_matrix, and upper_matrix to fill in middle_matrix.\n\n\n\nRetrieve Final Score:\n\nThe final alignment score is in middle_matrix[len(seq1) - 1][len(seq2) - 1], which represents the best alignment score for the entire length of both sequences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-79",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-79",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "80.1 Sample Dataset",
    "text": "80.1 Sample Dataset\n((((Aa,aa),(Aa,Aa)),((aa,aa),(aa,AA))),Aa);"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-80",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-80",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "80.2 Sample Output",
    "text": "80.2 Sample Output\n0.156 0.5 0.344"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-76",
    "href": "posts/md/Rosalind_stronghold.html#solution-76",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "80.3 Solution",
    "text": "80.3 Solution\n# Probabilities from an individual's ancestors (based on Mendelian inheritance)\nprob_dict = {\n    (\"AA\", \"AA\"): (1.0, 0.0, 0.0),\n    (\"AA\", \"Aa\"): (0.5, 0.5, 0.0),\n    (\"AA\", \"aa\"): (0.0, 1.0, 0.0),\n    (\"Aa\", \"AA\"): (0.5, 0.5, 0.0),\n    (\"Aa\", \"Aa\"): (0.25, 0.5, 0.25),\n    (\"Aa\", \"aa\"): (0.0, 0.5, 0.5),\n    (\"aa\", \"AA\"): (0.0, 1.0, 0.0),\n    (\"aa\", \"Aa\"): (0.0, 0.5, 0.5),\n    (\"aa\", \"aa\"): (0.0, 0.0, 1.0)\n}\n\ndef calculate_child_probabilities(parent1, parent2):\n    \"\"\"\n    Given the genotype probabilities of two parents, calculate the probabilities for the child.\n    \n    :param parent1: Dictionary containing the probabilities for the first parent (keys: 'AA', 'Aa', 'aa')\n    :param parent2: Dictionary containing the probabilities for the second parent (keys: 'AA', 'Aa', 'aa')\n    :return: Dictionary containing the probabilities for the child (keys: 'AA', 'Aa', 'aa')\n    \"\"\"\n    child_prob = {\"AA\": 0.0, \"Aa\": 0.0, \"aa\": 0.0}\n    \n    # Combine the genotype probabilities from both parents using the Mendelian inheritance rules\n    for p1_genotype, p1_prob in parent1.items():\n        for p2_genotype, p2_prob in parent2.items():\n            # Look up the probability distribution for the child based on the parent genotypes\n            child_distribution = prob_dict[(p1_genotype, p2_genotype)]\n            child_prob[\"AA\"] += p1_prob * p2_prob * child_distribution[0]\n            child_prob[\"Aa\"] += p1_prob * p2_prob * child_distribution[1]\n            child_prob[\"aa\"] += p1_prob * p2_prob * child_distribution[2]\n    \n    return child_prob\n\n# Known genotype probabilities for the leaves (ancestral individuals)\nAA = {\"AA\": 1.0, \"Aa\": 0.0, \"aa\": 0.0}\nAa = {\"AA\": 0.0, \"Aa\": 1.0, \"aa\": 0.0}\naa = {\"AA\": 0.0, \"Aa\": 0.0, \"aa\": 1.0}\n\n# Pedigree input in Newick format\npedigree = \"((((Aa,aa),(Aa,Aa)),((aa,aa),(aa,AA))),Aa);\"\n\n# Modify the Newick string to turn it into a Python expression using the `calculate_child_probabilities` function\npedigree_expression = pedigree.replace(\";\", \"\").replace(\"(\", \"calculate_child_probabilities(\")\n\n# Evaluate the expression to calculate the probabilities for the root individual\nroot_probabilities = eval(pedigree_expression)\n\n# Print the final probabilities for the root individual\nprint(f\"{root_probabilities['AA']:f} {root_probabilities['Aa']:f} {root_probabilities['aa']:f}\")"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-40",
    "href": "posts/md/Rosalind_stronghold.html#explanation-40",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "80.4 Explanation",
    "text": "80.4 Explanation\n\nprob_dict:\n\nThis dictionary contains the Mendelian inheritance probabilities for a child given the genotypes of both parents. Each tuple of parent genotypes maps to a tuple of probabilities representing the likelihood of the child being AA, Aa, or aa.\n\ncalculate_child_probabilities:\n\nThis function calculates the probabilities for a child’s genotype based on the genotype probabilities of their two parents.\nIt iterates over all combinations of the parents’ genotypes, looks up the probabilities from prob_dict, and accumulates the resulting probabilities for the child.\n\nLeaf Node Probabilities:\n\nThe genotype probabilities for the leaf nodes (the known ancestors) are defined:\n\nAA: 100% chance of being AA\nAa: 100% chance of being Aa\naa: 100% chance of being aa\n\n\nPedigree Parsing:\n\nThe input pedigree is provided in Newick format. This format is transformed into a Python expression that can be evaluated using eval. The key part of this transformation is replacing the ( characters with calls to the calculate_child_probabilities function. This allows the pedigree to be evaluated as a nested set of function calls, starting from the leaves and working up to the root.\n\nResult:\n\nThe program evaluates the pedigree, calculating the probabilities of the root individual being AA, Aa, or aa. These probabilities are then printed to three decimal places."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-80",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-80",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "81.1 Sample Dataset",
    "text": "81.1 Sample Dataset\nATTTGGATT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-81",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-81",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "81.2 Sample Output",
    "text": "81.2 Sample Output\n0.875"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-77",
    "href": "posts/md/Rosalind_stronghold.html#solution-77",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "81.3 Solution",
    "text": "81.3 Solution\nimport sys\nfrom math import log\n\nclas SuffixTree:\n    '''Creates a suffix tree for the provided word.'''\n\n    def __init__(self, input_word):\n        '''Initializes the suffix tree.'''\n        self.nodes = [self.Node(None, 0)]\n        self.edges = dict()\n        self.descendants_count = dict()\n        if isinstance(input_word, str):\n            self.build_suffix_tree(input_word)\n\n    clas Node(object):\n        '''Suffix tree node class.'''\n        def __init__(self, parent_node, node_number):\n            self.parent = parent_node\n            self.number = node_number\n            self.children = []\n\n        def add_child(self, child_node):\n            self.children.append(child_node)\n\n        def remove_child(self, child_node):\n            self.children.remove(child_node)\n\n        def update_parent(self, new_parent):\n            self.parent = new_parent\n\n    def build_suffix_tree(self, input_word):\n        '''Build the suffix tree from the input word.'''\n        if input_word[-1] != '$':\n            input_word += '$'\n        self.word = input_word\n        self.length = len(self.word)\n\n        for i in range(self.length):\n            parent_node, edge_start, overlap_exists = self.find_insert_position(i, self.nodes[0])\n\n            if overlap_exists:\n                old_edge_start, old_edge_end = self.edges[(parent_node.parent.number, parent_node.number)]\n\n                # Determine the length of the edge to be inserted\n                insertion_length = 0\n                while input_word[edge_start:edge_start + insertion_length] == input_word[old_edge_start:old_edge_start + insertion_length]:\n                    insertion_length += 1\n\n                # Create a new node for the insertion\n                new_node = self.Node(parent_node.parent, len(self.nodes))\n                new_node.add_child(parent_node)\n                self.add_edge(parent_node.parent, old_edge_start, old_edge_start + insertion_length - 1, new_node)\n\n                # Update the parent node since a new node is inserted above it\n                del self.edges[(parent_node.parent.number, parent_node.number)]\n                parent_node.parent.remove_child(parent_node)\n                parent_node.update_parent(new_node)\n                self.edges[(parent_node.parent.number, parent_node.number)] = [old_edge_start + insertion_length - 1, old_edge_end]\n\n                # Add new child node\n                self.add_edge(new_node, edge_start + insertion_length - 1, self.length)\n\n            else:\n                # No insertion necessary, just append the new node\n                self.add_edge(parent_node, edge_start, self.length)\n\n    def find_insert_position(self, start_index, parent_node):\n        '''Finds the appropriate position to insert a suffix into the suffix tree.'''\n        for child_node in parent_node.children:\n            edge_start, edge_end = self.edges[(parent_node.number, child_node.number)]\n            if self.word[start_index:start_index + edge_end - edge_start] == self.word[edge_start:edge_end]:\n                return self.find_insert_position(start_index + edge_end - edge_start, child_node)\n\n            elif self.word[edge_start] == self.word[start_index]:\n                return child_node, start_index, True\n\n        return parent_node, start_index, False\n\n    def add_edge(self, parent_node, edge_start, edge_end, child_node=None):\n        '''Adds a node and the associated edge to the suffix tree.'''\n        if child_node is None:\n            child_node = self.Node(parent_node, len(self.nodes))\n\n        self.nodes.append(child_node)\n        parent_node.add_child(child_node)\n        self.edges[(parent_node.number, child_node.number)] = [edge_start, edge_end]\n\n    def get_edge_strings(self):\n        '''Returns the string representations of the edges.'''\n        return [self.word[i:j] for i, j in self.edges.values()]\n\n    def calculate_descendants(self, base_node):\n        '''Calculates the total number of descendants of a given node.'''\n        if base_node not in self.descendants_count:\n            self.descendants_count[base_node] = len(base_node.children) + sum([self.calculate_descendants(c) for c in base_node.children])\n\n        return self.descendants_count[base_node]\n\n    def get_node_word(self, end_node):\n        '''Returns the prefix of the suffix tree word up to a given node.'''\n        accumulated_word = ''\n        while end_node.number != 0:\n            edge_indices = self.edges[(end_node.parent.number, end_node.number)]\n            accumulated_word = self.word[edge_indices[0]:edge_indices[1]] + accumulated_word\n            end_node = end_node.parent\n\n        return accumulated_word.strip('$')\n\n\n# Sample input\ndna_sequence = \"ATTTGGATT\"\nsequence_length = len(dna_sequence)\n\n# After removing the termination symbol $, if necessary, each edge corresponds to len(edge) substrings\nedge_lengths = [edge if edge[1] != sequence_length + 1 else [edge[0], sequence_length] for edge in SuffixTree(dna_sequence).edges.values()]\nobserved_substrings = float(sum([edge[1] - edge[0] for edge in edge_lengths]))\n\n# The number of possible substrings of length k is min(4^k, n-k-1)\nmax_possible_substrings = float(sum([sequence_length - k + 1 if k &gt; log(sequence_length + 1) / log(4) else 4 ** k for k in range(1, sequence_length + 1)]))\n\nprint(observed_substrings / max_possible_substrings)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-41",
    "href": "posts/md/Rosalind_stronghold.html#explanation-41",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "81.4 Explanation",
    "text": "81.4 Explanation\n\n81.4.1 1. SuffixTree Class\n\n__init__(self, input_word): Initializes the suffix tree with the given word. It sets up initial structures and calls build_suffix_tree to construct the tree.\nNode Class: Represents nodes in the suffix tree.\n\n__init__(self, parent_node, node_number): Initializes a node with a parent and a unique number.\nadd_child(self, child_node): Adds a child node.\nremove_child(self, child_node): Removes a child node.\nupdate_parent(self, new_parent): Updates the parent of the node.\n\nbuild_suffix_tree(self, input_word): Constructs the suffix tree for the given word. Adds a terminator character $ if not already present.\nfind_insert_position(self, start_index, parent_node): Determines where to insert a suffix starting at start_index under parent_node. Handles edge splitting and suffix insertion.\nadd_edge(self, parent_node, edge_start, edge_end, child_node=None): Adds an edge between parent_node and child_node with start and end indices.\nget_edge_strings(self): Returns the substrings represented by each edge in the suffix tree.\ncalculate_descendants(self, base_node): Computes the total number of descendants of base_node, caching results for efficiency.\nget_node_word(self, end_node): Constructs the string from the root to end_node.\n\n\n\n81.4.2 2. Calculations\n\nobserved_substrings: Computes the total length of all distinct substrings from the suffix tree.\nmax_possible_substrings: Calculates the maximum number of distinct substrings possible for a string of length n with an alphabet of size 4.\nprint(observed_substrings / max_possible_substrings): Outputs the linguistic complexity as the ratio of observed distinct substrings to the maximum possible distinct substrings."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-81",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-81",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "82.1 Sample Dataset",
    "text": "82.1 Sample Dataset\n&gt;Rosalind_80\nMEANLYPRTEINSTRING\n&gt;Rosalind_21\nPLEASANTLYEINSTEIN"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-82",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-82",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "82.2 Sample Output",
    "text": "82.2 Sample Output\n23\nLYPRTEINSTRIN\nLYEINSTEIN"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-78",
    "href": "posts/md/Rosalind_stronghold.html#solution-78",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "82.3 Solution",
    "text": "82.3 Solution\nimport numpy as np\n\nclas PAM250:\n    \"\"\"The PAM250 scoring matrix class.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the scoring matrix.\"\"\"\n        # Manually define the PAM250 scoring matrix.\n        self.scoring_matrix = {\n            ('A', 'A'): 2, ('A', 'C'): -2, ('A', 'D'): 0, ('A', 'E'): 0, ('A', 'F'): -3, ('A', 'G'): 1, ('A', 'H'): -1,\n            ('A', 'I'): -1, ('A', 'K'): -1, ('A', 'L'): -2, ('A', 'M'): -1, ('A', 'N'): 0, ('A', 'P'): 1, ('A', 'Q'): 0,\n            ('A', 'R'): -2, ('A', 'S'): 1, ('A', 'T'): 1, ('A', 'V'): 0, ('A', 'W'): -6, ('A', 'Y'): -3,\n            ('C', 'A'): -2, ('C', 'C'): 12, ('C', 'D'): -5, ('C', 'E'): -5, ('C', 'F'): -4, ('C', 'G'): -3, ('C', 'H'): -3,\n            ('C', 'I'): -2, ('C', 'K'): -5, ('C', 'L'): -6, ('C', 'M'): -5, ('C', 'N'): -4, ('C', 'P'): -3, ('C', 'Q'): -5,\n            ('C', 'R'): -4, ('C', 'S'): 0, ('C', 'T'): -2, ('C', 'V'): -2, ('C', 'W'): -8, ('C', 'Y'): 0,\n            ('D', 'A'): 0, ('D', 'C'): -5, ('D', 'D'): 4, ('D', 'E'): 3, ('D', 'F'): -6, ('D', 'G'): 1, ('D', 'H'): 1,\n            ('D', 'I'): -2, ('D', 'K'): 0, ('D', 'L'): -4, ('D', 'M'): -3, ('D', 'N'): 2, ('D', 'P'): -1, ('D', 'Q'): 2,\n            ('D', 'R'): -1, ('D', 'S'): 0, ('D', 'T'): 0, ('D', 'V'): -2, ('D', 'W'): -7, ('D', 'Y'): -4,\n            ('E', 'A'): 0, ('E', 'C'): -5, ('E', 'D'): 3, ('E', 'E'): 4, ('E', 'F'): -5, ('E', 'G'): 0, ('E', 'H'): 1,\n            ('E', 'I'): -2, ('E', 'K'): 0, ('E', 'L'): -3, ('E', 'M'): -2, ('E', 'N'): 1, ('E', 'P'): -1, ('E', 'Q'): 2,\n            ('E', 'R'): -1, ('E', 'S'): 0, ('E', 'T'): 0, ('E', 'V'): -2, ('E', 'W'): -7, ('E', 'Y'): -4,\n            ('F', 'A'): -3, ('F', 'C'): -4, ('F', 'D'): -6, ('F', 'E'): -5, ('F', 'F'): 9, ('F', 'G'): -5, ('F', 'H'): -2,\n            ('F', 'I'): 1, ('F', 'K'): -5, ('F', 'L'): 2, ('F', 'M'): 0, ('F', 'N'): -3, ('F', 'P'): -5, ('F', 'Q'): -5,\n            ('F', 'R'): -4, ('F', 'S'): -3, ('F', 'T'): -3, ('F', 'V'): -1, ('F', 'W'): 0, ('F', 'Y'): 7,\n            ('G', 'A'): 1, ('G', 'C'): -3, ('G', 'D'): 1, ('G', 'E'): 0, ('G', 'F'): -5, ('G', 'G'): 5, ('G', 'H'): -2,\n            ('G', 'I'): -3, ('G', 'K'): -2, ('G', 'L'): -4, ('G', 'M'): -3, ('G', 'N'): 0, ('G', 'P'): 0, ('G', 'Q'): -1,\n            ('G', 'R'): -3, ('G', 'S'): 1, ('G', 'T'): 0, ('G', 'V'): -1, ('G', 'W'): -7, ('G', 'Y'): -5,\n            ('H', 'A'): -1, ('H', 'C'): -3, ('H', 'D'): 1, ('H', 'E'): 1, ('H', 'F'): -2, ('H', 'G'): -2, ('H', 'H'): 6,\n            ('H', 'I'): -2, ('H', 'K'): 0, ('H', 'L'): -2, ('H', 'M'): -2, ('H', 'N'): 2, ('H', 'P'): 0, ('H', 'Q'): 3,\n            ('H', 'R'): 2, ('H', 'S'): -1, ('H', 'T'): -1, ('H', 'V'): -2, ('H', 'W'): -3, ('H', 'Y'): 0,\n            ('I', 'A'): -1, ('I', 'C'): -2, ('I', 'D'): -2, ('I', 'E'): -2, ('I', 'F'): 1, ('I', 'G'): -3, ('I', 'H'): -2,\n            ('I', 'I'): 5, ('I', 'K'): -2, ('I', 'L'): 2, ('I', 'M'): 2, ('I', 'N'): -2, ('I', 'P'): -2, ('I', 'Q'): -2,\n            ('I', 'R'): -2, ('I', 'S'): -1, ('I', 'T'): 0, ('I', 'V'): 4, ('I', 'W'): -5, ('I', 'Y'): -1,\n            ('K', 'A'): -1, ('K', 'C'): -5, ('K', 'D'): 0, ('K', 'E'): 0, ('K', 'F'): -5, ('K', 'G'): -2, ('K', 'H'): 0,\n            ('K', 'I'): -2, ('K', 'K'): 5, ('K', 'L'): -3, ('K', 'M'): 0, ('K', 'N'): 1, ('K', 'P'): -1, ('K', 'Q'): 1,\n            ('K', 'R'): 3, ('K', 'S'): 0, ('K', 'T'): 0, ('K', 'V'): -2, ('K', 'W'): -3, ('K', 'Y'): -4,\n            ('L', 'A'): -2, ('L', 'C'): -6, ('L', 'D'): -4, ('L', 'E'): -3, ('L', 'F'): 2, ('L', 'G'): -4, ('L', 'H'): -2,\n            ('L', 'I'): 2, ('L', 'K'): -3, ('L', 'L'): 6, ('L', 'M'): 4, ('L', 'N'): -3, ('L', 'P'): -3, ('L', 'Q'): -2,\n            ('L', 'R'): -3, ('L', 'S'): -3, ('L', 'T'): -2, ('L', 'V'): 2, ('L', 'W'): -2, ('L', 'Y'): -1,\n            ('M', 'A'): -1, ('M', 'C'): -5, ('M', 'D'): -3, ('M', 'E'): -2, ('M', 'F'): 0, ('M', 'G'): -3, ('M', 'H'): -2,\n            ('M', 'I'): 2, ('M', 'K'): 0, ('M', 'L'): 4, ('M', 'M'): 6, ('M', 'N'): -2, ('M', 'P'): -2, ('M', 'Q'): -1,\n            ('M', 'R'): 0, ('M', 'S'): -2, ('M', 'T'): -1, ('M', 'V'): 2, ('M', 'W'): -4, ('M', 'Y'): -2,\n            ('N', 'A'): 0, ('N', 'C'): -4, ('N', 'D'): 2, ('N', 'E'): 1, ('N', 'F'): -3, ('N', 'G'): 0, ('N', 'H'): 2,\n            ('N', 'I'): -2, ('N', 'K'): 1, ('N', 'L'): -3, ('N', 'M'): -2, ('N', 'N'): 2, ('N', 'P'): 0, ('N', 'Q'): 1,\n            ('N', 'R'): 0, ('N', 'S'): 1, ('N', 'T'): 0, ('N', 'V'): -2, ('N', 'W'): -4, ('N', 'Y'): -2,\n            ('P', 'A'): 1, ('P', 'C'): -3, ('P', 'D'): -1, ('P', 'E'): -1, ('P', 'F'): -5, ('P', 'G'): 0, ('P', 'H'): 0,\n            ('P', 'I'): -2, ('P', 'K'): -1, ('P', 'L'): -3, ('P', 'M'): -2, ('P', 'N'): 0, ('P', 'P'): 6, ('P', 'Q'): 0,\n            ('P', 'R'): 0, ('P', 'S'): 1, ('P', 'T'): 0, ('P', 'V'): -1, ('P', 'W'): -6, ('P', 'Y'): -5,\n            ('Q', 'A'): 0, ('Q', 'C'): -5, ('Q', 'D'): 2, ('Q', 'E'): 2, ('Q', 'F'): -5, ('Q', 'G'): -1, ('Q', 'H'): 3,\n            ('Q', 'I'): -2, ('Q', 'K'): 1, ('Q', 'L'): -2, ('Q', 'M'): -1, ('Q', 'N'): 1, ('Q', 'P'): 0, ('Q', 'Q'): 4,\n            ('Q', 'R'): 1, ('Q', 'S'): -1, ('Q', 'T'): -1, ('Q', 'V'): -2, ('Q', 'W'): -5, ('Q', 'Y'): -4,\n            ('R', 'A'): -2, ('R', 'C'): -4, ('R', 'D'): -1, ('R', 'E'): -1, ('R', 'F'): -4, ('R', 'G'): -3, ('R', 'H'): 2,\n            ('R', 'I'): -2, ('R', 'K'): 3, ('R', 'L'): -3, ('R', 'M'): 0, ('R', 'N'): 0, ('R', 'P'): 0, ('R', 'Q'): 1,\n            ('R', 'R'): 6, ('R', 'S'): 0, ('R', 'T'): -1, ('R', 'V'): -2, ('R', 'W'): 2, ('R', 'Y'): -4,\n            ('S', 'A'): 1, ('S', 'C'): 0, ('S', 'D'): 0, ('S', 'E'): 0, ('S', 'F'): -3, ('S', 'G'): 1, ('S', 'H'): -1,\n            ('S', 'I'): -1, ('S', 'K'): 0, ('S', 'L'): -3, ('S', 'M'): -2, ('S', 'N'): 1, ('S', 'P'): 1, ('S', 'Q'): -1,\n            ('S', 'R'): 0, ('S', 'S'): 2, ('S', 'T'): 1, ('S', 'V'): -1, ('S', 'W'): -2, ('S', 'Y'): -3,\n            ('T', 'A'): 1, ('T', 'C'): -2, ('T', 'D'): 0, ('T', 'E'): 0, ('T', 'F'): -3, ('T', 'G'): 0, ('T', 'H'): -1,\n            ('T', 'I'): 0, ('T', 'K'): 0, ('T', 'L'): -2, ('T', 'M'): -1, ('T', 'N'): 0, ('T', 'P'): 0, ('T', 'Q'): -1,\n            ('T', 'R'): -1, ('T', 'S'): 1, ('T', 'T'): 3, ('T', 'V'): 0, ('T', 'W'): -5, ('T', 'Y'): -3,\n            ('V', 'A'): 0, ('V', 'C'): -2, ('V', 'D'): -2, ('V', 'E'): -2, ('V', 'F'): -1, ('V', 'G'): -1, ('V', 'H'): -2,\n            ('V', 'I'): 4, ('V', 'K'): -2, ('V', 'L'): 2, ('V', 'M'): 2, ('V', 'N'): -2, ('V', 'P'): -1, ('V', 'Q'): -2,\n            ('V', 'R'): -2, ('V', 'S'): -1, ('V', 'T'): 0, ('V', 'V'): 4, ('V', 'W'): -6, ('V', 'Y'): -2,\n            ('W', 'A'): -6, ('W', 'C'): -8, ('W', 'D'): -7, ('W', 'E'): -7, ('W', 'F'): 0, ('W', 'G'): -7, ('W', 'H'): -3,\n            ('W', 'I'): -5, ('W', 'K'): -3, ('W', 'L'): -2, ('W', 'M'): -4, ('W', 'N'): -4, ('W', 'P'): -6, ('W', 'Q'): -5,\n            ('W', 'R'): 2, ('W', 'S'): -2, ('W', 'T'): -5, ('W', 'V'): -6, ('W', 'W'): 17, ('W', 'Y'): 0,\n            ('Y', 'A'): -3, ('Y', 'C'): 0, ('Y', 'D'): -4, ('Y', 'E'): -4, ('Y', 'F'): 7, ('Y', 'G'): -5, ('Y', 'H'): 0,\n            ('Y', 'I'): -1, ('Y', 'K'): -4, ('Y', 'L'): -1, ('Y', 'M'): -2, ('Y', 'N'): -2, ('Y', 'P'): -5, ('Y', 'Q'): -4,\n            ('Y', 'R'): -4, ('Y', 'S'): -3, ('Y', 'T'): -3, ('Y', 'V'): -2, ('Y', 'W'): 0, ('Y', 'Y'): 10\n        }\n\n    def __getitem__(self, pair):\n        \"\"\"Return the score for a given pair of amino acids.\"\"\"\n        return self.scoring_matrix[pair]\n\ndef local_alignment(v, w, scoring_matrix, sigma):\n    \"\"\"Returns the score and local alignment with the given scoring matrix and indel penalty sigma for strings v, w.\"\"\"\n    \n    # Initialize the matrices S (score) and backtrack.\n    S = np.zeros((len(v) + 1, len(w) + 1), dtype=int)\n    backtrack = np.zeros((len(v) + 1, len(w) + 1), dtype=int)\n\n    # Fill in the score matrix S and the backtrack matrix.\n    for i in range(1, len(v) + 1):\n        for j in range(1, len(w) + 1):\n            match_score = scoring_matrix[v[i - 1], w[j - 1]]\n            scores = [\n                S[i - 1][j] - sigma,          # Insertion\n                S[i][j - 1] - sigma,          # Deletion\n                S[i - 1][j - 1] + match_score, # Match/Mismatch\n                0                             # Local alignment can start anywhere, so 0 is a valid score.\n            ]\n            S[i][j] = max(scores)             # Maximum score for cell (i, j)\n            backtrack[i][j] = scores.index(S[i][j])  # Record which move was taken.\n\n    # Find the position of the highest scoring cell in the matrix.\n    max_pos = np.unravel_index(np.argmax(S), S.shape)\n    max_score = str(S[max_pos])\n\n    # Start backtracking from the position of the highest score.\n    i, j = max_pos\n    v_aligned, w_aligned = [], []\n\n    # Reconstruct the alignment.\n    while S[i][j] != 0:\n        if backtrack[i][j] == 0:  # Insertion\n            i -= 1\n            v_aligned.append(v[i])\n            w_aligned.append('-')\n        elif backtrack[i][j] == 1:  # Deletion\n            j -= 1\n            v_aligned.append('-')\n            w_aligned.append(w[j])\n        else:  # Match or mismatch\n            i -= 1\n            j -= 1\n            v_aligned.append(v[i])\n            w_aligned.append(w[j])\n\n    # Reverse the aligned sequences since they were constructed backwards.\n    v_aligned = ''.join(v_aligned[::-1])\n    w_aligned = ''.join(w_aligned[::-1])\n\n    return max_score, v_aligned, w_aligned\n\n# Sample Input\nsample_input = \"\"\"&gt;Rosalind_80\nMEANLYPRTEINSTRING\n&gt;Rosalind_21\nPLEASANTLYEINSTEIN\n\"\"\"\n\n# Parse input FASTA format data\ndef parse_fasta(data):\n    sequences = []\n    parts = data.strip().split('&gt;')\n    for part in parts:\n        if part:\n            lines = part.split('\\n')\n            sequence = ''.join(lines[1:])\n            sequences.append(sequence)\n    return sequences\n\nsequences = parse_fasta(sample_input)\nseq1, seq2 = sequences[0], sequences[1]\n\n# Get the local alignment with sigma = 5.\nalignment = local_alignment(seq1, seq2, PAM250(), 5)\n\n# Print the alignment.\nprint('\\n'.join(alignment))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-42",
    "href": "posts/md/Rosalind_stronghold.html#explanation-42",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "82.4 Explanation",
    "text": "82.4 Explanation\n\nPAM250 Class:\n\nThe PAM250 clas is defined to encapsulate the scoring matrix. The matrix is stored in a dictionary where the keys are tuples of amino acid pairs, and the values are their corresponding scores.\nThe __getitem__ method allows easy acces to the matrix using tuple indexing, e.g., scoring_matrix['A', 'A'].\n\nlocal_alignment Function:\n\nThe local_alignment function computes the local alignment using dynamic programming.\nS is the score matrix that keeps track of the best scores for subsequences of v and w.\nbacktrack keeps track of the choices made to achieve the score in each cell (insertion, deletion, match/mismatch, or starting a new local alignment).\nThe function iterates over the strings v and w, filling the matrices according to the local alignment rules.\nThe backtracking step reconstructs the aligned sequences from the highest scoring position until a score of zero is encountered, indicating the start of the local alignment.\n\nBacktracking and Alignment:\n\nThe backtracking loop constructs the aligned sequences by following the recorded moves in the backtrack matrix.\nThe sequences are built in reverse and then reversed at the end to give the correct alignment.\n\nparse_fasta:\n\nThe parse_fasta function reads the input FASTA format data and extracts the sequences. It returns a list of sequences to be used in the alignment.\n\nExecution:\n\nThe code parses the sample input, computes the local alignment, and prints the aligned sequences and the alignment score."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-82",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-82",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "83.1 Sample Dataset",
    "text": "83.1 Sample Dataset\n&gt;Rosalind_92\nAACGTA\n&gt;Rosalind_47\nACACCTA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-83",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-83",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "83.2 Sample Output",
    "text": "83.2 Sample Output\n3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-79",
    "href": "posts/md/Rosalind_stronghold.html#solution-79",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "83.3 Solution",
    "text": "83.3 Solution\ndef maximum_gap_symbols(v, w):\n    \"\"\"\n    Returns the maximum number of gap symbols in an optimal alignment of v and w.\n    The maximum number of gaps is derived from the lengths of the input strings and the longest common subsequence (LCS) between them.\n    \"\"\"\n    # Initialize a matrix to store the lengths of the longest common subsequence\n    len_v, len_w = len(v), len(w)\n    lcs_matrix = [[0] * (len_w + 1) for _ in range(len_v + 1)]\n\n    # Fill the matrix based on LCS dynamic programming\n    for i in range(1, len_v + 1):\n        for j in range(1, len_w + 1):\n            if v[i - 1] == w[j - 1]:\n                lcs_matrix[i][j] = lcs_matrix[i - 1][j - 1] + 1\n            else:\n                lcs_matrix[i][j] = max(lcs_matrix[i][j - 1], lcs_matrix[i - 1][j])\n\n    # Calculate the maximum number of gap symbols\n    lcs_length = lcs_matrix[len_v][len_w]\n    max_gaps = len_v + len_w - 2 * lcs_length\n\n    return max_gaps\n\ndef parse_fasta(data):\n    \"\"\"\n    Parses input FASTA format data and returns a list of sequences.\n    \"\"\"\n    sequences = []\n    entries = data.strip().split('&gt;')\n    \n    for entry in entries:\n        if entry:\n            lines = entry.splitlines()\n            sequence = ''.join(lines[1:])\n            sequences.append(sequence)\n    \n    return sequences\n\n# Sample input in FASTA format\nsample_input = \"\"\"\n&gt;Rosalind_92\nAACGTA\n&gt;Rosalind_47\nACACCTA\n\"\"\"\n\n# Parse the sequences from the sample input\nsequences = parse_fasta(sample_input)\nv, w = sequences[0], sequences[1]\n\n# Get the maximum number of gap symbols\nmax_gaps = maximum_gap_symbols(v, w)\n\n# Print the result\nprint(max_gaps)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-43",
    "href": "posts/md/Rosalind_stronghold.html#explanation-43",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "83.4 Explanation",
    "text": "83.4 Explanation\nmaximum_gap_symbols(v, w) function calculates the maximum number of gap symbols that can appear in any optimal alignment of two DNA strings v and w. It does so by using the Longest Common Subsequence (LCS) method.\n\nInitialization:\nlen_v, len_w = len(v), len(w)\nlcs_matrix = [[0] * (len_w + 1) for _ in range(len_v + 1)]\nHere, the lengths of the input strings v and w are stored in len_v and len_w. Then, a 2D matrix lcs_matrix is initialized with all elements set to 0. The matrix has dimensions (len_v + 1) x (len_w + 1).\nFilling the LCS Matrix:\nfor i in range(1, len_v + 1):\n    for j in range(1, len_w + 1):\n        if v[i - 1] == w[j - 1]:\n            lcs_matrix[i][j] = lcs_matrix[i - 1][j - 1] + 1\n        else:\n            lcs_matrix[i][j] = max(lcs_matrix[i][j - 1], lcs_matrix[i - 1][j])\nThis loop fills the lcs_matrix based on the LCS dynamic programming approach:\n\nIf the characters v[i-1] and w[j-1] are equal, the value at lcs_matrix[i][j] is updated to lcs_matrix[i-1][j-1] + 1.\nOtherwise, it takes the maximum of the values to the left (lcs_matrix[i][j-1]) and above (lcs_matrix[i-1][j]).\n\nCalculating Maximum Number of Gaps:\nlcs_length = lcs_matrix[len_v][len_w]\nmax_gaps = len_v + len_w - 2 * lcs_length\nAfter filling the matrix, the length of the LCS is stored in lcs_length. The maximum number of gaps is then calculated using the formula len(v) + len(w) - 2 * lcs_length."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-83",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-83",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "84.1 Sample Dataset",
    "text": "84.1 Sample Dataset\nTAGAGATAGAATGGGTCCAGAGTTTTGTAATTTCCATGGGTCCAGAGTTTTGTAATTTATTATATAGAGATAGAATGGGTCCAGAGTTTTGTAATTTCCATGGGTCCAGAGTTTTGTAATTTAT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-84",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-84",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "84.2 Sample Output",
    "text": "84.2 Sample Output\nATGGGTCCAGAGTTTTGTAATTT\nTAGAGATAGAATGGGTCCAGAGTTTTGTAATTTCCATGGGTCCAGAGTTTTGTAATTTAT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-80",
    "href": "posts/md/Rosalind_stronghold.html#solution-80",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "84.3 Solution",
    "text": "84.3 Solution\nimport functools\n\nclas SuffixTree(object):\n    '''Creates a suffix tree for the provided word.'''\n\n    def __init__(self, word):\n        '''Initializes the suffix tree.'''\n        self.nodes = [self.Node(None, 0)]\n        self.edges = dict()\n        self.descendants_dict = dict()\n        if type(word) == str:\n            self.add_word(word)\n\n    clas Node(object):\n        '''Suffix tree node class.'''\n        def __init__(self, parent, number):\n            self.parent = parent\n            self.number = number\n            self.children = []\n\n        def add_child(self, child):\n            self.children.append(child)\n\n        def remove_child(self, child):\n            self.children.remove(child)\n\n        def update_parent(self, parent):\n            self.parent = parent\n\n    def add_word(self, word):\n        '''Add a word to the suffix tree.'''\n        # Check to make sure word ends in '$'.\n        if word[-1] != '$':\n            word += '$'\n        self.word = word\n        self.n = len(self.word)\n\n        for i in range(self.n):\n            parent_node, edge_start, overlap = self.insert_position(i, self.nodes[0])\n\n            if overlap:\n                p_edge_start, p_edge_end = self.edges[(parent_node.parent.number, parent_node.number)]\n\n                # Get the edge to insert\n                insert_len = 0\n                while word[edge_start:edge_start + insert_len] == word[p_edge_start:p_edge_start + insert_len]:\n                    insert_len += 1\n\n                # Create a new node for insertion\n                new_node = self.Node(parent_node.parent, len(self.nodes))\n                new_node.add_child(parent_node)\n                self.add_node(parent_node.parent, p_edge_start, p_edge_start + insert_len - 1, new_node)\n\n                # Update the parent node since a new node is inserted above it\n                del self.edges[(parent_node.parent.number, parent_node.number)]\n                parent_node.parent.remove_child(parent_node)\n                parent_node.update_parent(new_node)\n                self.edges[(parent_node.parent.number, parent_node.number)] = [p_edge_start + insert_len - 1, p_edge_end]\n\n                # Add new child node\n                self.add_node(new_node, edge_start + insert_len - 1, self.n)\n\n            else:\n                # No insertion necessary, just append the new node.\n                self.add_node(parent_node, edge_start, self.n)\n\n    def insert_position(self, start_index, parent_node):\n        '''Determine the location and method to insert a suffix into the suffix tree.'''\n        for child_node in parent_node.children:\n            edge_start, edge_end = self.edges[(parent_node.number, child_node.number)]\n            if self.word[start_index:start_index + edge_end - edge_start] == self.word[edge_start:edge_end]:\n                return self.insert_position(start_index + edge_end - edge_start, child_node)\n\n            elif self.word[edge_start] == self.word[start_index]:\n                return child_node, start_index,  True\n\n        return parent_node, start_index, False\n\n    def add_node(self, parent_node, edge_start, edge_end, child_node=None):\n        '''Adds a node and the associated edge to the suffix tree.'''\n\n        # Create child node, if necessary\n        if child_node is None:\n            child_node = self.Node(parent_node, len(self.nodes))\n\n        # Add node to node list\n        self.nodes.append(child_node)\n\n        # Add child to parent\n        parent_node.add_child(child_node)\n\n        # Add edge to edge dict\n        self.edges[(parent_node.number, child_node.number)] = [\n            edge_start, edge_end]\n\n    def print_edges(self):\n        '''Returns the string representations of the edges.'''\n        return [self.word[i:j] for i, j in self.edges.values()]\n\n    def total_descendants(self, base_node):\n        '''Returns the total number of descendants of a given node.'''\n        if base_node not in self.descendants_dict:\n            self.descendants_dict[base_node] = len(base_node.children) + sum([self.total_descendants(c) for c in base_node.children])\n\n        return self.descendants_dict[base_node]\n\n    def node_word(self, end_node):\n        '''Returns the prefix of the suffix tree word up to a given node.'''\n        current_word = ''\n        while end_node.number != 0:\n            temp_indices = self.edges[(end_node.parent.number, end_node.number)]\n            current_word = self.word[temp_indices[0]:temp_indices[1]] + current_word\n            end_node = end_node.parent\n\n        return current_word.strip('$')\n\nclas Trie(object):\n    '''Constructs a trie.'''\n\n    def __init__(self, word=None):\n        self.nodes = [[self.Node('', 1)]]\n        self.edges = []\n        if word is not None:\n            self.add_word(word)\n\n    clas Node(object):\n        '''Trie node class.'''\n        def __init__(self, prefix, number):\n            self.prefix = prefix\n            self.number = number\n            self.depth = len(prefix)\n\n    clas Edge(object):\n        '''Trie edge class.'''\n        def __init__(self, letter, par_node, chi_node):\n            self.letter = letter\n            self.parent_node = par_node\n            self.child_node = chi_node\n\n        def get_info(self):\n            '''Return the edge information compactly.'''\n            return ' '.join(map(str, [self.parent_node, self.child_node, self.letter]))\n\n    def add_word(self, word):\n        '''Adds a word to the trie.'''\n        if type(word) == list:\n            for w in word:\n                self.add_word(w)\n        else:\n            parent = self.find_parent(word)\n            for i in range(len(parent.prefix), len(word)):\n                new_node = self.Node(word[:i + 1], self.node_count() + 1)\n                self.edges.append(self.Edge(word[i], parent.number, self.node_count() + 1))\n                self.insert_node(new_node)\n                parent = new_node\n\n    def insert_node(self, node):\n        '''Determine the location to insert the current node.'''\n        if node.depth &gt; self.depth():\n            self.nodes.append([node])\n        else:\n            self.nodes[node.depth].append(node)\n\n    def depth(self):\n        '''Returns the depth of the trie.'''\n        return len(self.nodes) - 1\n\n    def node_count(self):\n        '''Returns the total number of nodes.'''\n        count = 0\n        for trie_depth in self.nodes:\n            count += len(trie_depth)\n        return count\n\n    def find_parent(self, word):\n        '''Return the parent node of the word to be inserted.'''\n        for i in range(min(len(word), self.depth()), 0, -1):\n            for node in self.nodes[i]:\n                if word[:i] == node.prefix:\n                    return node\n\n        return self.nodes[0][0]\n\n# Read the input data.\nsample_input = \"\"\"\nTAGAGATAGAATGGGTCCAGAGTTTTGTAATTTCCATGGGTCCAGAGTTTTGTAATTTATTATATAGAGATAGAATGGGTCCAGAGTTTTGTAATTTCCATGGGTCCAGAGTTTTGTAATTTAT\n\"\"\"\n\ndna = sample_input.strip()\n\n# Create the Suffix Tree.\nsuff = SuffixTree(dna)\n\n# Store all multiple repeats of length at least 20 in a dictionary keyed on number of appearances.\nrepeat_dict = {}\nfor node in suff.nodes[1:]:\n    if suff.total_descendants(node) &gt;= 2 and len(suff.node_word(node)) &gt;= 20:\n        if suff.total_descendants(node) not in repeat_dict:\n            repeat_dict[suff.total_descendants(node)] = [suff.node_word(node)]\n        else:\n            repeat_dict[suff.total_descendants(node)].append(suff.node_word(node))\n\n# Filter out non-maximal repeats.\nrepeats = []\nfor values in repeat_dict.values():\n    if len(values) == 1:\n        repeats += values\n    else:\n        repeats += filter(lambda v: all(v not in word for word in values if word != v), values)\n\n# Print and save the answer.\nprint('\\n'.join(repeats))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-breakdown-of-the-code",
    "href": "posts/md/Rosalind_stronghold.html#detailed-breakdown-of-the-code",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "84.4 Detailed Breakdown of the Code",
    "text": "84.4 Detailed Breakdown of the Code\n\nsuff = SuffixTree(dna):\n\nConstructs a suffix tree for the DNA sequence.\n\nBuilding repeat_dict:\n\nCollects all repeats that appear at least twice and are at least 20 characters long.\nUses the total_descendants method to count occurrences.\n\nFiltering Non-Maximal Repeats:\n\nEnsures that each repeat is maximal by checking if it is not a substring of any other repeat in the same list.\n\nPrinting Results:\n\nPrints the filtered list of maximal repeats."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-84",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-84",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "85.1 Sample Dataset",
    "text": "85.1 Sample Dataset\n&gt;Rosalind_7\nATATCCG\n&gt;Rosalind_35\nTCCG\n&gt;Rosalind_23\nATGTACTG\n&gt;Rosalind_44\nATGTCTG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-85",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-85",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "85.2 Sample Output",
    "text": "85.2 Sample Output\n-18\nATAT-CCG\n-T---CCG\nATGTACTG\nATGT-CTG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-81",
    "href": "posts/md/Rosalind_stronghold.html#solution-81",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "85.3 Solution",
    "text": "85.3 Solution\nimport numpy as np\n\ndef score(chars, match=0, mismatch=-1):\n    \"\"\"\n    Calculate the alignment score for a list of characters.\n    :param chars: List of characters.\n    :param match: Score for matching characters.\n    :param mismatch: Score for mismatching characters.\n    :return: Total alignment score.\n    \"\"\"\n    return sum(match if chars[i] == chars[j] else mismatch for i in range(len(chars)) for j in range(i + 1, len(chars)))\n\n\ndef generate_indices(dimensions):\n    \"\"\"\n    Generate all possible indices for the given dimensions.\n    :param dimensions: List of dimensions for each sequence.\n    :return: Generator yielding tuples of indices.\n    \"\"\"\n    total_combinations = np.prod(dimensions)\n    indices = [0] * len(dimensions)\n    for _ in range(total_combinations):\n        yield tuple(indices)\n        for j in reversed(range(len(dimensions))):\n            indices[j] += 1\n            if indices[j] &lt; dimensions[j]:\n                break\n            indices[j] = 0\n\n\ndef generate_moves(num_sequences, options=[0, -1]):\n    \"\"\"\n    Generate all valid move combinations.\n    :param num_sequences: Number of sequences.\n    :param options: Possible move options (0 for match, -1 for gap).\n    :return: List of valid move combinations.\n    \"\"\"\n    def recursive_moves(m):\n        if m == 1:\n            return [[o] for o in options]\n        return [[o] + rest for o in options for rest in recursive_moves(m - 1)]\n\n    return [move for move in recursive_moves(num_sequences) if any(x != 0 for x in move)]\n\n\ndef add_tuples(u, v):\n    \"\"\"\n    Add two tuples element-wise.\n    :param u: First tuple.\n    :param v: Second tuple.\n    :return: Element-wise sum of the tuples.\n    \"\"\"\n    return tuple(a + b for a, b in zip(u, v))\n\n\ndef build_scoring_matrix(Strings, score_function=score):\n    \"\"\"\n    Build the scoring matrix for the alignment.\n    :param Strings: List of sequences to align.\n    :param score_function: Function to calculate alignment score.\n    :return: Scoring matrix, path dictionary, and move list.\n    \"\"\"\n    def calculate_scores(index):\n        def get_score(move):\n            previous = add_tuples(index, move)\n            if any(p &lt; 0 for p in previous):\n                return None\n            scorable = [Strings[j][previous[j]] if move[j] &lt; 0 else '-' for j in range(len(move))]\n            return scoring_matrix[previous] + score_function(scorable)\n\n        raw_scores = [(get_score(move), move) for move in available_moves]\n        return [(score, move) for score, move in raw_scores if score is not None]\n\n    dimensions = [len(S) + 1 for S in Strings]\n    scoring_matrix = np.zeros(dimensions, dtype=int)\n    path = {}\n    available_moves = generate_moves(len(Strings))\n\n    for index_set in generate_indices(dimensions):\n        scores_moves = calculate_scores(index_set)\n        if scores_moves:\n            scores, moves = zip(*scores_moves)\n            best_index = np.argmax(scores)\n            scoring_matrix[index_set] = scores[best_index]\n            path[index_set] = moves[best_index]\n\n    return scoring_matrix, path, available_moves\n\n\ndef backtrack_alignment(scoring_matrix, path, Strings):\n    \"\"\"\n    Perform backtracking to retrieve the optimal alignment.\n    :param scoring_matrix: Scoring matrix.\n    :param path: Path dictionary for moves.\n    :param Strings: List of sequences to align.\n    :return: Alignment score and aligned sequences.\n    \"\"\"\n    def reverse_string(s):\n        return ''.join(reversed(s))\n\n    position = tuple(len(S) for S in Strings)\n    alignment_score = scoring_matrix[position]\n    alignments = [[] for _ in Strings]\n\n    while any(p != 0 for p in position):\n        move = path[position]\n        for i, m in enumerate(move):\n            if m == 0:\n                alignments[i].append('-')\n            else:\n                alignments[i].append(Strings[i][position[i] - 1])\n        position = add_tuples(position, move)\n\n    return alignment_score, [reverse_string(s) for s in alignments]\n\n\ndef FindHighestScoringMultipleSequenceAlignment(Strings, score_function=score):\n    \"\"\"\n    Find the highest scoring multiple sequence alignment.\n    :param Strings: List of sequences to align.\n    :param score_function: Function to calculate alignment score.\n    :return: Alignment score and aligned sequences.\n    \"\"\"\n    scoring_matrix, path, _ = build_scoring_matrix(Strings, score_function)\n    return backtrack_alignment(scoring_matrix, path, Strings)\n\n\ndef parse_fasta(data):\n    \"\"\"\n    Parse FASTA format data into a list of sequences.\n    :param data: FASTA format input data.\n    :return: List of sequences.\n    \"\"\"\n    sequences = []\n    entries = data.strip().split('&gt;')\n    for entry in entries:\n        if entry:\n            lines = entry.splitlines()\n            sequence = ''.join(lines[1:])\n            sequences.append(sequence)\n    return sequences\n\n\n# Sample input in FASTA format\nsample_input = \"\"\"\n&gt;Rosalind_7\nATATCCG\n&gt;Rosalind_35\nTCCG\n&gt;Rosalind_23\nATGTACTG\n&gt;Rosalind_44\nATGTCTG\n\"\"\"\n\n# Parse the sequences from the sample input\nwords = parse_fasta(sample_input)\n\n# Get the alignment.\nscore, alignment = FindHighestScoringMultipleSequenceAlignment(words)\n\n# Print the alignment score and sequences.\nprint(score)\nfor line in alignment:\n    print(line)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explain-the-code",
    "href": "posts/md/Rosalind_stronghold.html#explain-the-code",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "85.4 Explain the code",
    "text": "85.4 Explain the code\n\nscore(chars, match, mismatch):\n\nThis function computes the alignment score based on matches and mismatches.\n\ngenerate_indices(dimensions):\n\nGenerates all possible index tuples for alignment, given the sequence lengths.\n\ngenerate_moves(num_sequences, options):\n\nGenerates valid move combinations for alignment, ensuring at least one non-gap move.\n\nadd_tuples(u, v):\n\nElement-wise addition of two tuples.\n\nbuild_scoring_matrix(Strings, score_function):\n\nConstructs the scoring matrix and paths for backtracking.\n\nbacktrack_alignment(scoring_matrix, path, Strings):\n\nReconstructs the alignment based on the scoring matrix and path.\n\nparse_fasta(data):\n\nParses FASTA format data into a list of sequences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-85",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-85",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "86.1 Sample Dataset",
    "text": "86.1 Sample Dataset\n2 2 3 3 4 5 6 7 8 10"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-86",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-86",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "86.2 Sample Output",
    "text": "86.2 Sample Output\n0 2 4 7 10"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-82",
    "href": "posts/md/Rosalind_stronghold.html#solution-82",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "86.3 Solution",
    "text": "86.3 Solution\nfrom math import sqrt\n\ndef reconstruct_set(input_data):\n    \"\"\"\n    Reconstruct the original set from the given differences.\n    \n    :param input_data: A string containing space-separated integers representing the differences\n    :return: A list of integers representing the reconstructed set\n    \"\"\"\n    # Convert input string to list of integers\n    differences = list(map(int, input_data.strip().split()))\n\n    # Calculate the number of elements in the original set\n    # using the quadratic formula: n(n-1)/2 = len(differences)\n    set_size = int(0.5 + 0.5 * sqrt(8.0 * len(differences) + 1))\n\n    # Initialize the result set with 0\n    result_set = [0]\n\n    # Add the largest difference to the result set\n    largest_difference = max(differences)\n    result_set.append(largest_difference)\n    differences.remove(largest_difference)\n\n    # Create a set of unique differences\n    unique_differences = set(differences)\n\n    for candidate in unique_differences:\n        # Check if the candidate fits with all existing elements in the result set\n        if sum([(abs(candidate - element) in differences) for element in result_set]) == len(result_set):\n            for element in result_set:\n                # Remove the differences we've already accounted for\n                differences.remove(abs(candidate - element))\n            # Add the new element to the result set\n            result_set.append(candidate)\n            if len(result_set) == set_size:\n                break\n\n    return sorted(result_set)\n\n# Example usage\ninput_data = \"\"\"\n2 2 3 3 4 5 6 7 8 10\n\"\"\"\n\nresult = reconstruct_set(input_data)\nprint(' '.join(map(str, result)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#step-by-step-explanation",
    "href": "posts/md/Rosalind_stronghold.html#step-by-step-explanation",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "86.4 Step-by-Step Explanation",
    "text": "86.4 Step-by-Step Explanation\n\nConvert Input to a List:\n\nThe input string is converted into a list of integers. These integers represent the differences between every pair of elements in the original set.\n\nDetermine the Number of Elements:\n\nThe code calculates how many numbers were in the original set using a mathematical formula related to the number of differences.\n\nStart with the Smallest Element:\n\nThe code assumes the smallest number in the set is 0 and starts the result_set with [0].\n\nAdd the Largest Difference:\n\nThe largest number in the original set is found by taking the largest difference from the list. This number is added to the result_set.\n\nReconstruct the Remaining Numbers:\n\nThe code checks each remaining difference to see if it can be used to find other numbers in the set. It does this by ensuring that each candidate number fits with all previously found numbers (i.e., the differences match).\n\nBuild the Set:\n\nAs valid numbers are found, they are added to the result_set, and the corresponding differences are removed from the list.\n\nReturn the Sorted Set:\n\nThe result_set is sorted and returned, which is the reconstructed original set."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-86",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-86",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "87.1 Sample Dataset",
    "text": "87.1 Sample Dataset\n4"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-87",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-87",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "87.2 Sample Output",
    "text": "87.2 Sample Output\n15"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-83",
    "href": "posts/md/Rosalind_stronghold.html#solution-83",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "87.3 Solution",
    "text": "87.3 Solution\ndef count_unrooted_binary_trees(n):\n    '''Returns the number of unrooted binary trees with n leaves.'''\n    # The total number is just the double factorial (2n - 5)!!\n    result = 1\n    for i in range(2 * n - 5, 1, -2):\n        result = (result * i) % 10**6\n    return result\n\ndef count_rooted_binary_trees(n):\n    '''Returns the number of rooted binary trees with n leaves.'''\n    # Can transform an unrooted binary tree into a rooted binary tree by inserting\n    # a node into any of its 2*n - 3 edges.\n    return (count_unrooted_binary_trees(n) * (2 * n - 3)) % 10**6\n\n# Read the input data.\ninput_data = \"\"\"\n4\n\"\"\"\n\nn = int(input_data.strip())\n\n# Get the number of unrooted binary trees.\ncount = count_rooted_binary_trees(n)\n\n# Print the answer.\nprint(count)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#count_unrooted_binary_treesn",
    "href": "posts/md/Rosalind_stronghold.html#count_unrooted_binary_treesn",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "87.4 count_unrooted_binary_trees(n)",
    "text": "87.4 count_unrooted_binary_trees(n)\n\nPurpose: This function calculates the number of possible unrooted binary trees with n leaves.\nLogic:\n\nThe number of unrooted binary trees with n leaves is given by the double factorial of (2n - 5), which is denoted as (2n - 5)!!.\nThe double factorial of a number is the product of all integers down to 1 that have the same parity (odd/even) as the starting number.\nFor example, if n = 4, (2n - 5) = 3, and the double factorial would be 3!! = 3.\nThe loop multiplies all odd numbers from 2n - 5 down to 3.\nThe result is taken modulo 10^6 to keep the number manageable and avoid overflow."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#count_rooted_binary_treesn",
    "href": "posts/md/Rosalind_stronghold.html#count_rooted_binary_treesn",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "87.5 count_rooted_binary_trees(n)",
    "text": "87.5 count_rooted_binary_trees(n)\n\nPurpose: This function calculates the number of possible rooted binary trees with n leaves.\nLogic:\n\nA rooted binary tree can be derived from an unrooted binary tree by adding a root to any of the 2n - 3 edges of the unrooted tree.\nTherefore, the number of rooted binary trees is the number of unrooted binary trees multiplied by (2n - 3).\nAgain, the result is taken modulo 10^6."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#how-it-works",
    "href": "posts/md/Rosalind_stronghold.html#how-it-works",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "87.6 How It Works",
    "text": "87.6 How It Works\n\nInput: The code reads the input value n from the string input_data. For example, if n = 4, the code calculates the number of binary trees for n = 4.\nExecution Flow:\n\ncount_rooted_binary_trees(n) is called with n = 4.\nInside this function, count_unrooted_binary_trees(n) is called.\nThe count_unrooted_binary_trees(n) function computes the product (2n - 5)!! modulo 10^6:\n\nFor n = 4, (2n - 5) = 3.\nThe loop runs from 3 to 1 (odd numbers only), resulting in 3!! = 3.\n\nThe result (3 in this case) is multiplied by (2 * n - 3) = 5, giving 3 * 5 = 15.\nThe final result is 15 % 10^6 = 15, which is returned and printed."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-87",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-87",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "88.1 Sample Dataset",
    "text": "88.1 Sample Dataset\n0.1 0.5 0.8"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-88",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-88",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "88.2 Sample Output",
    "text": "88.2 Sample Output\n0.18 0.5 0.32"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-84",
    "href": "posts/md/Rosalind_stronghold.html#solution-84",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "88.3 Solution",
    "text": "88.3 Solution\n# Read the input data.\ninput_data = \"\"\"\n0.1 0.5 0.8\n\"\"\"\n\n# Convert input data to a list of floats.\nnumbers = [float(x) for x in input_data.strip().split()]\n\n# Calculate the desired values.\nresults = [2 * (x - x**2) for x in numbers]\n\n# Format the results to two decimal places and print.\nformatted_results = ' '.join(f\"{result:f}\" for result in results)\nprint(formatted_results)\n\nunformatted_results = ' '.join(f\"{result}\" for result in results)\nprint(unformatted_results) # only unformatted_results accepted to answer\nCalculate Results: - For each number in numbers, the code calculates a new value using the formula 2 * (x - x**2). This formula computes the difference between a number and its square, doubles it, and stores it in the results list. - The calculations for each number would be: - For 0.1: 2 * (0.1 - 0.1**2) = 0.18 - For 0.5: 2 * (0.5 - 0.5**2) = 0.50 - For 0.8: 2 * (0.8 - 0.8**2) = 0.32 - The resulting list is: [0.18, 0.50, 0.32]."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-88",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-88",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "89.1 Sample Dataset",
    "text": "89.1 Sample Dataset\ndog rat elephant mouse cat rabbit\n(rat,(dog,cat),(rabbit,(elephant,mouse)));\n(rat,(cat,dog),(elephant,(mouse,rabbit)));"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-89",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-89",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "89.2 Sample Output",
    "text": "89.2 Sample Output\n2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-85",
    "href": "posts/md/Rosalind_stronghold.html#solution-85",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "89.3 Solution",
    "text": "89.3 Solution\nimport random\n\ndef get_fingerprints_list(taxa_dict, tree):\n    result = []\n    last_char = ''\n    taxon = ''\n    taxa_stack = []\n    \n    for char in tree:\n        if char in ('(', ',', ')'):\n            if last_char in ('(', ','):\n                if taxon:\n                    taxa_stack.append(taxa_dict[taxon])\n                    taxon = ''\n            elif last_char == ')':\n                t1 = taxa_stack.pop()\n                t2 = taxa_stack.pop()\n                result.append(t1 ^ t2)\n                taxa_stack.append(t1 ^ t2)\n            last_char = char\n        else:\n            if char != ' ':\n                taxon += char\n    \n    return result\n\ndef find_split_distance(taxa, tree1, tree2):\n    random.seed()  # Initialize random number generator\n    \n    taxa_dict = {taxon: random.randint(0, 2**12) for taxon in taxa}  # Adjusted bit range for randomness\n\n    fingerprints1 = sorted(get_fingerprints_list(taxa_dict, tree1))\n    fingerprints2 = sorted(get_fingerprints_list(taxa_dict, tree2))\n\n    shared_count = 0\n    i, j = len(fingerprints1) - 1, len(fingerprints2) - 1\n\n    while i &gt;= 0 and j &gt;= 0:\n        if fingerprints1[i] == fingerprints2[j]:\n            shared_count += 1\n            i -= 1\n            j -= 1\n        elif fingerprints1[i] &gt; fingerprints2[j]:\n            i -= 1\n        else:\n            j -= 1\n\n    return 2 * (len(taxa) - 3) - 2 * shared_count\n\n# Input data\nsample_input = \"\"\"\ndog rat elephant mouse cat rabbit\n(rat,(dog,cat),(rabbit,(elephant,mouse)));\n(rat,(cat,dog),(elephant,(mouse,rabbit)));\n\"\"\"\ninput_lines = sample_input.strip().split(\"\\n\")\ntaxa = input_lines[0].split()\ntree1 = input_lines[1]\ntree2 = input_lines[2]\n\n# Compute the maximum split distance over 500 iterations\nmax_distance = max(find_split_distance(taxa, tree1, tree2) for _ in range(500))\n\nprint(max_distance)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-44",
    "href": "posts/md/Rosalind_stronghold.html#explanation-44",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "89.4 Explanation",
    "text": "89.4 Explanation\n\nFunction get_fingerprints_list(taxa_dict, tree):\n\nPurpose: Converts a tree in Newick format into a list of fingerprints based on a dictionary of taxon identifiers.\nHow It Works:\n\nIterates through characters in the tree string.\nHandles tree structure symbols (, ,, ) and taxon names.\nUses a stack (taxa_stack) to keep track of taxon fingerprints.\nWhen encountering ), it combines the fingerprints of the last two taxa in the stack using the XOR operation (^), which is a common way to handle such trees.\n\n\nFunction find_split_distance(taxa, tree1, tree2):\n\nPurpose: Computes the split distance between two trees.\nHow It Works:\n\nGenerates a random dictionary mapping taxa to unique integer fingerprints.\nCalculates fingerprints for both trees and sorts them.\nFinds the number of shared fingerprints between the two trees.\nComputes the split distance using the formula 2 * (n - 3) - 2 * shared_count, where n is the number of taxa.\n\n\nMain Execution:\n\nInput Handling: Reads and parses input data.\nComputation: Runs the find_split_distance function 500 times with random initialization to determine the maximum split distance.\nOutput: Prints the maximum split distance found."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-89",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-89",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "90.1 Sample Dataset",
    "text": "90.1 Sample Dataset\n4 6 2 1"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-90",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-90",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "90.2 Sample Output",
    "text": "90.2 Sample Output\n0.772"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-86",
    "href": "posts/md/Rosalind_stronghold.html#solution-86",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "90.3 Solution",
    "text": "90.3 Solution\nfrom scipy.special import comb\n\ndef calculate_initial_probabilities(N, m):\n    \"\"\"\n    Calculate the probabilities of having a given number of recessive alleles in the first generation.\n    \"\"\"\n    p_rec = 1 - m / (2.0 * N)\n    return [comb(2 * N, i) * p_rec ** i * (1 - p_rec) ** (2 * N - i) for i in range(1, 2 * N + 1)]\n\ndef update_probabilities(previous_p, N):\n    \"\"\"\n    Update the probabilities of recessive alleles for the next generation.\n    \"\"\"\n    new_p = []\n    for j in range(1, 2 * N + 1):\n        temp = [comb(2 * N, j) * (x / (2 * N)) ** j * (1 - x / (2 * N)) ** (2 * N - j) for x in range(1, 2 * N + 1)]\n        new_p.append(sum(temp[i] * previous_p[i] for i in range(len(temp))))\n    return new_p\n\ndef calculate_final_probability(N, m, g, k):\n    \"\"\"\n    Calculate the probability of observing at least k recessive alleles after g generations.\n    \"\"\"\n    previous_p = calculate_initial_probabilities(N, m)\n    \n    for _ in range(2, g + 1):\n        previous_p = update_probabilities(previous_p, N)\n    \n    return sum(previous_p[k - 1:])\n\n# Sample input\nsample_input = \"\"\"\n4 6 2 1\n\"\"\"\ninput_lines = sample_input.strip().split(\"\\n\")\nN, m, g, k = [int(x) for x in input_lines[0].split()]\n\n# Calculate and print the final probability\nfinal_prob = calculate_final_probability(N, m, g, k)\nprint(final_prob)"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-45",
    "href": "posts/md/Rosalind_stronghold.html#explanation-45",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "90.4 Explanation",
    "text": "90.4 Explanation\n\nFunction calculate_initial_probabilities(N, m):\n\nPurpose: Computes the probabilities of having different numbers of recessive alleles in the first generation.\nHow It Works: Uses the comb function to calculate binomial probabilities based on the initial proportion of recessive alleles.\n\nFunction update_probabilities(previous_p, N):\n\nPurpose: Updates the probabilities for subsequent generations.\nHow It Works: For each possible number of recessive alleles, calculates the new probabilities based on the previous generation’s probabilities and the binomial distribution.\n\nFunction calculate_final_probability(N, m, g, k):\n\nPurpose: Computes the final probability of observing at least k recessive alleles after g generations.\nHow It Works: Iterates through generations, updating probabilities each time. After g generations, it sums up the probabilities for having at least k recessive alleles.\n\nMain Execution:\n\nInput Handling: Reads and parses input data.\nComputation: Uses the functions to calculate the final probability.\nOutput: Prints the result."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-90",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-90",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "91.1 Sample Dataset",
    "text": "91.1 Sample Dataset\n(((ostrich,cat)rat,(duck,fly)mouse)dog,(elephant,pikachu)hamster)robot;\n&gt;ostrich\nAC\n&gt;cat\nCA\n&gt;duck\nT-\n&gt;fly\nGC\n&gt;elephant\n-T\n&gt;pikachu\nAA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-91",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-91",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "91.2 Sample Output",
    "text": "91.2 Sample Output\n8\n&gt;rat\nAC\n&gt;mouse\nTC\n&gt;dog\nAC\n&gt;hamster\nAT\n&gt;robot\nAC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-87",
    "href": "posts/md/Rosalind_stronghold.html#solution-87",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "91.3 Solution",
    "text": "91.3 Solution\nfrom math import inf\nimport re\nfrom collections import defaultdict\n\ndef parse_newick(newick, directed=True):\n    newick = re.sub(\",,\", \",.,\", newick)\n    newick = re.sub(r\"\\(,\", \"(.,\", newick)\n    newick = re.sub(r\",\\)\", \",.)\", newick)\n    newick = re.sub(r\"\\(\\)\", \"(.)\", newick)\n    newick = re.sub(r\"^\\((.+)\\);\", r\"\\1\", newick)\n    m = re.finditer(r\"(\\(|[A-z_.]+|,|\\))\", newick)\n    tokens = [x.group() for x in m]\n\n    count = 0\n    node_stack = [\"0\"]\n    g = defaultdict(list)\n    i = len(tokens) - 1\n    while i &gt;= 0:\n        if tokens[i] == \"(\":\n            node_stack = node_stack[:-1]\n        elif tokens[i] == \")\":\n            if i + 1 &lt; len(tokens) and tokens[i + 1] not in \",)\":\n                node = tokens[i + 1]\n            else:\n                count += 1\n                node = str(count)\n            g[node_stack[-1]].append({\"n\": node, \"w\": 1})\n            if not directed:\n                g[node].append({\"n\": node_stack[-1], \"w\": 1})\n            node_stack += [node]\n        elif tokens[i] != \",\" and (i == 0 or tokens[i - 1] != \")\"):\n            if tokens[i] == \".\":\n                count += 1\n                tokens[i] = str(count)\n            g[node_stack[-1]].append({\"n\": tokens[i], \"w\": 1})\n            if not directed:\n                g[tokens[i]].append({\"n\": node_stack[-1], \"w\": 1})\n        i -= 1\n    return g\n\nclas Rec:\n    \"\"\"A simple FASTA record\"\"\"\n\n    def __init__(self, id, seq):\n        self.id = id\n        self.seq = seq\n\n    def __len__(self):\n        return len(self.seq)\n\ndef read_fasta(input_string):\n    lines = input_string.strip().split('\\n')\n    header, sequence = \"\", []\n    for line in lines:\n        if line.startswith(\"&gt;\"):\n            if sequence:\n                yield Rec(header, \"\".join(sequence))\n            header, sequence = line[1:], []\n        elif line.strip():  # 빈 줄 무시\n            sequence.append(line.strip())\n    if sequence:\n        yield Rec(header, \"\".join(sequence))\n\ndef nodes(graph):\n    s = list(graph.keys())\n    e = [y for v in graph.values() for y in v]\n    return set(s) | set(e)\n\n\n# return all leaves of a simple graph\ndef leaves(graph):\n    return nodes(graph) - set(graph.keys())\n\n\ndef extract_position(graph, seqs, pos):\n    chars = {}\n    for n in nodes(graph) - leaves(graph):\n        chars[n] = \"\"\n    for leaf in leaves(graph):\n        chars[leaf] = seqs[leaf][pos]\n    return chars\n\n\ndef traceback(skp, node, ind):\n    bases = [\"A\", \"C\", \"T\", \"G\", \"-\"]\n    chars = {}\n    chars[node] = bases[ind]\n    for k, v in skp[node][ind].items():\n        if k in skp:\n            chars = chars | traceback(skp, k, v)\n    return chars\n\n\ndef small_parsimony(graph, chars):\n    bases = [\"A\", \"C\", \"T\", \"G\", \"-\"]\n    sk = {}  # minimum parsimony score of the subtree over possible labels\n    skp = {}  # pointer to selected base for each child over possible labels\n    to_proces = nodes(graph)\n\n    # # initialise leaves\n    for leaf in leaves(graph):\n        sk[leaf] = [0 if chars[leaf] == c else inf for c in bases]\n        to_process.remove(leaf)\n\n    # iterate over available nodes till all are processed\n    while to_process:\n        for n in list(to_process):\n            if all(v in sk for v in graph[n]):\n                sk[n], skp[n] = [], []\n                for k in bases:\n                    tot = 0\n                    ptr = {}\n                    for d, sk_child in [(d, sk[d]) for d in graph[n]]:\n                        score = []\n                        for i, c in enumerate(bases):\n                            score += [sk_child[i] + (0 if c == k else 1)]\n                        tot += min(score)\n                        ptr[d] = score.index(min(score))\n                    skp[n] += [ptr]\n                    sk[n] += [tot]\n                to_process.remove(n)\n\n    # Recover sequence\n    node = \"0\"\n    score = min(sk[node])\n    return score, traceback(skp, node, sk[node].index(score))\n\ndef alph(tree, seqs, i):\n    # initialise sequences\n    for n in nodes(tree) - leaves(tree):\n        seqs[n] = \"\"\n\n    n = len(seqs[list(leaves(tree))[0]])\n    total_score = 0\n    for pos in range(n):\n        chars = extract_position(tree, seqs, pos)\n        score, tbchars = small_parsimony(tree, chars)\n        total_score += score\n        for k, v in tbchars.items():\n            seqs[k] += v\n\n    return total_score, seqs\n\ndef simplify_tree(graph):\n    return {k: [x[\"n\"] for x in v] for k, v in graph.items()}\n\n\nsample_input = \"\"\"\n(((ostrich,cat)rat,(duck,fly)mouse)dog,(elephant,pikachu)hamster)robot;\n&gt;ostrich\nAC\n&gt;cat\nCA\n&gt;duck\nT-\n&gt;fly\nGC\n&gt;elephant\n-T\n&gt;pikachu\nAA\n\"\"\"\n\ntree = parse_newick(sample_input.strip().split('\\n')[0])\ntree = simplify_tree(tree)\n\nseqs = read_fasta('\\n'.join(sample_input.strip().split('\\n')[1:]))\nseqs = {x.id: x.seq for x in seqs}\ntotal_score, seqs = alph(tree, seqs, 1)\nprint(total_score)\nfor node in tree.keys():\n    if node != \"0\":\n        print(f\"&gt;{node}\")\n        print(seqs[node])"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#working-principle",
    "href": "posts/md/Rosalind_stronghold.html#working-principle",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "91.4 Working Principle",
    "text": "91.4 Working Principle\n\nThe code implements the Small Parsimony algorithm for phylogenetic tree reconstruction.\nIt starts by parsing a Newick format string representation of a tree using the parse_newick function. This function creates a graph representation of the tree.\nThe small_parsimony function is the core of the algorithm. It calculates the most parsimonious ancestral sequences for internal nodes of the tree.\nThe algorithm works bottom-up, starting from the leaves and moving towards the root:\n\nFor leaves, it initializes scores based on their known character states.\nFor internal nodes, it calculates scores for each possible base by considering the scores of its children.\n\nThe traceback function is used to reconstruct the most parsimonious ancestral sequences by traversing the tree from root to leaves.\nThe alph function applies the Small Parsimony algorithm to each position in the sequences, building up the full ancestral sequences.\nFinally, the code reads a sample input (in FASTA format), constructs the tree, applies the algorithm, and prints the results.\n\nThis algorithm aims to find the ancestral sequences that minimize the total number of mutations (changes) along the branches of the phylogenetic tree, based on the principle of maximum parsimony."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-91",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-91",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "92.1 Sample Dataset",
    "text": "92.1 Sample Dataset\nGATTACA\nTACTACTAC\nATTGAT\nGAAGA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-92",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-92",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "92.2 Sample Output",
    "text": "92.2 Sample Output\n7 6"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-88",
    "href": "posts/md/Rosalind_stronghold.html#solution-88",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "92.3 Solution",
    "text": "92.3 Solution\ndef calculate_nxx(contigs, xx):\n    total_length = sum(len(contig) for contig in contigs)\n    target_length = total_length * xx / 100\n    \n    sorted_contigs = sorted(contigs, key=len, reverse=True)\n    cumulative_length = 0\n    \n    for contig in sorted_contigs:\n        cumulative_length += len(contig)\n        if cumulative_length &gt;= target_length:\n            return len(contig)\n    \n    return 0\n\n# Read input\nsample_input = \"\"\"\nGATTACA\nTACTACTAC\nATTGAT\nGAAGA\n\"\"\"\ncontigs = [line.strip() for line in sample_input.strip().split(\"\\n\")]\n\n# Calculate N50 and N75\nn50 = calculate_nxx(contigs, 50)\nn75 = calculate_nxx(contigs, 75)\n\n# Print results\nprint(f\"{n50} {n75}\")\nThe code calculates N50 and N75 values, which are measures used to asses the quality of DNA sequence assemblies."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#breaking-down-the-steps",
    "href": "posts/md/Rosalind_stronghold.html#breaking-down-the-steps",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "92.4 Breaking Down the Steps:",
    "text": "92.4 Breaking Down the Steps:\n\ncalculate_nxx(contigs, xx) Function:\n\nInput: A list of DNA sequences (contigs) and a percentage (xx like 50 for N50).\nOutput: The length of the sequence (contig) where the cumulative length reaches the specified percentage of the total length.\nHow It Works:\n\nStep 1: Add up the lengths of all sequences to get the total length.\nStep 2: Sort the sequences from longest to shortest.\nStep 3: Add lengths one by one from the sorted list until the sum reaches the specified percentage of the total length. The length of the last added sequence is the NXX value.\n\n\nMain Code:\n\nThe sample input is split into individual DNA sequences.\nThe code then calculates:\n\nN50: The sequence length where 50% of the total length is reached.\nN75: The sequence length where 75% of the total length is reached.\n\nFinally, it prints these N50 and N75 values."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-92",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-92",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "93.1 Sample Dataset",
    "text": "93.1 Sample Dataset\n100001\n000110\n111000\n100111"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-93",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-93",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "93.2 Sample Output",
    "text": "93.2 Sample Output\n000110\n100001\n100111"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-89",
    "href": "posts/md/Rosalind_stronghold.html#solution-89",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "93.3 Solution",
    "text": "93.3 Solution\nfrom collections import defaultdict\n\n\ndef conflict(c1, c2):\n    # 모든 인덱스에 대해 한 번에 비교하여 충돌 여부 확인\n    return any((c1[i] == 1 and c2[i] == 0) or (c1[i] == 0 and c2[i] == 1) for i in range(len(c1)))\n\n\ndef conflicts(characters):\n    count = defaultdict(int)\n    for i in range(len(characters)):\n        for j in range(i + 1, len(characters)):\n            if conflict(characters[i], characters[j]):\n                count[i] += 1\n                count[j] += 1\n    return count\n\n\n# 입력 처리\nsample_input = \"\"\"\n100001\n000110\n111000\n100111\n\"\"\"\n\nlines = sample_input.strip().split(\"\\n\")\ncharacters = [[int(x) for x in ch] for ch in lines]\n\n# 충돌 계산\ncount = conflicts(characters)\n\n# 가장 많은 충돌을 가진 행 제거\nrm = max(count, key=count.get)\n\n# 결과 출력\nprint(*lines[:rm], *lines[rm + 1:], sep=\"\\n\")\nThe code identifies and removes the row from a set of binary sequences (like 100001) that has the most conflicts with other rows. A “conflict” is defined as one sequence having a 1 where another has a 0, and vice versa."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#breaking-down-the-steps-1",
    "href": "posts/md/Rosalind_stronghold.html#breaking-down-the-steps-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "93.4 Breaking Down the Steps:",
    "text": "93.4 Breaking Down the Steps:\n\nconflict(c1, c2) Function:\n\nPurpose: Check if two sequences (c1 and c2) conflict with each other.\nHow It Works:\n\nIt compares the two sequences at each index.\nIf at any index, one sequence has 1 and the other has 0, they are in conflict.\nThe function returns True if there is any conflict; otherwise, it returns False.\n\n\nconflicts(characters) Function:\n\nPurpose: Count how many conflicts each sequence has with all other sequences.\nHow It Works:\n\nIt compares each sequence with every other sequence.\nIf two sequences conflict, it increments a conflict counter for both sequences.\nIt returns a dictionary where the key is the sequence index, and the value is the number of conflicts that sequence has.\n\n\nMain Code:\n\nInput Processing:\n\nThe binary sequences are read and converted into lists of integers.\n\nConflict Counting:\n\nThe code uses the conflicts function to count how many conflicts each sequence has.\n\nRemoving the Most Conflicting Sequence:\n\nThe sequence with the highest number of conflicts is identified.\nThis sequence is then removed from the list.\n\nOutput:\n\nThe remaining sequences (with the most conflicting one removed) are printed."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-93",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-93",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "94.1 Sample Dataset",
    "text": "94.1 Sample Dataset\n17\n0.1 0.2 0.3"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-94",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-94",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "94.2 Sample Output",
    "text": "94.2 Sample Output\n1.7 3.4 5.1"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-90",
    "href": "posts/md/Rosalind_stronghold.html#solution-90",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "94.3 Solution",
    "text": "94.3 Solution\nThe problem asks us to calculate the expected value of a binomial random variable given a population size \\(n\\) and an array \\(P\\) of allele frequencies. For each element \\(p\\) in the array \\(P\\), we need to compute the expected value \\(E(Bin(n, p))\\).\nFor a binomial random variable \\(Bin(n, p)\\), where: - \\(n\\) is the number of trials (in this case, the number of individuals in the population), - \\(p\\) is the probability of succes (or the allele frequency),\nThe expected value \\(E(Bin(n, p))\\) is calculated as:\n[ E(Bin(n, p)) = n p ]\nGiven that, the task is to compute this value for each probability in the array \\(P\\).We can implement this in Python as follows:\n# Sample Input\nsample_input = \"\"\"\n17\n0.1 0.2 0.3\n\"\"\"\n\n# Parse input\nlines = sample_input.strip().split('\\n')\nn = int(lines[0])\nP = list(map(float, lines[1].split()))\n\n# Calculate the expected values\nB = [n * p for p in P]\n\n# Print the result\nprint(' '.join(map(str, B)))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-46",
    "href": "posts/md/Rosalind_stronghold.html#explanation-46",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "94.4 Explanation",
    "text": "94.4 Explanation\n\nInput Parsing: We read the population size \\(n\\) and the array \\(P\\) of allele frequencies.\nComputation: For each \\(p\\) in \\(P\\), we compute \\(n \\times p\\) and store it in array \\(B\\).\nOutput: Finally, we print the values in \\(B\\) as a space-separated string."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-94",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-94",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "95.1 Sample Dataset",
    "text": "95.1 Sample Dataset\n4 3\n0 1 2"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-95",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-95",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "95.2 Sample Output",
    "text": "95.2 Sample Output\n0.0 -0.463935575821 -0.999509892866\n0.0 -0.301424998891 -0.641668367342\n0.0 -0.229066698008 -0.485798552456"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-91",
    "href": "posts/md/Rosalind_stronghold.html#solution-91",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "95.3 Solution",
    "text": "95.3 Solution\nfrom scipy.special import comb\nimport numpy as np\n\ndef wright_fisher_genetic_drift(N, m, g):\n    \"\"\"\n    Calculate the log10 of the probability that no copies of the recessive allele remain in the population\n    after g generations, given an initial count of m recessive alleles.\n    \"\"\"\n    q = m / (2 * N)  # Initial dominant allele frequency\n    p = 1 - q  # Initial recessive allele frequency\n\n    # Initialize probability of exactly t recessive alleles in the first generation\n    prob = np.array([comb(2 * N, i) * (q ** i) * (p ** (2 * N - i)) for i in range(1, 2 * N + 1)])\n\n    # Iterate through generations\n    for _ in range(1, g):\n        # Calculate probabilities for the next generation\n        next_prob = np.zeros(2 * N)\n        for t in range(1, 2 * N + 1):\n            # Calculate probability of having exactly t recessive alleles in the current generation\n            prob_t = np.array([comb(2 * N, t) * ((i / (2 * N)) ** t) * ((1 - (i / (2 * N))) ** (2 * N - t)) for i in range(1, 2 * N + 1)])\n            next_prob[t - 1] = np.sum(prob_t * prob)\n        prob = next_prob\n\n    # Return the log10 of the probability of no recessive alleles remaining\n    return np.log10(1 - np.sum(prob))\n\ndef calculate_genetic_drift_matrix(N, m, A):\n    \"\"\"\n    Generate the matrix B where B[i, j] represents the common logarithm of the probability that after i generations,\n    no copies of the recessive allele for the j-th factor will remain in the population.\n    \"\"\"\n    k = len(A)\n    B = np.zeros((m, k))\n\n    for i in range(m):\n        for j in range(k):\n            B[i, j] = wright_fisher_genetic_drift(N, A[j], i + 1)\n\n    return B\n\n# Sample Input\nsample_input = \"\"\"\n4 3\n0 1 2\n\"\"\"\n\n# Parse input\nlines = sample_input.strip().split('\\n')\nN, m = map(int, lines[0].split())\nA = list(map(int, lines[1].split()))\n\n# Calculate the matrix B\nB = calculate_genetic_drift_matrix(N, m, A)\n\n# Print the results\nfor row in B:\n    print(' '.join(map(str, row)))\nThis code calculates the probability of losing all copies of a recessive allele in a population over several generations, based on the Wright-Fisher model of genetic drift. The result is a matrix where each entry tells us the likelihood (in logarithmic form) that no recessive alleles remain after a given number of generations."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#functions-and-their-roles",
    "href": "posts/md/Rosalind_stronghold.html#functions-and-their-roles",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "95.4 Functions and Their Roles",
    "text": "95.4 Functions and Their Roles\n\nwright_fisher_genetic_drift(N, m, g)\nPurpose: Calculates the probability of losing all recessive alleles after g generations, starting with m recessive alleles in a population of size N.\nHow It Works:\n\nInitial Setup:\n\nq is the frequency of dominant alleles.\np is the frequency of recessive alleles.\n\nFirst Generation:\n\nCalculate the probability of having exactly t recessive alleles in the first generation.\n\nSubsequent Generations:\n\nFor each generation, update the probabilities based on the previous generation.\n\nFinal Calculation:\n\nCompute the probability that no recessive alleles remain after g generations and return its log base 10.\n\n\ncalculate_genetic_drift_matrix(N, m, A)\nPurpose: Creates a matrix where each entry shows the log probability that no recessive alleles remain after a certain number of generations for various initial counts of recessive alleles.\nHow It Works:\n\nMatrix Initialization:\n\nB is initialized as a zero matrix.\n\nFilling the Matrix:\n\nFor each possible number of generations and each initial count of recessive alleles, calculate the log probability using wright_fisher_genetic_drift and store it in the matrix B."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#key-points-1",
    "href": "posts/md/Rosalind_stronghold.html#key-points-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "95.5 Key Points",
    "text": "95.5 Key Points\n\nwright_fisher_genetic_drift focuses on updating probabilities generation by generation.\ncalculate_genetic_drift_matrix builds a matrix from these probabilities for different scenarios."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-95",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-95",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "96.1 Sample Dataset",
    "text": "96.1 Sample Dataset\n&gt;Rosalind_49\nPRTEINS\n&gt;Rosalind_47\nPRTWPSEIN"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-96",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-96",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "96.2 Sample Output",
    "text": "96.2 Sample Output\n8\nPRT---EINS\nPRTWPSEIN-"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-92",
    "href": "posts/md/Rosalind_stronghold.html#solution-92",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "96.3 Solution",
    "text": "96.3 Solution\n# BLOSUM62 matrix as a string\nblosum62_str = \"\"\"\nA  C  D  E  F  G  H  I  K  L  M  N  P  Q  R  S  T  V  W  Y\nA  4  0 -2 -1 -2  0 -2 -1 -1 -1 -1 -2 -1 -1 -1  1  0  0 -3 -2\nC  0  9 -3 -4 -2 -3 -3 -1 -3 -1 -1 -3 -3 -3 -3 -1 -1 -1 -2 -2\nD -2 -3  6  2 -3 -1 -1 -3 -1 -4 -3  1 -1  0 -2  0 -1 -3 -4 -3\nE -1 -4  2  5 -3 -2  0 -3  1 -2 -2  0 -1  2  0  0 -1 -2 -3 -2\nF -2 -2 -3 -3  6 -3 -1  0 -3  0  0 -3 -4 -3 -3 -2 -2 -1  1  3\nG  0 -3 -1 -2 -3  6 -2 -4 -2 -4 -3  0 -2 -2 -3  0 -2 -3 -2 -3\nH -2 -3 -1  0 -1 -2  8 -3 -1 -3 -2  1 -2  0  0 -1 -2 -3 -2  2\nI -1 -1 -3 -3  0 -4 -3  4 -3  2  1 -3 -3 -3 -3 -2 -1  3 -3 -1\nK -1 -3 -1  1 -3 -2 -1 -3  5 -2 -1  0 -1  1  2  0 -1 -2 -3 -2\nL -1 -1 -4 -2  0 -4 -3  2 -2  4  2 -3 -3 -2 -2 -2 -1  1 -2 -1\nM -1 -1 -3 -2  0 -3 -2  1 -1  2  5 -2 -2  0 -1 -1 -1  1 -1 -1\nN -2 -3  1  0 -3  0  1 -3  0 -3 -2  6 -2  0  0  1  0 -3 -4 -2\nP -1 -3 -1 -1 -4 -2 -2 -3 -1 -3 -2 -2  7 -1 -2 -1 -1 -3 -4 -3\nQ -1 -3  0  2 -3 -2  0 -3  1 -2  0  0 -1  5  1  0 -1 -2 -2 -1\nR -1 -3 -2  0 -3 -3  0 -3  2 -2 -1  0 -2  1  5 -1 -1 -3 -3 -2\nS  1 -1  0  0 -2  0 -1 -2  0 -2 -1  1 -1  0 -1  4  1 -2 -3 -2\nT  0 -1 -1 -1 -2 -2 -2 -1 -1 -1 -1  0 -1 -1 -1  1  5  0 -2 -2\nV  0 -1 -3 -2 -1 -3 -3  3 -2  1  1 -3 -3 -2 -3 -2  0  4 -3 -1\nW -3 -2 -4 -3  1 -2 -2 -3 -3 -2 -1 -4 -4 -2 -3 -3 -2 -3 11  2\nY -2 -2 -3 -2  3 -3  2 -1 -2 -1 -1 -2 -3 -1 -2 -2 -2 -1  2  7\n\"\"\"\n\ndef parse_blosum62(matrix_str):\n    \"\"\"Parse the BLOSUM62 matrix from a string into a dictionary.\"\"\"\n    lines = matrix_str.strip().split('\\n')\n    headers = lines[0].split()\n    matrix = {}\n    for line in lines[1:]:\n        values = line.split()\n        row = values[0]\n        scores = list(map(int, values[1:]))\n        matrix.update({(row, col): score for col, score in zip(headers, scores)})\n    return matrix\n\ndef parse_fasta(data):\n    \"\"\"Parse FASTA format data into a list of sequences.\"\"\"\n    sequences = []\n    seq = \"\"\n    for line in data.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if seq:\n                sequences.append(seq)\n                seq = \"\"\n        else:\n            seq += line.strip()\n    if seq:\n        sequences.append(seq)\n    return sequences\n\ndef match_score(scoring_matrix, a, b):\n    \"\"\"Return the score from the scoring matrix, defaulting to 0 if not found.\"\"\"\n    return scoring_matrix.get((a, b), 0)\n\ndef global_align_with_affine(s, t, scores, gap, gap_e):\n    \"\"\"Perform global alignment with affine gap penalties.\"\"\"\n    m, n = len(s), len(t)\n    # Initialize matrices\n    M = [[0] * (n + 1) for _ in range(m + 1)]\n    X = [[0] * (n + 1) for _ in range(m + 1)]\n    Y = [[0] * (n + 1) for _ in range(m + 1)]\n    traceM = [[0] * (n + 1) for _ in range(m + 1)]\n    traceX = [[0] * (n + 1) for _ in range(m + 1)]\n    traceY = [[0] * (n + 1) for _ in range(m + 1)]\n\n    # Initialize edges\n    for i in range(1, m + 1):\n        M[i][0] = gap + gap_e * (i - 1)\n        X[i][0] = Y[i][0] = float('-inf')\n    for j in range(1, n + 1):\n        M[0][j] = gap + gap_e * (j - 1)\n        X[0][j] = Y[0][j] = float('-inf')\n\n    # Fill matrices\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            costX = [M[i-1][j] + gap, X[i-1][j] + gap_e]\n            X[i][j] = max(costX)\n            traceX[i][j] = costX.index(X[i][j])\n            \n            costY = [M[i][j-1] + gap, Y[i][j-1] + gap_e]\n            Y[i][j] = max(costY)\n            traceY[i][j] = costY.index(Y[i][j])\n\n            costM = [M[i-1][j-1] + match_score(scores, s[i-1], t[j-1]), X[i][j], Y[i][j]]\n            M[i][j] = max(costM)\n            traceM[i][j] = costM.index(M[i][j])\n    \n    # Get maximum score and initialize aligned strings\n    max_score = M[m][n]\n    s_align, t_align = s, t\n\n    # Traceback\n    i, j = m, n\n    while i &gt; 0 or j &gt; 0:\n        traceback = max([(X[i][j], 0), (Y[i][j], 1), (M[i][j], 2)], key=lambda x: x[0])[1]\n        if traceback == 0:\n            t_align = t_align[:j] + '-' + t_align[j:]\n            i -= 1\n        elif traceback == 1:\n            s_align = s_align[:i] + '-' + s_align[i:]\n            j -= 1\n        elif traceback == 2:\n            if traceM[i][j] == 0:\n                traceback = 0\n            elif traceM[i][j] == 1:\n                traceback = 1\n            i -= 1\n            j -= 1\n\n    # Handle leading gaps\n    s_align = '-' * j + s_align\n    t_align = '-' * i + t_align\n\n    return str(max_score), s_align, t_align\n\n# Sample dataset in FASTA format\nsample_input = \"\"\"\n&gt;Rosalind_49\nPRTEINS\n&gt;Rosalind_47\nPRTWPSEIN\n\"\"\"\n\n# Parse the FASTA input to get the sequences\nsequences = parse_fasta(sample_input)\ns, t = sequences[0], sequences[1]\n\n# Parse the BLOSUM62 matrix\nblosum62 = parse_blosum62(blosum62_str)\n\n# Perform global alignment with affine gap penalties\nalignment = global_align_with_affine(s, t, blosum62, -11, -1)\n\nprint('\\n'.join(alignment))\nThis Python code performs global sequence alignment between two protein sequences using the BLOSUM62 substitution matrix and affine gap penalties. The alignment proces is a common technique in bioinformatics to compare two sequences and find the best way to align them by inserting gaps and matching characters."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#detailed-explanation-4",
    "href": "posts/md/Rosalind_stronghold.html#detailed-explanation-4",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "96.4 Detailed Explanation",
    "text": "96.4 Detailed Explanation\n\nBLOSUM62 Matrix as a String:\n\nThe BLOSUM62 matrix, a commonly used substitution matrix in bioinformatics, is provided as a multiline string. It contains scores representing how likely it is for each amino acid pair to substitute for each other.\n\nparse_blosum62(matrix_str) Function:\n\nPurpose: Converts the BLOSUM62 string into a dictionary for easier lookup.\nHow It Works:\n\nThe string is split into lines and then into individual elements.\nThe first line contains the amino acid headers.\nEach subsequent line contains scores for substituting one amino acid with others.\nA dictionary is created where each key is a tuple of two amino acids, and the value is the corresponding substitution score.\n\n\nparse_fasta(data) Function:\n\nPurpose: Parses sequences from the FASTA format, which is a standard text-based format for representing sequences.\nHow It Works:\n\nThe function reads the input line by line.\nLines starting with &gt; indicate sequence headers, which are ignored.\nSequence data is collected into a list of sequences.\n\n\nmatch_score(scoring_matrix, a, b) Function:\n\nPurpose: Retrieves the substitution score for a pair of amino acids from the BLOSUM62 matrix.\nHow It Works:\n\nIt looks up the score for the amino acid pair (a, b) in the dictionary. If the pair is not found, it returns 0.\n\n\nglobal_align_with_affine(s, t, scores, gap, gap_e) Function:\n\nPurpose: Performs global alignment of two sequences using affine gap penalties.\nHow It Works:\n\nInitialization: Three matrices (M, X, Y) are used to keep track of the scores for matches, gaps in one sequence, and gaps in the other sequence, respectively. traceM, traceX, and traceY track the path for traceback.\nMatrix Filling: The matrices are filled in a nested loop:\n\nM[i][j]: Maximum score considering a match or mismatch.\nX[i][j]: Maximum score considering a gap in sequence t.\nY[i][j]: Maximum score considering a gap in sequence s.\n\nTraceback: After filling the matrices, the function traces back from the last cell to reconstruct the aligned sequences, inserting gaps where needed.\nThe traceback ensures that the sequences are aligned optimally according to the scoring matrix and gap penalties.\n\n\nSample Input and Execution:\n\nSample Input: Two sequences (PRTEINS and PRTWPSEIN) are provided in FASTA format.\nExecution:\n\nThe sequences are parsed from the input.\nThe BLOSUM62 matrix is parsed.\nGlobal alignment with affine gap penalties is performed using the parsed sequences and matrix.\nThe alignment result, including the score and the aligned sequences, is printed."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-96",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-96",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "97.1 Sample Dataset",
    "text": "97.1 Sample Dataset\nCAG\nAGT\nGTT\nTTT\nTTG\nTGG\nGGC\nGCG\nCGT\nGTT\nTTC\nTCA\nCAA\nAAT\nATT\nTTC\nTCA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-97",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-97",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "97.2 Sample Output",
    "text": "97.2 Sample Output\nCAGTTCAATTTGGCGTT\nCAGTTCAATTGGCGTTT\nCAGTTTCAATTGGCGTT\nCAGTTTGGCGTTCAATT\nCAGTTGGCGTTCAATTT\nCAGTTGGCGTTTCAATT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-93",
    "href": "posts/md/Rosalind_stronghold.html#solution-93",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "97.3 Solution",
    "text": "97.3 Solution\nclas DeBruijnGraph:\n    clas Node:\n        def __init__(self, kmer):\n            self.kmer = kmer\n            self.neighbors = []\n            self.in_degree = 0\n            self.out_degree = 0\n\n    def __init__(self, reads):\n        self.graph = {}\n        self.start_kmer = reads[0]\n\n        for read in reads:\n            left_kmer, right_kmer = read[:-1], read[1:]\n            left_hash, right_hash = hash(left_kmer), hash(right_kmer)\n\n            left_node = self.graph.setdefault(left_hash, self.Node(left_kmer))\n            right_node = self.graph.setdefault(right_hash, self.Node(right_kmer))\n\n            left_node.neighbors.append(right_node)\n            left_node.out_degree += 1\n            right_node.in_degree += 1\n\n    def circular_string(self):\n        potential_starts = [node for node in self.graph.values() if node.out_degree &gt; 1]\n        assert potential_starts, \"No potential start nodes found!\"\n\n        contigs = []\n        strings = set()\n        k = len(self.start_kmer) - 1\n\n        def dfs(node, sequence):\n            if node.out_degree &gt; 1:\n                contigs.append(sequence + node.kmer[-1])\n            else:\n                dfs(node.neighbors[0], sequence + node.kmer[-1])\n\n        for start_node in potential_starts:\n            for neighbor in start_node.neighbors:\n                dfs(neighbor, start_node.kmer)\n\n        def find_circular_strings(current_sequence, accumulated_string, used_contigs):\n            if len(used_contigs) == len(contigs):\n                strings.add(accumulated_string)\n            else:\n                for idx in set(range(len(contigs))).difference(used_contigs):\n                    if not current_sequence.endswith(contigs[idx][:k]):\n                        continue\n                    find_circular_strings(contigs[idx], accumulated_string + contigs[idx][:-k], used_contigs + (idx,))\n\n        for i, contig in enumerate(contigs):\n            if contig.startswith(self.start_kmer):\n                find_circular_strings(contig, contig[:-k], (i,))\n                break\n\n        return strings\n\n\n# Sample input\nsample_input = \"\"\"\nCAG\nAGT\nGTT\nTTT\nTTG\nTGG\nGGC\nGCG\nCGT\nGTT\nTTC\nTCA\nCAA\nAAT\nATT\nTTC\nTCA\n\"\"\"\n\nreads = sample_input.strip().split(\"\\n\")\ngraph = DeBruijnGraph(reads)\nprint(*graph.circular_string(), sep='\\n')"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-47",
    "href": "posts/md/Rosalind_stronghold.html#explanation-47",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "97.4 Explanation",
    "text": "97.4 Explanation\n\nGraph Construction: The code constructs a De Bruijn graph using the provided k-mers (reads). Each k-mer’s prefix (all but the last character) and suffix (all but the first character) are treated as nodes in the graph. The graph edges represent transitions from one k-mer to another based on these prefixes and suffixes.\nNode Structure: Each node in the graph stores its k-mer, its neighbors (other nodes it can connect to), and its in-degree and out-degree (how many edges enter and leave the node).\nGraph Traversal: The code identifies nodes with more than one outgoing edge (out_degree &gt; 1) as potential starting points for generating circular sequences (possible cyclic paths in the graph).\nDepth-First Search (DFS): The code uses DFS to traverse from these potential starting nodes to build “contigs,” which are sequences representing possible paths through the graph.\nGenerating Circular Strings: After building contigs, the code recursively combines these contigs to generate complete circular strings that encompas all the original k-mers.\nOutput: The final set of circular strings that represent possible solutions is printed."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-97",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-97",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "98.1 Sample Dataset",
    "text": "98.1 Sample Dataset\n&gt;Rosalind_54\nGCAAACCATAAGCCCTACGTGCCGCCTGTTTAAACTCGCGAACTGAATCTTCTGCTTCACGGTGAAAGTACCACAATGGTATCACACCCCAAGGAAAC\n&gt;Rosalind_46\nGCCGTCAGGCTGGTGTCCG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-98",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-98",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "98.2 Sample Output",
    "text": "98.2 Sample Output\n5\nACCATAAGCCCTACGTG-CCG\nGCCGTCAGGC-TG-GTGTCCG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-94",
    "href": "posts/md/Rosalind_stronghold.html#solution-94",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "98.3 Solution",
    "text": "98.3 Solution\nfrom typing import List, Tuple\n\nGAP_PENALTY = 1\nMATCH_SCORE = 1\nMISMATCH_PENALTY = 1\n\ndef parse_fasta(data: str) -&gt; List[str]:\n    \"\"\"Parse FASTA format data into a list of sequences.\"\"\"\n    sequences = []\n    current_seq = []\n    for line in data.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if current_seq:\n                sequences.append(''.join(current_seq))\n                current_seq = []\n        else:\n            current_seq.append(line.strip())\n    if current_seq:\n        sequences.append(''.join(current_seq))\n    return sequences\n\ndef initialize_dp_matrix(m: int, n: int) -&gt; List[List[int]]:\n    \"\"\"Initialize the dynamic programming matrix.\"\"\"\n    return [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n\ndef fill_dp_matrix(c: str, d: str, dp: List[List[int]]) -&gt; None:\n    \"\"\"Fill the dynamic programming matrix.\"\"\"\n    for i in range(len(c) + 1):\n        for j in range(1, len(d) + 1):\n            ans = float('-inf')\n            if i &gt; 0:\n                ans = max(ans, dp[i - 1][j] - GAP_PENALTY)\n            if j &gt; 0:\n                ans = max(ans, dp[i][j - 1] - GAP_PENALTY)\n            if i &gt; 0 and j &gt; 0:\n                if c[i - 1] == d[j - 1]:\n                    ans = max(ans, dp[i - 1][j - 1] + MATCH_SCORE)\n                else:\n                    ans = max(ans, dp[i - 1][j - 1] - MISMATCH_PENALTY)\n            dp[i][j] = ans\n\ndef find_best_score(dp: List[List[int]], m: int, n: int) -&gt; Tuple[int, int, int]:\n    \"\"\"Find the best score and its position in the DP matrix.\"\"\"\n    score = float('-inf')\n    bi, bj = -1, -1\n    for i in range(m + 1):\n        if score &lt; dp[i][n]:\n            score = dp[i][n]\n            bi, bj = i, n\n    return int(score), bi, bj\n\ndef backtrack(c: str, d: str, dp: List[List[int]], bi: int, bj: int) -&gt; Tuple[str, str]:\n    \"\"\"Backtrack to find the aligned sequences.\"\"\"\n    s1, s2 = [], []\n    while bj &gt; 0:\n        if bi &gt; 0 and dp[bi - 1][bj] - GAP_PENALTY == dp[bi][bj]:\n            s1.append(c[bi - 1])\n            s2.append('-')\n            bi -= 1\n        elif bj &gt; 0 and dp[bi][bj - 1] - GAP_PENALTY == dp[bi][bj]:\n            s1.append('-')\n            s2.append(d[bj - 1])\n            bj -= 1\n        else:\n            s1.append(c[bi - 1])\n            s2.append(d[bj - 1])\n            bi -= 1\n            bj -= 1\n    return ''.join(s1[::-1]), ''.join(s2[::-1])\n\nsample_input = \"\"\"\n&gt;Rosalind_54\nGCAAACCATAAGCCCTACGTGCCGCCTGTTTAAACTCGCGAACTGAATCTTCTGCTTCACGGTGAAAGTACCACAATGGTATCACACCCCAAGGAAAC\n&gt;Rosalind_46\nGCCGTCAGGCTGGTGTCCG\n\"\"\"\n\nsequences = parse_fasta(sample_input)\nc, d = sequences\n\ndp = initialize_dp_matrix(len(c), len(d))\nfill_dp_matrix(c, d, dp)\n\nscore, bi, bj = find_best_score(dp, len(c), len(d))\naligned_c, aligned_d = backtrack(c, d, dp, bi, bj)\n\nprint(score)\nprint(aligned_c)\nprint(aligned_d)\nThis code performs a sequence alignment between two DNA sequences using a dynamic programming approach. Sequence alignment is a method used in bioinformatics to compare two sequences and determine the best match between them, accounting for matches, mismatches, and gaps."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#how-the-code-works",
    "href": "posts/md/Rosalind_stronghold.html#how-the-code-works",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "98.4 How the Code Works",
    "text": "98.4 How the Code Works\n\nConstants:\n\nGAP_PENALTY: Penalty for introducing a gap (insertion/deletion) in the sequence alignment.\nMATCH_SCORE: Score for matching characters between the two sequences.\nMISMATCH_PENALTY: Penalty for mismatched characters.\n\nFunctions:\n\nparse_fasta(data: str) -&gt; List[str]:\n\nPurpose: Converts a FASTA format string into a list of sequences.\nHow It Works: The function reads the input data, ignoring lines starting with ‘&gt;’ (which are headers), and combines the remaining lines into sequences.\n\ninitialize_dp_matrix(m: int, n: int) -&gt; List[List[int]]:\n\nPurpose: Initializes a matrix for dynamic programming (DP) with dimensions (m+1) x (n+1), where m and n are the lengths of the two sequences.\nHow It Works: Creates a 2D list filled with zeros.\n\nfill_dp_matrix(c: str, d: str, dp: List[List[int]]) -&gt; None:\n\nPurpose: Fills the DP matrix with scores based on the alignment of sequences c and d.\nHow It Works:\n\nIterates over all possible alignments of c and d.\nFor each position, it calculates the best score considering three possible moves: match/mismatch, insertion, and deletion.\nThe score is updated based on whether characters match or mismatch, and whether gaps are introduced.\n\n\nfind_best_score(dp: List[List[int]], m: int, n: int) -&gt; Tuple[int, int, int]:\n\nPurpose: Finds the best alignment score and its position in the DP matrix.\nHow It Works:\n\nScans the last column of the DP matrix to find the highest score and its position. This represents the optimal alignment score.\n\n\nbacktrack(c: str, d: str, dp: List[List[int]], bi: int, bj: int) -&gt; Tuple[str, str]:\n\nPurpose: Traces back through the DP matrix to construct the aligned sequences based on the best alignment score.\nHow It Works:\n\nStarting from the best score position, it determines the path that led to this score, reconstructing the aligned sequences with gaps (-) where necessary.\n\n\n\nExecution:\n\nSample Input: The FASTA input contains two sequences labeled Rosalind_54 and Rosalind_46.\nProcessing:\n\nThe sequences are extracted using parse_fasta.\nA DP matrix is initialized using initialize_dp_matrix.\nThe matrix is filled with alignment scores using fill_dp_matrix.\nThe best alignment score and position are found with find_best_score.\nThe best alignment itself is reconstructed using backtrack.\n\nOutput:\n\nThe alignment score and the two aligned sequences are printed."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-98",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-98",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "99.1 Sample Dataset",
    "text": "99.1 Sample Dataset\n&gt;Rosalind_35\nATAGATA\n&gt;Rosalind_5\nACAGGTA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-99",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-99",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "99.2 Sample Output",
    "text": "99.2 Sample Output\n3\n-139"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-95",
    "href": "posts/md/Rosalind_stronghold.html#solution-95",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "99.3 Solution",
    "text": "99.3 Solution\nfrom typing import List, Tuple\n\ndef parse_fasta(data: str) -&gt; List[str]:\n    \"\"\"Parse FASTA format data into a list of sequences.\"\"\"\n    sequences = []\n    current_seq = []\n    for line in data.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if current_seq:\n                sequences.append(''.join(current_seq))\n                current_seq = []\n        else:\n            current_seq.append(line.strip())\n    if current_seq:\n        sequences.append(''.join(current_seq))\n    return sequences\n\ndef initialize_score_matrix(rows: int, cols: int) -&gt; List[List[int]]:\n    \"\"\"Initialize the score matrix with gap penalties.\"\"\"\n    S = [[0 for _ in range(cols)] for _ in range(rows)]\n    for i in range(1, rows):\n        S[i][0] = -i\n    for j in range(1, cols):\n        S[0][j] = -j\n    return S\n\ndef calculate_cell_score(S: List[List[int]], i: int, j: int, s: str, t: str) -&gt; int:\n    \"\"\"Calculate the score for a cell in the alignment matrix.\"\"\"\n    match_score = 1 if s[i-1] == t[j-1] else -1\n    return max(\n        S[i-1][j-1] + match_score,\n        S[i-1][j] - 1,\n        S[i][j-1] - 1\n    )\n\ndef global_alignment(s: str, t: str) -&gt; List[List[int]]:\n    \"\"\"Perform global alignment and return the score matrix.\"\"\"\n    rows, cols = len(s) + 1, len(t) + 1\n    S = initialize_score_matrix(rows, cols)\n\n    for i in range(1, rows):\n        for j in range(1, cols):\n            S[i][j] = calculate_cell_score(S, i, j, s, t)\n\n    return S\n\ndef align_to_symbols(s: str, t: str) -&gt; Tuple[int, int]:\n    \"\"\"Compute the maximum alignment score and sum of all alignment scores.\"\"\"\n    prefix_matrix = global_alignment(s, t)\n    suffix_matrix = global_alignment(s[::-1], t[::-1])\n\n    total = 0\n    best = -(len(s) + len(t))\n\n    for i in range(len(s)):\n        for j in range(len(t)):\n            match_score = 1 if s[i] == t[j] else -1\n            score = prefix_matrix[i][j] + match_score + suffix_matrix[len(s)-1-i][len(t)-1-j]\n            total += score\n            best = max(best, score)\n\n    return best, total\n\nsample_input = \"\"\"\n&gt;Rosalind_35\nATAGATA\n&gt;Rosalind_5\nACAGGTA\n\"\"\"\n\nsequences = parse_fasta(sample_input)\nif len(sequences) != 2:\n    raise ValueError(\"Expected exactly two sequences in the input.\")\n\ns, t = sequences\nbest_score, total_score = align_to_symbols(s, t)\n\nprint(f\"{best_score}\")\nprint(f\"{total_score}\")"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#code-breakdown",
    "href": "posts/md/Rosalind_stronghold.html#code-breakdown",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "99.4 Code Breakdown",
    "text": "99.4 Code Breakdown\n\nParsing FASTA Format (parse_fasta function):\n\nPurpose: Extracts DNA sequences from a FASTA formatted string.\nHow It Works:\n\nInput: A string with lines that include sequences starting with &gt; (sequence headers) and followed by sequence data.\nProcess:\n\nIt reads the input line by line.\nWhen it encounters a line starting with &gt;, it recognizes it as a header and finishes the current sequence.\nIt collects sequence data lines and joins them into a single sequence string.\nIt returns a list of sequences.\n\n\nOutput: A list of DNA sequences.\n\nInitializing the Score Matrix (initialize_score_matrix function):\n\nPurpose: Set up a matrix to track alignment scores between two sequences, initializing with gap penalties.\nHow It Works:\n\nInput: Number of rows (sequence length + 1) and columns (sequence length + 1).\nProcess:\n\nCreates a matrix of zeros.\nFills the first row and column with penalties for gaps (negative values), representing the cost of inserting gaps.\n\n\nOutput: A score matrix with initialized gap penalties.\n\nCalculating Cell Scores (calculate_cell_score function):\n\nPurpose: Determine the alignment score for a specific cell in the matrix.\nHow It Works:\n\nInput: Current cell indices, the score matrix, and the sequences being aligned.\nProcess:\n\nCalculates the score for matching or mismatching characters, and the penalty for gaps.\nTakes the maximum score among possible scenarios: match/mismatch, gap in one sequence, or gap in the other sequence.\n\n\nOutput: The best score for the current cell.\n\nGlobal Alignment (global_alignment function):\n\nPurpose: Create and fill a score matrix for global alignment of two sequences.\nHow It Works:\n\nInput: Two sequences.\nProcess:\n\nInitializes the score matrix with gap penalties.\nFills the matrix using calculate_cell_score for each cell.\n\n\nOutput: A filled score matrix representing global alignment scores.\n\nAligning to Symbols (align_to_symbols function):\n\nPurpose: Calculate the best alignment score and total score sum by considering both the original and reversed sequences.\nHow It Works:\n\nInput: Two sequences.\nProcess:\n\nComputes the alignment score matrices for both the original sequences and their reversed versions.\nCalculates scores by combining the prefix (forward alignment) and suffix (reverse alignment) matrices.\nFinds the best score and sums all scores from the matrix.\n\n\nOutput: The highest alignment score and the total sum of all scores."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-99",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-99",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "100.1 Sample Dataset",
    "text": "100.1 Sample Dataset\n2\nACGTAG\nACGGATCGGCATCGT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-100",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-100",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "100.2 Sample Output",
    "text": "100.2 Sample Output\n1 4\n1 5\n1 6"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-96",
    "href": "posts/md/Rosalind_stronghold.html#solution-96",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "100.3 Solution",
    "text": "100.3 Solution\nimport sys\nimport re\nimport multiprocessing as mp\n\ndef get_seeds(x, seq, k):\n    seed_size = len(x) // (k + 1)\n    for s1 in range(0, len(x) - seed_size + 1, seed_size):\n        px = (s1, s1 + seed_size)\n        seed = x[px[0] : px[1]]\n        for m in re.finditer(rf\"(?=({seed}))\", seq):\n            ps = (m.span()[0], m.span()[0] + seed_size)\n            yield (px, ps)\n\ndef process_seed(args):\n    def extend_fwd(i, j, score):\n        if (i, j, score) not in seen:\n            seen.update([(i, j, score)])\n            if score &lt;= k:\n                if i == len(x) - 1:\n                    yield i, j, score\n                if i + 1 &lt; len(x):\n                    yield from extend_fwd(i + 1, j, score + 1)\n                if j + 1 &lt; len(seq):\n                    yield from extend_fwd(i, j + 1, score + 1)\n                if i + 1 &lt; len(x) and j + 1 &lt; len(seq):\n                    yield from extend_fwd(\n                        i + 1, j + 1, score + int(x[i + 1] != seq[j + 1])\n                    )\n\n    def extend_rev(i, j, score):\n        if (i, j, score) not in seen:\n            seen.update([(i, j, score)])\n            if score &lt;= k:\n                if i == 0:\n                    yield i, j, score\n                if i - 1 &gt;= 0:\n                    yield from extend_rev(i - 1, j, score + 1)\n                if j - 1 &gt;= 0:\n                    yield from extend_rev(i, j - 1, score + 1)\n                if i - 1 &gt;= 0 and j - 1 &gt;= 0:\n                    yield from extend_rev(\n                        i - 1, j - 1, score + int(x[i - 1] != seq[j - 1])\n                    )\n\n    print(\".\", end=\"\", file=sys.stderr)\n    sys.stderr.flush()\n    sys.setrecursionlimit(10000)\n    seed, k, x, seq = args\n    xcoord, seqcoord = seed\n    res = set()\n    seen = set()\n    fwds = list(extend_fwd(xcoord[1] - 1, seqcoord[1] - 1, 0))\n    if not fwds:\n        return set()\n    seen = set()\n    revs = list(extend_rev(xcoord[0], seqcoord[0], 0))\n    if not revs:\n        return set()\n    for i0, j0, s0 in revs:\n        for i1, j1, s1 in fwds:\n            if s0 + s1 &lt;= k:\n                res.add((j0 + 1, j1 - j0 + 1))\n    return res\n\n# Sample input\nsample_input = \"\"\"\n1\nACGTAG\nGGACGATAGGTAAAGTAGTAGCGACGTAGG\n\"\"\"\n\nk, x, seq = sample_input.strip().split(\"\\n\")\nk = int(k)\nseeds = list(get_seeds(x, seq, k))\nprint(f\"found {len(seeds)} seeds\", file=sys.stderr)\n\npool = mp.Pool(mp.cpu_count())\nargs = ([seed, k, x, seq] for seed in seeds)\nres = pool.map(process_seed, args)\nres = set().union(*res)\n\n# 결과 출력\nfor start, length in sorted(list(res)):\n    print(f\"{start} {length}\")\n\n\n\n\n\n\nNote\n\n\n\n해당 문제는 계산량이 많아 파이썬을 사용하는 것이 적절하지 않습니다. 그래도 위 코드를 download dataset 에 적용해 실행한 결과 1 분 45 초 정도가 소요되어 통과할 수 있었습니다.(사용한 CPU: 13th Gen Intel i9-13900F (32) @ 5.3GHz)\n\n\nThis Python code is designed to find approximate matches of a short DNA sequence x within a longer DNA sequence seq. It does this by breaking down the problem into smaller “seed” sequences and then extending those seeds to find matches, even if there are a few mismatches allowed (controlled by the parameter k)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#overview-of-the-code",
    "href": "posts/md/Rosalind_stronghold.html#overview-of-the-code",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "100.4 Overview of the Code",
    "text": "100.4 Overview of the Code\n\nSeeding (get_seeds function):\n\nThe get_seeds function divides the sequence x into smaller overlapping segments called “seeds.”\nFor each seed, it searches within the longer sequence seq to find exact matches of the seed.\nIt returns the positions in both x and seq where these seeds match.\n\nExtending Seeds (process_seed function):\n\nThis function takes a seed and tries to extend it in both directions (forward and backward) to see if a longer match can be found between x and seq, even with up to k mismatches.\nTwo helper functions, extend_fwd and extend_rev, recursively extend the seed by comparing characters in x and seq while keeping track of mismatches.\nThe results are stored as starting positions and lengths of the matching segments.\n\nParallel Processing:\n\nThe script uses multiprocessing to speed up the search by running the seed extension proces in parallel acros multiple CPU cores.\nEach seed is processed independently, and the results are combined.\n\nResult Compilation:\n\nThe final matching segments are collected, and the unique results are sorted and printed as the start position and length of each matching segment in seq.\n\n\n\nThe code searches for parts of a short DNA sequence (x) within a longer DNA sequence (seq), allowing for a small number of mismatches (k).\nIt does this by first finding small exact matches (seeds) and then extending these matches to find longer sequences with few mismatches.\nThe proces is parallelized to improve performance, especially when dealing with large DNA sequences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-100",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-100",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "101.1 Sample Dataset",
    "text": "101.1 Sample Dataset\n&gt;Rosalind_54\nCTAAGGGATTCCGGTAATTAGACAG\n&gt;Rosalind_45\nATAGACCATATGTCAGTGACTGTGTAA"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-101",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-101",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "101.2 Sample Output",
    "text": "101.2 Sample Output\n1\nATTAGAC-AG\nAT-AGACCAT"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-97",
    "href": "posts/md/Rosalind_stronghold.html#solution-97",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "101.3 Solution",
    "text": "101.3 Solution\nimport numpy as np\n\ndef oap(s1, s2, penalty=-2):\n    score = np.empty((len(s2) + 1, len(s1) + 1), dtype=int)\n    ptr = np.empty((len(s2) + 1, len(s1) + 1), dtype=int)\n\n    for j in range(len(s2) + 1):\n        score[j][0] = j * penalty\n        ptr[j][0] = 1\n    for i in range(len(s1) + 1):\n        score[0][i] = 0\n        ptr[0][i] = 2\n\n    score[0][0] = 0\n    for j in range(len(s2)):\n        for i in range(len(s1)):\n            opt = [\n                score[j][i] + (1 if s1[i] == s2[j] else penalty),\n                score[j][i + 1] + penalty,\n                score[j + 1][i] + penalty,\n            ]\n            best = max(opt)\n            score[j + 1][i + 1] = best\n            ptr[j + 1][i + 1] = opt.index(best)\n\n    sc = [score[j][len(s1)] for j in range(len(s2) + 1)]\n    max_score = max(sc)\n    j = [j for j, s in enumerate(sc) if s == max_score][-1]\n    i = len(s1)\n    a1, a2 = \"\", \"\"\n    while i &gt; 0 and j &gt; 0:\n        if ptr[j][i] == 0:\n            a1 += s1[i - 1]\n            a2 += s2[j - 1]\n            j, i = j - 1, i - 1\n        elif ptr[j][i] == 1:\n            a1 += \"-\"\n            a2 += s2[j - 1]\n            j = j - 1\n        elif ptr[j][i] == 2:\n            a1 += s1[i - 1]\n            a2 += \"-\"\n            i = i - 1\n\n    return max_score, a1[::-1], a2[::-1]\n\ndef read_fasta(fasta_string):\n    \"\"\"\n    Parses a FASTA formatted string and returns a list of sequences.\n    \"\"\"\n    sequences = []\n    current_sequence = []\n    for line in fasta_string.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            if current_sequence:\n                sequences.append(''.join(current_sequence))\n                current_sequence = []\n        else:\n            current_sequence.append(line.strip())\n    if current_sequence:\n        sequences.append(''.join(current_sequence))\n    return sequences\n\nsample_input = \"\"\"\n&gt;Rosalind_54\nCTAAGGGATTCCGGTAATTAGACAG\n&gt;Rosalind_45\nATAGACCATATGTCAGTGACTGTGTAA\n\"\"\"\n\ns1, s2 = read_fasta(sample_input)\nprint(*oap(s1, s2, -2), sep=\"\\n\")\nThis code implements a semi-global sequence alignment algorithm, also known as overlap alignment. Here’s a brief explanation of how it works:\n\nInitialization:\n\nCreates two matrices: ‘score’ for alignment scores and ‘ptr’ for backtracking.\nInitializes the first row and column of these matrices.\n\nFilling the matrices:\n\nIterates through both sequences, filling the ‘score’ and ‘ptr’ matrices.\nFor each cell, calculates three possible scores: match/mismatch, gap in s1, gap in s2.\nChooses the maximum score and stores it along with a pointer to its origin.\n\nFinding the best alignment:\n\nFinds the maximum score in the last column of the ‘score’ matrix.\nThis allows for free end gaps in s2 (overlap alignment).\n\nTraceback:\n\nStarts from the position of the maximum score.\nFollows the pointers back to construct the aligned sequences.\nAdds gaps (‘-’) where necessary.\n\nResult:\n\nReturns the maximum alignment score and the two aligned sequences.\n\n\nThe ‘read_fasta’ function parses a FASTA-formatted string into sequences.\nFinally, it applies this alignment algorithm to two sequences from the sample input and prints the results.\nThis algorithm is particularly useful for finding the best overlap between two sequences, allowing for free end gaps in one of the sequences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-101",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-101",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "102.1 Sample Dataset",
    "text": "102.1 Sample Dataset\nA B C D E\n(A,C,((B,D),E));\n(C,(B,D),(A,E));"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-102",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-102",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "102.2 Sample Output",
    "text": "102.2 Sample Output\n4"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-98",
    "href": "posts/md/Rosalind_stronghold.html#solution-98",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "102.3 Solution",
    "text": "102.3 Solution\nimport re\nimport math\n\n\ndef quartet_distance(taxa, t1, t2):\n    def parse_newick(taxa, tree_str):\n        tree = {}\n        parent = {}\n        node_names = {}\n        new_node_id = 0\n        root = new_node_id\n        tree[root] = []\n        current_node = root\n\n        for match in re.finditer(r\"\\(|\\)|,|;|([^\\(\\),;]+)\", tree_str):\n            token = match.group()\n            if token == '(':\n                new_node_id += 1\n                tree[current_node].append(new_node_id)\n                parent[new_node_id] = current_node\n                current_node = new_node_id\n                tree[current_node] = []\n            elif token == ',':\n                new_node_id += 1\n                tree[parent[current_node]].append(new_node_id)\n                parent[new_node_id] = parent[current_node]\n                current_node = new_node_id\n                tree[current_node] = []\n            elif token == ')':\n                current_node = parent[current_node]\n            elif token == ';':\n                break\n            else:\n                node_names[current_node] = token\n        \n        return tree, parent, node_names\n\n    def get_children(tree, parent, edge_id):\n        if edge_id &gt; 0:\n            return tree[edge_id]\n        else:\n            parent_id = parent[-edge_id]\n            if parent_id == 0:\n                return [e for e in tree[parent_id] if e != -edge_id]\n            else:\n                return [-parent_id] + [e for e in tree[parent_id] if e != -edge_id]\n\n    def compute_shared_leaves(i, j):\n        if shared_leaves[i][j] is None:\n            if not children1[i] and not children2[j]:  # Both are leaves\n                shared_leaves[i][j] = int(leaves1[i] == leaves2[j])\n            elif not children1[i]:  # i is a leaf\n                j1, j2 = children2[j]\n                shared_leaves[i][j] = compute_shared_leaves(i, j1) + compute_shared_leaves(i, j2)\n            elif not children2[j]:  # j is a leaf\n                i1, i2 = children1[i]\n                shared_leaves[i][j] = compute_shared_leaves(i1, j) + compute_shared_leaves(i2, j)\n            else:  # Both are internal nodes\n                i1, i2 = children1[i]\n                j1, j2 = children2[j]\n                shared_leaves[i][j] = (\n                    compute_shared_leaves(i1, j1) + compute_shared_leaves(i1, j2) +\n                    compute_shared_leaves(i2, j1) + compute_shared_leaves(i2, j2)\n                )\n        return shared_leaves[i][j]\n\n    def calculate_quartet_distances():\n        for i in all_edges1:\n            for j in all_edges2:\n                compute_shared_leaves(i, j)\n\n        total_distance = 0\n        for c1 in internal_edges1:\n            for c2 in internal_edges2:\n                a1, b1 = children1[-c1]\n                a2, b2 = children2[-c2]\n                quartet_value = (\n                    shared_leaves[a1][a2] * shared_leaves[b1][b2] +\n                    shared_leaves[a1][b2] * shared_leaves[b1][a2]\n                )\n                total_distance += quartet_value * (shared_leaves[c1][c2] * (shared_leaves[c1][c2] - 1) / 2)\n\n        return total_distance\n\n    # Parse Newick trees\n    tree1, parent1, leaves1 = parse_newick(taxa, t1)\n    tree2, parent2, leaves2 = parse_newick(taxa, t2)\n\n    # Number of taxa\n    n = len(taxa)\n\n    # Get children of each edge\n    children1 = [None] * (4 * n - 5)\n    children2 = [None] * (4 * n - 5)\n\n    for i in range(1, 2 * n - 2):\n        children1[i] = get_children(tree1, parent1, i)\n    for i in range(3 - 2 * n, 0):\n        children1[i] = get_children(tree1, parent1, i)\n\n    for j in range(1, 2 * n - 2):\n        children2[j] = get_children(tree2, parent2, j)\n    for j in range(3 - 2 * n, 0):\n        children2[j] = get_children(tree2, parent2, j)\n\n    # Initialize shared leaves matrix\n    shared_leaves = [[None] * (4 * n - 5) for _ in range(4 * n - 5)]\n\n    # List of all edges and internal edges\n    all_edges1 = list(range(1, 2 * n - 2)) + [edge for edge in range(3 - 2 * n, 0) if leaves1.get(-edge) is None]\n    all_edges2 = list(range(1, 2 * n - 2)) + [edge for edge in range(3 - 2 * n, 0) if leaves2.get(-edge) is None]\n    internal_edges1 = [edge for edge in all_edges1 if leaves1.get(edge) is None]\n    internal_edges2 = [edge for edge in all_edges2 if leaves2.get(edge) is None]\n\n    # Calculate quartet distances\n    total_quartets = calculate_quartet_distances()\n\n    # Calculate and return the quartet distance\n    max_possible_quartets = 2 * math.comb(n, 4)\n    return max_possible_quartets - total_quartets\n\n\n# Sample input\nsample_input = \"\"\"\nA B C D E\n(A,C,((B,D),E));\n(C,(B,D),(A,E));\n\"\"\".strip().split(\"\\n\")\n\ntaxa = sample_input[0].split()\nnwck1 = sample_input[1]\nnwck2 = sample_input[2]\n\nprint(quartet_distance(taxa, nwck1, nwck2))"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#explanation-of-how-it-works",
    "href": "posts/md/Rosalind_stronghold.html#explanation-of-how-it-works",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "102.4 Explanation of How It Works",
    "text": "102.4 Explanation of How It Works\n\nNewick Parsing (parse_newick):\n\nThe function parse_newick parses a Newick-formatted tree string and constructs a representation of the tree using a dictionary tree, which maps node IDs to their children. It also maintains a parent dictionary to track parent-child relationships and a name dictionary to map node IDs to taxa names.\nThis parsed tree allows us to later traverse and compare the structures of the two trees.\n\nChildren Function (get_children):\n\nThe get_children function retrieves the children of a given edge in the tree. If the edge is positive, it directly retrieves children from the tree structure. If the edge is negative, it handles the reversed direction by excluding the edge itself from the parent’s list.\n\nShared Leaves Calculation (compute_shared_leaves):\n\nThe compute_shared_leaves function computes the number of shared leaves between two subtrees (one from each tree) by recursively exploring their child nodes. It caches results to avoid redundant calculations, significantly optimizing performance.\n\nQuartet Calculation (calculate_quartet_distances):\n\nThe calculate_quartet_distances function iterates over all pairs of internal edges from the two trees and calculates the quartet distances. A quartet distance measures the difference in tree structure by comparing the shared leaves for each quartet configuration.\nThis function sums up these quartet values, representing the differences between the two trees.\n\nQuartet Distance Calculation:\n\nFinally, the quartet_distance function subtracts the calculated quartet differences from the total possible quartets for the number of taxa (given by math.comb(n, 4)). This provides the quartet distance, a measure of how dissimilar the two trees are in terms of their quartets."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-102",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-102",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "103.1 Sample Dataset",
    "text": "103.1 Sample Dataset\n&gt;Rosalind_79\nCAGCACTTGGATTCTCGG\n&gt;Rosalind_98\nCAGCGTGG"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-103",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-103",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "103.2 Sample Output",
    "text": "103.2 Sample Output\n4\nCAGCA-CTTGGATTCTCGG\n---CAGCGTGG--------"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-99",
    "href": "posts/md/Rosalind_stronghold.html#solution-99",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "103.3 Solution",
    "text": "103.3 Solution\ndef semiglobal_alignment(seq1, seq2):\n    seq1 = \"-\" + seq1\n    seq2 = \"-\" + seq2\n\n    score_matrix = [[0 for j in range(len(seq2))] for i in range(len(seq1))]\n    direction_matrix = [[None for j in range(len(seq2))] for i in range(len(seq1))]\n\n    for i in range(1, len(seq1)):\n        for j in range(1, len(seq2)):\n\n            match_score = score_matrix[i - 1][j - 1] + (1 if seq1[i] == seq2[j] else -1)\n            delete_score = score_matrix[i - 1][j] - 1\n            insert_score = score_matrix[i][j - 1] - 1\n            score_matrix[i][j] = max(match_score, delete_score, insert_score)\n            if score_matrix[i][j] == match_score:\n                direction_matrix[i][j] = \"diagonal\"\n            elif score_matrix[i][j] == delete_score:\n                direction_matrix[i][j] = \"up\"\n            else:\n                direction_matrix[i][j] = \"left\"\n\n    last_row_max = max(range(len(seq2)), key=lambda x: score_matrix[len(seq1) - 1][x])\n    last_col_max = max(range(len(seq1)), key=lambda x: score_matrix[x][len(seq2) - 1])\n    if score_matrix[len(seq1) - 1][last_row_max] &gt;= score_matrix[last_col_max][len(seq2) - 1]:\n        i = len(seq1) - 1\n        j = last_row_max\n    else:\n        i = last_col_max\n        j = len(seq2) - 1\n    max_score = score_matrix[i][j]\n\n    insert_gap = lambda word, i: word[:i] + '-' + word[i:]\n\n    # Initialize the aligned sequences as the input sequences.\n    aligned_seq1, aligned_seq2 = seq1[1:], seq2[1:]\n\n    for _ in range(len(seq1) - 1 - i):\n        aligned_seq2 += '-'\n    for _ in range(len(seq2) - 1 - j):\n        aligned_seq1 += '-'\n\n    while i * j != 0:\n        if direction_matrix[i][j] == \"up\":\n            i -= 1\n            aligned_seq2 = insert_gap(aligned_seq2, j)\n        elif direction_matrix[i][j] == \"left\":\n            j -= 1\n            aligned_seq1 = insert_gap(aligned_seq1, i)\n        else:\n            i -= 1\n            j -= 1\n\n    for _ in range(i):\n        aligned_seq2 = insert_gap(aligned_seq2, 0)\n    for _ in range(j):\n        aligned_seq1 = insert_gap(aligned_seq1, 0)\n\n    return max_score, aligned_seq1, aligned_seq2\n\ndef parse_fasta(fasta_string):\n    sequences = {}\n    current_label = None\n    for line in fasta_string.strip().split('\\n'):\n        if line.startswith('&gt;'):\n            current_label = line[1:].strip()\n            sequences[current_label] = ''\n        else:\n            sequences[current_label] += line.strip()\n    return list(sequences.values())\n\nsample_input = \"\"\"\n&gt;Rosalind_79\nCAGCACTTGGATTCTCGG\n&gt;Rosalind_98\nCAGCGTGG\n\"\"\"\n\nsequence_A, sequence_B = parse_fasta(sample_input)\nfinal_score, aligned_sequence_A, aligned_sequence_B = semiglobal_alignment(sequence_A, sequence_B)\n\nprint(final_score)\nprint(aligned_sequence_A)\nprint(aligned_sequence_B)\n\nThe semiglobal_alignment function implements a semi-global alignment algorithm for two sequences:\n\nIt adds a gap character “-” at the beginning of both sequences.\nIt creates two matrices: score_matrix for alignment scores and direction_matrix for backtracking.\n\nThe function then fills these matrices:\n\nIt calculates scores for matches (1), mismatches (-1), and gaps (-1).\nIt chooses the maximum score among match, deletion, and insertion for each cell.\nIt records the direction (diagonal, up, or left) in the direction_matrix.\n\nAfter filling the matrices, it finds the best alignment end point:\n\nIt checks the maximum score in the last row and last column.\nIt chooses the higher of these two as the ending point of the alignment.\n\nThe function then performs a traceback to construct the aligned sequences:\n\nIt starts from the best end point and follows the directions in direction_matrix.\nIt adds gaps to the sequences as needed during the traceback.\n\nFinally, it returns the maximum score and the two aligned sequences.\nThe parse_fasta function reads a FASTA-formatted string:\n\nIt separates the sequences and their labels.\nIt returns a list of sequences without the labels.\n\nThe main part of the code:\n\nDefines a sample input in FASTA format.\nParses the input using parse_fasta.\nCalls semiglobal_alignment with the parsed sequences.\nPrints the final score and the aligned sequences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-103",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-103",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "104.1 Sample Dataset",
    "text": "104.1 Sample Dataset\n&gt;Rosalind_8\nPLEASANTLY\n&gt;Rosalind_18\nMEANLY"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-104",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-104",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "104.2 Sample Output",
    "text": "104.2 Sample Output\n12\nLEAS\nMEAN"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-100",
    "href": "posts/md/Rosalind_stronghold.html#solution-100",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "104.3 Solution",
    "text": "104.3 Solution\nBLOSUM62 = {\n    ('W', 'F'): 1, ('L', 'R'): -2, ('S', 'P'): -1, ('V', 'T'): 0,\n    ('Q', 'Q'): 5, ('N', 'A'): -2, ('Z', 'Y'): -2, ('W', 'R'): -3,\n    ('Q', 'A'): -1, ('S', 'D'): 0, ('H', 'H'): 8, ('S', 'H'): -1,\n    ('H', 'D'): -1, ('L', 'N'): -3, ('W', 'A'): -3, ('Y', 'M'): -1,\n    ('G', 'R'): -2, ('Y', 'I'): -1, ('Y', 'E'): -2, ('B', 'Y'): -3,\n    ('Y', 'A'): -2, ('V', 'D'): -3, ('B', 'S'): 0, ('Y', 'Y'): 7,\n    ('G', 'N'): 0, ('E', 'C'): -4, ('Y', 'Q'): -1, ('Z', 'Z'): 4,\n    ('V', 'A'): 0, ('C', 'C'): 9, ('M', 'R'): -1, ('V', 'E'): -2,\n    ('T', 'N'): 0, ('P', 'P'): 7, ('V', 'I'): 3, ('V', 'S'): -2,\n    ('Z', 'P'): -1, ('V', 'M'): 1, ('T', 'F'): -2, ('V', 'Q'): -2,\n    ('K', 'K'): 5, ('P', 'D'): -1, ('I', 'H'): -3, ('I', 'D'): -3,\n    ('T', 'R'): -1, ('P', 'L'): -3, ('K', 'G'): -2, ('M', 'N'): -2,\n    ('P', 'H'): -2, ('F', 'Q'): -3, ('Z', 'G'): -2, ('X', 'L'): -1,\n    ('T', 'M'): -1, ('Z', 'C'): -3, ('X', 'H'): -1, ('D', 'R'): -2,\n    ('B', 'W'): -4, ('X', 'D'): -1, ('Z', 'K'): 1, ('F', 'A'): -2,\n    ('Z', 'W'): -3, ('F', 'E'): -3, ('D', 'N'): 1, ('B', 'K'): 0,\n    ('X', 'X'): -1, ('F', 'I'): 0, ('B', 'G'): -1, ('X', 'T'): 0,\n    ('F', 'M'): 0, ('B', 'C'): -3, ('Z', 'I'): -3, ('Z', 'V'): -2,\n    ('S', 'S'): 4, ('L', 'Q'): -2, ('W', 'E'): -3, ('Q', 'R'): 1,\n    ('N', 'N'): 6, ('W', 'M'): -1, ('Q', 'C'): -3, ('W', 'I'): -3,\n    ('S', 'C'): -1, ('L', 'A'): -1, ('S', 'G'): 0, ('L', 'E'): -3,\n    ('W', 'Q'): -2, ('H', 'G'): -2, ('S', 'K'): 0, ('Q', 'N'): 0,\n    ('N', 'R'): 0, ('H', 'C'): -3, ('Y', 'N'): -2, ('G', 'Q'): -2,\n    ('Y', 'F'): 3, ('C', 'A'): 0, ('V', 'L'): 1, ('G', 'E'): -2,\n    ('G', 'A'): 0, ('K', 'R'): 2, ('E', 'D'): 2, ('Y', 'R'): -2,\n    ('M', 'Q'): 0, ('T', 'I'): -1, ('C', 'D'): -3, ('V', 'F'): -1,\n    ('T', 'A'): 0, ('T', 'P'): -1, ('B', 'P'): -2, ('T', 'E'): -1,\n    ('V', 'N'): -3, ('P', 'G'): -2, ('M', 'A'): -1, ('K', 'H'): -1,\n    ('V', 'R'): -3, ('P', 'C'): -3, ('M', 'E'): -2, ('K', 'L'): -2,\n    ('V', 'V'): 4, ('M', 'I'): 1, ('T', 'Q'): -1, ('I', 'G'): -4,\n    ('P', 'K'): -1, ('M', 'M'): 5, ('K', 'D'): -1, ('I', 'C'): -1,\n    ('Z', 'D'): 1, ('F', 'R'): -3, ('X', 'K'): -1, ('Q', 'D'): 0,\n    ('X', 'G'): -1, ('Z', 'L'): -3, ('X', 'C'): -2, ('Z', 'H'): 0,\n    ('B', 'L'): -4, ('B', 'H'): 0, ('F', 'F'): 6, ('X', 'W'): -2,\n    ('B', 'D'): 4, ('D', 'A'): -2, ('S', 'L'): -2, ('X', 'S'): 0,\n    ('F', 'N'): -3, ('S', 'R'): -1, ('W', 'D'): -4, ('V', 'Y'): -1,\n    ('W', 'L'): -2, ('H', 'R'): 0, ('W', 'H'): -2, ('H', 'N'): 1,\n    ('W', 'T'): -2, ('T', 'T'): 5, ('S', 'F'): -2, ('W', 'P'): -4,\n    ('L', 'D'): -4, ('B', 'I'): -3, ('L', 'H'): -3, ('S', 'N'): 1,\n    ('B', 'T'): -1, ('L', 'L'): 4, ('Y', 'K'): -2, ('E', 'Q'): 2,\n    ('Y', 'G'): -3, ('Z', 'S'): 0, ('Y', 'C'): -2, ('G', 'D'): -1,\n    ('B', 'V'): -3, ('E', 'A'): -1, ('Y', 'W'): 2, ('E', 'E'): 5,\n    ('Y', 'S'): -2, ('C', 'N'): -3, ('V', 'C'): -1, ('T', 'H'): -2,\n    ('P', 'R'): -2, ('V', 'G'): -3, ('T', 'L'): -1, ('V', 'K'): -2,\n    ('K', 'Q'): 1, ('R', 'A'): -1, ('I', 'R'): -3, ('T', 'D'): -1,\n    ('P', 'F'): -4, ('I', 'N'): -3, ('K', 'I'): -3, ('M', 'D'): -3,\n    ('V', 'W'): -3, ('W', 'W'): 11, ('M', 'H'): -2, ('P', 'N'): -2,\n    ('K', 'A'): -1, ('M', 'L'): 2, ('K', 'E'): 1, ('Z', 'E'): 4,\n    ('X', 'N'): -1, ('Z', 'A'): -1, ('Z', 'M'): -1, ('X', 'F'): -1,\n    ('K', 'C'): -3, ('B', 'Q'): 0, ('X', 'B'): -1, ('B', 'M'): -3,\n    ('F', 'C'): -2, ('Z', 'Q'): 3, ('X', 'Z'): -1, ('F', 'G'): -3,\n    ('B', 'E'): 1, ('X', 'V'): -1, ('F', 'K'): -3, ('B', 'A'): -2,\n    ('X', 'R'): -1, ('D', 'D'): 6, ('W', 'G'): -2, ('Z', 'F'): -3,\n    ('S', 'Q'): 0, ('W', 'C'): -2, ('W', 'K'): -3, ('H', 'Q'): 0,\n    ('L', 'C'): -1, ('W', 'N'): -4, ('S', 'A'): 1, ('L', 'G'): -4,\n    ('W', 'S'): -3, ('S', 'E'): 0, ('H', 'E'): 0, ('S', 'I'): -2,\n    ('H', 'A'): -2, ('S', 'M'): -1, ('Y', 'L'): -1, ('Y', 'H'): 2,\n    ('Y', 'D'): -3, ('E', 'R'): 0, ('X', 'P'): -2, ('G', 'G'): 6,\n    ('G', 'C'): -3, ('E', 'N'): 0, ('Y', 'T'): -2, ('Y', 'P'): -3,\n    ('T', 'K'): -1, ('A', 'A'): 4, ('P', 'Q'): -1, ('T', 'C'): -1,\n    ('V', 'H'): -3, ('T', 'G'): -2, ('I', 'Q'): -3, ('Z', 'T'): -1,\n    ('C', 'R'): -3, ('V', 'P'): -2, ('P', 'E'): -1, ('M', 'C'): -1,\n    ('K', 'N'): 0, ('I', 'I'): 4, ('P', 'A'): -1, ('M', 'G'): -3,\n    ('T', 'S'): 1, ('I', 'E'): -3, ('P', 'M'): -2, ('M', 'K'): -1,\n    ('I', 'A'): -1, ('P', 'I'): -3, ('R', 'R'): 5, ('X', 'M'): -1,\n    ('L', 'I'): 2, ('X', 'I'): -1, ('Z', 'B'): 1, ('X', 'E'): -1,\n    ('Z', 'N'): 0, ('X', 'A'): 0, ('B', 'R'): -1, ('B', 'N'): 3,\n    ('F', 'D'): -3, ('X', 'Y'): -1, ('Z', 'R'): 0, ('F', 'H'): -1,\n    ('B', 'F'): -3, ('F', 'L'): 0, ('X', 'Q'): -1, ('B', 'B'): 4\n}\n\ndef local_alignment_with_affine_gap(str1, str2, gap_open=11, gap_extend=1):\n    m, n = len(str1), len(str2)\n    \n    # Initialize score matrices\n    M = [[0] * (n + 1) for _ in range(m + 1)]\n    X = [[-float('inf')] * (n + 1) for _ in range(m + 1)]\n    Y = [[-float('inf')] * (n + 1) for _ in range(m + 1)]\n    \n    # Initialize backtrack matrices\n    B = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    max_score, max_i, max_j = 0, 0, 0\n    \n    # Fill matrices\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            X[i][j] = max(X[i][j-1] - gap_extend, M[i][j-1] - gap_open - gap_extend)\n            Y[i][j] = max(Y[i-1][j] - gap_extend, M[i-1][j] - gap_open - gap_extend)\n            \n            key = (str1[i-1], str2[j-1]) if (str1[i-1], str2[j-1]) in BLOSUM62 else (str2[j-1], str1[i-1])\n            match_score = BLOSUM62[key]\n            \n            M[i][j] = max(0, M[i-1][j-1] + match_score, X[i][j], Y[i][j])\n            \n            if M[i][j] &gt; max_score:\n                max_score, max_i, max_j = M[i][j], i, j\n            \n            if M[i][j] == 0:\n                B[i][j] = 0\n            elif M[i][j] == M[i-1][j-1] + match_score:\n                B[i][j] = 1\n            elif M[i][j] == X[i][j]:\n                B[i][j] = 2\n            else:\n                B[i][j] = 3\n    \n    # Backtrack\n    i, j = max_i, max_j\n    aligned_1, aligned_2 = [], []\n    \n    while B[i][j] != 0:\n        if B[i][j] == 1:\n            aligned_1.append(str1[i-1])\n            aligned_2.append(str2[j-1])\n            i -= 1\n            j -= 1\n        elif B[i][j] == 2:\n            aligned_1.append('-')\n            aligned_2.append(str2[j-1])\n            j -= 1\n        else:\n            aligned_1.append(str1[i-1])\n            aligned_2.append('-')\n            i -= 1\n    \n    return max_score, ''.join(reversed(aligned_1)), ''.join(reversed(aligned_2))\n\ndef parse_fasta(fasta_str):\n    sequences = {}\n    for record in fasta_str.strip().split('&gt;')[1:]:\n        lines = record.split('\\n')\n        sequences[lines[0]] = ''.join(lines[1:])\n    return list(sequences.values())\n\nsample_input = \"\"\"\n&gt;Rosalind_8\nPLEASANTLY\n&gt;Rosalind_18\nMEANLY\n\"\"\"\n\nstringA, stringB = parse_fasta(sample_input)\nscore, aligned_strA, aligned_strB = local_alignment_with_affine_gap(stringA, stringB)\n\nprint(score)\nprint(aligned_strA.replace(\"-\", \"\"))\nprint(aligned_strB.replace(\"-\", \"\"))\n\nBLOSUM62 Matrix:\n\nBLOSUM62 is a dictionary containing scores for amino acid substitutions based on the BLOSUM62 matrix. It provides a scoring scheme for amino acid matches and mismatches.\n\nlocal_alignment_with_affine_gap Function:\n\nInputs: str1 and str2 (the sequences to align), gap_open and gap_extend (penalties for opening and extending gaps).\nInitialization:\n\nM, X, and Y are matrices used to store scores for alignments and gaps.\nB is a backtracking matrix to reconstruct the optimal alignment.\n\nMatrix Filling:\n\nIterates over each position in the matrices, computing scores based on the BLOSUM62 matrix and gap penalties.\nUpdates the matrices to reflect the best alignment score at each position.\n\nBacktracking:\n\nConstructs the optimal local alignment by following the backtracking matrix.\n\n\nparse_fasta Function:\n\nInput: A string in FASTA format containing sequences.\nOutput: A list of sequences parsed from the FASTA format.\n\nExample Usage:\n\nsample_input: A FASTA formatted string with two example sequences.\nExecution:\n\nParses the FASTA string into sequences.\nPerforms local alignment on these sequences.\nPrints the alignment score and the aligned sequences (with gaps removed)."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-dataset-104",
    "href": "posts/md/Rosalind_stronghold.html#sample-dataset-104",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "105.1 Sample Dataset",
    "text": "105.1 Sample Dataset\n(((ostrich,cat)rat,mouse)dog,elephant)robot;\n&gt;robot\nAATTG\n&gt;dog\nGGGCA\n&gt;mouse\nAAGAC\n&gt;rat\nGTTGT\n&gt;cat\nGAGGC\n&gt;ostrich\nGTGTC\n&gt;elephant\nAATTC"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#sample-output-105",
    "href": "posts/md/Rosalind_stronghold.html#sample-output-105",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "105.2 Sample Output",
    "text": "105.2 Sample Output\ndog mouse 1 A-&gt;G-&gt;A\ndog mouse 2 A-&gt;G-&gt;A\nrat ostrich 3 G-&gt;T-&gt;G\nrat cat 3 G-&gt;T-&gt;G\ndog rat 3 T-&gt;G-&gt;T"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#solution-101",
    "href": "posts/md/Rosalind_stronghold.html#solution-101",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "105.3 Solution",
    "text": "105.3 Solution\nclas Node:\n    def __init__(self, number, parent, name=None):\n        self.number = number\n        self.parent = parent\n        self.children = []\n        self.name = name or f\"Node_{number}\"\n\n    def __repr__(self):\n        return f\"Node_{self.number}({self.name})\" if self.name != f\"Node_{self.number}\" else f\"Node_{self.number}\"\n\n    def add_child(self, child):\n        self.children.append(child)\n\nclas Newick:\n    def __init__(self, data):\n        self.nodes = []\n        self.edges = []\n        self.construct_tree(data)\n        self.name_index = {node.name: node.number for node in self.nodes}\n        self.inv_name_index = {node.number: node.name for node in self.nodes}\n\n    def construct_tree(self, data):\n        tokens = data.replace(',', ' ').replace('(', '( ').replace(')', ' )').strip(';').split()\n        stack = [Node(-1, None)]\n        for token in tokens:\n            if token == '(':\n                new_node = Node(len(self.nodes), stack[-1].number)\n                self.nodes.append(new_node)\n                if len(self.nodes) &gt; 1:\n                    self.nodes[new_node.parent].add_child(new_node.number)\n                    self.edges.append((new_node.parent, new_node.number))\n                stack.append(new_node)\n            elif token == ')':\n                stack.pop()\n            elif token.startswith(')'):\n                stack[-1].name = token[1:]\n                stack.pop()\n            else:\n                new_node = Node(len(self.nodes), stack[-1].number, token)\n                self.nodes.append(new_node)\n                self.nodes[new_node.parent].add_child(new_node.number)\n                self.edges.append((new_node.parent, new_node.number))\n\n    def traverse(self, node_index=0, order='pre'):\n        node = self.nodes[node_index]\n        if order == 'pre':\n            result = [node]\n            for child in node.children:\n                result.extend(self.traverse(child, order))\n        else:  # post-order\n            result = []\n            for child in node.children:\n                result.extend(self.traverse(child, order))\n            result.append(node)\n        return result\n\n    def max_depth(self, node):\n        return max([self.max_depth(self.nodes[child]) for child in node.children], default=-1) + 1\n\n    def all_paths(self, node):\n        if not node.children:\n            return []\n\n        paths = []\n        stack = [(node, [node.name])]\n        while stack:\n            current, path = stack.pop()\n            for child_idx in current.children:\n                child = self.nodes[child_idx]\n                new_path = path + [child.name]\n                if len(new_path) &gt;= 3:\n                    paths.append(new_path)\n                stack.append((child, new_path))\n        return paths\n\n    def rsub(self, DNA_strings):\n        rsub_list = []\n        pre_order = self.traverse(order='pre')\n        k = len(next(iter(DNA_strings.values())))\n        \n        for pos in range(k):\n            for node in pre_order:\n                if node.children:\n                    for path in self.all_paths(node):\n                        nucs = [DNA_strings[label][pos] for label in path]\n                        if nucs[0] == nucs[-1] != nucs[1] and all(x == nucs[1] for x in nucs[1:-1]):\n                            rsub_list.append([path[1], path[-1], str(pos + 1), \"-&gt;\".join([nucs[0], nucs[1], nucs[-1]])])\n        return rsub_list\n\ndef parse_fasta(lines):\n    sequences = {}\n    current_seq = []\n    current_name = \"\"\n    for line in lines:\n        line = line.strip()\n        if line.startswith(\"&gt;\"):\n            if current_name:\n                sequences[current_name] = \"\".join(current_seq)\n            current_name = line[1:]\n            current_seq = []\n        else:\n            current_seq.append(line)\n    if current_name:\n        sequences[current_name] = \"\".join(current_seq)\n    return sequences\n\n\nsample_input = \"\"\"\n(((ostrich,cat)rat,mouse)dog,elephant)robot;\n&gt;robot\nAATTG\n&gt;dog\nGGGCA\n&gt;mouse\nAAGAC\n&gt;rat\nGTTGT\n&gt;cat\nGAGGC\n&gt;ostrich\nGTGTC\n&gt;elephant\nAATTC\n\"\"\".strip().split(\"\\n\")\n\nnewick = sample_input[0]\nDNA_strings = parse_fasta(sample_input[1:])\n\ntree = Newick(newick)\nresult = tree.rsub(DNA_strings)\nfor r in result:\n    print(\" \".join(r))\nHere’s a refactored version of the provided code with explanations:\nclas Node:\n    def __init__(self, number, parent, name=None):\n        self.number = number\n        self.parent = parent\n        self.children = []\n        self.name = name or f\"Node_{number}\"\n\n    def __repr__(self):\n        return f\"Node_{self.number}({self.name})\" if self.name != f\"Node_{self.number}\" else f\"Node_{self.number}\"\n\n    def add_child(self, child):\n        self.children.append(child)\n\nclas Newick:\n    def __init__(self, data):\n        self.nodes = []\n        self.edges = []\n        self.construct_tree(data)\n        self.name_index = {node.name: node.number for node in self.nodes}\n        self.inv_name_index = {node.number: node.name for node in self.nodes}\n\n    def construct_tree(self, data):\n        tokens = data.replace(',', ' ').replace('(', '( ').replace(')', ' )').strip(';').split()\n        stack = [Node(-1, None)]\n        for token in tokens:\n            if token == '(':\n                new_node = Node(len(self.nodes), stack[-1].number)\n                self.nodes.append(new_node)\n                if len(self.nodes) &gt; 1:\n                    self.nodes[new_node.parent].add_child(new_node.number)\n                    self.edges.append((new_node.parent, new_node.number))\n                stack.append(new_node)\n            elif token == ')':\n                stack.pop()\n            elif token.startswith(')'):\n                stack[-1].name = token[1:]\n                stack.pop()\n            else:\n                new_node = Node(len(self.nodes), stack[-1].number, token)\n                self.nodes.append(new_node)\n                self.nodes[new_node.parent].add_child(new_node.number)\n                self.edges.append((new_node.parent, new_node.number))\n\n    def traverse(self, node_index=0, order='pre'):\n        node = self.nodes[node_index]\n        if order == 'pre':\n            result = [node]\n            for child in node.children:\n                result.extend(self.traverse(child, order))\n        else:  # post-order\n            result = []\n            for child in node.children:\n                result.extend(self.traverse(child, order))\n            result.append(node)\n        return result\n\n    def max_depth(self, node):\n        return max([self.max_depth(self.nodes[child]) for child in node.children], default=-1) + 1\n\n    def all_paths(self, node):\n        if not node.children:\n            return []\n\n        paths = []\n        stack = [(node, [node.name])]\n        while stack:\n            current, path = stack.pop()\n            for child_idx in current.children:\n                child = self.nodes[child_idx]\n                new_path = path + [child.name]\n                if len(new_path) &gt;= 3:\n                    paths.append(new_path)\n                stack.append((child, new_path))\n        return paths\n\n    def rsub(self, DNA_strings):\n        rsub_list = []\n        pre_order = self.traverse(order='pre')\n        k = len(next(iter(DNA_strings.values())))\n        \n        for pos in range(k):\n            for node in pre_order:\n                if node.children:\n                    for path in self.all_paths(node):\n                        nucs = [DNA_strings[label][pos] for label in path]\n                        if nucs[0] == nucs[-1] != nucs[1] and all(x == nucs[1] for x in nucs[1:-1]):\n                            rsub_list.append([path[1], path[-1], str(pos + 1), \"-&gt;\".join([nucs[0], nucs[1], nucs[-1]])])\n        return rsub_list\n\ndef parse_fasta(lines):\n    sequences = {}\n    current_seq = []\n    current_name = \"\"\n    for line in lines:\n        line = line.strip()\n        if line.startswith(\"&gt;\"):\n            if current_name:\n                sequences[current_name] = \"\".join(current_seq)\n            current_name = line[1:]\n            current_seq = []\n        else:\n            current_seq.append(line)\n    if current_name:\n        sequences[current_name] = \"\".join(current_seq)\n    return sequences"
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#classes-and-their-functions",
    "href": "posts/md/Rosalind_stronghold.html#classes-and-their-functions",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "105.4 Classes and Their Functions",
    "text": "105.4 Classes and Their Functions\n\nNode Class:\n\nRepresents a single node in the tree.\nEach node has a unique number, a parent node, a list of children, and a name.\n\nNewick Class:\n\nPurpose: Parses and builds a tree from a Newick format string.\nKey Methods:\n\nconstruct_tree(data): Builds the tree structure from the Newick format string.\ntraverse(): Gets all nodes in a specific order (pre-order or post-order).\nmax_depth(node): Finds the maximum depth of the tree from a given node.\nall_paths(node): Lists all paths starting from a node.\nrsub(DNA_strings): Finds specific patterns in the DNA sequences based on the tree.\n\n\nparse_fasta(lines):\n\n\nConverts FASTA formatted sequence data into a dictionary. Keys are sequence names, and values are the sequences."
  },
  {
    "objectID": "posts/md/Rosalind_stronghold.html#how-it-works-1",
    "href": "posts/md/Rosalind_stronghold.html#how-it-works-1",
    "title": "Rosalind Stronghold 문제풀이",
    "section": "105.5 How It Works",
    "text": "105.5 How It Works\n\nParse the Tree and Sequences:\n\nNewick Tree: Convert the Newick format string into a tree structure.\nFASTA Sequences: Read and store DNA sequences.\n\nFind Patterns:\n\nUse the tree structure and DNA sequences to find and list patterns where:\n\nThe first and last characters of a pattern are the same.\nThe middle characters are all the same but different from the first/last character."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html",
    "href": "posts/md/Cheatsheet_Seurat.html",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "",
    "text": "Seurat 은 scRNA-seq 분석에서 가장 널리 사용되는 R 패키지로, 다양한 기능을 통해 데이터를 처리하고 시각화합니다. 이 문서는 주요 명령어와 기능을 요약하여 사용자가 데이터를 효율적으로 다룰 수 있도록 지원합니다. 특히, 데이터 로드, 전처리, 클러스터링 및 시각화와 같은 일반적인 작업에 대한 예제를 제공합니다. Seurat v5.0.1 버전을 사용한 환경에서 테스트되었습니다. 다양한 명령어와 튜토리얼은 공식 페이지 에서 확인하세요."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#메타데이터에-새로운-열-추가",
    "href": "posts/md/Cheatsheet_Seurat.html#메타데이터에-새로운-열-추가",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "2.1 메타데이터에 새로운 열 추가",
    "text": "2.1 메타데이터에 새로운 열 추가\n새로운 열을 추가하려면 $ 기호를 사용하여 벡터를 메타데이터 열에 할당할 수 있습니다.\nSeuratObject$NEW_COLUMN_NAME &lt;- setNames(\n    colnames(SeuratObject),\n    vector_NEW_DATA\n)\n또는 AddMetaData 함수를 사용하여 열을 추가할 수 있습니다.\nSeuratObject &lt;- AddMetaData(\n    object   = SeuratObject,\n    metadata = vector_NEW_DATA,\n    col.name = \"NEW_COLUMN_NAME\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#seurat-객체-하위-집합-만들기",
    "href": "posts/md/Cheatsheet_Seurat.html#seurat-객체-하위-집합-만들기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "2.2 Seurat 객체 하위 집합 만들기",
    "text": "2.2 Seurat 객체 하위 집합 만들기\n일부 데이터를 따로 때어내는 방법은 R 의 데이터프레임 문법을 사용하는 방법과 Seurat 함수를 사용하는 방법이 있습니다. 먼저 데이터 프레임 문법은 아래와 같습니다.\n# Feature 하위 집합\nSeuratObject[vector_FEATURES_TO_USE, ]\n# 세포 하위 집합\nSeuratObject[, vector_CELLS_TO_USE]\nSeurat 에서 제공하는 함수는 아래와 같습니다.\n# Identity 클래스를 기반으로 Seurat 객체를 하위 집합으로 추출합니다. 자세한 내용은 ?SubsetData를 참조하세요.\nsubset(x = SeuratObject, idents = \"Bcell\")\nsubset(x = SeuratObject, idents = c(\"CD4T\", \"CD8T\"), invert = TRUE)\n\n# 유전자/특성의 발현 수준을 기준으로 하위 집합을 만듭니다.\nsubset(x = SeuratObject, subset = MS4A1 &gt; 3)\n\n# 기준의 조합에 따라 하위 집합을 만듭니다.\nsubset(x = SeuratObject, subset = MS4A1 &gt; 3 & PC1 &gt; 5)\nsubset(x = SeuratObject, subset = MS4A1 &gt; 3, idents = \"B 세포\")\n\n# 객체 메타 데이터의 값에 따라 하위 집합을 만듭니다.\nsubset(x = SeuratObject, subset = orig.ident == \"Replicate1\")\n\n# 각 identity 클래스당 세포 수를 다운샘플링합니다.\nsubset(x = SeuratObject, downsample = 100)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#시각화하기",
    "href": "posts/md/Cheatsheet_Seurat.html#시각화하기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "2.3 시각화하기",
    "text": "2.3 시각화하기\n가장 일반적으로 사용되는 시각화 함수는 바이올린 플롯과 UMAP(차원 축소 후의 산점도) 입니다.\n\n\n\n\n\n\nWarning\n\n\n\nUMAP 을 그리기 전에 반드시 먼저 차원 축소를 계산해야 합니다.\n\n\n연속 변수에 대하여 바이올린 플롯 시각화하는 코드\nVlnPlot(\n    object = SeuratObject,\n    group.by = \"orig.ident\",\n    features = c(\"percent_mito\"),\n    pt.size = 0.1,\n    ncol = 4,\n    y.max = 100\n) + NoLegend()\n연속 변수에 대한 UMAP 을 그리는 방법 (예시: 유전자 발현량):\nFeaturePlot(\n    object = SeuratObject,\n    features = c(\"FEATURE_1\", \"FEATURE_2\", \"FEATURE_3\"),\n    reduction = \"umap\",\n    dims = c(1, 2),\n    order = TRUE,\n    pt.size = 0.1,\n    ncol = 3\n)\n범주형 변수에 대한 UMAP 을 그리는 방법 (예시: 세포 유형):\nDimPlot(\n    object = SeuratObject,\n    group.by = c(\"DATASET\"),\n    reduction = \"umap\",\n    dims = c(1, 2),\n    pt.size = 0.1,\n    label = TRUE,\n    ncol = 3\n)\n이것 말고도 다양한 시각화 함수가 있으니 자세한 것은 공식 문서 를 참고하세요."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#데이터-합치기",
    "href": "posts/md/Cheatsheet_Seurat.html#데이터-합치기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "2.4 데이터 합치기",
    "text": "2.4 데이터 합치기\n두 개이상의 데이터셋을 결합하려면:\nmerged_obj &lt;- merge(\n    x = SeuratObject1,\n    y = c(SeuratObject2, SeuratObject3, SeuratObject4),\n    add.cell.ids = c(\"Dataset1\", \"Dataset2\", \"Dataset3\", \"Dataset4\")\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#미토콘드리아-유전자-비율",
    "href": "posts/md/Cheatsheet_Seurat.html#미토콘드리아-유전자-비율",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "4.1 미토콘드리아 유전자 비율",
    "text": "4.1 미토콘드리아 유전자 비율\n높은 비율의 미토콘드리아 유전자는 일반적으로 손상된 세포를 의미합니다. 이는 세포질 속의 RNA 가 쉽게 소실되는 반면, 미토콘드리아 속의 RNA 는 비교적 덜 손실되기 때문입니다. 다음은 미토콘드리아 유전자 비율을 계산하고 메타데이터 테이블에 추가하는 예시입니다.\n# 미토콘드리아 유전자의 비율 계산\nSeuratObject &lt;- PercentageFeatureSet(\n    object = SeuratObject,\n    pattern = \"^MT\",\n    assay = \"RNA\",\n    col.name = \"percent_mito\"\n)\n\n\n\n\n\n\nNote\n\n\n\n유전자 관련 품질 관리에 대한 설명은 사람 (human) 에 해당하는 내용으로 다른 종의 데이터를 처리하는 경우에는 코드가 수정되어야 합니다. 예시로 생쥐 (mouse) 의 경우 미토콘드리아 (mt-), 리보소말 (Rpl) 입니다."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#리보솜-유전자-비율",
    "href": "posts/md/Cheatsheet_Seurat.html#리보솜-유전자-비율",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "4.2 리보솜 유전자 비율",
    "text": "4.2 리보솜 유전자 비율\n리보솜 유전자는 모든 세포에서 가장 많이 발현되는 유전자 중 하나이며, 미토콘드리아 유전자와는 반대로 미토콘드리아 수에 반비례하는 경향이 있습니다. 즉, 미토콘드리아의 수가 많을수록 리보솜 유전자의 감지 수준은 낮아집니다 (다만, 이 관계는 비선형적입니다). 리보솜 단백질에서 유래하는 유전자 발현 비율을 계산하는 방법은 미토콘드리아 유전자 비율을 계산하는 방법과 유사합니다.\n# 리보솜 유전자의 비율 계산\nSeuratObject &lt;- PercentageFeatureSet(\n    SeuratObject,\n    pattern  = \"^RP[SL]\",\n    col.name = \"percent_ribo\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#유전자-유형-및-염색체-위치-정보",
    "href": "posts/md/Cheatsheet_Seurat.html#유전자-유형-및-염색체-위치-정보",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "4.3 유전자 유형 및 염색체 위치 정보",
    "text": "4.3 유전자 유형 및 염색체 위치 정보\nRNA 시퀀싱에서 얻은 유전자는 여러 유형으로 구분할 수 있습니다. 이에는 단백질 발현 정보를 포함하는 Coding 유전자, 단백질 정보를 포함하지 않는 Non-coding 유전자, BCR 및 TCR 과 관련된 VDJ 영역 유전자 (세포 분화에 대한 정보를 포함), 그리고 siRNA 등이 포함됩니다. 이러한 정보를 활용하면 원하는 분석 유형에 따라 관심 없는 특정 유전자 범주를 필터링할 수 있습니다.\n특히, scRNA-seq 는 일반적으로 poly-A 를 사용하여 RNA 를 농축하기 때문에 데이터에 존재하는 유전자의 약 80-90% 가 단백질 코딩 유전자입니다. 또한, 유전자의 염색체 위치 정보를 수집하면 성 염색체에 의한 효과를 식별하는 데 유용할 수 있습니다.\n# 사람 유전자로 변경하기 위해 먼저 ENSEMBL에서 마우스 유전자 주석 검색\nlibrary(biomaRt)\n\n# ENSEMBL 마트 설정\nmart &lt;- biomaRt::useMart(\n    biomart = \"ensembl\",\n    dataset = \"hsapiens_gene_ensembl\",\n    host= \"www.ensembl.org\"\n)\n\n# 선택된 속성으로 마우스 유전자 주석 검색\nannot &lt;- biomaRt::getBM(\n    mart = mart,\n    attributes = c(\"external_gene_name\", \"gene_biotype\", \"chromosome_name\")\n)\n유전자 이름을 해당 생물 유형과 일치시킵니다.\n# 유전자 이름과 해당 염색체 위치 일치\nitem &lt;- annot[match(rownames(SeuratObject), annot[, 1]), \"chromosome_name\"]\n\n# NA 값을 \"unknown\"으로 대체\nitem[is.na(item)] &lt;- \"unknown\"\n\n# 유효하지 않은 염색체 값을 \"other\"로 대체\nitem[!item %in% as.character(c(1:23, \"X\", \"Y\", \"MT\"))] &lt;- \"other\"\n예를 들어 단백질 코딩 유전자에만 초점을 맞추려면 다음과 같이 수행 할 수 있습니다.\n# SeuratObject의 차원 확인\ndim(SeuratObject)\n\n# 유전자 주석에서 단백질 코딩 유전자 선택\nsel &lt;- annot[match(rownames(SeuratObject), annot[, 1]), 2] == \"protein_coding\"\n\n# 선택된 유전자 이름 가져오기\ngenes_use &lt;- rownames(SeuratObject)[sel]\ngenes_use &lt;- as.character(na.omit(genes_use))\n\n# SeuratObject에서 단백질 코딩 유전자만 필터링\nSeuratObject &lt;- SeuratObject[genes_use, ]\n\n# 필터링 후 SeuratObject의 차원 확인\ndim(SeuratObject)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#데이터-정규화",
    "href": "posts/md/Cheatsheet_Seurat.html#데이터-정규화",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "7.1 데이터 정규화",
    "text": "7.1 데이터 정규화\nscRNA-seq 에서 가장 일반적인 정규화 방법은 로그 정규화입니다. 이 방법은 각 유전자의 카운트를 모든 유전자 카운트의 합 (라이브러리 크기라고도 함) 으로 나누어 라이브러리 크기의 차이를 보상합니다. 그 후, 결과에 상수를 곱하여 모든 세포가 동일한 시퀀싱 깊이를 가지도록 합니다. 대부분의 bulk RNA-seq 에서는 상수가 보통 1e6 이며, 이로 인해 CPM(백만 단위 카운트) 이 생성됩니다. 그러나 단일 세포 라이브러리 크기는 그보다 훨씬 작기 때문에 1e3 에서 1e4(10,000 단위 카운트) 까지 사용됩니다.\n\\[ NormCounts = \\frac{(GeneCounts \\times 10000)}{LibrarySize} \\]\n라이브러리 크기에 대해 보정된 값은 로그 변환하여 로그 정규 분포로 만듭니다.\n\\[ logNormCounts = \\ln(NormCounts + 1) \\]\nSeurat에서는 NormalizeData 함수를 사용하여 이 과정을 수행할 수 있습니다.\n# 분산이 0인 유전자 제거\nSeuratObject &lt;- SeuratObject[Matrix::rowSums(SeuratObject) &gt; 0, ]\n\n# 데이터 정규화 (LogNormalize 사용)\nSeuratObject &lt;- NormalizeData(\n    object = SeuratObject,\n    scale.factor = 10000,\n    normalization.method = \"LogNormalize\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#데이터-회귀-regression",
    "href": "posts/md/Cheatsheet_Seurat.html#데이터-회귀-regression",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "7.2 데이터 회귀 (Regression)",
    "text": "7.2 데이터 회귀 (Regression)\n데이터에서 혼란 요인을 제거하기 위해 회귀 분석을 수행할 수 있습니다. 예를 들어, 세포 주기나 품질 지표 (예: 미토콘드리아 비율 또는 감지된 유전자 수) 와 같은 혼란 요인을 제거할 수 있습니다. Seurat에서는 ScaleData 함수를 사용하여 이러한 회귀를 수행할 수 있습니다.\nSeuratObject &lt;- ScaleData(\n    SeuratObject,\n    vars.to.regress = c(\"nFeature_RNA\", \"percent_mito\"),\n    verbose = FALSE\n)\n이렇게 하면 데이터에 회귀가 적용되며, 이제 데이터가 준비되었으므로 추가 분석을 진행할 수 있습니다."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#선형-스케일링-및-중심화",
    "href": "posts/md/Cheatsheet_Seurat.html#선형-스케일링-및-중심화",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "9.1 (선형) 스케일링 및 중심화",
    "text": "9.1 (선형) 스케일링 및 중심화\n각 유전자는 서로 다른 발현 수준을 가지므로, 높은 발현 값을 가진 유전자는 자연스럽게 더 높은 변동성을 보입니다. 이는 후속 분석에서 잘 포착될 수 있습니다. 따라서 각 유전자에 대해 유사한 가중치를 부여하는 것이 중요합니다. 일반적으로 PCA 를 수행하기 전에 각 유전자를 중심화하고 스케일링하는 것이 권장됩니다. 이러한 정밀한 스케일링 방법은 Z- 점수 정규화라고 하며, PCA, 클러스터링 및 히트맵 시각화에 매우 유용합니다.\n또한, 세포 주기, 시퀀싱 깊이, 미토콘드리아 비율 등과 같은 데이터셋에서 원하지 않는 변동 소스를 제거하기 위해 회귀 분석을 사용할 수 있습니다. 이는 이러한 매개변수를 모형의 공변량으로 사용하여 일반화된 선형 회귀 (GLM) 를 수행함으로써 달성됩니다. 이후 모델의 잔차는 “회귀된 데이터”로 간주됩니다.\nSeuratObject &lt;- ScaleData(\n    object = SeuratObject,\n    vars.to.regress = c(\"nCount_RNA\", \"percent_mito\", \"nFeatures_RNA\"),\n    model.use = \"linear\",\n    assay = \"RNA\",\n    do.scale = TRUE,\n    do.center = TRUE\n)\nSeurat 은 기본적으로 Seurat 객체에 있는 모든 변수 기능에 대해 스케일링을 실행합니다. 변수 기능이 있을 경우 이러한 스케일링 단계의 성능을 크게 향상시킬 수 있습니다. 그러나 결과는 변수 유전자 자체에 기반하여 PCA, 차원 축소 또는 클러스터링과 같은 후속 분석의 결과가 변경되지 않습니다. 전체 유전자 수에 대해 데이터를 스케일링하려는 경우 FindVariableFeatures 를 사용한 후 함수 호출에서 이를 지정할 수 있습니다.\nSeuratObject &lt;- ScaleData(\n    object = SeuratObject,\n    vars.to.regress = c(\"nCount_RNA\", \"percent_mito\", \"nFeatures_RNA\"),\n    model.use  = \"linear\",\n    assay = \"RNA\",\n    do.scale = TRUE,\n    do.center = TRUE,\n    features = rownames(SeuratObject)\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#포아송-스케일링-및-중심화",
    "href": "posts/md/Cheatsheet_Seurat.html#포아송-스케일링-및-중심화",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "9.2 (포아송) 스케일링 및 중심화",
    "text": "9.2 (포아송) 스케일링 및 중심화\n위의 선형 스케일링과 중심화는 로그 - 선형 데이터 분포를 가정하기 때문에, RNA-seq 데이터 (단일 세포 포함) 는 보다 음이항 분포에 가까울 수 있으며, 이로 인해 변이를 올바르게 회귀하지 못할 수 있습니다. 위의 절차의 대안으로 “포아송” 또는 “음이항” 분포를 사용하여 원시 UMI 카운트 데이터에 대해 실행할 수 있습니다. 이는 포아송 모델을 사용하여 유전자별 GLM 회귀를 수행하는 방법입니다. \nSeuratObject &lt;- ScaleData(\n    object = SeuratObject,\n    vars.to.regress = c(\"nCount_RNA\", \"mito.percent\", \"nFeatures_RNA\"),\n    model.use = \"poisson\",\n    assay = \"RNA\",\n    do.scale = TRUE,\n    do.center = TRUE\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#pca",
    "href": "posts/md/Cheatsheet_Seurat.html#pca",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "11.1 PCA",
    "text": "11.1 PCA\n주성분 분석 (PCA) 은 데이터를 새로운 좌표계로 변환하는 직교 선형 변환입니다. 이 과정에서 데이터의 가장 큰 분산이 첫 번째 주성분에 위치하고, 두 번째로 큰 분산은 두 번째 주성분에 위치하도록 합니다. 이러한 과정을 통해 데이터의 분산을 가장 잘 설명하는 방식으로 내부 구조를 드러냅니다. 일반적으로 이 동작은 데이터의 차원을 줄여 변환된 데이터의 차원을 축소하는 방식으로 이해될 수 있습니다.\n# 주성분 분석 실행\nSeuratObject &lt;- RunPCA(\n    object = SeuratObject,\n    assay = \"RNA\",\n    npcs = 100,\n    verbose = FALSE\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#t-sne",
    "href": "posts/md/Cheatsheet_Seurat.html#t-sne",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "11.2 t-SNE",
    "text": "11.2 t-SNE\nt- 분포 확률적 이웃 임베딩 (t-distributed stochastic neighbor embedding, t-SNE) 은 고차원 데이터를 저차원 공간으로 임베딩하여 시각화하는 데 적합한 비선형 차원 축소 기술입니다. 이 방법은 각 고차원 객체를 유사한 객체와 가까운 점으로 모델링하고, 서로 다른 객체는 먼 확률적 점으로 모델링하여 두 개 또는 세 개의 차원으로 표현합니다.2\n# tSNE 실행\nSeuratObject &lt;- RunTSNE(\n    object = SeuratObject,\n    reduction = \"pca\",\n    perplexity = 30,\n    max_iter = 1000,\n    theta = 0.5,\n    eta = 200,\n    exaggeration_factor = 12,\n    dims.use = 1:50,\n    verbose = FALSE,\n    num_threads = 0\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#umap",
    "href": "posts/md/Cheatsheet_Seurat.html#umap",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "11.3 UMAP",
    "text": "11.3 UMAP\nUMAP(Uniform Manifold Approximation and Projection) 은 t-SNE 와 유사하게 시각화를 위한 차원 축소에 사용할 수 있는 기술이며, 일반적인 비선형 차원 축소에도 활용될 수 있습니다.3\nUMAP 알고리즘은 실제 데이터에 적용할 수 있는 실용적이고 확장 가능한 방법입니다. 이 알고리즘은 시각화 품질에서 t-SNE 와 경쟁할 만큼 우수하며, 더 나은 실행 시간 성능을 바탕으로 전역 구조를 더 잘 보존한다고 알려져 있습니다. 또한 UMAP 은 임베딩 차원에 대한 계산 제한이 없어 머신러닝을 위한 일반적인 목적의 차원 축소 기술로 널리 사용될 수 있습니다.4\nSeuratObject &lt;- RunUMAP(\n    object = SeuratObject,\n    reduction = \"pca\",\n    dims = 1:50,\n    n.components = 2,\n    n.neighbors = 20,\n    repulsion.strength = 1,\n    verbose = FALSE,\n    n.epochs = 200,\n    metric = \"euclidean\",\n    seed.use = 42,\n    reduction.name = \"umap\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#확산-맵-diffusion-maps",
    "href": "posts/md/Cheatsheet_Seurat.html#확산-맵-diffusion-maps",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "11.4 확산 맵 (Diffusion Maps)",
    "text": "11.4 확산 맵 (Diffusion Maps)\n확산 맵 (Diffusion Maps, DM) 은 데이터 집합을 유클리드 공간 (일반적으로 저차원) 으로 임베딩하는 차원 축소 기법 중 하나입니다. 이 기법은 데이터에 대한 확산 연산자의 고유 벡터와 고유값을 기반으로 임베딩을 계산합니다. 임베딩된 공간에서의 점들 사이의 유클리드 거리는 해당 점들을 중심으로 하는 확률 분포 간의 “확산 거리”와 동일합니다. 이 방법은 주어진 데이터에서 랜덤 워크를 수행할 때, 인접한 데이터 포인트로 이동하는 것이 멀리 떨어진 데이터 포인트로 이동하는 것보다 더 자주 발생한다는 기본적인 관찰에 기반하여 작동합니다.확산 맵은 주로 데이터가 추출된 기저 매니폴드를 발견하는 데 중점을 두고 있으며, 이는 주성분 분석 (PCA) 이나 다차원 스케일링 (MDS) 과 같은 선형 차원 축소 방법과는 다른 접근 방식입니다.5\n# 추가 라이브러리 로드\nlibrary(destiny)\n\n# destiny 패키지를 사용하여 확산 맵 실행\ndm &lt;- DiffusionMap(\n    data = SeuratObject@reductions[[\"pca\"]]@cell.embeddings[, 1:50],\n    k = 20,\n    n_eigs = 20)\n\n# DM 임베딩에서 셀 이름 수정\nrownames(dm@eigenvectors) &lt;- colnames(SeuratObject)\n\n# SeuratObject에 DM 임베딩 추가\nSeuratObject@reductions[[\"dm\"]] &lt;- CreateDimReducObject(\n    embeddings = dm@eigenvectors,                                \n    key = \"DC_\",\n    assay = \"RNA\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#독립-성분-분석-ica",
    "href": "posts/md/Cheatsheet_Seurat.html#독립-성분-분석-ica",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "11.5 독립 성분 분석 (ICA)",
    "text": "11.5 독립 성분 분석 (ICA)\n독립 성분 분석 (Independent Component Analysis, ICA) 은 다변량 신호를 가산성 하위 구성 요소로 분리하는 계산 방법입니다. 이 기법은 하위 구성 요소가 가우시안이 아닌 신호이며 서로 통계적으로 독립적이라는 가정을 기반으로 합니다. ICA 는 블라인드 소스 분리 (blind source separation) 의 특별한 경우로 간주됩니다.6\nSeuratObject &lt;- RunICA(\n    object = SeuratObject,\n    assay = \"pca\",\n    nics = 20,\n    reduction.name = \"ica\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#knn",
    "href": "posts/md/Cheatsheet_Seurat.html#knn",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "12.1 KNN",
    "text": "12.1 KNN\nKNN 은 “K Nearest Neighbors”의 약자로, 데이터 마이닝 및 기계 학습 분야에서 기본적이고 인기 있는 주제입니다. KNN 그래프는 두 정점 p 와 q 가 연결되는 그래프로, 그 거리가 K 번째로 작은 거리 중 하나인 경우에만 연결됩니다. 이때 쌍별 거리 측정치는 해밍 거리, 코사인 거리, 유클리드 거리 등 다양한 방법을 사용할 수 있습니다. 본 논문에서는 벡터 간 유사성을 측정하기 위해 유클리드 거리를 사용합니다. KNN 그래프 데이터 구조는 데이터 마이닝에서 많은 이점을 제공합니다. 예를 들어, 수십억 개의 데이터 세트에서는 KNN 그래프를 오프라인으로 사전 구축하여 인덱스로 사용하는 것이 여러 번의 온라인 KNN 검색보다 훨씬 효율적입니다.\nSeuratObject &lt;- FindNeighbors(\n    SeuratObject,\n    assay = \"RNA\",\n    compute.SNN = FALSE, # FALSE로 설정하면 KNN 그래프만 계산됩니다.\n    reduction = \"pca\",\n    dims = 1:50,\n    graph.name = \"nn\",\n    prune.SNN = 1/15,\n    k.param = 20,\n    force.recalc = TRUE\n)\nKNN 그래프는 모든 셀 간의 연결이 1 로 표시된 행렬로 나타납니다. 이를 가중치가 없는 그래프라고 하며, Seurat 의 기본 설정입니다. 그러나 일부 셀 간의 연결은 다른 연결보다 더 중요할 수 있으며, 이 경우 그래프의 가중치는 0 에서 최대 거리까지 다양해집니다. 일반적으로 거리가 작을수록 두 점이 가까워지고, 그들의 연결이 강해집니다. 이러한 경우를 가중치가 있는 그래프라고 합니다.\n가중치가 있는 그래프와 가중치가 없는 그래프 모두 클러스터링에 적합하지만, 대규모 데이터 세트 (100,000 개 이상의 세포) 에서는 가중치가 없는 그래프에서 클러스터링하는 것이 더 빠릅니다.\nlibrary(pheatmap)\n\npheatmap(\n    SeuratObject@graphs$nn[1:100, 1:100],\n    col = c(\"white\", \"black\"),\n    border_color = \"grey90\",\n    legend = FALSE,\n    cluster_rows = FALSE,\n    cluster_cols = FALSE,\n    fontsize = 2\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#snn",
    "href": "posts/md/Cheatsheet_Seurat.html#snn",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "12.2 SNN",
    "text": "12.2 SNN\n추가로 KNN 그래프 외에도 모든 두 점 간에 공유된 최근접 이웃의 수를 계산합니다. 이를 통해 “공유된 최근접 이웃” 그래프를 형성합니다. 이 과정에서는 두 점 간의 각 링크의 가중치를 해당 점들이 공유하는 이웃의 수로 대체합니다. 다시 말해, 이는 최근접 이웃 그래프 내에서 모든 두 점 사이의 길이가 2 인 경로의 수를 나타냅니다.공유된 최근접 이웃 그래프가 생성되면 모든 점 쌍이 비교됩니다. 두 점이 T 이상의 이웃을 공유하는 경우, 즉 공유된 최근접 이웃 그래프에서 가중치가 임계값 T 보다 많은 링크를 가지면, 두 점과 그들이 속한 모든 클러스터가 병합됩니다. 다시 말해, 클러스터는 임계값을 사용하여 희소화한 후 공유된 최근접 이웃 그래프에서의 연결 요소입니다.\nSeuratObject &lt;- FindNeighbors(\n    SeuratObject,\n    assay = \"RNA\",\n    compute.SNN = TRUE, # KNN 그래프와 SNN 그래프 둘 다 계산합니다.\n    reduction = \"pca\",\n    dims = 1:50,\n    graph.name = \"SNN\",\n    prune.SNN = 1/15,\n    k.param = 20,\n    force.recalc = TRUE\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#scanorama-사용하기",
    "href": "posts/md/Cheatsheet_Seurat.html#scanorama-사용하기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "13.1 Scanorama 사용하기",
    "text": "13.1 Scanorama 사용하기\n\n\n\n\n\n\nNote\n\n\n\n최근 여러 방법들이 데이터셋을 성공적으로 통합할 수 있다는 것을 보여주고 있습니다. 그러나 이러한 접근법들은 데이터셋에 적어도 하나의 공통 세포 유형이 존재한다고 가정하기 때문에, 세포 구성이 상당히 다른 데이터셋을 결합할 경우 과도한 보정이 발생할 수 있습니다.\n\n\nScanorama 의 접근 방식은 두 데이터 세트 간의 유사한 요소를 찾는 상호 최근접 이웃 일치 기술을 일반화하여 여러 데이터 세트 간에 유사한 요소를 탐색합니다. 이 방식은 휴대폰으로 파노라마 사진을 촬영하는 기술과 유사합니다. Scanorama 는 자동으로 유사한 전사 프로필을 가진 세포를 식별하고 이를 일괄 보정 및 통합에 활용합니다. 이 방법은 강력한 특징을 가지고 있으며, 데이터셋의 크기나 출처에 크게 영향을 받지 않으며, 모든 데이터셋이 적어도 하나의 공통 세포 모집단을 공유할 필요가 없습니다.9\nlibrary(reticulate)\nscanorama &lt;- import(\"scanorama\")\n\n# 오브젝트를 배치 별로 분할\nSeuratObject.list &lt;- SplitObject(\n    object   = SeuratObject,\n    split.by = \"BATCH\"\n)\n\n# 어레이 및 제네릭 데이터 만들기\nassaylist &lt;- list()\ngenelist  &lt;- list()\n\nfor (i in seq_along(SeuratObject.list)) {\n    assaylist[[i]] &lt;- t(\n        as.matrix(GetAssayData(SeuratObject.list[[i]], \"data\"))\n    )\n    genelist[[i]] &lt;- rownames(SeuratObject.list[[i]])\n}\n\n# 데이터 통합 및 보정\nintegrated.data &lt;- scanorama$integrate(assaylist, genelist)\ncorrected.data &lt;- scanorama$correct(\n    assaylist,\n    genelist,\n    return_dense = TRUE\n)\nintegrated.corrected.data &lt;- scanorama$correct(\n    assaylist,\n    genelist,\n    return_dimred = TRUE,\n    return_dense = TRUE\n)\n\n# 데이터 결합 및 이름 지정\nintdata &lt;- lapply(integrated.corrected.data[[2]], t)\npanorama &lt;- do.call(cbind, intdata)\nrownames(panorama) &lt;- as.character(integrated.corrected.data[[3]])\ncolnames(panorama) &lt;- unlist(sapply(assaylist, rownames))\n\n# 차원 축소 결과 결합\nintdimred &lt;- do.call(rbind, integrated.corrected.data[[1]])\ncolnames(intdimred) &lt;- paste0(\"PC_\", 1:100)\n\n# Seurat에서 Elbow Plot을 그리기 위해 표준 편차 추가\nstdevs &lt;- apply(intdimred, MARGIN = 2, FUN = sd)\n\n# 새로운 어레이 오브젝트 만들기\nSeuratObject@assays[[\"pano\"]] &lt;- CreateAssayObject(\n    data = panorama,\n    min.cells = 0,\n    min.features = 0\n)\n\n# PCA 결과 추가\nSeuratObject[[\"pca_scanorama\"]] &lt;- CreateDimReducObject(\n    embeddings = intdimred,\n    stdev = stdevs,\n    key = \"PC_\",\n    assay = \"pano\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#harmony-사용하기",
    "href": "posts/md/Cheatsheet_Seurat.html#harmony-사용하기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "13.2 Harmony 사용하기",
    "text": "13.2 Harmony 사용하기\nHarmony 는 주어진 저차원 임베딩 (예: 주성분 분석) 을 기반으로 시작합니다. Harmony 는 먼저 이 임베딩을 사용하여 셀을 여러 데이터 세트 클러스터로 그룹화합니다. 부드러운 전이를 고려하기 위해 소프트 클러스터링을 사용하여 셀을 잠재적으로 여러 클러스터에 할당합니다. 이러한 클러스터는 이산적인 세포 유형을 식별하기 위한 것이 아니라, 세포 상태 간의 부드러운 전환을 반영하는 대리 변수로 기능합니다. 클러스터링이 완료되면 각 데이터셋에는 클러스터별 중심점이 생성됩니다. 이 중심점은 클러스터별 선형 보정 계수를 계산하는 데 사용됩니다. 클러스터가 세포 유형과 상태에 해당하므로, 클러스터별 보정 계수는 개별 세포 유형 및 세포 상태에 특정한 보정 계수로 작용합니다. 이를 통해 Harmony 는 미묘한 세포 표현형에 민감한 간단한 선형 조정 함수를 학습합니다. 마지막으로, 각 세포는 이러한 클러스터의 가중 평균을 할당받고, 세포별 선형 요소에 의해 보정됩니다. 각 세포가 여러 클러스터에 속할 수 있기 때문에, 각 세포에는 잠재적으로 고유한 보정 계수가 부여됩니다. Harmony 는 셀 클러스터 할당이 안정될 때까지 이 네 단계를 반복합니다. Harmony 는 대규모 데이터셋으로 확장 가능하며, 다양한 인구 집단과 세부적인 하위 집단을 동시에 식별할 수 있는 유연성을 갖추고 있습니다. 또한 복잡한 실험 설계를 수용할 수 있으며, 다양한 모드 간 통합이 가능한 강력한 알고리즘입니다.10\n# 추가 라이브러리 로드\nlibrary(harmony)\nlibrary(SeuratWrappers)\n\n# Harmony 실행\nSeuratObject &lt;- RunHarmony(\n  SeuratObject,\n  group.by.vars = \"batch\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#루브앵-louvain",
    "href": "posts/md/Cheatsheet_Seurat.html#루브앵-louvain",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "14.1 루브앵 (Louvain)",
    "text": "14.1 루브앵 (Louvain)\n루브앵 (Louvain) 방법은 Blondel 이 개발한 대규모 네트워크에서 커뮤니티를 추출하는 기법입니다.12 이 방법은 네트워크 노드 수에 대해 \\(O(n.log2n)\\) 의 시간 복잡도를 가지는 탐욕적 최적화 방법입니다. 최적화하는 값은 모듈러리티 (modularity) 로 이는 커뮤니티 내 링크 밀도를 커뮤니티 간 링크와 비교하여 측정하는 범위 내 값입니다. 이 값을 최적화하는 것은 이론적으로 주어진 네트워크의 노드를 가장 잘 그룹화하는 결과를 도출하지만 모든 노드를 그룹으로 나누는 모든 가능한 반복 과정을 거치는 것은 비실용적이므로 휴리스틱 알고리즘이 사용됩니다. \nSeuratObject &lt;- FindClusters(\n  object = SeuratObject,\n  graph.name = \"SNN\",\n  resolution = 0.8,\n  algorithm = 1 # 1로 설정하면 Louvain을 의미합니다.\n)\n클러스터 수는 resolution 매개변수를 사용하여 제어할 수 있으며 값이 높을수록 더 많은 (작은) 클러스터가 생성됩니다."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#라이든-leiden",
    "href": "posts/md/Cheatsheet_Seurat.html#라이든-leiden",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "14.2 라이든 (Leiden)",
    "text": "14.2 라이든 (Leiden)\n라이든 알고리즘은 반복적으로 적용되어 모든 커뮤니티의 하위 집합이 지역적으로 최적화된 분할로 수렴합니다.13 이 알고리즘은 빠른 로컬 이동 접근 방식을 기반으로 하여 루브앵 알고리즘보다 더 빠르게 실행됩니다. 라이든 알고리즘은 다음과 같은 세 단계로 구성됩니다:\n\n노드의 로컬 이동: 네트워크 내의 각 노드는 주변 노드와의 연결을 고려하여 위치를 조정합니다. 이 과정에서 각 노드는 자신의 이웃 중에서 가장 높은 모듈러리티를 제공하는 위치로 이동하게 됩니다. 이를 통해 각 노드가 속한 커뮤니티가 더욱 명확해집니다.\n분할의 정제: 초기 클러스터링 결과를 바탕으로, 각 커뮤니티 내에서 노드의 재배치를 통해 더 나은 분할을 찾습니다. 이 단계에서는 커뮤니티 간의 경계를 명확히 하고, 중복된 연결을 줄이는 과정이 포함됩니다.\n정제된 분할을 바탕으로 네트워크를 집계: 정제된 분할을 사용하여 새로운 집계 네트워크를 생성합니다. 이때, 정제되지 않은 분할을 기반으로 초기 집계 네트워크의 분할을 생성하여, 전체 네트워크 구조를 반영합니다.\n\n이러한 과정을 통해 라이든 알고리즘은 데이터의 구조를 효과적으로 파악하고, 커뮤니티 간의 관계를 명확히 하여 최적화된 결과를 도출합니다. 라이든 알고리즘은 대규모 네트워크에서도 효율적으로 작동하며, 다양한 분야에서 널리 사용되고 있습니다.\nSeuratObject &lt;- FindClusters(\n  object = SeuratObject,\n  graph.name = \"SNN\",\n  resolution = 0.8,\n  algorithm = 4 # 4로 설정하면 Leiden을 의미합니다.\n)\n클러스터 수는 resolution 매개변수를 사용하여 제어할 수 있으며 값이 높을수록 더 많은 (작은) 클러스터가 생성됩니다."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#계층적-클러스터링",
    "href": "posts/md/Cheatsheet_Seurat.html#계층적-클러스터링",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "14.3 계층적 클러스터링",
    "text": "14.3 계층적 클러스터링\n계층적 클러스터링 (Hierarchical Clustering, HC) 은 클러스터의 계층 구조를 구축하는 클러스터 분석 방법입니다. 이 방법은 일반적으로 두 가지 전략으로 나뉘며, 각각 합병적 (agglomerative) 또는 분할적 (divisive) 입니다. 병합과 분할은 보통 탐욕적인 방식으로 결정됩니다. 계층적 클러스터링의 결과는 일반적으로 덴드로그램 (dendrogram) 으로 시각화됩니다.\n계층적 병합 클러스터링 (Hierarchical Agglomerative Clustering, HAC) 의 표준 알고리즘은 \\(O(n^3)\\) 의 시간 복잡도를 가지며 \\(O(n^2)\\) 의 메모리를 필요로 하기 때문에, 중간 규모의 데이터 세트에서도 느릴 수 있습니다. 병합적인 경우에는 어떤 클러스터를 결합할 것인지, 분할적인 경우에는 어떤 클러스터를 분할할 것인지를 결정하기 위해 관측값 집합 간의 불일치 측정이 필요합니다. 대부분의 계층적 클러스터링 방법에서는 적절한 메트릭 (관측값 쌍 간의 거리 측정) 과 연결 기준을 사용하여 관측값 집합의 불일치를 관측값 쌍 간 거리의 함수로 정의합니다.\nR 의 기본 stats 패키지에는 모든 샘플 쌍 사이의 거리를 계산하는 dist() 함수가 포함되어 있습니다. dist() 에서 사용할 수 있는 거리 방법은 다음과 같습니다:\n\n유클리드 거리 (euclidean)\n최대 거리 (maximum)\n맨하탄 거리 (manhattan)\n캔버라 거리 (canberra)\n이진 거리 (binary)\n민코프스키 거리 (minkowski)\n\n또한, 이미 셀 간 거리 정보를 포함하는 그래프 (KNN 또는 SNN) 를 사용하여 직접 계층적 클러스터링을 수행할 수도 있습니다. 그러나 그래프의 거리는 반전되어 있으므로 (0 은 멀리 떨어져 있음을 나타내고 1 은 가깝다는 것을 나타냄), 그래프의 최대값 (인접 SNN 의 경우 1) 을 뺀 후 0 이 가까운 거리를 나타내고 1 이 먼 거리를 나타내도록 조정해야 합니다.\n샘플 간의 거리를 계산한 후에는 본격적인 계층적 클러스터링을 진행할 수 있습니다. 이를 위해 hclust() 함수를 사용하며, 위에서 생성한 거리 객체를 활용하여 간단히 실행할 수 있습니다. 사용 가능한 방법에 대한 자세한 내용은 HC for networks 에서 확인할 수 있습니다.\n# PCA에서 HC 실행\nh_pca &lt;- hclust(\n    d = dist(\n        SeuratObject@reductions[[\"pca\"]]@cell.embeddings[, 1:30],\n        method = \"euclidean\"\n    ),\n    method = \"ward.D2\"\n)\n\n# 그래프에서 HC 실행\nh_graph &lt;- hclust(\n    d = 1 - as.dist(SeuratObject@graphs$SNN),\n    method = \"ward.D2\"\n)\n클러스터 계층이 정의되면, 다음 단계는 특정 클러스터에 속하는 샘플을 식별하는 것입니다.\nplot(h, labels = FALSE)\n세포에 대한 덴드로그램을 확인한 후, 고정된 높이 (threshold) 에서 나무 (tree) 를 자를 수 있습니다. 이 작업은 cutree 함수를 사용하여 수행할 수 있습니다. 또한, 해상도에 따라 다른 수준에서 나무를 자르는 것도 가능합니다.\n# 덴드로그램 높이를 기반으로 나무 자르기\nSeuratObject$HC_res &lt;- cutree(\n    tree = h,\n    k = 18\n)\n\n# 클러스터 수를 기반으로 나무 자르기\nSeuratObject$HC_res &lt;- cutree(\n    tree = h,\n    h = 3\n)\n\n# 각 클러스터에 속하는 셀 수 확인\ntable(SeuratObject$HC_res)\n클러스터의 수는 높이 (h) 를 기준으로 하거나 직접 k 매개변수를 설정하여 조절할 수 있습니다."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#k--평균-클러스터링",
    "href": "posts/md/Cheatsheet_Seurat.html#k--평균-클러스터링",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "14.4 K- 평균 클러스터링",
    "text": "14.4 K- 평균 클러스터링\nK- 평균 클러스터링 (k-means clustering) 은 신호 처리에서 시작된 벡터 양자화 방법으로, n 개의 관측값을 k 개의 클러스터로 분할하여 각 관측값이 평균 (클러스터 중심) 과 가장 가까운 클러스터에 속하도록 하는 것을 목표로 합니다. 이 평균은 클러스터의 프로토타입 역할을 합니다. 그러나 알고리즘은 전역 최적값으로 수렴하는 것을 보장하지 않으며, 결과는 초기 클러스터 설정에 따라 달라질 수 있습니다. 일반적으로 알고리즘은 빠르게 실행되므로 서로 다른 초기 조건으로 여러 번 실행하는 것이 일반적입니다. K- 평균 클러스터링은 주로 비슷한 공간 범위의 클러스터 (모두 같은 크기) 를 찾지만, 기대값 최대화 메커니즘 덕분에 클러스터가 다양한 형태를 가질 수 있습니다.\nK- 평균은 많은 응용 분야에서 널리 사용되는 클러스터링 알고리즘입니다. R 에서는 kmeans 함수를 사용하여 쉽게 적용할 수 있습니다. 일반적으로 이 알고리즘은 표현 데이터의 축소된 차원 표현 (대부분 PCA 를 사용함) 에 적용되며, 이는 저차원 거리의 해석 가능성 덕분입니다. K- 평균 클러스터링을 수행할 때는 클러스터의 수를 미리 정의해야 하며, 결과는 클러스터 중심의 초기화에 따라 달라지므로 일반적으로 다양한 시작 구성으로 k-means 를 실행하는 것이 권장됩니다 (nstart 인수를 통해 설정 가능).\nset.seed(42)  # 재현성을 위해 설정합니다.\n\n# K-means 클러스터링 실행\nSeuratObject$kmeans_12 &lt;- kmeans(\n    x = SeuratObject@reductions[[\"pca\"]]@cell.embeddings[, 1:50],\n    centers = 12,\n    iter.max = 50,\n    nstart = 10\n)$cluster\n클러스터의 수는 centers 매개변수를 사용하여 설정할 수 있습니다. 그렇다면 어떤 클러스터링 해상도를 선택해야 할까요?클러스터링 해상도를 결정하는 것은 간단한 문제가 아니며, 여러 방법을 통해 접근할 수 있습니다. 합리적인 클러스터 수를 정하는 것은 복잡한 과정으로, 샘플에 대한 생물학적 지식과 조사가 필요합니다.차별적 유전자 발현 (Differential gene expression) 분석은 이 과정에 큰 도움이 될 수 있습니다. 만약 두 클러스터가 동일한 차별적 유전자 발현을 보이고 구별할 만한 유전자가 없다면, 이들을 개별 클러스터로 분리하는 것은 바람직하지 않을 수 있습니다. 또한, 품질 관리 지표 (QC metrics) 를 통해 생성된 클러스터를 검토하여, 일부 클러스터가 낮은 품질의 세포나 이중체로 구성되지 않았는지 확인하는 것이 중요합니다."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#클러스터링-결과-비교하기",
    "href": "posts/md/Cheatsheet_Seurat.html#클러스터링-결과-비교하기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "14.5 클러스터링 결과 비교하기",
    "text": "14.5 클러스터링 결과 비교하기\n클러스터링 결과를 다른 클러스터링 기법과 비교하는 것은 매우 유용합니다. 이를 다음과 같은 방식으로 수행할 수 있습니다: 위 코드는 비교 대상 클래스에 속하는 세포 수에 대한 정보를 포함하는 테이블을 생성합니다. 이 데이터를 막대 그래프로 시각화하여 클러스터링 결과를 쉽게 비교할 수 있습니다.\n# 비교 테이블 생성\ncomparison_table &lt;- table(\n    list(\n        SeuratObject@meta.data[,\"METADATA_FACTOR_1\"],\n        SeuratObject@meta.data[,\"METADATA_FACTOR_2\"]\n    )\n)\n\n# 데이터를 백분율로 변환\ncomparison_table &lt;- t(t(comparison_table) / colSums(comparison_table)) * 100\n\n# 막대 그래프 그리기\nbarplot(\n    comparison_table,\n    col = 1:nrow(comparison_table),  # 색상 설정\n    border = NA,\n    las = 2                         # 축 라벨 방향 설정\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#degdifferential-expression-gene-찾기",
    "href": "posts/md/Cheatsheet_Seurat.html#degdifferential-expression-gene-찾기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "15.1 DEG(Differential expression gene) 찾기",
    "text": "15.1 DEG(Differential expression gene) 찾기\n차별적 발현 유전자 (DEG) 는 종종 “마커 유전자”로 언급되지만, 대부분의 DE 테스트는 한 그룹의 세포와 다른 그룹의 세포 간에 발현이 더 높은 유전자를 감지하도록 설계되었다는 점을 인식해야 합니다. DEG 가 자동으로 세포 유형을 위한 고유한 마커가 되지는 않습니다.Seurat 패키지에는 DE 분석을 위한 여러 가지 테스트가 구현되어 있으며, 일부는 scRNA-seq 에 특화되어 있고, 다른 일부는 벌크 RNA-seq 에서 사용됩니다.\n\nwilcox: Wilcoxon 순위 합 검정을 사용하여 두 그룹의 세포 간 차이가 있는 유전자 식별\nbimod: 단일 세포 유전자 발현에 대한 우도 비율 검정 15\nroc: ROC 분석을 사용하여 유전자 발현의 ’마커’를 식별합니다. 각 유전자에 대해 해당 유전자를 기반으로 구축된 분류기를 평가하여 두 그룹의 세포를 분류합니다. AUC 를 사용하여 각 유전자에 대한 분류기의 성능을 평가하며, AUC 값이 1 인 경우 해당 유전자의 발현 값만으로 두 그룹을 완벽하게 구분할 수 있음을 의미합니다. AUC 값이 0 인 경우 완벽한 분류가 이루어지지만 반대 방향입니다. AUC 값이 0.5 인 경우 해당 유전자가 두 그룹을 구분하는 데 예측력이 없음을 나타냅니다. 이 방법은 예상되는 차별적 발현 유전자의 순위를 지정된 예측력\\((abs(AUC-0.5) * 2)\\) 행렬로 반환합니다.\nt: 두 그룹의 세포 간 차이가 있는 유전자 식별을 위한 Student 의 t-test 사용\nnegbinom: 음이항 일반화 선형 모델을 사용하여 두 그룹의 세포 간 차이가 있는 유전자 식별 (UMI 기반 데이터 세트에만 사용)\npoisson: 포아송 일반화 선형 모델을 사용하여 두 그룹의 세포 간 차이가 있는 유전자 식별 (UMI 기반 데이터 세트에만 사용)\nLR: 로지스틱 회귀 모델을 사용하여 차이가 있는 유전자 식별. 각 기능을 개별적으로 사용하여 그룹 소속을 예측하는 로지스틱 회귀 모델을 구성하고 이를 가능도 비율 검정과 비교합니다.\nMAST: scRNA-seq 데이터에 특화된 허들 모델을 사용하여 두 그룹의 세포 간 차이가 있는 유전자 식별. DE 테스트를 실행하기 위해 MAST 패키지를 사용합니다.\nDESeq2: DESeq2 를 사용하여 두 그룹의 세포 간 차이가 있는 유전자 식별. DESeq2 는 음이항 분포를 사용하는 모델에 기초하며, 이 테스트는 세포 그룹 간 평균 차이 (또는 감지율의 백분율) 에 따른 유전자의 사전 필터링을 지원하지 않습니다. 그러나 유전자는 세포 그룹 모두에서 최소 감지율 (min.pct) 기준으로 사전 필터링될 수 있습니다. 이 방법을 사용하려면 DESeq2 를 설치하십시오.\n\nSeurat 객체의 모든 클러스터에 대해 각 클러스터와 모든 다른 세포 간의 DE 예측을 실행하려면 FindAllMarkers 함수를 사용하세요.\nmarkers &lt;- FindAllMarkers(\n    object = SeuratObject,\n    assay = \"RNA\",\n    logfc.threshold = 0.25,\n    test.use = \"wilcox\",\n    slot = \"data\",\n    min.pct = 0.1,\n    min.diff.pct = -Inf,\n    only.pos = FALSE,\n    max.cells.per.ident = Inf,\n    latent.vars = NULL,\n    min.cells.feature = 3,\n    min.cells.group = 3,\n    pseudocount.use = 1,\n    return.thresh = 0.01\n)\n아래에 나타낸 여러 가지 기준을 통해 특정 유전자를 포함하거나 출력을 필터링할 수 있습니다. 예를 들어, 상향 조절된 유전자만 테스트하면 분석 속도를 높일 수 있습니다:\n\nonly.pos = TRUE 를 사용하여 상향 조절된 유전자만 테스트\n유전자가 발현된 최소 세포 수를 설정하는 min.cells.feature\n유전자가 발현된 클러스터의 최소 세포 수를 나타내는 min.cells.group\n클러스터에서 min.pct 이상 발현된 유전자만 테스트\n두 그룹 간의 발현 비율 차이가 min.pct.diff 인 유전자만 테스트\np.value &lt; return.thresh 인 유전자만 반환\nlogFoldchange &gt; logfc.threshold 인 유전자만 반환\n\n또한 고려해야 할 몇 가지 중요한 사항이 있습니다:\n\n객체에 여러 분석이 포함된 경우, 올바른 분석 결과에서 차등발현 분석을 실행해야 합니다. 예를 들어, 데이터가 통합된 경우에는 Batch 를 고려해야 합니다.\n\ncell_selection &lt;- SeuratObject[, SeuratObject$seurat_clusters == 4]\ncell_selection &lt;- SetIdent(cell_selection, value = \"Batch\")\n\n# 차별적 발현 계산\nDGE_cell_selection &lt;- FindAllMarkers(\n    object = cell_selection,\n    logfc.threshold = 0.2,\n    test.use = \"wilcox\",\n    min.pct = 0.1,\n    min.diff.pct = -Inf,\n    only.pos = FALSE,\n    max.cells.per.ident = Inf,\n    latent.vars = NULL,\n    min.cells.feature = 3,\n    min.cells.group = 3,\n    pseudocount.use = 1,\n    return.thresh = 0.01\n)\n또한 FindMarkers 함수를 사용하여 두 세트의 세포 간의 차이를 테스트할 수 있으며, 각 그룹에 대한 셀 이름을 cells 및 cells 로 지정할 수 있습니다."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#deg-시각화하기",
    "href": "posts/md/Cheatsheet_Seurat.html#deg-시각화하기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "15.2 DEG 시각화하기",
    "text": "15.2 DEG 시각화하기\nDE 테스트를 실행한 후, 다양한 방법으로 유전자를 시각화하고 싶을 수 있습니다. 그러나 먼저 각 클러스터에서 상위 DEG 를 추출해야 합니다. 각 클러스터별로 상위 5 개의 유전자를 선택하는 방법은 다음과 같습니다:\ntop5 &lt;- markers %&gt;% group_by(cluster) %&gt;% top_n(-5, p_val_adj)\n이를 히트맵으로 시각화할 수 있습니다:\nDoHeatmap(\n    object = SeuratObject,\n    features = as.character(unique(top5$gene)),\n    group.by = \"seurat_clusters\",\n    assay = \"RNA\"\n)\n또는 각 클러스터가 평균 발현으로 색상별로 나타내고, 해당 유전자를 발현하는 세포의 비율로 크기별로 나타내는 점 도플롯을 사용할 수 있습니다:\n# coord_flip() 함수를 위해 필요\nlibrary(ggplot2) \n\nDotPlot(\n    object = SeuratObject,\n    features = as.character(unique(top5$gene)),\n    group.by = \"seurat_clusters\",\n    assay = \"RNA\"\n) + coord_flip()\n또한 각 유전자에 대해 바이올린 플롯을 그릴 수 있습니다:\nVlnPlot(\n    object = SeuratObject,\n    features = as.character(unique(top5$gene)),\n    ncol = 3,\n    group.by = \"seurat_clusters\",\n    assay = \"RNA\"\n)\n바이올린 플롯을 배치별로 분할할 수도 있습니다. 이것은 감지된 DEG 가 단일 배치로 인해 주도되는 것이 아닌지 확인하는 데 매우 유용할 수 있습니다.\nVlnPlot(\n    object = SeuratObject,\n    features = as.character(unique(top5$gene)),\n    ncol = 3,\n    group.by = \"seurat_clusters\",\n    assay = \"RNA\",\n    split.by = \"Batch\"\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#slingshot-사용하기",
    "href": "posts/md/Cheatsheet_Seurat.html#slingshot-사용하기",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "17.1 Slingshot 사용하기",
    "text": "17.1 Slingshot 사용하기\n슬링샷 (Slingshot) 은 차원 축소, 클러스터링 및 주요 곡선 (Principal curves) 과 같은 다른 경로 분석 방법에서 발견되는 여러 인기 있는 구성 요소를 포함하고 있어 가장 전형적인 방법 중 하나입니다. 슬링샷은 특정 차원 축소 방법에 제한되지 않으며, 어떤 방법을 사용할지 결정할 때 고려해야 할 중요한 세 가지 사항이 있습니다:\n\n데이터의 전체 복잡성을 포착하기에 충분한 차원을 사용해야 합니다. 이는 특히 PCA 및 MDS 와 같은 선형 차원 축소에서 중요하며, 경로의 위상이 이분화보다 복잡할 경우 더욱 그렇습니다. 두 차원에서 경로가 명확하게 보이지 않을 때도 여전히 다차원에서 경로를 시각화할 수 있습니다.\n일부 차원 축소 방법은 고밀도 영역에서 거리를 확대할 수 있습니다. t-SNE 와 UMAP 은 이러한 문제를 겪을 수 있습니다. 우리가 사용하는 데이터셋에서는 이러한 문제가 발생하지 않지만, 생물학적 모집단을 무작위로 샘플링할 경우 문제가 발생할 수 있습니다.\n일부 차원 축소 방법은 셀의 그룹화를 강제하고 연속성을 제거하려고 합니다. t-SNE 와 UMAP 도 이러한 문제를 일부 가지고 있으며, 이로 인해 실제로는 그렇지 않은 셀들이 함께 묶일 수 있습니다.\n\n이러한 이유로 MDS 와 확산 맵이 종종 선호되는 선택지입니다. 그러나 샘플이 균형 잡힌 경우 t-SNE 와 UMAP 도 효과적으로 작동할 수 있습니다.\ndimred &lt;- SeuratObject@reductions[['pca']]@cell.embeddings  # 사용될 embedding 지정\n그 답은 간단합니다. 클러스터링은 문제를 단순화하며, 경로 상에서 거의 동일한 위치에 있는 세포 그룹을 찾아냅니다. 이러한 그룹은 이후 세포가 어떤 경로를 통해 이동하고 분기되는지를 파악하는 데 연결될 수 있습니다. 이상적으로 TI 를 위한 클러스터링 방법은 밀도를 기반으로 셀을 그룹화해서는 안 됩니다. 예를 들어 DBSCAN 과 같은 밀도 기반 클러스터링은 정의상 모든 세포가 고밀도 영역을 통해 연결되지만 유전자 발현에서는 차이가 발생할 수 있습니다. 여기서는 Seurat의 표준 클러스터링 방법인 라이든 (Leiden) 클러스터링을 사용할 것입니다.\nclustering &lt;- factor(\n    SeuratObject@meta.data[, \"seurat_clusters\"]\n) \n이제부터 이 클러스터링 결과를 경로 분석에 적용할 것입니다. 전체 프로세스는 단일 함수인 slingshot 을 사용하여 수행할 수 있으며, 이는 경로 분석을 위한 두 가지 주요 단계를 간단히 묶어 놓은 래퍼 (wrapper) 입니다. 프로세스의 첫 번째 단계는 세포 계통 (cell lineages) 을 정의하고, 그 다음 데이터를 통해 경로를 정의하는 곡선을 맞추는 것입니다. 이러한 단계들은 아래에서 설명합니다.\n\n17.1.1 Slingshot 으로 세포 계통 정의하기\n# 기본 설정으로 Slingshot 세포 계통 식별을 실행\nlibrary(slingshot)\n\nlineages &lt;- getLineages(\n     data = dimred,\n     clusterLabels = clustering\n)\n\n# Lineages를 시각화.\npar(mfrow = c(1,2))\nplot(\n     dimred[, 1:2],\n     col = clustering,\n     cex = ,\n     pch = 16\n)\n여기서 우리는 경로 분석의 한 가지 중요한 문제를 살펴볼 수 있습니다: 경로 분석이 어디에서 시작되는지를 알 수 없다는 점입니다. 추가 정보가 없이는 이를 파악하기가 거의 불가능합니다. 경로 분석의 시작점과 끝점을 정의하기 위해서는 사전 생물학적 정보가 필요합니다.\n# Cluster 5에서 Slingshot을 시작하기.\nlineages &lt;- getLineages(\n    data = dimred,\n    clusterLabels = clustering,\n    start.clus = \"5\"\n)\n\nlineages\n\n\n17.1.2 주요 경로 정의하기\n시작 클러스터를 정한 뒤에는 Slingshot 을 사용하여 경로 분석을 수행할 수 있습니다. Slingshot 의 알고리즘은 초기 경로를 반복적으로 수정하여 데이터 포인트와 더 잘 일치되도록 합니다. 구체적으로 Slingshot 은 두 가지 기능을 사용합니다.\n\n각 “계통”마다 주요 경로를 찾습니다: 여기서 “계통”이란 특정 시작 클러스터에서 특정 종단 클러스터로 이어지는 클러스터 집합을 의미합니다. 즉, 각 계통은 세포들이 어떤 경로를 따라 이동하는지를 나타내며, 이를 통해 세포의 발달 과정을 이해할 수 있습니다.\n동일한 클러스터 집합을 가진 계통은 주요 곡선이 겹치는 클러스터 주위에 묶이도록 제약됩니다: 이는 서로 연결된 세포 그룹이 같은 경로를 공유하도록 하여, 분석의 일관성을 높이는 역할을 합니다. 이렇게 하면 각 계통이 서로 다른 경로를 따라 이동하더라도, 동일한 클러스터 내에서의 관계를 유지할 수 있습니다.\n\nSlingshot 의 getCurves() 함수는 실행하는 데 시간이 오래 걸리므로 각 계통에서 사용할 셀의 수를 줄여 곡선 적합 프로세스의 속도를 높일 수 있습니다. 이상적으로는 모든 셀을 사용하는 것이 좋지만 여기서는 approx_points 를 300 으로 설정하여 속도를 높였습니다.\n# 곡선 데이터 가져오기\ncurves &lt;- getCurves(\n    lineages,\n    approx_points = 300,\n    thresh = 0.01,\n    stretch = NULL,  # stretch 인수에 NULL 설정 (예시)\n    allow.breaks = FALSE,\n    shrink = NULL    # shrink 인수에 NULL 설정 (예시)\n)\n\n# 곡선 데이터 확인\ncurves\n\n# 차원 축소 결과 플롯\nplot(\n    dimred,\n    col = clustering,\n    asp = 1,\n    pch = 16\n)\n\n# 곡선 추가\nlines(\n    curves,\n    lwd = 3,\n    col = 'black'\n)"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#경로-분석를-따라-변하는-유전자를-찾기-differential-expression-along-trajectories",
    "href": "posts/md/Cheatsheet_Seurat.html#경로-분석를-따라-변하는-유전자를-찾기-differential-expression-along-trajectories",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "17.2 경로 분석를 따라 변하는 유전자를 찾기 (Differential expression along trajectories)",
    "text": "17.2 경로 분석를 따라 변하는 유전자를 찾기 (Differential expression along trajectories)\n경로 분석을 해석하는 주요 방법 중 하나는 경로를 따라 변하는 유전자를 찾는 것입니다. 경로 분석에서 차별적 발현 (differential expression) 을 정의하는 방법은 여러 가지가 있습니다:\n\n특정 경로를 따라 발현 변화: 예를 들어, 세포가 유사한 시간에 따라 발현이 어떻게 변화하는지를 분석합니다.\n가지 사이의 발현 차이: 서로 다른 경로 (또는 가지) 간의 유전자 발현 차이를 비교합니다.\n분기점에서의 발현 변화: 세포가 분기점에 도달했을 때 발현이 어떻게 변화하는지를 살펴봅니다.\n경로 분석의 어느 지점에서든 발현 변화: 경로의 모든 지점에서 유전자 발현의 변화를 평가합니다.\n\ntradeSeq 는 경로 분석 결과에서 차별적 발현 유전자를 찾는 도구 중 하나입니다. 이 도구는 일반화된 가법 모형 (Generalized Additive Models, GAMs) 을 사용하여 경로를 따라 유전자 발현을 조정하고 경로의 여러 지점 사이에서 특정 계수가 통계적으로 다른지를 테스트합니다. GAM 모델을 적합시키는 과정은 시간이 많이 걸립니다. 따라서 여기에서는 매우 엄격한 필터링을 통해 많은 유전자를 제거하였습니다. 실제로는 모든 유전자 데이터를 사용하는 것이 좋습니다.\n# 추가 패키지 로드\nBiocParallel::register(BiocParallel::SerialParam())\nlibrary(tradeSeq)\n\n# 연산 속도를 높이기 위해 일부 유전자 제거\ncounts &lt;- as.matrix(SeuratObject[[\"RNA\"]]$counts)\ndim(counts)  # 원본 카운트 행렬 차원 확인\n\n# 필터링된 카운트 행렬 생성\nfilt_counts &lt;- counts[rowSums(counts &gt; 5) &gt; ncol(counts) / 100, ]\ndim(filt_counts)  # 필터링 후 카운트 행렬 차원 확인\n\n# 감마 분포 fitting\nsce &lt;- fitGAM(\n    counts = as.matrix(filt_counts),\n    sds = curves\n)\n\n# 결과 플롯\nplotGeneCount(\n    curve = curves,\n    counts = filt_counts,\n    clusters = clustering,\n    models = sce\n)\n\n17.2.1 두 시간점 사이에서 변하는 유전자\n원하는 경우 사용자 정의 유사 시간 (Pseudotime) 값을 설정하여 두 시간점 사이에서 변하는 유전자를 찾을 수 있습니다. 예를 들어, 전구 세포 표시자와 같은 특정 유전자를 분석할 수 있습니다. 기본적으로는 시작점과 끝점 사이의 차이를 살펴보는 방식으로 진행됩니다.\n# Pseudotime 시작과 끝 간의 연관성 테스트\npseudotime_start_end_association &lt;- startVsEndTest(\n    sce,\n    pseudotimeValues = c(0, 1)\n)\n\n# feature_id 추가\npseudotime_start_end_association$feature_id &lt;- rownames(pseudotime_start_end_association)\n\n# p-value가 0.05 미만인 feature_id 필터링 및 선택\nfeature_id &lt;- pseudotime_start_end_association %&gt;%\n    filter(pvalue &lt; 0.05) %&gt;%\n    top_n(1, waldStat) %&gt;%\n    pull(feature_id)\n\n# 차별 발현 플롯 그리기\nplot_differential_expression(feature_id)\n\n\n17.2.2 경로가 갈라지는 사이에서 변하는 유전자\n두 가지 사이에서 차별적으로 발현되는 유전자는 특히 흥미로운 연구 대상입니다. 이러한 유전자 중 일부는 이전의 두 시간점 사이에서 변하는 유전자 분석에서 이미 발견되었을 수 있습니다. “경로가 갈라지는 사이에서 변하는 유전자”를 정의하는 여러 가지 방법이 있으며 방법에 따라 함수가 다릅니다.17\n\ndiffEndTest: 각 가지의 끝점에서 발현 차이를 분석합니다.\nearlyDETest: 가지의 분기점에서 발현 차이를 평가합니다.\npatternTest: 경로 분석의 어느 지점에서든 발현 차이를 탐색합니다. 이 마지막 함수는 두 가지 간의 유사 시간이 정렬되어 있어야 함을 요구합니다.\n\n# 차별적 끝 연관성 테스트 수행\ndifferent_end_association &lt;- diffEndTest(sce)\ndifferent_end_association$feature_id &lt;- rownames(different_end_association)\n\n# p-value가 0.05 미만인 feature_id 필터링 및 정렬\nfeature_id &lt;- different_end_association %&gt;%\n    filter(pvalue &lt; 0.05) %&gt;%\n    arrange(desc(waldStat)) %&gt;%\n    dplyr::slice(1) %&gt;%\n    pull(feature_id)\n\n# 차별 발현 플롯 그리기\nplot_differential_expression(feature_id)\n\n# 가지점 연관성 테스트 수행\nbranch_point_association &lt;- earlyDETest(sce)\nbranch_point_association$feature_id &lt;- rownames(branch_point_association)\n\n# p-value가 0.05 미만인 feature_id 필터링 및 정렬\nfeature_id &lt;- branch_point_association %&gt;%\n    filter(pvalue &lt; 0.05) %&gt;%\n    arrange(desc(waldStat)) %&gt;%\n    dplyr::slice(1) %&gt;%\n    pull(feature_id)\n\n# 차별 발현 플롯 그리기\nplot_differential_expression(feature_id)\n더 자세한 것은 공식 문서 를 확인하십시오."
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#온라인-강의",
    "href": "posts/md/Cheatsheet_Seurat.html#온라인-강의",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "18.1 온라인 강의",
    "text": "18.1 온라인 강의\n\nExercises for the NBIS scRNA-seq course\nThe GitHub repository for the NBIS scRNA-seq course\nSingle cell RNA-seq course at from Hemberg lab\nSingle cell RNA-seq course in Python\nSingle cell RNA-seq course at Broad"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#프레젠테이션-및-강의",
    "href": "posts/md/Cheatsheet_Seurat.html#프레젠테이션-및-강의",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "18.2 프레젠테이션 및 강의",
    "text": "18.2 프레젠테이션 및 강의\n\nSingle-cell methodologies\nQuaility controls\nNormalisation\nDimensionality reduction\nBatch correction and data integration\nClustering techniques\nDifferential expression analysis\nTrajectory inference analyses\nAll of the above in a YouTube playlist"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#도구-및-파이프라인",
    "href": "posts/md/Cheatsheet_Seurat.html#도구-및-파이프라인",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "18.3 도구 및 파이프라인",
    "text": "18.3 도구 및 파이프라인\n\nRepository listing many scRNA-seq tools\nBitbucket repository for QC report scripts\nBitbucket repository for an NBIS scRNA-seq pipeline\nA catalogue of many scRNA-seq tools"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#웹사이트",
    "href": "posts/md/Cheatsheet_Seurat.html#웹사이트",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "18.4 웹사이트",
    "text": "18.4 웹사이트\n\nSingleCellExperiment objects for many datasets\nConquer datasets - many different datasets based on a salmon pipeline\nThe Human Cell Atlas project\nThe EBI Single-cell expression atlas"
  },
  {
    "objectID": "posts/md/Cheatsheet_Seurat.html#footnotes",
    "href": "posts/md/Cheatsheet_Seurat.html#footnotes",
    "title": "Seurat 라이브러리 치트시트(Cheatsheet)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.pnas.org/content/116/20/9775)↩︎\nhttps://distill.pub/2016/misread-tsne, http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf↩︎\nhttps://umap-learn.readthedocs.io/en/latest↩︎\nhttps://arxiv.org/abs/1802.03426↩︎\nhttps://projecteuclid.org/euclid.ajm/1253539864↩︎\nhttps://en.wikipedia.org/wiki/Independent_component_analysis↩︎\nhttps://www.nature.com/articles/nbt.4091↩︎\nhttps://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1850-9↩︎\nhttps://www.nature.com/articles/s41587-019-0113-3↩︎\nhttps://www.nature.com/articles/s41592-019-0619-00↩︎\nhttps://f1000research.com/articles/7-1141↩︎\nhttps://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008/pdf↩︎\nhttps://www.nature.com/articles/s41598-019-41695-z.pdf↩︎\nhttps://www.nature.com/articles/nmeth.4612↩︎\nMcDavid et al., Bioinformatics, 2013↩︎\nhttps://www.nature.com/articles/s41587-019-0071-9↩︎\nCannoodt, Robrecht, Wouter Saelens, and Yvan Saeys. 2016.”Computational Methods for Trajectory Inference from Single-Cell Transcriptomics.”European Journal of Immunology 46 (11): 2496–2506.↩︎"
  },
  {
    "objectID": "posts/md/Install_UV.html",
    "href": "posts/md/Install_UV.html",
    "title": "pip 대신에 uv 사용하기",
    "section": "",
    "text": "pip 는 파이썬 패키지 관리자로 유용하지만 아래와 같은 몇 가지 근본적인 문제가 있습니다. 특히 패키지 의존성과 환경 관리는 문제가 터지면 해결하기가 아주 어렵습니다."
  },
  {
    "objectID": "posts/md/Install_UV.html#가상-환경-만들기",
    "href": "posts/md/Install_UV.html#가상-환경-만들기",
    "title": "pip 대신에 uv 사용하기",
    "section": "3.1 가상 환경 만들기",
    "text": "3.1 가상 환경 만들기\nuv venv  # `.venv`에 가상 환경을 만듭니다."
  },
  {
    "objectID": "posts/md/Install_UV.html#가상-환경-활성화",
    "href": "posts/md/Install_UV.html#가상-환경-활성화",
    "title": "pip 대신에 uv 사용하기",
    "section": "3.2 가상 환경 활성화",
    "text": "3.2 가상 환경 활성화\n# On macOS and Linux.\nsource .venv/bin/activate\n\n# On Windows.\n.venv\\Scripts\\activate"
  },
  {
    "objectID": "posts/md/Install_UV.html#가상-환경에-패키지-설치",
    "href": "posts/md/Install_UV.html#가상-환경에-패키지-설치",
    "title": "pip 대신에 uv 사용하기",
    "section": "3.3 가상 환경에 패키지 설치",
    "text": "3.3 가상 환경에 패키지 설치\nuv pip install flask                # Flask 설치.\nuv pip install flask==2.0.0 # 특정 버전의 Flask 설치.\nuv pip install -r requirements.txt  # requirements.txt 파일을 통한 패키지 설치.\nuv pip install -e .                 # 현재 프로젝트를 편집 가능한 모드로 설치.\nuv pip install \"package @ .\"        # 디스크에 저장된 패키지를 현재 프로젝트를 설치.\nuv pip install \"flask[dotenv]\"      # dotenv 의존성이 추가된 Flask를 설치.\n의존성을 고정한 requirements.txt 파일을 생성합니다:\nuv pip compile requirements.in -o requirements.txt    # requirements.in 파일을 읽고 생성.\nuv pip compile pyproject.toml -o requirements.txt     # pyproject.toml 파일을 읽고 생성.\nuv pip compile setup.py -o requirements.txt           # setup.py 파일을 읽고 생성.\necho flask | uv pip compile - -o requirements.txt     # stdin을 읽고 생성.\nuv pip freeze | uv pip compile - -o requirements.txt  # 현재 환경의 의존성을 고정.\n고정한 의존성 파일을 가상 환경과 동기화합니다:\nuv pip sync requirements.txt  # requirements.txt 파일에서 설치합니다.\nuv 의 pip-install 및 pip-compile 명령은 -r requirements.txt, -c constraints.txt, -e .(편집 가능한 설치용), --index-url 등 기존 도구와 동일한 커맨드를 지원합니다."
  },
  {
    "objectID": "posts/md/Install_devEnv.html#pip-virtualenv",
    "href": "posts/md/Install_devEnv.html#pip-virtualenv",
    "title": "파이썬과 R 개발 환경 설정하기",
    "section": "2.1 pip + virtualenv",
    "text": "2.1 pip + virtualenv\npip와 virtualenv는 파이썬 개발 환경에서 가장 오래 사용되었고 또한 자주 사용되는 도구입니다. pip는 파이썬에 기본적으로 포함되어 있는 패키지 관리자로 PyPI(Python Package Index) 에 등록된 패키지를 설치하고 관리할 수 있습니다. virtualenv 는 가상 환경을 생성하여 프로젝트마다 독립된 파이썬 환경을 가질 수 있게 해주는 패키지입니다. 다른 프로젝트와 충돌 없이 패키지를 관리하려면 필수적으로 사용해야 합니다.\n\n\n\n\n\n\nWarning\n\n\n\npip 는 아래와 같은 몇 가지 근본적인 문제를 가지고 있습니다.\n\n\n\n의존성 충돌 (Dependency Conflicts) 여러 패키지를 설치할 때 각 패키지가 서로 다른 버전의 동일한 종속성을 요구할 경우 의존성 충돌이 발생하며 프로젝트에서 예기치 않은 동작이나 오류를 발생시킬 수 있습니다.\n글로벌 설치 문제 기본적으로 pip 는 전역 파이썬 환경에 패키지를 설치합니다. 이로 인해, 여러 프로젝트에서 동일한 패키지 버전을 공유하게 되어 프로젝트 간에 종속성 충돌이 발생할 수 있습니다. 특히, 프로젝트마다 요구되는 패키지 버전이 다른 경우에 문제가 됩니다.\n환경 관리의 어려움 pip 는 파이썬 환경을 별도로 관리하지 않으므로 전역 환경에 설치된 패키지들이 서로 영향을 미칠 수 있습니다. 이로 인해, 동일한 시스템에서 여러 프로젝트를 작업할 때 각 프로젝트의 요구 사항을 일관되게 유지하기 어려워집니다. 이를 해결하기 위해 virtualenv 와 같은 도구를 사용해 가상 환경을 생성해야 합니다.\n보안 문제 pip 는 기본적으로 모든 패키지를 신뢰합니다. 악의적인 사용자가 배포한 패키지가 PyPI 에 업로드되면, 이를 설치하는 과정에서 보안 문제가 발생할 수 있습니다.\n복잡한 패키지 관리 복잡한 프로젝트에서는 수많은 패키지와 버전들을 관리해야 하는데, pip 는 기본적으로 이러한 복잡한 종속성 트리를 관리하는 데 최적화되어 있지 않습니다. 이는 특정 패키지 설치 시 종속성 문제가 발생할 때 수동으로 해결해야 하는 번거로움으로 이어질 수 있습니다. 이러한 문제들로 인해, 많은 개발자들이 pipenv 또는 poetry 같은 도구들을 사용하여 환경과 패키지 관리를 하고 있습니다.\n\npip 는 파이썬에 기본적으로 설치되어 있어 상관이 없지만, virtualenv 는 pip 를 사용해 설치할 수 있습니다. 다음 단계를 따라 설치하면 됩니다. 먼저 터미널 (또는 명령 프롬프트) 을 열고 다음 명령어를 입력합니다. 이 명령어는 최신 버전의 virtualenv 를 설치합니다.\npip install virtualenv\nvirtualenv 가 올바르게 설치되었는지 확인하려면, 다음 명령어를 입력하여 버전을 확인할 수 있습니다.\nvirtualenv --version\n버전 번호가 출력되면 virtualenv 가 성공적으로 설치된 것입니다.\n\n2.1.1 가상환경\n기본적으로 pip 는 전역 (global) 파이썬 환경에 패키지를 설치하기 때문에 필연적으로 의존성 문제가 발생합니다. 그러나 virtualenv 를 사용하면 각 프로젝트마다 별도의 가상 환경을 만들어 서로 다른 패키지 버전이나 설정을 유지할 수 있습니다. 이는 프로젝트 간의 종속성 충돌을 방지하고, 개발 환경을 안정적으로 유지하는 데 도움이 됩니다.\n\n2.1.1.1 virtualenv 주요 커맨드\n\nvirtualenv 설치: bash pip install virtualenv\n가상 환경 생성: bash python -m virtualenv 가상환경명 가상환경명 은 원하는 이름으로 지정할 수 있습니다. 이 명령어는 가상환경명 이라는 폴더를 생성하고, 그 안에 독립된 파이썬 실행 파일과 pip 를 포함한 환경을 구성합니다.\n가상 환경 활성화:\n\nWindows: bash 가상환경명\\Scripts\\activate\nMac/Linux: bash source 가상환경명/bin/activate 가상 환경이 활성화되면, 터미널 프롬프트에 (가상환경명) 이 표시됩니다. 이 상태에서 pip 로 설치된 패키지는 전역 환경이 아닌 해당 가상 환경에 설치됩니다.\n\n가상 환경 비활성화: bash deactivate 가상 환경에서 벗어나기 위해 이 명령어를 사용합니다.\n가상 환경 내 패키지 관리:\n\n패키지 설치: bash pip install 패키지명 활성화된 가상 환경에 패키지를 설치합니다.\n설치된 패키지 목록 저장 (dependencies list): bash pip freeze &gt; requirements.txt 현재 가상 환경에 설치된 모든 패키지의 목록을 requirements.txt 파일로 저장합니다.\n목록에 따른 패키지 설치: bash pip install -r requirements.txt requirements.txt 파일에 기록된 패키지들을 한 번에 설치합니다.\n\n\n\n\n\n2.1.2 의존성 관리하기\npip 는 다음과 같은 기능을 지원합니다:\n\n패키지 설치: 파이썬 패키지를 설치합니다.\n패키지 업그레이드: 기존에 설치된 패키지를 최신 버전으로 업그레이드합니다.\n패키지 제거: 더 이상 필요하지 않은 패키지를 삭제합니다.\n패키지 목록 보기: 현재 설치된 패키지의 목록을 볼 수 있습니다.\n\n\n2.1.2.1 주요 pip 커맨드\n\n패키지 설치: bash pip install 패키지명\n패키지 업그레이드: bash pip install --upgrade 패키지명\n패키지 제거: bash pip uninstall 패키지명\n설치된 패키지 목록 보기: bash pip list\n패키지의 특정 버전 설치: bash pip install 패키지명==버전\n\n\n\n\n\n\n\nTip\n\n\n\n가상환경과 라이브러리 관리 팁 1. 프로젝트마다 가상 환경 생성: 프로젝트를 시작할 때마다 새로운 가상 환경을 생성하여, 프로젝트 간에 라이브러리 충돌을 방지하는 것이 좋습니다. 2. requirements.txt 활용: 프로젝트의 의존성을 공유하거나 배포할 때 requirements.txt 파일을 사용하면, 다른 사람이 동일한 환경을 쉽게 설정할 수 있습니다. 3. 가상 환경 폴더 관리: virtualenv 로 생성된 가상 환경 폴더는 일반적으로 프로젝트 폴더 내에 두거나, 중앙 관리 폴더를 만들어 관리할 수 있습니다."
  },
  {
    "objectID": "posts/md/Install_devEnv.html#conda",
    "href": "posts/md/Install_devEnv.html#conda",
    "title": "파이썬과 R 개발 환경 설정하기",
    "section": "2.2 Conda",
    "text": "2.2 Conda\nconda 는 패키지 매니저로 가상환경을 만들고 파이썬 패키지의 의존성을 관리하고, 설치, 삭제, 업데이트 등을 할 수 있습니다. conda 는 기본적으로 Anaconda 라는 파이썬 기반의 데이터 분석에 특화된 각종 패키지를 모아놓은 플랫폼에서 개발되었습니다. 다시말해 Anaconda 를 설치하면 자동으로 conda 도 설치가 됩니다.\n만약 Anaconda 에서 패키지 관리 메니저인 conda 만을 설치 하고 싶다면 miniconda 를 설치 하면 됩니다. 저는 개인적으로 Miniconda 를 사용하는 것을 추천합니다. Anaconda 는 라이센스 문제와 설치 용량이 크다는 단점이 있기 때문입니다.\n\n2.2.1 Conda 사용법\n\n2.2.1.1 가상환경\n새로운 프로젝트를 시작할때면 항상 새로운 가상환경을 만들어 작업하는것이 좋습니다. 새로운 가상환경 ENV_NAME 을 만드는 예시를 들어보겠습니다.\n\n터미널에 다음과 같이 입력해 새로운 가상환경을 만듭니다.\n\nconda env create -n ENV_NAME\nconda env create -n ENV_NAME -f requirements.txt # 의존성 파일로 부터 가상환경을 만들때 사용하는 코드\n\n가상환경을 사용하기 위해서는 활성화 (activate) 시켜줘야 합니다. 활성화되면 터미널에 (ENV_NAME) 이 표시됩니다.\n\nconda activate ENV_NAME\n\n만약 가상환경을 종료하고 싶다면 비활성화 (deactivate) 하면 됩니다.\n\nconda deactivate\n\n이미 생성한 가상환경의 목록은 다음과 같이 확인 할 수 있습니다.\n\nconda env list\n\n생성한 가상환경을 제거하는 명령어는 다음과 같습니다.\n\nconda env remove -n ENV_NAME\n\n\n2.2.1.2 의존성 관리하기\n자주 사용하는 conda 명령어는 아래와 같습니다. 예시로 numpy 를 설치하는 상황을 가정합니다.\n\n\n\n기능\n명령어\n\n\n\n\n패키지 설치\nconda install numpy\n\n\n동시에 여러 패키지를 설치\nconda install numpy scipy pandas\n\n\n특정 버전 설치\nconda install “numpy=1.10”\n\n\n패키지 제거\nconda remove numpy\n\n\n패키지 업데이트\nconda update numpy\n\n\n모든 패키지 업데이트\nconda update –all\n\n\n설치된 목록 출력\nconda list\n\n\n설치하려는 패키지 검색\nconda search numpy\n\n\n\n만약 Conda 에서 패키지를 찾을 수 없다면 pip 을 사용해보세요\n\n\n\n2.2.2 Conda 제거하기\n만약 conda 를 제거하고 싶다면 다음과 같이 하면 됩니다.\n\nWindowsLinux, MacOS\n\n\n제어판에 들어가서 Python(Anaconda) 혹은 Python(Miniconda) 를 제거합니다.\n\n\n단순하게 아래 디렉토리만 삭제하면 됩니다.\nrm -rf ~/miniconda\nrm -rf ~/anaconda"
  },
  {
    "objectID": "posts/md/Install_devEnv.html#rstudio-설치",
    "href": "posts/md/Install_devEnv.html#rstudio-설치",
    "title": "파이썬과 R 개발 환경 설정하기",
    "section": "3.1 RStudio 설치",
    "text": "3.1 RStudio 설치\nR 의 기본 콘솔 외에 통합 개발 환경 (IDE) 를 사용하고 싶다면 RStudio 를 설치할 수 있습니다. 다만 개인용도로만 무료임으로 라이센스를 항상 확인하세요.\n\nRStudio 다운로드:\n\nRStudio 공식 웹사이트 에서 운영 체제에 맞는 설치 파일을 다운로드합니다.\n\nRStudio 설치:\n\n다운로드한 파일을 실행하여 설치를 진행합니다. 설치 과정은 간단하며, 기본값으로 진행하면 됩니다.\n\nRStudio 실행:\n\n설치가 완료되면, RStudio 를 실행하여 R 환경을 더욱 효율적으로 사용할 수 있습니다."
  },
  {
    "objectID": "posts/md/Install_devEnv.html#가상환경-2",
    "href": "posts/md/Install_devEnv.html#가상환경-2",
    "title": "파이썬과 R 개발 환경 설정하기",
    "section": "4.1 가상환경",
    "text": "4.1 가상환경\n\n4.1.1 renv\nrenv 는 R 의 가상환경 관리 도구로, Python 의 virtualenv 나 conda 와 유사한 역할을 합니다. renv 는 각 프로젝트마다 독립된 라이브러리 환경을 설정하여, 서로 다른 프로젝트에서 사용하는 패키지가 충돌하지 않도록 도와줍니다.\nrenv 의 주요 기능\n\n프로젝트별로 패키지 라이브러리 폴더를 생성하여, 각 프로젝트마다 고유한 환경을 유지.\nrenv.lock 파일을 사용하여 프로젝트에 필요한 패키지 버전과 의존성을 기록.\nrenv 를 사용하면 특정 시점의 패키지 상태를 다른 시스템에서 동일하게 재현 가능.\n\n\nrenv 설치:\ninstall.packages(\"renv\")\n프로젝트 초기화: 프로젝트 디렉토리에서 renv 를 초기화합니다. 이는 새로운 가상환경을 생성하고, renv.lock 파일을 생성합니다.\nrenv::init()\n패키지 설치: 일반적인 방법으로 패키지를 설치하면 해당 패키지는 현재 renv 환경에 설치됩니다.\ninstall.packages(\"ggplot2\")\n환경 스냅샷 생성: 프로젝트의 현재 상태를 기록하여 renv.lock 파일에 저장합니다.\nrenv::snapshot()\n환경 복원: 다른 시스템에서 동일한 환경을 재현하기 위해, renv.lock 파일을 사용하여 패키지 환경을 복원합니다.\nrenv::restore()\n가상환경 해제: 프로젝트의 가상환경을 해제하고 전역 라이브러리를 사용하도록 변경합니다.\nrenv::deactivate()"
  },
  {
    "objectID": "posts/md/Install_devEnv.html#의존성-관리하기-2",
    "href": "posts/md/Install_devEnv.html#의존성-관리하기-2",
    "title": "파이썬과 R 개발 환경 설정하기",
    "section": "4.2 의존성 관리하기",
    "text": "4.2 의존성 관리하기\n\n4.2.1 CRAN\n대부분의 R 패키지는 CRAN(Comprehensive R Archive Network) 을 통해 설치 및 관리됩니다. CRAN 은 R 패키지의 공식 저장소로 일반적인 분석 작업에 필요한 대부분의 패키지를 제공합니다.\n\n4.2.1.1 주요 CRAN 커맨드\n\ninstall.packages(): 기본적으로 CRAN 에서 패키지를 설치하는 함수입니다.\nlibrary(): 패키지를 로드하는 함수입니다. 한 번 설치된 패키지는 프로젝트에서 사용할 때마다 library(패키지명) 을 통해 불러올 수 있습니다.\nupdate.packages(): 설치된 모든 패키지를 최신 버전으로 업데이트하는 함수입니다.\nremove.packages(): 설치된 패키지를 제거하는 함수입니다.\n\n\n\n\n4.2.2 Bioconductor: 생물정보학용 패키지 매니저\n생물정보학 연구에서 자주 사용되는 R 패키지는 Bioconductor 를 통해 관리됩니다. Bioconductor 는 CRAN 을 통해 설치해 사용할 수 있습니다. 주요 명령어는 아래와 같습니다.\n\nBiocManager 설치:\ninstall.packages(\"BiocManager\")\nBioconductor 패키지 설치:\nBiocManager::install(\"패키지명\")\nBioconductor 패키지 제거\n\n특정 Bioconductor 패키지를 제거하려면 remove.packages() 함수를 사용합니다:\nremove.packages(\"패키지명\")\n\n특정 Bioconductor 버전을 사용하고 싶다면 (예: 3.13):\n\nBiocManager::install(version = \"3.13\")\n\nBioconductor 패키지 검색\n\nBioconductor 에서 특정 패키지를 검색하려면:\nBiocManager::available(\"패키지명\")\n이러한 명령어들을 활용하면 Bioconductor 패키지를 효과적으로 관리하고, 필요한 생물정보학 도구들을 손쉽게 사용할 수 있습니다."
  },
  {
    "objectID": "posts/md/Debug_mindfullness.html",
    "href": "posts/md/Debug_mindfullness.html",
    "title": "마음챙김과 효율을 위한 생산성 디버깅",
    "section": "",
    "text": "우리 모두 경험하고 있습니다. 일을 하려고 자리에 앉았는데 갑자기 휴대폰이 울리는 거예요. 어느새 고양이 동영상과 나무 위키 문서를 보게 되죠. 그러고 나면 하루가 어떻게 지나갔는지 벌써 저녁입니다. 낯익은 이야기인가요?\n이 거친 세상에서 집중력을 유지하는 것은 마치 미친 고양이를 쫓아다니는 것 같아요. 하지만 걱정하지 마세요! 이 글에서는 그 미친 고양이를 츄르로 유인하는 지혜를 나누려고 합니다."
  },
  {
    "objectID": "posts/md/Debug_mindfullness.html#미루기-내일-할-거야-라는-함정",
    "href": "posts/md/Debug_mindfullness.html#미루기-내일-할-거야-라는-함정",
    "title": "마음챙김과 효율을 위한 생산성 디버깅",
    "section": "2.1 미루기: “내일 할 거야” 라는 함정",
    "text": "2.1 미루기: “내일 할 거야” 라는 함정\n미루는 것은 단순한 게으름이 아닙니다.(가끔 낮잠을 자는 것은 오히려 좋은 방법이지요). 미루는 것은 오히려 성인이 되었다는 것과 비슷합니다.3\n다음과 같은 경우가 미루기에 속합니다: - 쿠키 통에 손을 넣은 정치인보다 더 많은 핑계를 대는 경우. - 일을 하지 않는 것에 대한 죄책감이 실제로 일하는 것만큼이나 심할때. - 갑자기 책상 정리를 엄청하고 싶을 때. - 가만히 있는 데 마감일이 다가올때.\n왜 우리는 스스로에게 이런 짓을 할까요? 그럴 수도 있습니다:\n\n실수에 대한 두려움 (안녕, 완벽주의자, 나의 오랜 친구)\n소스에서 길을 잃은 느낌 (목표란 도대체 무엇인가요?)\n게으름뱅이를 효율적으로 보이게 하는 시간 관리 기술\n감정?"
  },
  {
    "objectID": "posts/md/Debug_mindfullness.html#번아웃-일과-삶의-균형이-깨졌을-때",
    "href": "posts/md/Debug_mindfullness.html#번아웃-일과-삶의-균형이-깨졌을-때",
    "title": "마음챙김과 효율을 위한 생산성 디버깅",
    "section": "2.2 번아웃: 일과 삶의 균형이 깨졌을 때",
    "text": "2.2 번아웃: 일과 삶의 균형이 깨졌을 때\n번아웃은 완전히 다른 차원의 문제입니다. 마치 뇌가 파업을 일으켜 어떤 협상도 받아들이지 않는 것과 같습니다. 다음과 같은 경우 번아웃 상태일 수 있습니다:\n\n할 일 목록을 생각만 해도 자고 싶을 때\n재미있는 일조차 집안일처럼 느껴질때 (넷플릭스를 보며 멍하니 벽만 쳐다보는 게 더 낫겠죠?).\n몸이 SOS 신호를 보내는 경우 (불면증 혹은 불안증세)\n\n번아웃의 결과는 아래와 비슷할 겁니다:\n\n끝이 보이지 않는 업무 스트레스\n줄에 매달린 꼭두각시 같은 느낌\n진흙탕같이 불명확한 업무 지시\n일과 삶의 불균형\n\n나의 생산성이 어떤 것에 의해 저하되는 지를 알아야 해결이 가능합니다. 이제 어떻게 극복할 수 있는지 알아보죠."
  },
  {
    "objectID": "posts/md/Debug_mindfullness.html#footnotes",
    "href": "posts/md/Debug_mindfullness.html#footnotes",
    "title": "마음챙김과 효율을 위한 생산성 디버깅",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://dev.to/benborla/debugging-your-productivity-a-guide-to-mindfulness-and-efficiency-4le1↩︎\nhttps://jamesclear.com/atomic-habits↩︎\nhttps://www.goodreads.com/book/show/6708.The_Power_of_Now↩︎\nhttps://www.amazon.com/Discipline-Equals-Freedom-Field-Manual/dp/1250156947↩︎"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html",
    "href": "posts/ipynb/R_ggpubr.html",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "",
    "text": "모든 내용은 공식문서에서 간추린 것입니다. 자세한것은 공식문서를 읽어주세요.\nggpubr은 ggplot2에 기반한 R 패키지입니다. 연구자들이 쉽게 높은 질의 도표를 그리는 것을 목표로 하고 있는 시각화 패키지죠. 주요 특징은 다음과 같습니다:\n\nggplot2 패키지를 기반으로해서 좀 더 명확한 문법으로 보다 쉽게 사용할 수 있습니다.\nR 언어를 잘 모르더라도 높은 질의 도표를 만들수 있습니다.\n자동으로 p-values 나 통계적 유의성을 표시할 할 수 있습니다.\n여러 도표를 한 페이지에 배열 할 수 있는 기능을 가지고 있습니다.\n레이블이나 색상을 쉽게 변경할 수 있습니다.\n\n먼저 ggpubr 로 시각화를 하는 간단한 방법을 살펴보고, 이후에 다양한 예시 도표를 보여드리겠습니다."
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#데이터-불러오기",
    "href": "posts/ipynb/R_ggpubr.html#데이터-불러오기",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "4.1 3.1. 데이터 불러오기",
    "text": "4.1 3.1. 데이터 불러오기\n\n# 필요한  패키지 불러오기\n# suppressPackageStartupMessages(library(tidyverse))\nlibrary(\"dplyr\") \nlibrary(\"ggpubr\")\noptions(warn=-1) # 경고메세지 무시하기\n\ndata(\"ToothGrowth\") # 예제 데이터 불러오기\nhead(ToothGrowth,4) # 데이터 테이블 확인\n\n\n\n\nlen\nsupp\ndose\n\n\n\n\n4.2\nVC\n0.5\n\n\n11.5\nVC\n0.5\n\n\n7.3\nVC\n0.5\n\n\n5.8\nVC\n0.5"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#시각화-설정하기",
    "href": "posts/ipynb/R_ggpubr.html#시각화-설정하기",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "4.2 3.2. 시각화 설정하기",
    "text": "4.2 3.2. 시각화 설정하기\n\noptions(repr.plot.width = 6, repr.plot.height = 4)\n\nggline(ToothGrowth, x = \"dose\", y = \"len\", add = \"mean_se\", # 각각의 축설정 \n      color = \"supp\", palette = \"npg\")+  # 색상 설정하기\n      stat_compare_means(aes(group = supp), label = \"p.signif\", label.y = c(16, 25, 29)) + # 통계적 유의성 표시\n      labs(list(x = 'Dose', y = 'Length', fill = 'Supp')) # 레이블 변경"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#한페이지에-여러-도표-넣기",
    "href": "posts/ipynb/R_ggpubr.html#한페이지에-여러-도표-넣기",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "4.3 3.3. 한페이지에 여러 도표 넣기",
    "text": "4.3 3.3. 한페이지에 여러 도표 넣기\n여러 도표를 한페이지에 넣는 기능은 ggarrange()입니다. cowplot의 plot_grid()함수에 기반하고 있죠. 그래서 사용법도 동일합니다. 아래의 예시 코드를 확인하세요.\nggarrange(a, b, c ,  \n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#분포distribution-시각화",
    "href": "posts/ipynb/R_ggpubr.html#분포distribution-시각화",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "5.1 4.1. 분포(Distribution) 시각화",
    "text": "5.1 4.1. 분포(Distribution) 시각화\n\n# 예제 데이터 만들기\nset.seed(1234)\nwdata = data.frame(\n   sex = factor(rep(c(\"F\", \"M\"), each=200)),\n   weight = c(rnorm(200, 55), rnorm(200, 58)))\nhead(wdata, 4)\n\n\n\n\nsex\nweight\n\n\n\n\nF\n53.79293\n\n\nF\n55.27743\n\n\nF\n56.08444\n\n\nF\n52.65430\n\n\n\n\n\n\na1 &lt;- ggdensity(wdata, x = \"weight\",\n   add = \"mean\", rug = TRUE, # Density plot with mean lines and marginal rug\n   color = \"sex\", fill = \"sex\",  # Change outline and fill colors by groups (\"sex\")\n   palette = c(\"#00AFBB\", \"#E7B800\")) # Use custom palette\n\na2 &lt;- gghistogram(wdata, x = \"weight\",\n   add = \"mean\", rug = TRUE,\n   color = \"sex\", fill = \"sex\",\n   palette = c(\"#00AFBB\", \"#E7B800\"))\n\na3 &lt;- ggdensity(wdata, x = \"weight\",\n   add = \"mean\", rug = TRUE,\n   fill = \"lightgray\")\n\n# Combine histogram and density plots\na4 &lt;-  gghistogram(wdata, x = \"weight\",\n   add = \"mean\", rug = FALSE,\n   fill = \"sex\", palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\n\n# 한페이지에 넣기\nggarrange(a1, a2, a3 , a4,\n          labels = c(\"A\", \"B\", \"C\", \"D\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#막대-그래프bar-plots",
    "href": "posts/ipynb/R_ggpubr.html#막대-그래프bar-plots",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "6.1 4.3. 막대 그래프(Bar plots)",
    "text": "6.1 4.3. 막대 그래프(Bar plots)\n\n6.1.1 4.3.1 간단한 막대 그래프\n\n# example Data\ndf &lt;- data.frame(dose=c(\"D0.5\", \"D1\", \"D2\"),\n   len=c(4.2, 10, 29.5))\ndf2 &lt;- data.frame(supp=rep(c(\"VC\", \"OJ\"), each=3),\n   dose=rep(c(\"D0.5\", \"D1\", \"D2\"),2),\n   len=c(6.8, 15, 33, 4.2, 10, 29.5))\ndf3 &lt;- ToothGrowth\n\n# Change position: Interleaved (dodged) bar plot\np1 &lt;- ggbarplot(df2, \"dose\", \"len\",\n        fill = \"supp\", color = \"supp\", palette = \"Paired\",\n        position = position_dodge(0.8))\n\n# Change fill and outline color add labels inside bars\np2 &lt;- ggbarplot(df, \"dose\", \"len\",\n        fill = \"dose\", color = \"dose\",\n        palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n        label = TRUE, lab.pos = \"in\", lab.col = \"white\")\n\n# Add jitter points and errors (mean_se)\np3 &lt;- ggbarplot(df3, x = \"dose\", y = \"len\",\n        add = c(\"mean_se\", \"jitter\"))\n\n# Multiple groups with error bars and jitter point\np4 &lt;- ggbarplot(df3, x = \"dose\", y = \"len\", color = \"supp\",\n         add = \"mean_se\", palette = c(\"#00AFBB\", \"#E7B800\"),\n         position = position_dodge(0.8))\n\nggarrange(p1, p2, p3, p4,\n          labels = c(\"A\", \"B\", \"C\", \"D\"),\n          ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.2 4.3.2 정돈된(Ordered) 바 그래프\ncyl에 따라서 그룹화하고, 전체적으로 정렬한 그래프(A)와 그룹별로 정렬한 그래프(B)의 시각화입니다.\n\n# 샘플 데이터 불러오기\ndata(\"mtcars\")\ndfm &lt;- mtcars\ndfm$cyl &lt;- as.factor(dfm$cyl) # Convert the cyl variable to a factor\ndfm$name &lt;- rownames(dfm) # Add the name colums\nhead(dfm[, c(\"name\", \"wt\", \"mpg\", \"cyl\")]) # 데이터 살펴보기\n\n\n\n\n\nname\nwt\nmpg\ncyl\n\n\n\n\nMazda RX4\nMazda RX4\n2.620\n21.0\n6\n\n\nMazda RX4 Wag\nMazda RX4 Wag\n2.875\n21.0\n6\n\n\nDatsun 710\nDatsun 710\n2.320\n22.8\n4\n\n\nHornet 4 Drive\nHornet 4 Drive\n3.215\n21.4\n6\n\n\nHornet Sportabout\nHornet Sportabout\n3.440\n18.7\n8\n\n\nValiant\nValiant\n3.460\n18.1\n6\n\n\n\n\n\n\na1 &lt;- ggbarplot(dfm, x = \"name\", y = \"mpg\",\n          fill = \"cyl\",               # change fill color by cyl\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"desc\",          # Sort the value in dscending order\n          sort.by.groups = FALSE,     # Don't sort inside each group\n          x.text.angle = 90)           # Rotate vertically x axis texts\n\na2 &lt;- ggbarplot(dfm, x = \"name\", y = \"mpg\",\n          fill = \"cyl\",               # change fill color by cyl\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"asc\",           # Sort the value in dscending order\n          sort.by.groups = TRUE,      # Sort inside each group\n          x.text.angle = 90)           # Rotate vertically x axis texts\n\nggarrange(a1, a2,\n          labels = c(\"A\", \"B\"),\n          ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.3 4.3.3. 편차(Deviation) 그래프\n편차(deviation) 그래프는 각각의 값들이 평균값 대비 얼마나 차이가 나는지를 시각화 합니다. 여기서는 연비 평균값에 비교해서 각 차량의 편차가 얼마인지 계산해(Z-score) 도표를 그려보겠습니다.\n\n# Calculate the z-score of the mpg data\ndfm$mpg_z &lt;- (dfm$mpg -mean(dfm$mpg))/sd(dfm$mpg)\ndfm$mpg_grp &lt;- factor(ifelse(dfm$mpg_z &lt; 0, \"low\", \"high\"), \n                     levels = c(\"low\", \"high\"))\n# Inspect the data\nhead(dfm[, c(\"name\", \"wt\", \"mpg\", \"mpg_z\", \"mpg_grp\", \"cyl\")])\n\n\n\n\n\nname\nwt\nmpg\nmpg_z\nmpg_grp\ncyl\n\n\n\n\nMazda RX4\nMazda RX4\n2.620\n21.0\n0.1508848\nhigh\n6\n\n\nMazda RX4 Wag\nMazda RX4 Wag\n2.875\n21.0\n0.1508848\nhigh\n6\n\n\nDatsun 710\nDatsun 710\n2.320\n22.8\n0.4495434\nhigh\n4\n\n\nHornet 4 Drive\nHornet 4 Drive\n3.215\n21.4\n0.2172534\nhigh\n6\n\n\nHornet Sportabout\nHornet Sportabout\n3.440\n18.7\n-0.2307345\nlow\n8\n\n\nValiant\nValiant\n3.460\n18.1\n-0.3302874\nlow\n6\n\n\n\n\n\n\n# Create an ordered bar plot, colored according to the level of mpg:\nggbarplot(dfm, x = \"name\", y = \"mpg_z\",\n          fill = \"mpg_grp\",           # change fill color by mpg_level\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"desc\",          # Sort the value in descending order\n          sort.by.groups = FALSE,     # Don't sort inside each group\n          x.text.angle = 90,          # Rotate vertically x axis texts\n          ylab = \"MPG z-score\",\n          legend.title = \"MPG Group\",\n          rotate = TRUE,\n          ggtheme = theme_minimal())"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#막대사탕lollipop-plot",
    "href": "posts/ipynb/R_ggpubr.html#막대사탕lollipop-plot",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "7.1 4.4.1 막대사탕(Lollipop) plot",
    "text": "7.1 4.4.1 막대사탕(Lollipop) plot\n막대사탕 그래프는 많은 양의 데이터를 시각화하는데 적합합니다. 아래 예시에서는 cyl 그룹에 맞춰서 색상을 구분하였습니다.\n\nggdotchart(dfm, x = \"name\", y = \"mpg\",\n           color = \"cyl\",                                # Color by groups\n           palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), # Custom color palette\n           sorting = \"descending\",                       # Sort value in descending order\n           add = \"segments\",                             # Add segments from y = 0 to dots\n           rotate = TRUE,                                # Rotate vertically\n           group = \"cyl\",                                # Order by groups\n           dot.size = 6,                                 # Large dot size\n           label = round(dfm$mpg),                        # Add mpg values as dot labels\n           font.label = list(color = \"white\", size = 9, \n           vjust = 0.5),               # Adjust label parameters\n           ggtheme = theme_pubr())                        # ggplot2 theme"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#도표에-설명figure-legend-넣기",
    "href": "posts/ipynb/R_ggpubr.html#도표에-설명figure-legend-넣기",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "7.2 4.5. 도표에 설명(figure legend) 넣기",
    "text": "7.2 4.5. 도표에 설명(figure legend) 넣기\n도표 밑에 설명을 넣는 방법입니다. 한줄단위로 내용을 끊어서 작성해야, 산출물에서 줄이 잘 맞게 할 수 있습니다. 아래의 예시 코드를 확인하세요.\nggparagraph(text, color = NULL, size = NULL, face = NULL, family = NULL,\n  lineheight = NULL)\n# S3 method for splitText\ndrawDetails(x, recording)\n\n# Density plot\ndensity.p &lt;- ggdensity(iris, x = \"Sepal.Length\",\n                      fill = \"Species\", palette = \"jco\")\n# Text plot\ntext &lt;- paste(\"Iris data set gives the measurements in cm\",\n             \"of the variables sepal length and width\",\n             \"and petal length and width, respectively,\",\n             \"for 50 flowers from each of 3 species of iris.\",\n             \"The species are Iris setosa, versicolor, and virginica.\", sep = \" \")\ntext.p &lt;- ggparagraph(text, face = \"italic\", size = 12)\n\n# Arrange the plots on the same page\nggarrange(density.p, text.p,\n         ncol = 1, nrow = 2,\n         heights = c(1, 0.3))"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#선-그래프",
    "href": "posts/ipynb/R_ggpubr.html#선-그래프",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "7.3 4.6. 선 그래프",
    "text": "7.3 4.6. 선 그래프\n\n# Data: ToothGrowth data set we'll be used.\ndf3 &lt;- ToothGrowth\n# Add error bars: mean_se\n# (other values include: mean_sd, mean_ci, median_iqr, ....)\n# Add labels\np1 &lt;- ggline(df3, x = \"dose\", y = \"len\", add = \"mean_se\")\n# Add jitter points and errors (mean_se)\np2 &lt;- ggline(df3, x = \"dose\", y = \"len\",\n add = c(\"mean_se\",'jitter'))\n# Multiple groups with error bars\np3 &lt;- ggline(df3, x = \"dose\", y = \"len\", color = \"supp\",\n add = \"mean_se\", palette = c(\"#00AFBB\", \"#FC4E07\"))\n\nggarrange(p1, p2, p3,\n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#히스토그램과-산포도scatter-plot-with-histograms",
    "href": "posts/ipynb/R_ggpubr.html#히스토그램과-산포도scatter-plot-with-histograms",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "7.4 4.7. 히스토그램과 산포도(Scatter Plot with Histograms)",
    "text": "7.4 4.7. 히스토그램과 산포도(Scatter Plot with Histograms)\n히스토그램과 산포도를 하나의 도표에 합쳐서 그려보도록 하겠습니다.\n\n# Grouped data\nggscatterhist(\n iris, x = \"Sepal.Length\", y = \"Sepal.Width\",\n color = \"Species\", size = 3, alpha = 0.6,\n palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n margin.params = list(fill = \"Species\", color = \"black\", size = 0.2))"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#상관분석correlation-coefficients과-p-values-추가하기",
    "href": "posts/ipynb/R_ggpubr.html#상관분석correlation-coefficients과-p-values-추가하기",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "7.5 4.8. 상관분석(Correlation Coefficients)과 P-values 추가하기",
    "text": "7.5 4.8. 상관분석(Correlation Coefficients)과 P-values 추가하기\n산포도에 상관분석과 p-values를 추가하는 방법입니다.\n\n# Load data\ndata(\"mtcars\")\ndf &lt;- mtcars\ndf$cyl &lt;- as.factor(df$cyl)\n\n# Scatter plot with correlation coefficient\nsp &lt;- ggscatter(df, x = \"wt\", y = \"mpg\",\n   add = \"reg.line\",  # Add regressin line\n   add.params = list(color = \"blue\", fill = \"lightgray\"), # Customize reg. line\n   conf.int = TRUE) # Add confidence interval\n# Add correlation coefficient\np1 &lt;- sp + stat_cor(method = \"pearson\", label.x = 3, label.y = 30)\n# Color by groups and facet\nsp &lt;- ggscatter(df, x = \"wt\", y = \"mpg\",\n   color = \"cyl\", palette = \"jco\",\n   add = \"reg.line\", conf.int = TRUE)\np2 &lt;- sp + stat_cor(aes(color = cyl), label.x = 3)\n# Scatter plot with ellipses and group mean points\np3 &lt;- ggscatter(df, x = \"wt\", y = \"mpg\",\n   color = \"cyl\", shape = \"cyl\",\n   mean.point = TRUE, ellipse = TRUE)+\n   stat_stars(aes(color = cyl))\n\nggarrange(p1, p2, p3,\n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#plot-paired-data",
    "href": "posts/ipynb/R_ggpubr.html#plot-paired-data",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "7.6 4.9. Plot Paired Data",
    "text": "7.6 4.9. Plot Paired Data\n\n# Example data\nbefore &lt;-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\nafter &lt;-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\n\nd &lt;- data.frame(before = before, after = after)\np1 &lt;- ggpaired(d, cond1 = \"before\", cond2 = \"after\", width = 0.0,\n    line.color = \"gray\", line.size = 0.4, palette = \"npg\")\np2 &lt;- ggpaired(d, cond1 = \"before\", cond2 = \"after\", width = 0.2,\n    line.color = \"gray\", line.size = 0.4, palette = \"aaas\")\np3 &lt;- ggpaired(d, cond1 = \"before\", cond2 = \"after\", width = 0.2,\n    line.color = \"gray\", line.size = 0.4, fill = \"condition\",palette = \"npg\")\nggarrange(p1, p2, p3,\n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/ipynb/R_ggpubr.html#p-values-를-박스-그래프에-추가하기",
    "href": "posts/ipynb/R_ggpubr.html#p-values-를-박스-그래프에-추가하기",
    "title": "ggpubr로 논문에 사용할 플랏그리기",
    "section": "7.7 4.10. P-values 를 박스 그래프에 추가하기",
    "text": "7.7 4.10. P-values 를 박스 그래프에 추가하기\n\n# Load data\ndata(\"ToothGrowth\")\nhead(ToothGrowth)\n\n\n\n\nlen\nsupp\ndose\n\n\n\n\n4.2\nVC\n0.5\n\n\n11.5\nVC\n0.5\n\n\n7.3\nVC\n0.5\n\n\n5.8\nVC\n0.5\n\n\n6.4\nVC\n0.5\n\n\n10.0\nVC\n0.5\n\n\n\n\n\n\n# Two independent groups\np &lt;- ggboxplot(ToothGrowth, x = \"supp\", y = \"len\",\n    color = \"supp\", palette = \"npg\", add = \"jitter\")\n\n#  Add p-value\np1 &lt;- p + stat_compare_means(method = \"t.test\")\n\n# Paired samples\np2 &lt;- ggpaired(ToothGrowth, x = \"supp\", y = \"len\",\n    color = \"supp\", line.color = \"gray\", line.size = 0.4,\n    palette = \"npg\")+\n    stat_compare_means(paired = TRUE, method = \"t.test\")\n\n# More than two groups, Pairwise comparisons: Specify the comparisons you want\nmy_comparisons &lt;- list( c(\"0.5\", \"1\"), c(\"1\", \"2\"), c(\"0.5\", \"2\") )\np3 &lt;- ggboxplot(ToothGrowth, x = \"dose\", y = \"len\",\n          color = \"dose\", palette = \"npg\")+\n# Add pairwise comparisons p-value\n    stat_compare_means(comparisons = my_comparisons, label.y = c(29, 35, 40))+\n    stat_compare_means(label.y = 45)     # Add global Anova p-value\n\n# Multiple pairwise test against a reference group\np4 &lt;- ggboxplot(ToothGrowth, x = \"dose\", y = \"len\",\n    color = \"dose\", palette = \"npg\")+\n    stat_compare_means(method = \"anova\", label.y = 40)+ # Add global p-value\n    stat_compare_means(aes(label = ..p.signif..),\n                      method = \"t.test\", ref.group = \"0.5\")\n\nggarrange(p1, p2, p3, p4,  ncol = 2, nrow = 2,\n          labels = c(\"A\", \"B\",\"C\",\"D\"))\n\n\n\n\n\n\n\n\n\n\n\n\n# Multiple grouping variables\np &lt;- ggboxplot(ToothGrowth, x = \"supp\", y = \"len\",\n              color = \"supp\", palette = \"npg\",\n              add = \"jitter\",\n              facet.by = \"dose\", short.panel.labs = FALSE)\n# Use only p.format as label. Remove method name.\np5 &lt;- p + stat_compare_means(aes(label = paste0(\"p = \", ..p.format..)))\np5"
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#anndata-객체에-대하여",
    "href": "posts/ipynb/scanpy_adata2mtx.html#anndata-객체에-대하여",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "1.1 AnnData 객체에 대하여",
    "text": "1.1 AnnData 객체에 대하여\nScanpy는 고차원 단일세포 RNA 시퀀싱 데이터를 다루기 위한 파이썬 라이브러리입니다. 이 라이브러리는 데이터를 저장하고 관리하기 위해 AnnData 객체를 활용합니다. 이번 포스트는 Scanpy를 사용하여 10x Genomics 에서 제공하는 텍스트 형식의 데이터를 먼저 불러오고 UMAP을 그린다음 다시 텍스트 형식의 데이터로 내보내는 작업을 해보겠습니다. 이런 방법을 알게 되면 Seurat으로 데이터를 변환하는 작업도 훨씬 쉬워집니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#mtx-형식에-대하여",
    "href": "posts/ipynb/scanpy_adata2mtx.html#mtx-형식에-대하여",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "1.2 mtx 형식에 대하여",
    "text": "1.2 mtx 형식에 대하여\nscRNA-seq count 데이터는 희소 행렬입니다. 다시 말해 대부분의 데이터가 0인 형태입니다. 이런 형태의 데이터를 효율적으로 저장하기 위해서 우리는 MEX(Market Exchange Format)을 사용합니다.\n\nMEX 형식은 행렬 데이터의 교환을 쉽게 할 수 있도록 고안되었습니다. 가능한 쉽게 분석할 수 있도록 최소한의 ASCII 형식을 가진 파일로 구성 됩니다.\n\n공개된 scRNA-seq 데이터에서 mtx로 저장되어 있는 것을 많이 볼 수 있습니다. 간략하게 설명하자면 matrix.mtx파일은 생성된 유전자 x 세포 count 행렬을 나타내며, 각 유전자의 발현 수준을 담고 있습니다.\n추가로 각 행과 열 인덱스에 해당하는 유전자 및 바코드 시퀀스가 TSV(Tab separated values) 형식 파일로 포함됩니다. TSV파일은 가장 단순하고 범용적인 파일 형식으로, 각 유전자의 이름과 세포 데이터가 텝으로 구분되어 저장되어 있습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#기본적인-데이터-qc-하기",
    "href": "posts/ipynb/scanpy_adata2mtx.html#기본적인-데이터-qc-하기",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "3.1 기본적인 데이터 QC 하기",
    "text": "3.1 기본적인 데이터 QC 하기\n\n# 기본 필터링: 최소 유전자가 200개 이상인 세포 필터링\nsc.pp.filter_cells(adata, min_genes=200)\n# 최소 3개 이상의 세포에서 발현되는 유전자 필터링\nsc.pp.filter_genes(adata, min_cells=3)\n\n# 미토콘드리아 유전자 그룹을 \"mt\"로 주석\nadata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n\n# QC 지표 계산\nsc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\"], percent_top=None, log1p=False, inplace=True)\n\n# 유전자 별 발현 수가 2500개 미만인 데이터 선택\nadata = adata[adata.obs.n_genes_by_counts &lt; 2500, :]\n# 미토콘드리아 발현 비율이 5% 미만인 데이터 선택 후 복사\nadata = adata[adata.obs.pct_counts_mt &lt; 5, :].copy()\n\nadata\n\nAnnData object with n_obs × n_vars = 2638 × 13714\n    obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt'\n    var: 'gene_ids', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'\n\n\n위 결과를 통해 데이터 QC를 거치자 데이터가 조금 감소한 것(바코드 2,638개, 유전자 13,714개)을 알 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#데이터-정규화하기",
    "href": "posts/ipynb/scanpy_adata2mtx.html#데이터-정규화하기",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "3.2 데이터 정규화하기",
    "text": "3.2 데이터 정규화하기\nUMAP을 그리기 위해 원시 데이터를 정규화하고 highly variable 유전자만 남긴 다음 클러스터링을 수행합니다. 계산량을 줄이기 위해 전체 유전자중 일부 highly variable 유전자만으로 진행합니다.\n\n\n\n\n\n\nNote\n\n\n\n전체 유전자를 다 사용한 것과 클러스터링 결과는 동일합니다.\n\n\n\n# 총 카운트 정규화\nsc.pp.normalize_total(adata, target_sum=1e4)\n# 데이터 로그 스케일 변환\nsc.pp.log1p(adata)\n# 고변동성 유전자 식별\nsc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n\n# 나중에 사용하기 위해 AnnData 객체의 .raw 속성을 정규화되고 로그 스케일링된 원시 유전자 발현으로 설정\nadata.raw = adata\n\n# 필터링 실행\nadata = adata[:, adata.var.highly_variable]\n\n# 각 세포의 총 카운트 및 발현된 미토콘드리아 유전자의 백분율 효과를 회귀로 제거하고 데이터를 단위 분산으로 스케일링\nsc.pp.regress_out(adata, [\"total_counts\", \"pct_counts_mt\"])\n# 각 유전자를 단위 분산으로 스케일링하고 표준 편차가 10을 초과하는 값을 클리핑\nsc.pp.scale(adata, max_value=10)\n# PCA 수행\nsc.tl.pca(adata, svd_solver=\"arpack\")\n# 이웃 찾기\nsc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\n# UMAP 수행\nsc.tl.umap(adata)\n# Leiden 알고리즘을 사용하여 클러스터링 수행\nsc.tl.leiden(\n    adata,\n    resolution=0.4,\n    random_state=42,\n)\n\n# 처리된 데이터셋 반환\nadata\n\nAnnData object with n_obs × n_vars = 2638 × 1838\n    obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'leiden'\n    var: 'gene_ids', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'log1p', 'hvg', 'pca', 'neighbors', 'umap', 'leiden'\n    obsm: 'X_pca', 'X_umap'\n    varm: 'PCs'\n    obsp: 'distances', 'connectivities'\n\n\n이제 유전자의 갯수가 13,714개에서 1,838개로 줄어다는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#umap-그리기",
    "href": "posts/ipynb/scanpy_adata2mtx.html#umap-그리기",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "3.3 UMAP 그리기",
    "text": "3.3 UMAP 그리기\n\n# UMAP 시각화\nsc.pl.umap(\n    adata,\n    color=\"leiden\",  # 클러스터링 결과에 따라 색상 지정\n    add_outline=True,  # 클러스터 경계 추가\n    legend_loc=\"on data\",  # 범례 위치 설정\n    legend_fontsize=12,  # 범례 글꼴 크기 설정\n    legend_fontoutline=2,  # 범례 글꼴 외곽선 굵기 설정\n    frameon=False,  # 테두리 제거\n    palette=\"Set1\",  # 색상 팔레트 설정\n)\n\n\n\n\n\n\n\n\n이 데이터셋은 이미 많이 알려진 것이라. 클러스터에 대한 마커 유전자를 찾고 어떤 세포인지 알아내는 과정은 생략하겠습니다.\n아래와 같이 수동으로 이름을 지정해줍니다. 숫자로 표시되는 각각의 클러스터에 대하여 아래 코드를 통해 이름을 붙여줍니다.\n\nnew_cluster_names = [\n    \"CD4T\",  # CD4T 세포\n    \"Monocytes\",  # 단핵구\n    \"CD8T\",  # CD8T 세포\n    \"B cells\",  # B 세포\n    \"Dendritic\",  # 수상돌기세포\n    \"Megakaryocytes\",  # 거대혈소판세포\n]\nadata.rename_categories(\"leiden\", new_cluster_names)\n\n# 클러스터링된 세포의 UMAP 다시 시각화\nsc.pl.umap(\n    adata,\n    color=\"leiden\",  # 클러스터링 결과에 따라 색상 지정\n    add_outline=True,  # 클러스터 경계 추가\n    legend_loc=\"on data\",  # 범례 위치 설정\n    legend_fontsize=12,  # 범례 글꼴 크기 설정\n    legend_fontoutline=2,  # 범례 글꼴 외곽선 굵기 설정\n    frameon=False,  # 테두리 제거\n    palette=\"Set1\",  # 색상 팔레트 설정\n)\n\n\n\n\n\n\n\n\n이후 데이터를 다시 읽어왔을때와 동일한지 확인하기 위해 adata 객체에 포함되어 있는 메타데이터의 데이터형을 확인합니다.\n\n# adata 메타데이터의 데이터 타입\nadata.obs.dtypes\n\nn_genes                 int64\nn_genes_by_counts       int32\ntotal_counts          float32\ntotal_counts_mt       float32\npct_counts_mt         float32\nleiden               category\ndtype: object"
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#h5ad-파일로-저장하기",
    "href": "posts/ipynb/scanpy_adata2mtx.html#h5ad-파일로-저장하기",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "3.4 H5AD 파일로 저장하기",
    "text": "3.4 H5AD 파일로 저장하기\nAnnData 객체를 저장할때는 H5AD(Hierarchical Data Format 5) 파일을 사용합니다. 이 파일 형식은 대용량 데이터셋의 주요 구성 요소(예: 표현형 데이터, 주석, 메타데이터)를 계층적으로 구조화해서 효율적으로 저장하고 처리할 수 있게 해 줍니다.\n\n# 분석 결과를 저장할 파일 경로\noutput_path = \"../output/pbmc3k/\"\n# `compression='gzip'`는 디스크 공간을 절약하지만 쓰기와 이후의 읽기가 약간 느려집니다.\nadata.write(output_path + \"pbmc3k.h5ad\", compression=\"gzip\")"
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#h5ad-파일-다시-불러오기",
    "href": "posts/ipynb/scanpy_adata2mtx.html#h5ad-파일-다시-불러오기",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "3.5 H5AD 파일 다시 불러오기",
    "text": "3.5 H5AD 파일 다시 불러오기\nsc.read_h5ad 함수는 H5AD 파일 형식을 AnnData 객체로 직접 불러옵니다.\n\n# 저장했던 h5ad파일을 다시 불러옵니다.\nadata = sc.read_h5ad(output_path + \"pbmc3k.h5ad\")\nadata\n\nAnnData object with n_obs × n_vars = 2638 × 1838\n    obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'leiden'\n    var: 'gene_ids', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'hvg', 'leiden', 'leiden_colors', 'log1p', 'neighbors', 'pca', 'umap'\n    obsm: 'X_pca', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\n데이터를 다시 불러와보니 유전자의 갯수가 1,838개입니다. highly variable 유전자가 아닌 원래 유전자 데이터로 되돌리려면 아래와 같이 해줍니다.\n\n# AnnData 객체를 불러온 후 원시 데이터로 변환\nadata = adata.raw.to_adata()\nadata\n\nAnnData object with n_obs × n_vars = 2638 × 13714\n    obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'leiden'\n    var: 'gene_ids', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n    uns: 'hvg', 'leiden', 'leiden_colors', 'log1p', 'neighbors', 'pca', 'umap'\n    obsm: 'X_pca', 'X_umap'\n    obsp: 'connectivities', 'distances'\n\n\n이제 유전자의 갯수가 13,714개로 돌아왔습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#read_10x_mtx함수-사용",
    "href": "posts/ipynb/scanpy_adata2mtx.html#read_10x_mtx함수-사용",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "5.1 read_10x_mtx()함수 사용",
    "text": "5.1 read_10x_mtx()함수 사용\n만약 다른 분석 도구를 통해 처리된 데이터의 경우라면 그리 추천하지는 않는 방법입니다.\n\n# 데이터 읽기\nadata = sc.read_10x_mtx(\n    output_path,  # `.mtx` 파일이 있는 디렉토리\n    var_names=\"gene_symbols\",  # 변수 이름에 유전자 기호 사용 (변수 축 인덱스)\n)\n\n# `sc.read_10x_mtx`에서 `var_names='gene_ids'`를 사용하는 경우 이 작업이 필요하지 않음\nadata.var_names_make_unique()\nadata\n\nAnnData object with n_obs × n_vars = 2638 × 13714\n    var: 'gene_ids'\n\n\nadata.var 출력을 확인해 정상적으로 유전자 이름을 불러왔는지도 확인합니다.\n\nadata.var\n\n\n\n\n\n\n\n\ngene_ids\n\n\n\n\nAL627309.1\nENSG00000237683\n\n\nAP006222.2\nENSG00000228463\n\n\nRP11-206L10.2\nENSG00000228327\n\n\nRP11-206L10.9\nENSG00000237491\n\n\nLINC00115\nENSG00000225880\n\n\n...\n...\n\n\nAC145212.1\nENSG00000215750\n\n\nAL592183.1\nENSG00000220023\n\n\nAL354822.1\nENSG00000215615\n\n\nPNRC2-1\nENSG00000215700\n\n\nSRSF10-1\nENSG00000215699\n\n\n\n\n13714 rows × 1 columns\n\n\n\n기존에 작업한 메타데이터와 UMAP 좌표 정보도 불러옵니다.\n\n# 세포 메타데이터 불러오기\ncell_meta = pd.read_csv(output_path + \"metadata.tsv\", sep=\"\\t\", index_col=0)\n# AnnData 객체에 메타데이터 및 변수 이름 설정\nadata.obs = cell_meta\n\n# UMAP 데이터 불러오기\numap = pd.read_csv(\n    output_path + \"UMAP.tsv\",\n    sep=\"\\t\",\n    index_col=0,\n)\n# UMAP 데이터 설정\nadata.obsm[\"X_umap\"] = umap.loc[adata.obs.index].values\nadata\n\nAnnData object with n_obs × n_vars = 2638 × 13714\n    obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'leiden'\n    var: 'gene_ids'\n    obsm: 'X_umap'\n\n\n위 결과를 통해 정상적으로 데이터를 불러왔음을 확인 할 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_adata2mtx.html#수작업으로-anndata-객체를-만드는-방법",
    "href": "posts/ipynb/scanpy_adata2mtx.html#수작업으로-anndata-객체를-만드는-방법",
    "title": "AnnData 객체를 10X MEX 형식으로 저장하기",
    "section": "5.2 수작업으로 Anndata 객체를 만드는 방법",
    "text": "5.2 수작업으로 Anndata 객체를 만드는 방법\n수작업을 통해 만드는 방법을 해봅니다. 10X MEX format과 동일하지는 않지만 유사한 형태의 경우에는 이 방법이 유용합니다.\n\n# mtx 희소 행렬 불러오기\nX = mmread(output_path + \"matrix.mtx\")\n\n# AnnData 객체 생성\nadata = anndata.AnnData(X=X.transpose().tocsr())\n\n# 세포 메타데이터 불러오기\ncell_meta = pd.read_csv(output_path + \"metadata.tsv\", sep=\"\\t\", index_col=0)\n\n# 유전자 이름 불러오기\ngene_names = pd.read_csv(\n    output_path + \"genes.tsv\",\n    sep=\"\\t\",\n    index_col=None,\n    header=None,\n    names=[\"gene_ids\", \"gene_symbols\"],\n)\n\n# 바코드 불러오기\nbarcodes = pd.read_csv(\n    input_path + \"barcodes.tsv\",\n    sep=\"\\t\",\n    index_col=0,\n    header=None,\n)\n\n# AnnData 객체에 메타데이터 및 변수 이름 설정\nadata.obs = cell_meta\nadata.obs.index = list(barcodes.index)\nadata.var.index = gene_names[\"gene_symbols\"].values\nadata.var[\"gene_ids\"] = gene_names[\"gene_ids\"].values\n\n# UMAP 데이터 불러오기\numap = pd.read_csv(\n    output_path + \"UMAP.tsv\",\n    sep=\"\\t\",\n    index_col=0,\n)\n# UMAP 데이터 설정\nadata.obsm[\"X_umap\"] = umap.loc[adata.obs.index].values\n\n# adata 데이터 확인\nadata\n\nAnnData object with n_obs × n_vars = 2638 × 13714\n    obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'leiden'\n    var: 'gene_ids'\n    obsm: 'X_umap'\n\n\n\n5.2.1 메타 데이터의 데이터형 확인하기\n위의 방법을 통해 메타 데이터를 불러오면 데이터의 형이 잘못 지정되는 경우가 있습니다.\n\n# adata.obs의 데이터 타입 확인\nadata.obs.dtypes\n\nn_genes                int64\nn_genes_by_counts      int64\ntotal_counts         float64\ntotal_counts_mt      float64\npct_counts_mt        float64\nleiden                object\ndtype: object\n\n\n예를 들면 leiden 클러스터는 기본적으로 숫자로 지정되는데 다시 불러오는 과정 중에 category가 아닌 int형으로 되는 경우가 있습니다. 이번에는 object로 되어 있습니다. 아래와 같이 category로 수정해줍니다.\n\n# adata.obs의 'leiden' 열을 범주형으로 변환한 후 데이터 타입 확인\nadata.obs.astype({\"leiden\": \"category\"}).dtypes\n\nn_genes                 int64\nn_genes_by_counts       int64\ntotal_counts          float64\ntotal_counts_mt       float64\npct_counts_mt         float64\nleiden               category\ndtype: object\n\n\nUMAP을 그려서 마지막으로 확인해 봅니다.\n\n# UMAP 시각화\nsc.pl.umap(\n    adata,\n    color=\"leiden\",  # 클러스터링 결과에 따라 색상 지정\n    add_outline=True,  # 클러스터 경계 추가\n    legend_loc=\"on data\",  # 범례 위치 설정\n    legend_fontsize=12,  # 범례 글꼴 크기 설정\n    legend_fontoutline=2,  # 범례 글꼴 외곽선 굵기 설정\n    frameon=False,  # 테두리 제거\n    palette=\"Set1\",  # 색상 팔레트 설정\n)\n\n\n\n\n\n\n\n\nUMAP이 문제없이 재현되는 것을 확인 할 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/cellranger_alignment.html#가상환경-사용하기",
    "href": "posts/ipynb/cellranger_alignment.html#가상환경-사용하기",
    "title": "10X genomics scRNA-seq alignment",
    "section": "1.1 가상환경 사용하기",
    "text": "1.1 가상환경 사용하기\n정확하게는 velocyto를 사용하기 위해 가상환경을 만들어 사용합니다. 먼저 TutorialEnvironment.yml 파일을 다음 명령어로 다운받습니다.\nwget https://cf.10xgenomics.com/supp/cell-exp/neutrophils/TutorialEnvironment.yml\n다음 명령어로 가상환경을 만들어 줍니다.\nconda env create --file TutorialEnvironment.yml\n만들어진 가상환경을 활성화 시킵니다.\nconda activate tutorial\n다운로드 받은 yml파일은 삭제해줍니다.\nrm TutorialEnvironment.yml"
  },
  {
    "objectID": "posts/ipynb/cellranger_alignment.html#system-requirements",
    "href": "posts/ipynb/cellranger_alignment.html#system-requirements",
    "title": "10X genomics scRNA-seq alignment",
    "section": "2.1 System Requirements",
    "text": "2.1 System Requirements\nHardware Cell Ranger pipelines run on Linux systems that meet these minimum requirements:\n\n8-core Intel or AMD processor (16 cores recommended).\n64GB RAM (128GB recommended).\n1TB free disk space.\n64-bit CentOS/RedHat 7.0 or Ubuntu 14.04 - See the 10x Genomics OS Support page for details.\n\n\n2.1.1 Cell Ranger 7.2.0 (Sep 13, 2023)\nChromium Single Cell Software Suite Self-contained, relocatable tar file. Does not require centralized installation. Contains binaries pre-compiled for CentOS/RedHat 7.0 and Ubuntu 14.04. Runs on Linux systems that meets the minimum compute requirements.\n다음 명령어를 통해 cell ranger를 다운로드 합니다.\nwget -O cellranger-7.2.0.tar.gz \"https://cf.10xgenomics.com/releases/cell-exp/cellranger-7.2.0.tar.gz?Expires=1695129121&Key-Pair-Id=APKAI7S6A5RYOXBWRPDA&Signature=ZipqR8Pg4YvVDQ3MvAVuuiPwEOC5c39~Wj0WTAxfoJW6xtrxqIFgIDySFSFnsWcwDpmAovJGrHXU24Y9Cptt88OJSPdEupyFRXoGKJvVzRtDJChmuMbSpVCy-2N-QnMKwxNtd8Yt8Mdp2Vcq4wxx1hVC0Yx54c7U9o~RFXIVsIp48thKR6JnKhJmCAC5U4dFLa86~NcB4s5Ic4HATrQP2KyWexyYZWgmBEw13mlKYtlVRUil0zseoq0-CZyGmE8oB0iDBSUBAyIqo~XMVjv~lkMz4cRcyCbQKBRDr~U36FM2KnE3rhv-Rlp4KD-uXCReRnBsY6N6t-HxpS2YDpN3mQ__\"\n그리고 나서 다음 명령어로 압축을 풀어줍니다.\ntar -xvf cellranger-7.2.0.tar.gz\ncellranger-7.2.0 폴더로 이동하기\ncd cellranger-7.2.0\n정상적으로 되었다면 아래와 같은 폴더 구조를 볼 수 있습니다.\n.\n├── bin\n├── builtwith.json\n├── cellranger -&gt; bin/cellranger\n├── external\n├── lib\n├── LICENSE\n├── mro\n├── probe_sets -&gt; external/tenx_feature_references/targeted_panels\n├── sourceme.bash\n├── sourceme.csh\n└── THIRD-PARTY-LICENSES.cellranger.txt\n\n5 directories, 6 files\n이제 ./cellranger명령어를 입력하면 아래와 같이 cell ranger 실행이 가능합니다.\n./cellranger\ncellranger cellranger-7.2.0\n\nProcess 10x Genomics Gene Expression, Feature Barcode, and Immune Profiling data\n\nUsage: cellranger &lt;COMMAND&gt;\n\nCommands:\n  count           Count gene expression and/or feature barcode reads from a single sample and GEM well\n  multi           Analyze multiplexed data or combined gene expression/immune profiling/feature barcode data\n  multi-template  Output a multi config CSV template\n  vdj             Assembles single-cell VDJ receptor sequences from 10x Immune Profiling libraries\n  aggr            Aggregate data from multiple Cell Ranger runs\n  reanalyze       Re-run secondary analysis (dimensionality reduction, clustering, etc)\n  mkvdjref        Prepare a reference for use with CellRanger VDJ\n  mkfastq         Run Illumina demultiplexer on sample sheets that contain 10x-specific sample index sets\n  testrun         Execute the 'count' pipeline on a small test dataset\n  mat2csv         Convert a gene count matrix to CSV format\n  mkref           Prepare a reference for use with 10x analysis software. Requires a GTF and FASTA\n  mkgtf           Filter a GTF file by attribute prior to creating a 10x reference\n  upload          Upload analysis logs to 10x Genomics support\n  sitecheck       Collect linux system configuration information\n  help            Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help     Print help\n  -V, --version  Print version"
  },
  {
    "objectID": "posts/ipynb/cellranger_alignment.html#scanpy-사용",
    "href": "posts/ipynb/cellranger_alignment.html#scanpy-사용",
    "title": "10X genomics scRNA-seq alignment",
    "section": "6.1 Scanpy 사용",
    "text": "6.1 Scanpy 사용\n\n# conda install pandas scanpy\nimport scanpy as sc\n\n# Cell ranger 결과 파일이 들어있는 경로\nfile_path = \"../input/run_count_mNeuron1k/outs/filtered_feature_bc_matrix\"\n\n# read_10x_mtx 함수를 사용\nadata = sc.read_10x_mtx(file_path, var_names=\"gene_symbols\", cache=True)\nadata\n\nAnnData object with n_obs × n_vars = 1311 × 32285\n    var: 'gene_ids', 'feature_types'\n\n\n\n# conda install -c bioconda scvelo\nimport scvelo as scv\n\n# load loom files for spliced/unspliced matrices for each sample\nloom = scv.read(f\"{file_path}/run_count_mNeuron1k.loom\", validate=False, cache=False)\n\n# rename barcodes in order to merge:\nbarcodes = [bc.split(\":\")[1] for bc in loom.obs.index.tolist()]\nbarcodes = [bc[0 : len(bc) - 1] + \"-1\" for bc in barcodes]\nloom.obs.index = barcodes\n\n# make variable names unique\nloom.var_names_make_unique()\n\n# merge matrices into the original adata object\nadata = scv.utils.merge(adata, loom)\nadata\n\nAnnData object with n_obs × n_vars = 1311 × 32285\n    obs: 'initial_size_unspliced', 'initial_size_spliced', 'initial_size'\n    var: 'gene_ids', 'feature_types', 'Accession', 'Chromosome', 'End', 'Start', 'Strand'\n    layers: 'matrix', 'ambiguous', 'spliced', 'unspliced'\n\n\n\nscv.pl.proportions(adata)\n\n\n\n\n\n\n\n\n\nadata.write(\"../output/mNeuron1k.h5ad\", compression=\"gzip\")"
  },
  {
    "objectID": "posts/ipynb/cellranger_alignment.html#seurat-사용",
    "href": "posts/ipynb/cellranger_alignment.html#seurat-사용",
    "title": "10X genomics scRNA-seq alignment",
    "section": "6.2 Seurat 사용",
    "text": "6.2 Seurat 사용\nR에 좀 더 익숙하시다면 Seurat을 사용하세요. 저는 파이썬 문법을 더 좋아하지만 솔직히 분석 도구의 완성도 측면에서는 Seurat이 더 나은 것 같습니다. 아래 코드는 jupyter notebook에서 R코드를 사용하기 위해 필요한 것으로 rStudio를 사용하신다면 %%R이 포함된 셀의 코드만 사용하시면 됩니다.\n\nimport logging\n\nimport rpy2.rinterface_lib.callbacks as rcb\n\n# import rpy2.robjects as ro\n\nrcb.logger.setLevel(logging.ERROR)\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(dplyr)\nlibrary(Seurat)\n\n\n    WARNING: The R package \"reticulate\" only fixed recently\n    an issue that caused a segfault when used with rpy2:\n    https://github.com/rstudio/reticulate/pull/1188\n    Make sure that you use a version of that package that includes\n    the fix.\n    \n\n\n\n%%R\n# Load dataset\nfile_path &lt;- \"../input/run_count_mNeuron1k/outs/filtered_feature_bc_matrix\"\nneuron_data &lt;- Read10X(data_dir=file_path)\n# Initialize the Seurat object.\nobj &lt;- CreateSeuratObject(counts = neuron_data)\nobj\n\nAn object of class Seurat \n32285 features across 1311 samples within 1 assay \nActive assay: RNA (32285 features, 0 variable features)\n\n\nSeurat object를 파일로 저장하는 방법에도 여러가지가 있지만 R에서 많이 사용되는 *.rds를 사용하는 코드는 아래와 같습니다.\n\n%%R\nsaveRDS(obj, file = \"../output/mNeuron1k.rds\")"
  },
  {
    "objectID": "posts/ipynb/jupyter_notebook.html",
    "href": "posts/ipynb/jupyter_notebook.html",
    "title": "Jupyter notebook 소개",
    "section": "",
    "text": "ipython는 파이썬을 위한 향상된 대화형 커맨드라인 콘솔입니다. Jupyter notebook은 그것에서 보다 발전된 형태로 코딩과 문서화를 동시에 해서 생산성을 극대화 하는 도구입니다. 이 블로그도 jupyter notebook을 기반으로 해서 만들어 졌습니다.\n\nprint(\"Hello!\")\n\nHello!\n\n\n\n\n파일 시스템과 상호작용을 할 수 있는 명령어로 % 기호로 시작합니다. 현재 디렉토리 위치를 출력해 보겠습니다.\n\n%pwd\n\n'/home/partrita/Documents/blog/partrita.github.io/posts'\n\n\n더 많은 매직 명령어는 %lsmagic으로 확인 할 수 있고, 각 명령어에 ?를 추가하면 추가 정보를 보여줍니다.\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n\n\n\n\n\n자동완성은 믿을 수 없을 정도로 아주 유용한 기능입니다. 코딩을 하면서 모든 것을 타이핑하는것보다 tab키를 누르면 알아서 완성해주는 기능입니다."
  },
  {
    "objectID": "posts/ipynb/jupyter_notebook.html#매직-명령어magic-commands",
    "href": "posts/ipynb/jupyter_notebook.html#매직-명령어magic-commands",
    "title": "Jupyter notebook 소개",
    "section": "",
    "text": "파일 시스템과 상호작용을 할 수 있는 명령어로 % 기호로 시작합니다. 현재 디렉토리 위치를 출력해 보겠습니다.\n\n%pwd\n\n'/home/partrita/Documents/blog/partrita.github.io/posts'\n\n\n더 많은 매직 명령어는 %lsmagic으로 확인 할 수 있고, 각 명령어에 ?를 추가하면 추가 정보를 보여줍니다.\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics."
  },
  {
    "objectID": "posts/ipynb/jupyter_notebook.html#자동완성-tab-completion",
    "href": "posts/ipynb/jupyter_notebook.html#자동완성-tab-completion",
    "title": "Jupyter notebook 소개",
    "section": "",
    "text": "자동완성은 믿을 수 없을 정도로 아주 유용한 기능입니다. 코딩을 하면서 모든 것을 타이핑하는것보다 tab키를 누르면 알아서 완성해주는 기능입니다."
  },
  {
    "objectID": "posts/ipynb/jupyter_notebook.html#seaborn-시각화",
    "href": "posts/ipynb/jupyter_notebook.html#seaborn-시각화",
    "title": "Jupyter notebook 소개",
    "section": "2.1 Seaborn 시각화",
    "text": "2.1 Seaborn 시각화\nseaborn은 사용하기 쉬운 발전된 기능을 제공 합니다.\n\n# 예제에 사용될 데이터를 읽어오자\ndf = sns.load_dataset(\"iris\")\ndf.head()  # 데이터의 모양을 확인\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n사용한 데이터셋은 붓꽃(iris) 의 3가지 종(setosa, versicolor, virginica)에 대해 꽃받침(sepal)과 꽃잎(petal)의 넓이와 길이를 정리한 데이터입니다.\n\n위 그림을 참고하시면 이해가 되실 겁니다.\n\n# pair plot을 그려본다\nsns.pairplot(df, hue=\"species\", size=2.5)\n\n\n\n\n\n\n\n\n각각의 붓꽃종에 따라 꽃받침(sepal)과 꽃잎(petal)에 어떠한 연관성이 있는 지 확인 할 수 있습니다. 예를 들면 꽃잎의 길이가 길면 넓이도 넓어지는것은 모든종에서 연관관계가 있는 것을 볼수 있습니다."
  },
  {
    "objectID": "posts/ipynb/python_StatisticalTesting.html",
    "href": "posts/ipynb/python_StatisticalTesting.html",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "",
    "text": "통계적 추론이라는 것은 제한된 실험 데이터에서 얻은 결과를 모집단에도 적용하려는 것입니다. 이번 포스트에서는 통계적 추론에 사용되는 검정법을 배워봅니다."
  },
  {
    "objectID": "posts/ipynb/python_StatisticalTesting.html#순열-검정을-통한-ab-검정",
    "href": "posts/ipynb/python_StatisticalTesting.html#순열-검정을-통한-ab-검정",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "2.1 1.1. 순열 검정을 통한 A/B 검정",
    "text": "2.1 1.1. 순열 검정을 통한 A/B 검정\n\n순열검정(permutaion test): 두 개 이상의 표본을 함께 결합하여 관측값들을 무작위로 재표본으로 추출하는 과정을 말한다.\n\n파이썬에서 순열 검정을 구현하기 위해 아래와 같이 perm_fun 함수를 정의합니다.\n\n# Permutation test example with stickiness\ndef perm_fun(x, nA, nB):\n    n = nA + nB\n    idx_B = set(random.sample(range(n), nB))\n    idx_A = set(range(n)) - idx_B\n    return x.loc[idx_B].mean() - x.loc[idx_A].mean()\n\n\nnA = session_times[session_times.Page == \"Page A\"].shape[0]\nnB = session_times[session_times.Page == \"Page B\"].shape[0]\nprint(perm_fun(session_times.Time, nA, nB))\n\n24.23809523809524\n\n\n단 한번의 계산을 통해서 24초라는 차이가 발생하였습니다. 계산을 반복해서 페이지A와 페이지B 사이의 시간 차이에 대한 도수 분포표를 그려봅시다.\n\nrandom.seed(1)\nperm_diffs = [perm_fun(session_times.Time, nA, nB) for _ in range(1000)]\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.hist(perm_diffs, bins=11, rwidth=0.9)\nax.axvline(x=mean_b - mean_a, color=\"black\", lw=2)\nax.text(50, 190, \"Observed\\ndifference\", bbox={\"facecolor\": \"white\"})\nax.set_xlabel(\"Session time differences (in seconds)\")\nax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n위 그림에서 수직선은 관측된 차이입니다. 이것을 통해 순열 검정에서 가끔 실제 관찰된 차이를 넘어가는 것을 알 수 있습니다. 그렇다면 어느정도의 확률로 그런 일이 벌어질까요?\n\nprint(np.mean(perm_diffs &gt; mean_b - mean_a))\n\n0.121\n\n\n답은 12.1% 입니다. 이것을 통해 페이지A와 페이지B의 차이인 36초가 통계적으로 봤을때는 차이가 없어도 약 12% 확률로 발생할 수 있다는 결론을 얻었습니다."
  },
  {
    "objectID": "posts/ipynb/python_StatisticalTesting.html#t-test를-사용한-ab-검정",
    "href": "posts/ipynb/python_StatisticalTesting.html#t-test를-사용한-ab-검정",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "2.2 1.1.2. t-Test를 사용한 A/B 검정",
    "text": "2.2 1.1.2. t-Test를 사용한 A/B 검정\n\nt-테스트(t-test) 또는 t-검정 또는 스튜던트 t-테스트(Student’s t-test)는 검정통계량이 귀무가설 하에서 t-분포를 따르는 통계적 가설 검정법이다. t-테스트는 일반적으로 검정통계량이 정규 분포를 따르며 분포와 관련된 스케일링 변숫값들이 알려진 경우에 사용한다. 이때 모집단의 분산과 같은 스케일링 항을 알 수 없으나, 이를 데이터를 기반으로 한 추정값으로 대체하면 검정통계량은 t-분포를 따른다. 예를 들어 t-테스트를 사용하여 두 데이터 세트(집단)의 평균이 서로 유의하게 다른지 여부를 판별할 수 있다. -wiki\n\n파이썬에서는 ttest_ind 함수를 사용하면 손쉽게 구해볼 수 있습니다.\n\nres = stats.ttest_ind(\n    session_times[session_times.Page == \"Page A\"].Time,\n    session_times[session_times.Page == \"Page B\"].Time,\n    equal_var=False,\n)\nprint(f\"p-value for single sided test: {res.pvalue / 2:.4f}\")\n\np-value for single sided test: 0.1408\n\n\n결과 값이 앞서 구한 순열 검정의 확률인 12%과 유사한 수치인 14%임을 확인 할 수 있습니다. 컴퓨터가 보급되기 전에 순열 검정은 실용적이지 않았고 그래서 통계학자들에게 t-Test가 널리 사용되었습니다.\n\ntstat, pvalue, df = sm.stats.ttest_ind(\n    session_times[session_times.Page == \"Page A\"].Time,\n    session_times[session_times.Page == \"Page B\"].Time,\n    usevar=\"unequal\",\n    alternative=\"smaller\",\n)\nprint(f\"p-value: {pvalue:.4f}\")\n\np-value: 0.1408"
  },
  {
    "objectID": "posts/ipynb/python_StatisticalTesting.html#p-value-구하기",
    "href": "posts/ipynb/python_StatisticalTesting.html#p-value-구하기",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "3.1 2.1. P-Value 구하기",
    "text": "3.1 2.1. P-Value 구하기\np-value는 순열 검정에서 얻은 결과 중에 관찰된 차이와 같거나 더 큰 차이를 보이는 경우의 비율이라고 할 수 있기에 다음과 같이 추정할 수 있습니다.\n\nprint(np.mean([diff &gt; obs_pct_diff for diff in perm_diffs]))\n\n0.332\n\n\n예상한 것처럼 30%의 확률로 우연에 의해서 나타날 수 있는 차이였습니다. 따라서 페이지A와 페이지B의 차이는 통계적으로 유의미하지 않다고 말할 수 있겠습니다."
  },
  {
    "objectID": "posts/ipynb/python_StatisticalTesting.html#순열검정을-통한-one-way-anova",
    "href": "posts/ipynb/python_StatisticalTesting.html#순열검정을-통한-one-way-anova",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "4.1 3.1. 순열검정을 통한 one-way ANOVA",
    "text": "4.1 3.1. 순열검정을 통한 one-way ANOVA\n파이썬에서는 다음 코드를 사용해 순열 검정을 통해 ANOVA 분석을 진행할 수 있습니다.\n\nobserved_variance = four_sessions.groupby(\"Page\").mean().var()[0]\nprint(\"Observed means:\", four_sessions.groupby(\"Page\").mean().values.ravel())\nprint(\"Variance:\", observed_variance)\n\n\n# Permutation test example with stickiness\ndef perm_test(df):\n    df = df.copy()\n    df[\"Time\"] = np.random.permutation(df[\"Time\"].values)\n    return df.groupby(\"Page\").mean().var()[0]\n\n\nprint(perm_test(four_sessions))\n\nObserved means: [172.8 182.6 175.6 164.6]\nVariance: 55.426666666666655\n57.02666666666678\n\n\n\nrandom.seed(1)\nperm_variance = [perm_test(four_sessions) for _ in range(3000)]\nprint(\"Pr(Prob)\", np.mean([var &gt; observed_variance for var in perm_variance]))\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.hist(perm_variance, bins=11, rwidth=0.9)\nax.axvline(x=observed_variance, color=\"black\", lw=2)\nax.text(60, 200, \"Observed\\nvariance\", bbox={\"facecolor\": \"white\"})\nax.set_xlabel(\"Variance\")\nax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\nPr(Prob) 0.07766666666666666\n\n\n\n\n\n\n\n\n\nPr(Prob)의 값은 p-value이며 결과는 0.07입니다. 통상적인 임계 p-value 값인 0.05이상임으로 네 페이지간의 차이가 우연히 발생할 수 있다고 결론 내릴 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/python_StatisticalTesting.html#f-통계량을-통한-one-way-anova",
    "href": "posts/ipynb/python_StatisticalTesting.html#f-통계량을-통한-one-way-anova",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "4.2 3.2. F-통계량을 통한 one-way ANOVA",
    "text": "4.2 3.2. F-통계량을 통한 one-way ANOVA\nF-통계량은 잔차 오류에 인한 분산과 그룹 평균의 분산에 대한 비율을 기초로 합니다. 비율이 높으면 통계적으로 유의미 하다고 할 수 있고 이를 토대로 p-value를 계산할 수 있습니다.\n\nmodel = smf.ols(\"Time ~ Page\", data=four_sessions).fit()\n\naov_table = sm.stats.anova_lm(model)\nprint(aov_table)\n\n            df  sum_sq     mean_sq         F    PR(&gt;F)\nPage       3.0   831.4  277.133333  2.739825  0.077586\nResidual  16.0  1618.4  101.150000       NaN       NaN\n\n\ndf는 자유도, sum_sq는 제곱합, mean_sq는 평균제곱, F는 F-통계량을 나타냅니다.\n\nres = stats.f_oneway(\n    four_sessions[four_sessions.Page == \"Page 1\"].Time,\n    four_sessions[four_sessions.Page == \"Page 2\"].Time,\n    four_sessions[four_sessions.Page == \"Page 3\"].Time,\n    four_sessions[four_sessions.Page == \"Page 4\"].Time,\n)\nprint(f\"F-Statistic: {res.statistic / 2:.4f}\")\nprint(f\"p-value: {res.pvalue / 2:.4f}\")\n\nF-Statistic: 1.3699\np-value: 0.0388\n\n\nF-통계량을 사용한 방법은 p-value 값이 더 적게나와 임계값인 0.05 이하입니다. 그러나 ANOVA 분석의 p-value가 낮게 나왔다고 해서 모든 그룹에서 통계적으로 차이가 있다고 할 수는 없습니다. 추가적인 Ad hoc 분석을 진행해 어떤 그룹에서 차이가 있는지 확인해보아야 합니다."
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html",
    "href": "posts/ipynb/python_Statistics.html",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "통계 계산을 위한 프로그래밍 언어에 R 프로그래밍 언어(줄여서 R)가 있는데, 왜 파이썬을 써야 할까요?\nR은 문법 자체부터 통계에 특화되어 있고 여러가지 통계분석을 할 수 있습니다. 그럼에도 불구하고 제가 파이썬을 통계분석에 사용하는 이유는 간단합니다. 파이썬은 보다 범용적인 언어이고 라이브러리가 풍부해서 제가 원하는 기능은 거의 이미 다 있기 때문이죠.\n\n\n여기, brain_size 라는 데이터를 살펴 보겠습니다.\n\n# 필요한 라이브러리를 불러옵니다.\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\n\nbrain_size 데이터는 Willerman이 1991년에 사람의 뇌 크기와 무게, 그리고 IQ에 대하여 측정한 값입니다. 범주형의 데이터와 수치형 데이터로 구성 되어 있죠.\nPandas의 read_csv 기능을 이용해 데이터프레임을 만들어 보겠습니다.\n\ndf = pd.read_csv(\n    \"http://www.scipy-lectures.org/_downloads/brain_size.csv\",\n    sep=\";\",\n    index_col=0,\n    na_values=\".\",\n)\ndf.head()  # 상단의 5개의 데이터 확인하기\n\n\n\n\n\n\n\n\nGender\nFSIQ\nVIQ\nPIQ\nWeight\nHeight\nMRI_Count\n\n\n\n\n1\nFemale\n133\n132\n124\n118.0\n64.5\n816932\n\n\n2\nMale\n140\n150\n124\nNaN\n72.5\n1001121\n\n\n3\nMale\n139\n123\n150\n143.0\n73.3\n1038437\n\n\n4\nMale\n133\n129\n128\n172.0\n68.8\n965353\n\n\n5\nFemale\n137\n132\n134\n147.0\n65.0\n951545\n\n\n\n\n\n\n\n간단히 살펴보면, 총 40명의 사람들의 성별, IQ, 몸무게, 키 그리고 MRI_count(total pixel Count from the 18 MRI scans) 값이 측정되어 있습니다. IQ의 경우 3종류로 세분화 되어있는데 각각을 알아 보면 아래와 같습니다.\n\nFull Scale Intelligence Quotient (FSIQ) : VIQ와 PIQ의 종합적인 수치입니다\nVerbal IQ (VIQ) : 언어적인 측면을 측정합니다.\nPerformance IQ (PIQ) : 논리, 계산적인 측면을 측정\n\n\n\n\npandas에서는 간단하게 평균값과 표준편차등을 계산해주는 기능이 있습니다.\ndescribe() 함수를 사용하면 모든 열에 대한 설명통계값을 보여줍니다.\n\n# padas 에서 제공하는 설명 통계\ndf.describe()\n\n\n\n\n\n\n\n\nFSIQ\nVIQ\nPIQ\nWeight\nHeight\nMRI_Count\n\n\n\n\ncount\n40.000000\n40.000000\n40.00000\n38.000000\n39.000000\n4.000000e+01\n\n\nmean\n113.450000\n112.350000\n111.02500\n151.052632\n68.525641\n9.087550e+05\n\n\nstd\n24.082071\n23.616107\n22.47105\n23.478509\n3.994649\n7.228205e+04\n\n\nmin\n77.000000\n71.000000\n72.00000\n106.000000\n62.000000\n7.906190e+05\n\n\n25%\n89.750000\n90.000000\n88.25000\n135.250000\n66.000000\n8.559185e+05\n\n\n50%\n116.500000\n113.000000\n115.00000\n146.500000\n68.000000\n9.053990e+05\n\n\n75%\n135.500000\n129.750000\n128.00000\n172.000000\n70.500000\n9.500780e+05\n\n\nmax\n144.000000\n150.000000\n150.00000\n192.000000\n77.000000\n1.079549e+06\n\n\n\n\n\n\n\nIQ의 평균값은 113이군요. 몸무게는 kg으로 변환하면 약 70kg쯤 됩니다.\n\n\n\n전체적인 데이터의 양상을 보기에는 시각화가 중요합니다. 파이썬에서는 간단하게 산포 행렬(sactter matrix)를 그려 볼 수 있습니다.\n먼저 키와 몸무게, MRI_count 간의 상관관계를 보겠습니다.\n\n# Plotting data\nfrom pandas.plotting import scatter_matrix\n\n# 키와 몸무게, MRI_count\nscatter_matrix(df[[\"Weight\", \"Height\", \"MRI_Count\"]])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A1D2B70&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A443668&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A47E588&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A4B9588&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A56D4A8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A56D4E0&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A5DC978&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A614EB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A643B70&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n키와 몸무계는 서로 연관이 있는듯 하고 나머지는 그다지 서로 연관이 없어 보입니다.\n그 다음으로는 여러 IQ 수치간에 상관관계를 알아 보죠.\n\nscatter_matrix(df[[\"PIQ\", \"VIQ\", \"FSIQ\"]])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A9AD6A0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A9F34A8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AA2E3C8&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AA64358&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AAA0358&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AAA0390&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB01EB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB3DEB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB72EB8&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n당연하지만, FSIQ는 VIQ, PIQ 각각과 연관성이 있어 보입니다. VIQ와 PIQ간에는 애매하게 연관성이 없어 보이네요. FSIQ의 히스토그램에서는 100 - 125 사이에는 데이터가 없는 것을 확인 할 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#예를-들어봅시다.",
    "href": "posts/ipynb/python_Statistics.html#예를-들어봅시다.",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "여기, brain_size 라는 데이터를 살펴 보겠습니다.\n\n# 필요한 라이브러리를 불러옵니다.\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#csv-파일-읽어오기",
    "href": "posts/ipynb/python_Statistics.html#csv-파일-읽어오기",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "brain_size 데이터는 Willerman이 1991년에 사람의 뇌 크기와 무게, 그리고 IQ에 대하여 측정한 값입니다. 범주형의 데이터와 수치형 데이터로 구성 되어 있죠.\nPandas의 read_csv 기능을 이용해 데이터프레임을 만들어 보겠습니다.\n\ndf = pd.read_csv(\n    \"http://www.scipy-lectures.org/_downloads/brain_size.csv\",\n    sep=\";\",\n    index_col=0,\n    na_values=\".\",\n)\ndf.head()  # 상단의 5개의 데이터 확인하기\n\n\n\n\n\n\n\n\nGender\nFSIQ\nVIQ\nPIQ\nWeight\nHeight\nMRI_Count\n\n\n\n\n1\nFemale\n133\n132\n124\n118.0\n64.5\n816932\n\n\n2\nMale\n140\n150\n124\nNaN\n72.5\n1001121\n\n\n3\nMale\n139\n123\n150\n143.0\n73.3\n1038437\n\n\n4\nMale\n133\n129\n128\n172.0\n68.8\n965353\n\n\n5\nFemale\n137\n132\n134\n147.0\n65.0\n951545\n\n\n\n\n\n\n\n간단히 살펴보면, 총 40명의 사람들의 성별, IQ, 몸무게, 키 그리고 MRI_count(total pixel Count from the 18 MRI scans) 값이 측정되어 있습니다. IQ의 경우 3종류로 세분화 되어있는데 각각을 알아 보면 아래와 같습니다.\n\nFull Scale Intelligence Quotient (FSIQ) : VIQ와 PIQ의 종합적인 수치입니다\nVerbal IQ (VIQ) : 언어적인 측면을 측정합니다.\nPerformance IQ (PIQ) : 논리, 계산적인 측면을 측정"
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#pandas-설명통계",
    "href": "posts/ipynb/python_Statistics.html#pandas-설명통계",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "pandas에서는 간단하게 평균값과 표준편차등을 계산해주는 기능이 있습니다.\ndescribe() 함수를 사용하면 모든 열에 대한 설명통계값을 보여줍니다.\n\n# padas 에서 제공하는 설명 통계\ndf.describe()\n\n\n\n\n\n\n\n\nFSIQ\nVIQ\nPIQ\nWeight\nHeight\nMRI_Count\n\n\n\n\ncount\n40.000000\n40.000000\n40.00000\n38.000000\n39.000000\n4.000000e+01\n\n\nmean\n113.450000\n112.350000\n111.02500\n151.052632\n68.525641\n9.087550e+05\n\n\nstd\n24.082071\n23.616107\n22.47105\n23.478509\n3.994649\n7.228205e+04\n\n\nmin\n77.000000\n71.000000\n72.00000\n106.000000\n62.000000\n7.906190e+05\n\n\n25%\n89.750000\n90.000000\n88.25000\n135.250000\n66.000000\n8.559185e+05\n\n\n50%\n116.500000\n113.000000\n115.00000\n146.500000\n68.000000\n9.053990e+05\n\n\n75%\n135.500000\n129.750000\n128.00000\n172.000000\n70.500000\n9.500780e+05\n\n\nmax\n144.000000\n150.000000\n150.00000\n192.000000\n77.000000\n1.079549e+06\n\n\n\n\n\n\n\nIQ의 평균값은 113이군요. 몸무게는 kg으로 변환하면 약 70kg쯤 됩니다."
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#산포-행렬을-그려보겠습니다.",
    "href": "posts/ipynb/python_Statistics.html#산포-행렬을-그려보겠습니다.",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "전체적인 데이터의 양상을 보기에는 시각화가 중요합니다. 파이썬에서는 간단하게 산포 행렬(sactter matrix)를 그려 볼 수 있습니다.\n먼저 키와 몸무게, MRI_count 간의 상관관계를 보겠습니다.\n\n# Plotting data\nfrom pandas.plotting import scatter_matrix\n\n# 키와 몸무게, MRI_count\nscatter_matrix(df[[\"Weight\", \"Height\", \"MRI_Count\"]])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A1D2B70&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A443668&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A47E588&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A4B9588&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A56D4A8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A56D4E0&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A5DC978&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A614EB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A643B70&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n키와 몸무계는 서로 연관이 있는듯 하고 나머지는 그다지 서로 연관이 없어 보입니다.\n그 다음으로는 여러 IQ 수치간에 상관관계를 알아 보죠.\n\nscatter_matrix(df[[\"PIQ\", \"VIQ\", \"FSIQ\"]])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A9AD6A0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A9F34A8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AA2E3C8&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AA64358&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AAA0358&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AAA0390&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB01EB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB3DEB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB72EB8&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n당연하지만, FSIQ는 VIQ, PIQ 각각과 연관성이 있어 보입니다. VIQ와 PIQ간에는 애매하게 연관성이 없어 보이네요. FSIQ의 히스토그램에서는 100 - 125 사이에는 데이터가 없는 것을 확인 할 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#sample-t-test",
    "href": "posts/ipynb/python_Statistics.html#sample-t-test",
    "title": "파이썬 통계분석하기",
    "section": "2.1 1-sample T-test",
    "text": "2.1 1-sample T-test\n하나의 집단의 평균이 특정 기준보다 유의미하게 다른지 를 알아보는 분석 방법입니다. Student T-test이라고도 하는 1-sample T-test 를 사용하려면 scipy.stats.ttest_1samp() 함수를 사용하면 됩니다.\n\nfrom scipy import stats\n\n\n## Student’s t-test: the simplest statistical test\nstats.ttest_1samp(df[\"VIQ\"], 0)\n# VIQ의 평균값이 0과 통계적으로 유의미하게 다른지 알아 보겠습니다.\n\nTtest_1sampResult(statistic=30.088099970849328, pvalue=1.3289196468728067e-28)\n\n\n간단하게 결론만 말하자면, p-value가 아주 낮음(10의 -28제곱) 으로 VIQ의 평균은 0이 아니라고 말할 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#sample-t-test-1",
    "href": "posts/ipynb/python_Statistics.html#sample-t-test-1",
    "title": "파이썬 통계분석하기",
    "section": "2.2 2-sample t-test",
    "text": "2.2 2-sample t-test\n서로 다른 두개의 그룹 간 평균의 차이가 유의미 한지 여부를 판단하기 위해 시행합니다. 2-sample t-test 는 scipy.stats.ttest_ind(): 함수를 사용합니다.\n예를 들어 여자의 VIQ와 남자의 VIQ의 평균은 통계적으로 차이가 있는지 알아 보겠습니다.\n\n# 여자의 VIQ\nfemale_viq = df[df[\"Gender\"] == \"Female\"][\"VIQ\"]\n# 남자의 VIQ\nmale_viq = df[df[\"Gender\"] == \"Male\"][\"VIQ\"]\n# 두개의 리스트를 가지고 t-test실행\nstats.ttest_ind(female_viq, male_viq)\n\nTtest_indResult(statistic=-0.77261617232750113, pvalue=0.44452876778583217)\n\n\np-value가 0.44로 아주 높게 나왔습니다. 따라서 기무가설이었던 남자와 여자의 VIQ 평균에는 차이가 있다. 는 기각되고 차이가 없다 라고 결론을 낼 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#paired-tests",
    "href": "posts/ipynb/python_Statistics.html#paired-tests",
    "title": "파이썬 통계분석하기",
    "section": "2.3 Paired tests:",
    "text": "2.3 Paired tests:\nPaired t-test는 동일한 집단에서의 반복적인 측정에 의한 차이를 비교하기 위해 사용됩니다. 예를 들면 커피가 수면시간에 미치는 영향을 보기 위해 커피를 마시지 않고 측정하고 커피를 마시고 측정한 데이터를 수집하여 사용합니다. &gt; 전제조건을 충족하기 위해서는 실험이 길어지는 단점이 있습니다"
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#f-test",
    "href": "posts/ipynb/python_Statistics.html#f-test",
    "title": "파이썬 통계분석하기",
    "section": "4.1 F-test",
    "text": "4.1 F-test\nF-test는 두 표본의 분산에 대한 차이가 통계적으로 유의한가를 판별하는 검정기법입니다. 다른 이름으로 var-test로도 불립니다.\n\nfrom statsmodels.formula.api import ols\n\nmodel = ols(\"VIQ ~ Gender + MRI_Count + Height\", df).fit()\nprint(model.summary())\nprint(model.f_test([0, 1, 0, 0]))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    VIQ   R-squared:                       0.246\nModel:                            OLS   Adj. R-squared:                  0.181\nMethod:                 Least Squares   F-statistic:                     3.809\nDate:                Thu, 21 Dec 2017   Prob (F-statistic):             0.0184\nTime:                        15:34:54   Log-Likelihood:                -172.34\nNo. Observations:                  39   AIC:                             352.7\nDf Residuals:                      35   BIC:                             359.3\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept        166.6258     88.824      1.876      0.069     -13.696     346.948\nGender[T.Male]     8.8524     10.710      0.827      0.414     -12.890      30.595\nMRI_Count          0.0002   6.46e-05      2.615      0.013    3.78e-05       0.000\nHeight            -3.0837      1.276     -2.417      0.021      -5.674      -0.494\n==============================================================================\nOmnibus:                        7.373   Durbin-Watson:                   2.109\nProb(Omnibus):                  0.025   Jarque-Bera (JB):                2.252\nSkew:                           0.005   Prob(JB):                        0.324\nKurtosis:                       1.823   Cond. No.                     2.40e+07\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.4e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n&lt;F test: F=array([[ 0.68319608]]), p=0.4140878441244722, df_denom=35, df_num=1&gt;\n\n\nF test 결과, p-value 가 0.41로 아주 높게 나왔습니다. 따라서 기무가설을 기각하지 못합니다. 다시 말해 성별에 의한 VIQ 차이는 없다 라고 할 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/python_Statistics.html#시각화",
    "href": "posts/ipynb/python_Statistics.html#시각화",
    "title": "파이썬 통계분석하기",
    "section": "4.2 시각화",
    "text": "4.2 시각화\n시각화를 하면 통계분석에 사용된 변수간의 상관관계를 손쉽게 확인할 수 있습니다. 아래 코드는 scatter_matrix를 그리는 방법입니다.\n\n# This plotting is useful to get an intuitions on the relationships between\n# our different variables\n\n# Fill in the missing values for Height for plotting\ndf[\"Height\"].fillna(method=\"pad\", inplace=True)\n\n# The parameter 'c' is passed to plt.scatter and will control the color\n# The same holds for parameters 'marker', 'alpha' and 'cmap', that\n# control respectively the type of marker used, their transparency and\n# the colormap\nscatter_matrix(\n    df[[\"VIQ\", \"MRI_Count\", \"Height\"]],\n    c=(df[\"Gender\"] == \"Female\"),\n    marker=\"o\",\n    alpha=0.7,\n)\n\nfig = plt.gcf()\nfig.suptitle(\"purple: male, yellow: female\", size=13)\n\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html",
    "href": "posts/ipynb/scanpy_workshop_01.html",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "",
    "text": "단일 세포 RNA 시퀀싱(single-cell RNA sequencing, scRNA-seq)은 개별 세포의 전사체(transcriptome)를 분석하여 세포 간의 이질성을 탐구하는 강력한 도구이지만, 기술적 잡음과 변동성이 높아 데이터 품질 저하의 위험이 큽니다. 따라서 낮은 품질의 데이터를 사용하면 잘못된 생물학적 결론을 도출할 수 있으며 이는 후속 연구 및 임상 적용에 부정적인 영향을 미칠 수 있습니다. 그러므로 이번 글에서 scRNA-seq 데이터의 품질 관리에 대해서 알아보겠습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#품질-지표-계산하기",
    "href": "posts/ipynb/scanpy_workshop_01.html#품질-지표-계산하기",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.1 품질 지표 계산하기",
    "text": "3.1 품질 지표 계산하기\n데이터가 확보되면 이제 몇 가지 데이터 품질 지표를 계산합니다. 예를 들어 세포당 미토콘드리아 유전자의 비율, 세포당 리보솜 유전자의 비율, 세포당 헤모글로빈 유전자의 비율(적혈구 오염을 나타낼 수 있습니다.)등을 계산해 메타데이터에 추가해보죠. 이것들은 계산하는 방법에는 여러 가지가 있지만 기본적으로 calculate_qc_metrics함수를 사용하는게 편리합니다.\n일반적으로 사용되는 품질 지표의 이름과 설명은 아래 목록을 참고하세요.\n\nn_genes_by_counts: 각 세포에서 1개 이상 검출된 유전자의 수. Seurat에서는 nFeature_RNA에 해당합니다.\ntotal_counts: 각 세포에서 읽은 Read의 수. Seurat에서는 nCount_RNA에 해당합니다.\npct_counts_mt: total counts 중 미토콘드리아 유전자의 비율.\n\n\n\n\n\n\n\nNote\n\n\n\ntotal_counts와 n_genes_by_counts는 같은 의미가 아닙니다. 예를 들어 두개의 세포를 하나로 잘못 인식한 이중체(Doublet)의 경우 total_counts수는 높지만 검출된 유전자의 수는 상대적으로 적겠죠.\n\n\n\n# 미토콘드리아 유전자\nadata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n# 리보솜 유전자\nadata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n# 적혈구 유전자\nadata.var[\"hb\"] = adata.var_names.str.contains(\"^HB[^(P|E|S)]\")\n\nsc.pp.calculate_qc_metrics(\n    adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], percent_top=None, log1p=True, inplace=True\n)\n\n이제 메타데이터 슬롯에 데이터 품질 지표가 추가되었습니다. 바이올린 플롯을 통해 발현된 전체 유전자의 수, 셀당 총 유전자의 수, 미토콘드리아 유전자의 비율을 살펴보죠."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#qc-플롯",
    "href": "posts/ipynb/scanpy_workshop_01.html#qc-플롯",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.2 QC 플롯",
    "text": "3.2 QC 플롯\n\nsc.pl.violin(\n    adata,\n    [\n        \"n_genes_by_counts\",\n        \"total_counts\",\n        \"pct_counts_mt\",\n        \"pct_counts_ribo\",\n        \"pct_counts_hb\",\n    ],\n    jitter=0.4,\n    groupby=\"sample\",\n    rotation=90,\n    multi_panel=True,\n)\n\n\n\n\n\n\n\n\n위 그림에서 알 수 있듯이 샘플의 품질에 상당한 차이가 있습니다. 예를 들어 covid_15와 covid_16 샘플은 미토콘드리아 함량이 더 많은 세포를 가지고 있습니다. 반면에 리보솜 유전자의 비율은 다른 샘플보다 적게 검출되었습니다.\n다른 시각화 방법으로 QC 측정값을 산점도로 표시해봅니다.\n\nsc.pl.scatter(adata, x=\"total_counts\", y=\"n_genes_by_counts\", color=\"pct_counts_mt\")\n\n\n\n\n\n\n\n\n위 그림을 보면 미토콘드리아 유전자의 비율이 높은 세포들은 total_counts와 n_genes_by_counts의 수가 적음을 알 수 있습니다. 이것을 통해 해당 세포들의 상태가 좋지 않다는 것을 유추할 수 있습니다.\n따라서 임계값을 설정해 미토콘드리아 유전자가 너무 많이 발현되거나 유전자의 총 개수가 너무 많은 세포들을 제거합니다. 여기서 주의할 점은 가끔 QC 지표가 좋지 않은 것처럼 보이는 세포도 생물학적 의미를 가지고 있을 수 있다는 것입니다. 그러므로 가능하다면 관대한 필터링를 적용하면서 시작하는 게 좋습니다. 뿐만 아니라 여러 배치가 포함된 데이터셋의 경우 임계값이 배치 간에 매우 크게 달라질 수 있어서 각 배치에 대해서 개별적으로 데이터 품질 관리를 수행해야 합니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#필터링",
    "href": "posts/ipynb/scanpy_workshop_01.html#필터링",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.3 필터링",
    "text": "3.3 필터링\n필터링은 임계값 이하의 데이터들을 제거하는 단계입니다. 데이터의 노이즈를 줄이는 것이라고도 생각할 수 있습니다.\n\n3.3.1 검출 기반 필터링\n검출 기반 필터링은 판독 횟수가 적은 세포와 최소 지정된 수의 세포에 존재하는 유전자를 필터링하는 것입니다. 일반적으로 200개 이상의 유전자가 검출된 세포와 유전자는 최소 3개 이상의 세포 조건을 많이 사용합니다. 이 값은 사용된 라이브러리 준비 방법에 따라 크게 달라질 수 있다는 점을 염두해 두세요.\n\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\n\n# 데이터 추가\nnew_data = {\n    \"단계\": \"검출 기반 필터링\",\n    \"바코드의 수\": adata.n_obs,\n    \"유전자의 수\": adata.n_vars,\n}\ndf_report = pd.concat([df_report, pd.DataFrame([new_data])], ignore_index=True)\ndf_report\n\n\n\n\n\n\n\n\n단계\n바코드의 수\n유전자의 수\n\n\n\n\n0\n원본\n12000\n33538\n\n\n1\n검출 기반 필터링\n10664\n19491\n\n\n\n\n\n\n\n결과를 보니 바코드의 수와 유전자의 수가 동시에 줄어들었다는 것을 알 수 있습니다. 이제 어떤 유전자가 시퀀서에서 많이 읽혔는지도 확인해보죠.\n\nsc.pl.highest_expr_genes(adata, n_top=20)\n\n\n\n\n\n\n\n\n위 결과를 통해 MALAT1라는 유전자가 아주 많이 읽혔다는 것을 알 수 있으며 다른 상위 유전자들은 미토콘드리아 혹은 리보솜 유전자라는 것을 확인할 수 있습니다.\n\nMALAT1은 poly-A 서열을 가지고 있어 scRNA-seq에서 상대적으로 높게 검출되는 것으로 알려져 있습니다.\n\n이제 품질 관리 및 필터링에 중요한 몇가지 유전자에 대한 정보를 모아보겠습니다.\n\n\n3.3.2 미토콘드리아/리보솜 유전자 필터링\n미토콘드리아와 리보솜 유전자에 관련된 필터링에 대한 3가지 접근법이 있습니다.\n\n미토콘드리아 유전자 비율이 높고 리보솜 유전자 비율이 낮은 세포를 제거하기.\n데이터에서 미토콘드리아 유전자 정보를 제거하고 나머지 유전자 데이터만 사용하기.\n데이터를 스케일링을 할 때 percent_mito를 회귀시키기.(다만 이 경우 일부 세포에서 유전자 발현 정보가 매우 적게 남을 가능성이 있습니다.)\n\n만약 필터링 후에도 충분한 세포가 남아 있다면 첫번째 방법을 사용하는게 가장 현명한 방법입니다. 이 예제의 경우 대부분의 세포가 미토콘드리아 판독값이 20% 미만이기 때문에 이 값을 임계값으로 사용해 제거하겠습니다. 리보솜 유전자도 마찬가지로 판독률이 5% 미만인 세포는 제거합니다.\n\n# 미토콘드리아 유전자 필터\nadata = adata[adata.obs[\"pct_counts_mt\"] &lt; 20, :]\n\n# 리보솜 유전자 필터\nadata = adata[adata.obs[\"pct_counts_ribo\"] &gt; 5, :]\n\n# 데이터 추가\nnew_data = {\n    \"단계\": \"MT/Ribo 유전자 비율 필터링\",\n    \"바코드의 수\": adata.n_obs,\n    \"유전자의 수\": adata.n_vars,\n}\ndf_report = pd.concat([df_report, pd.DataFrame([new_data])], ignore_index=True)\ndf_report\n\n\n\n\n\n\n\n\n단계\n바코드의 수\n유전자의 수\n\n\n\n\n0\n원본\n12000\n33538\n\n\n1\n검출 기반 필터링\n10664\n19491\n\n\n2\nMT/Ribo 유전자 비율 필터링\n7431\n19491\n\n\n\n\n\n\n\n미토콘드리아와 리보솜 유전자 비율에 대한 필터링으로 바코드의 수가 감소되었다는 것을 알 수 있습니다. 다만 유전자의 수는 그대로입니다. 이제 환자 샘플별로 얼마나 줄었는지 확인해보겠습니다. 처음 바코드의 숫자는 1500개였다는 기억하세요.\n\nprint(adata.obs[\"sample\"].value_counts())\n\nsample\nctrl_13     1173\nctrl_19     1170\ncovid_17    1101\nctrl_14     1063\nctrl_5      1052\ncovid_1      900\ncovid_15     599\ncovid_16     373\nName: count, dtype: int64\n\n\n역시 covid_15, 16 샘플에서 많은 데이터가 필터링 되었네요.\n\n\n3.3.3 유전자 데이터 필터링\n이 예제 데이터에서는 미토콘드리아, 적혈구(HB) 그리고 MALAT1 유전자는 주로 기술적인 문제에 의한 것으로 보입니다. 따라서 해당 유전자에 대한 데이터를 필터링하는것이 현명해 보입니다. 아래는 해당 유전자의 데이터를 제거하는 코드입니다.\n\nmalat1 = adata.var_names.str.startswith(\"MALAT1\")\n# 미토콘드리아 유전자를 처음에 정의한 이후 다시 정의해야 합니다.\n# 발현이 낮은 유전자를 제거하기 전에 전체 오브젝트에서 계산한 값입니다.\nmito_genes = adata.var_names.str.startswith(\"MT-\")\nhb_genes = adata.var_names.str.contains(\"^HB[^(P|E|S)]\")\n\nremove = np.add(mito_genes, malat1)\nremove = np.add(remove, hb_genes)\nkeep = np.invert(remove)\n\nadata = adata[:, keep]\n\n# 행이나 열방향으로 0으로만 존재하는 값이 있는지 확인\nprint(np.any(adata.X.sum(axis=0) == 0))\nprint(np.any(adata.X.sum(axis=1) == 0))\n\nTrue\nFalse\n\n\n위 결과를 보니 유전자를 전혀 포함하지 않는 바코드 데이터가 (np.any(adata.X.sum(axis=0) == 0)값이 True임으로) 존재하는 것 같네요. 다시 filter_genes을 적용하고 확인합니다.\n\nsc.pp.filter_genes(adata, min_cells=3)\n\n# 행이나 열방향으로 0으로만 존재하는 값이 있는지 확인\nprint(np.any(adata.X.sum(axis=0) == 0))\nprint(np.any(adata.X.sum(axis=1) == 0))\n\nFalse\nFalse\n\n\n이제 모두 False임으로 문제가 없습니다. 얼마나 많은 유전자 데이터가 필터링 되었는지 확인해봅시다.\n\n# 데이터 추가\nnew_data = {\n    \"단계\": \"특정 유전자 필터링\",\n    \"바코드의 수\": adata.n_obs,\n    \"유전자의 수\": adata.n_vars,\n}\ndf_report = pd.concat([df_report, pd.DataFrame([new_data])], ignore_index=True)\ndf_report\n\n\n\n\n\n\n\n\n단계\n바코드의 수\n유전자의 수\n\n\n\n\n0\n원본\n12000\n33538\n\n\n1\n검출 기반 필터링\n10664\n19491\n\n\n2\nMT/Ribo 유전자 비율 필터링\n7431\n19491\n\n\n3\n특정 유전자 필터링\n7431\n19094\n\n\n\n\n\n\n\n유전자의 수가 약 400개 정도 감소하였습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#이중체doublet-예측",
    "href": "posts/ipynb/scanpy_workshop_01.html#이중체doublet-예측",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.4 이중체(Doublet) 예측",
    "text": "3.4 이중체(Doublet) 예측\n드롭렛(Droplet)을 만들어 분석하는 scRNA-seq 기술에서 동일한 드롭렛에 여러개의 세포가 들어가는 것은 생각보다 자주 발생하는 문제이며 이중체(Doublet)라고 부릅니다. 10X genomics사의 일반적인 Single Cell 3’ Gene Expression v3.1 assay 실험법의 경우 이중체의 비율은 세포의 양에 선형적으로 비례한다고 알려져 있습니다. 아래는 사용자 가이드에 명시된 더블렛의 비율입니다.\n\n\n\nMultiplet Rate (%)\n# of Cells Loaded\n# of Cells Recovered\n\n\n\n\n0.4%\n~825\n~500\n\n\n0.8%\n~1,650\n~1,000\n\n\n2.4%\n~4,950\n~3,000\n\n\n3.2%\n~6,300\n~4,000\n\n\n4.0%\n~8,250\n~5,000\n\n\n5.6%\n~11,550\n~7,000\n\n\n6.4%\n~13,200\n~8000\n\n\n7.2%\n~14,850\n~9,000\n\n\n8.0%\n~16,500\n~10,000\n\n\n\n예시 데이터셋은 샘플당 약 5,000개의 세포가 있었습니다. 따라서 약 9,000개 세포로 드롭렛을 만들었다고 보여지며 약 4%의 더블렛이 포함되어 있을 것으로 예측됩니다.\n하나의 세포에서 검출된 유전자 총 수가 너무 많은 경우는 이중체(Doublet; 2개의 세포가 1개로 분석되는 경우)일 가능성이 있습니다. 그러나 세포 유형 구성에 따라 더 많은 수의 유전자를 가진 세포가 있을 수도 있기 때문에 문제가 복잡해집니다.\n이중체를 예측하는 방법에는 여러가지가 있지만 최신 버전 Scanpy에는 이중체 감지를 위한 Scrublet 패키지가 포함되어 있습니다. Scrublet은 관찰된 전사체와 시뮬레이션된 이중체의 KNN을 만들어 이중체를 예측합니다. sc.pp.scrublet함수를 사용하면 doublet_score와 predicted_doublet을 .obs에 추가되고 바로 필터링하거나 클러스터링을 하고나서 나중에 doublet_score를 사용해 높은 이중점수를 가진 클러스터를 필터링할 수 있습니다.\n\n\n\n\n\n\nNote\n\n\n\n더블렛 감지를 위한 다른 방법으로 scverse 생태계에는 DoubletDetection 및 SOLO가 있습니다. 더 자세한 내용은 sc-best-practice 문서에서 확인할 수 있습니다.\n\n\n아래 코드를 사용해 이중체를 예측하고 감지된 세포의 수를 나타내겠습니다.\n\nsc.pp.scrublet(adata, batch_key=\"sample\")\nnum_doublet = sum(adata.obs[\"predicted_doublet\"])\nprint(f\"이중체로 감지된 세포의 수: {num_doublet}\")\n\n이중체로 감지된 세포의 수: 204\n\n\n이중체로 예측된 세포에서 더 많은 유전자가 검출되는지 시각화를 통해 살펴보죠.\n\n# 참/거짓 대신 싱글렛/더블렛으로 열에 추가합니다.\nadata.obs[\"doublet_info\"] = adata.obs[\"predicted_doublet\"].astype(str)\nsc.pl.violin(adata, \"n_genes_by_counts\", size=0, groupby=\"doublet_info\", rotation=45)\n\n\n\n\n\n\n\n\n확실히 이중체로 예측된 세포들이 n_genes_by_counts 값이 높다는 것을 알 수 있습니다. 이제 데이터들을 클러스터링 해서 이중체가 독자적인 클러스터를 형성하는지 확인해보죠. 가장 먼저 유전자 카운트 데이터를 정규화해야 합니다.\n\n# 카운트 데이터 저장\nadata.layers[\"counts\"] = adata.X.copy()\n# 총 개수 중앙값으로 정규화\nsc.pp.normalize_total(adata)\n# 로그값으로 변환\nsc.pp.log1p(adata)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#특징feature-선택",
    "href": "posts/ipynb/scanpy_workshop_01.html#특징feature-선택",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.5 특징(Feature) 선택",
    "text": "3.5 특징(Feature) 선택\n사실 scRNA-seq 데이터에 포함된 많은 유전자들은 모든 세포에서 발현량이 높거나 낮아서 생물학적으로 큰 의미는 없고 데이터셋의 차원(크기)을 높이기만 하는 경우가 많습니다. 따라서 특징 선택이라는 단계를 통해서 차이가 있는 유전자들만 남겨서 이후 분석의 계산량을 줄여줍니다. 여기서는 차이가 큰 유전자(highly variable genes) 2000개를 고릅니다.\n\n\n\n\n\n\nNote\n\n\n\n예전 버전의 scanpy에서는 특징 선택후 adata객체를 subset하는 과정(adata = adata[:, adata.var.highly_variable])이 필요했지만 최근 버전에는 불필요하게 변경되었습니다.\n\n\n\nsc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=\"sample\")\nsc.pl.highly_variable_genes(adata)\n\n\n\n\n\n\n\n\n\n# `regress_out` 함수를 사용한 회귀는 데이터셋에 따라 정보가 소실될 수 있습니다.\n# 따라서 여기서는 사용하지 않고 아래 코드를 주석 처리하였습니다.\n# sc.pp.regress_out(adata, [\"total_counts\", \"pct_counts_mt\"])\n# sc.pp.scale(adata, max_value=10)\n\nsc.tl.pca(adata)\nsc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\nsc.tl.umap(adata)\n\n\nsc.pl.umap(adata, color=[\"doublet_score\", \"doublet_info\", \"sample\"])\n\n\n\n\n\n\n\n\n위 그림을 비교해보면 이중체 점수가 높은 세포들은 샘플 혹은 클러스터와 연관은 없어보입니다. 따라서 doublet_info값을 사용해 예상되는 이중체를 모두 제거해 보겠습니다.\n\nadata = adata[adata.obs[\"doublet_info\"] == \"False\", :]\n\n# 데이터 추가\nnew_data = {\n    \"단계\": \"이중체 필터링\",\n    \"바코드의 수\": adata.n_obs,\n    \"유전자의 수\": adata.n_vars,\n}\ndf_report = pd.concat([df_report, pd.DataFrame([new_data])], ignore_index=True)\ndf_report\n\n\n\n\n\n\n\n\n단계\n바코드의 수\n유전자의 수\n\n\n\n\n0\n원본\n12000\n33538\n\n\n1\n검출 기반 필터링\n10664\n19491\n\n\n2\nMT/Ribo 유전자 비율 필터링\n7431\n19491\n\n\n3\n특정 유전자 필터링\n7431\n19094\n\n\n4\n이중체 필터링\n7227\n19094\n\n\n\n\n\n\n\n이중체 필터링을 통해 대략적으로 200개의 바코드가 감소하였습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#성염색체-편향bias-확인",
    "href": "posts/ipynb/scanpy_workshop_01.html#성염색체-편향bias-확인",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.6 성염색체 편향(Bias) 확인",
    "text": "3.6 성염색체 편향(Bias) 확인\n분석 결과에 성염색체에 의한 바이어스가 포함되지 않도록 애초에 scRNA-seq 실험을 계획 할때 단일한 성별로 제한하는 것이 이상적입니다. 그렇지 않다면 아래 같이 추가적인 작업을 통해 성염색체에 대한 편향 확인이 필요합니다. 예를들어 염색체Y(수컷)와 X(주로 여성) 발현량을 샘플별로 파악해보면 샘플의 성별을 쉽게 파악할 수 있습니다. 또한 샘플 메타데이터에 성별 정보가 있다면 예측 결과와 비교해 레이블이 잘못되었는지 확인할 수 있습니다.\n유전자의 염색체 위치 정보는 서열 데이터를 alingment할 때 사용한 gtf파일에서 가져오는 것이 가장 이상적입니다. 그러나 여기에서는 Biomart 도구를 사용해 염색체의 위치 정보를 얻어오는 작업을 해봅니다.\n\n# pybiomart 설치 필요\nannot = sc.queries.biomart_annotations(\n    \"hsapiens\",\n    [\n        \"ensembl_gene_id\",\n        \"external_gene_name\",\n        \"start_position\",\n        \"end_position\",\n        \"chromosome_name\",\n    ],\n).set_index(\"external_gene_name\")\n\n# adata.var[annot.columns] = annot\n\n이제 염색체 정보를 얻었으므로 세포당 염색체 Y에서 나온 읽기 비율을 계산할 수 있습니다.\n\nchrY_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"Y\"])\n\nadata.obs[\"percent_chrY\"] = (\n    np.sum(adata[:, chrY_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1 * 100\n)\n\n# 플랏을 위한 색상을 추가합니다.\nadata.obs[\"XIST-counts\"] = adata.X[:, adata.var_names.str.match(\"XIST\")].toarray()\n\n바이올린 플롯을 그려 XIST과 chrY 비율을 살펴보죠.\n\nsc.pl.violin(adata, [\"XIST-counts\", \"percent_chrY\"], jitter=0.4, groupby=\"sample\", rotation=45)\n\n\n\n\n\n\n\n\n위 그림을 통해 남성의 시료가 3개 여성의 시료가 5개임을 알 수 있습니다. 다행히도 한 쪽에 쏠려있지는 않아서 성염색체에 의한 편향은 무시해도 괜찮을 것 같습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#세포-주기-예측",
    "href": "posts/ipynb/scanpy_workshop_01.html#세포-주기-예측",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.7 세포 주기 예측",
    "text": "3.7 세포 주기 예측\n이제 세포 주기에 대한 예측을 해봅니다. 세포 주기를 예측하는 원리는 세포 주기 참조 유전자의 목록과 비교해서 유전자의 평균 발현에 대한 차이를 계산하는 것입니다. 따라서 세포 주기 참조 유전자 목록이 필요합니다. 여기에서는 Regev 연구소에서 만든 파일을 사용해 메타데이터에 S 단계에 대한 점수, G2M 단계에 대한 점수, 예측된 세포 주기 단계를 추가하겠습니다.\n\ncell_cycle_genes = [x.strip() for x in open(path_parent + \"/regev_lab_cell_cycle_genes.txt\")]\n\n# 2개의 목록으로 분할\ns_genes = cell_cycle_genes[:43]\ng2m_genes = cell_cycle_genes[43:]\n\ncell_cycle_genes = [x for x in cell_cycle_genes if x in adata.var_names]\nprint(f\"포함되어 있는 세포 주기 유전자의 수: {len(cell_cycle_genes)}\")\n\n포함되어 있는 세포 주기 유전자의 수: 94\n\n\n세포 주기를 예측하기 전에 먼저 유전자 카운트 데이터를 정규화해야 합니다. 이미 앞선 코드에서 정규화 및 로그 변환까지 되어 있는 상황임으로 데이터 스케일링만 실행하고 세포 주기를 예측합니다.\n\nsc.pp.scale(adata, max_value=10)\n# 세포 주기 예측\nsc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes)\n\n세포 주기 점수에 대해 시각화를 해봅니다. 일반적인 세포 주기는 다음 3가지로 구분됩니다.\n\nG1: 단백질 합성기, 세포의 성장기.\nS: DNA 복제기, DNA의 양이 두배가 된다.\nG2: 세포 분열 준비기\n\n\n# 바이올린 플랏\n# sc.pl.violin(adata, [\"S_score\", \"G2M_score\"], jitter=0.4, groupby=\"sample\", rotation=60)\n# 산포도 플랏\nsc.pl.scatter(adata, x=\"S_score\", y=\"G2M_score\", color=\"phase\")\n\n\n\n\n\n\n\n\n일반적인 사람 세포들은 G1기가 가장 긴것으로 알려져있습니다. 그러나 예시 데이터셋에는 G1기의 세포가 상대적으로 적어 보입니다. 이것은 세포 주기 예측의 기본 임계값이 0으로 고정되어 있기 때문으로 임계값 수정이 필요하다고 말할 수 있습니다. 따라서 세포 주기 예측은 결과 해석을 할 때 항상 주의해야 합니다.\n이제 QC과정을 거치면서 제거된 데이터들의 비율을 계산해보죠. 이것을 통해 어떤 단계를 다시 조정하는 것이 좋을지 판단할 수 있습니다.\n\n# 첫 번째 행의 바코드 수와 유전자 수 값 저장\ninitial_num_barcode = df_report.loc[0, \"바코드의 수\"]\ninitial_num_gene = df_report.loc[0, \"유전자의 수\"]\n\n# 비율 계산 후 새로운 열 추가\ndf_report[\"바코드 필터 비율\"] = df_report[\"바코드의 수\"] / initial_num_barcode\ndf_report[\"유전자 필터 비율\"] = df_report[\"유전자의 수\"] / initial_num_gene\n\ndf_report\n\n\n\n\n\n\n\n\n단계\n바코드의 수\n유전자의 수\n바코드 필터 비율\n유전자 필터 비율\n\n\n\n\n0\n원본\n12000\n33538\n1.0\n1.0\n\n\n1\n검출 기반 필터링\n10664\n19491\n0.888667\n0.581162\n\n\n2\nMT/Ribo 유전자 비율 필터링\n7431\n19491\n0.61925\n0.581162\n\n\n3\n특정 유전자 필터링\n7431\n19094\n0.61925\n0.569324\n\n\n4\n이중체 필터링\n7227\n19094\n0.60225\n0.569324\n\n\n\n\n\n\n\n위를 통해 검출 기반 필터링과 유전자 비율 필터링에서 많은 데이터가 제거되었다는 것을 알 수 있습니다. 따라서 다시 시작한다면 그 쪽의 매개변수를 조절해보는 것이 좋겠습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#데이터-정리하기",
    "href": "posts/ipynb/scanpy_workshop_01.html#데이터-정리하기",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.8 데이터 정리하기",
    "text": "3.8 데이터 정리하기\n파일을 저장하기전 전체 파일의 크기를 줄이기 위해 필요하지 않아 보이는 데이터를 제거합니다.\n\n# 정규화 된 값을 원본 Count값으로 다시 되돌리기\nadata.X = adata.layers[\"counts\"].copy()\ndel adata.layers[\"counts\"]\n\n# 필요한 obs와 var 컬럼 지정\nrequired_obs_columns = [\"type\", \"sample\", \"batch\"]  # 실제 필요한 열로 대체\nrequired_var_columns = [\"gene_ids\", \"feature_types\", \"genome\"]  # 실제 필요한 열로 대체\n\n# obs에서 필요한 열만 남기고 삭제\nobs_columns_to_drop = [col for col in adata.obs.columns if col not in required_obs_columns]\nadata.obs.drop(columns=obs_columns_to_drop, inplace=True)\n\n# var에서 필요한 열만 남기고 삭제\nvar_columns_to_drop = [col for col in adata.var.columns if col not in required_var_columns]\nadata.var.drop(columns=var_columns_to_drop, inplace=True)\n\n# uns, obsm, varm, obsp 속성 삭제\nadata.uns = {}\nadata.obsm = {}\nadata.varm = {}\nadata.obsp = {}\n\n# 데이터 확인\nprint(adata)\n\nAnnData object with n_obs × n_vars = 7227 × 19094\n    obs: 'type', 'sample', 'batch'\n    var: 'gene_ids', 'feature_types', 'genome'"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_01.html#데이터-저장",
    "href": "posts/ipynb/scanpy_workshop_01.html#데이터-저장",
    "title": "Scanpy로 scRNA-seq 분석 01",
    "section": "3.9 데이터 저장",
    "text": "3.9 데이터 저장\n마지막으로 QC된 데이터를 저장해 이후 분석에 사용합니다. Scanpy는 기본적으로 h5ad형식으로 파일을 저장합니다.\n\nadata.write_h5ad(\"./output/covid/results/scanpy_covid_qc.h5ad\", compression=\"gzip\")"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html",
    "href": "posts/ipynb/scanpy_workshop_03.html",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "",
    "text": "scRNA-seq 기술의 발달로 개별 세포 수준에서의 유전자 발현 데이터를 얻는 것이 가능해졌습니다. 세포 수준의 데이터는 생명과학 연구에서 세포 유형 및 기능적 특성을 이해하는데 중요한 정보를 제공합니다. 그러나 동시에 데이터가 너무 방대하고 복잡해 새로운 분석 도구와 알고리즘이 필요하게 되었습니다. 이번 강좌는 scanpy를 활용하여 그래프 커뮤니티 감지 알고리즘을 통해 세포를 그룹화하고 각 세포 집단의 기능적 특성을 파악하는 방법을 살펴보겠습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#라이덴leiden",
    "href": "posts/ipynb/scanpy_workshop_03.html#라이덴leiden",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "2.1 라이덴(Leiden)",
    "text": "2.1 라이덴(Leiden)\n라이덴은 루바인 알고리즘의 개선 버전으로 연결성과 높은 모듈성, 세분화 단계를 추가해 더 정확한 커뮤니티를 탐지하는 것으로 알려져 있습니다. 따라서 라이덴을 사용하는 것을 권장합니다.\n\nsc.tl.leiden(adata, key_added=\"leiden_1.0\")  # default resolution in 1.0\nsc.tl.leiden(adata, resolution=0.6, key_added=\"leiden_0.6\")\nsc.tl.leiden(adata, resolution=0.4, key_added=\"leiden_0.4\")\nsc.tl.leiden(adata, resolution=1.4, key_added=\"leiden_1.4\")\n\n시각화를 해보면 설정된 해상도 값이 높아질수록 클러스터가 세분화 된다는 것을 알 수 있습니다.\n\nsc.pl.umap(\n    adata,\n    color=[\"leiden_0.4\", \"leiden_0.6\", \"leiden_1.0\", \"leiden_1.4\"],\n    ncols=2,\n)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#루바인louvain",
    "href": "posts/ipynb/scanpy_workshop_03.html#루바인louvain",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "2.2 루바인(Louvain)",
    "text": "2.2 루바인(Louvain)\n루바인 알고리즘은 네트워크 그래프에서 클러스터를 탐지하는 데 사용되는 방법으로 주로 네트워크 모듈성 최적화를 통해 작동됩니다.\n\nsc.tl.louvain(adata, key_added=\"louvain_1.0\")  # default resolution in 1.0\nsc.tl.louvain(adata, resolution=0.6, key_added=\"louvain_0.6\")\nsc.tl.louvain(adata, resolution=0.4, key_added=\"louvain_0.4\")\nsc.tl.louvain(adata, resolution=1.4, key_added=\"louvain_1.4\")\n\nsc.pl.umap(\n    adata,\n    color=[\"louvain_0.4\", \"louvain_0.6\", \"louvain_1.0\", \"louvain_1.4\"],\n    ncols=2,\n)\n\n\n\n\n\n\n\n\n루바인도 라이덴과 마찬가지로 해상도 값이 높아지면 클러스터가 세분화됩니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#k-평균-클러스터링",
    "href": "posts/ipynb/scanpy_workshop_03.html#k-평균-클러스터링",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "2.3 K-평균 클러스터링",
    "text": "2.3 K-평균 클러스터링\nK-평균 클러스터링은 다양한 분야에서 일반적으로 사용되어온 전통적인 클러스터링 알고리즘입니다. 클러스터 수를 미리 정의해야 한다는 점이 다르며 클러스터링 결과는 초기 클러스터 중심 값(nstart)에 따라 달라지므로 여러가지 값으로 시도해보는 것이 좋습니다. 여기에서는 scikit learn패키지의 KMeans함수를 이용해 구현하겠습니다.\n\n# 저장된 PCA값을 사용\nX_pca = adata.obsm[\"Scanorama\"]\n\n# kmeans with k=5\nkmeans = KMeans(n_clusters=5, random_state=0).fit(X_pca)\nadata.obs[\"kmeans5\"] = kmeans.labels_.astype(str)\n\n# kmeans with k=10\nkmeans = KMeans(n_clusters=10, random_state=0).fit(X_pca)\nadata.obs[\"kmeans10\"] = kmeans.labels_.astype(str)\n\n# kmeans with k=15\nkmeans = KMeans(n_clusters=15, random_state=0).fit(X_pca)\nadata.obs[\"kmeans15\"] = kmeans.labels_.astype(str)\n\nsc.pl.umap(\n    adata,\n    color=[\"kmeans5\", \"kmeans10\", \"kmeans15\"],\n    ncols=2,\n)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#계층적-클러스터링",
    "href": "posts/ipynb/scanpy_workshop_03.html#계층적-클러스터링",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "2.4 계층적 클러스터링",
    "text": "2.4 계층적 클러스터링\n또 다른 일반적인 형태의 클러스터링으로 계층적 클러스터링(Hierarchical clustering)을 사용해봅니다. 계층적 클러스터링은 다음 단계로 수행됩니다.\n\n샘플 사이의 거리를 정의합니다. 가장 일반적인 방법은 유클리드 거리(두 점 사이의 직선 거리) 또는 상관 계수입니다.\n클러스터 간 거리 측정값을 연결 기준이라고 정의합니다. 예를 들어 클러스터 간의 평균 거리가 될 수 있습니다. 일반적으로 사용되는 방법은 단일, 완전, 평균, 중앙값, 중심 및 방향입니다.\n상향식 또는 하향식 방식을 사용하여 전체 샘플 중 덴드로그램을 정의합니다. 상향식은 샘플이 자체 클러스터로 시작하여 하나의 클러스터만 남을 때까지 한 쌍씩 병합하는 방식입니다. 하향식은 샘플이 모두 동일한 클러스터에서 시작하여 각 샘플이 자체 클러스터를 가질 때까지 2개로 분할되는 방식입니다.\n\n여기에서는 scikit learn패키지의 AgglomerativeClustering함수를 이용해 구현하겠습니다. K-평균 클러스터링과 마찬가지로 클러스터의 수를 지정해야 하며 사용된 매개변수인 linkage=\"ward\"은 병합되는 클러스터의 편차를 최소화하기 위함입니다.\n\ncluster = AgglomerativeClustering(n_clusters=5, linkage=\"ward\")\nadata.obs[\"hclust_5\"] = cluster.fit_predict(X_pca).astype(str)\n\ncluster = AgglomerativeClustering(n_clusters=10, linkage=\"ward\")\nadata.obs[\"hclust_10\"] = cluster.fit_predict(X_pca).astype(str)\n\ncluster = AgglomerativeClustering(n_clusters=15, linkage=\"ward\")\nadata.obs[\"hclust_15\"] = cluster.fit_predict(X_pca).astype(str)\n\nsc.pl.umap(adata, color=[\"hclust_5\", \"hclust_10\", \"hclust_15\"], ncols=2)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#t-test",
    "href": "posts/ipynb/scanpy_workshop_03.html#t-test",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "4.1 T-test",
    "text": "4.1 T-test\nt-test는 두 집단 간의 평균 차이를 비교하는 통계적 검정입니다. scRNA-seq 데이터에서는 두 그룹(예: 서로 다른 세포 유형 또는 조건) 간에 특정 유전자의 발현 수준이 유의하게 다른지 확인하는 데 사용됩니다.\n\n장점: 간단하고 계산이 빠르며, 데이터가 정규 분포를 따를 때 강력한 성능을 발휘합니다.\n단점: 데이터가 정규 분포를 따르지 않거나 분산이 다를 때 성능이 저하될 수 있습니다.\n\n\nadata.X = adata.layers[\"log1p\"].copy()\n\nsc.tl.rank_genes_groups(adata, \"louvain_0.6\", method=\"t-test\", key_added=\"t-test\")\nsc.pl.rank_genes_groups(\n    adata,\n    n_genes=25,\n    sharey=False,\n    key=\"t-test\",\n    ncols=3,\n)\n\n# 결과는 adata.uns[\"t-test\"] 슬롯에 저장됩니다.\nadata\n\n\n\n\n\n\n\n\nAnnData object with n_obs × n_vars = 7227 × 19094\n    obs: 'type', 'sample', 'batch', 'n_counts', 'leiden_1.0', 'leiden_0.6', 'leiden_0.4', 'leiden_1.4', 'louvain_1.0', 'louvain_0.6', 'louvain_0.4', 'louvain_1.4', 'kmeans5', 'kmeans10', 'kmeans15', 'hclust_5', 'hclust_10', 'hclust_15'\n    var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std', 'highly_variable_nbatches', 'highly_variable_intersection'\n    uns: 'hvg', 'log1p', 'neighbors', 'pca', 'sample_colors', 'tsne', 'umap', 'leiden_1.0', 'leiden_0.6', 'leiden_0.4', 'leiden_1.4', 'leiden_0.4_colors', 'leiden_0.6_colors', 'leiden_1.0_colors', 'leiden_1.4_colors', 'louvain_1.0', 'louvain_0.6', 'louvain_0.4', 'louvain_1.4', 'louvain_0.4_colors', 'louvain_0.6_colors', 'louvain_1.0_colors', 'louvain_1.4_colors', 'kmeans5_colors', 'kmeans10_colors', 'kmeans15_colors', 'hclust_5_colors', 'hclust_10_colors', 'hclust_15_colors', 't-test'\n    obsm: 'Scanorama', 'X_pca', 'X_tsne', 'X_umap'\n    varm: 'PCs'\n    layers: 'counts', 'log1p'\n    obsp: 'connectivities', 'distances'"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#과대추정-분산을-고려한-t-test",
    "href": "posts/ipynb/scanpy_workshop_03.html#과대추정-분산을-고려한-t-test",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "4.2 과대추정 분산을 고려한 T-test",
    "text": "4.2 과대추정 분산을 고려한 T-test\n이 방법은 표준 t-test의 변형으로, 분산을 과대추정하여 보수적인 결과를 도출합니다. 이는 특히 소규모 표본에서 통계적 검정의 신뢰도를 높이는 데 도움이 됩니다.\n\n장점: 소규모 표본에서의 검정의 신뢰도를 높이고, 분산이 과대추정될 때 발생할 수 있는 문제를 완화합니다.\n단점: 분산을 과대추정함으로 인해 실제 차이가 있는 유전자도 덜 민감하게 검출될 수 있습니다.\n\n\nsc.tl.rank_genes_groups(adata, \"louvain_0.6\", method=\"t-test_overestim_var\", key_added=\"t-test_ov\")\nsc.pl.rank_genes_groups(\n    adata,\n    n_genes=25,\n    sharey=False,\n    key=\"t-test_ov\",\n    ncols=3,\n)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#윌콕슨-순위-합계",
    "href": "posts/ipynb/scanpy_workshop_03.html#윌콕슨-순위-합계",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "4.3 윌콕슨 순위 합계",
    "text": "4.3 윌콕슨 순위 합계\n윌콩슨(wilcoxon) 순위 합계 검정은 비모수 검정으로, 두 독립된 집단의 순위합을 비교하여 차이를 평가합니다. 이는 데이터가 정규 분포를 따르지 않을 때 유용합니다.\n\n장점: 데이터가 정규 분포를 따르지 않아도 사용할 수 있으며, 이상치에 덜 민감합니다.\n단점: 순위 기반 방법이기 때문에 데이터의 세밀한 차이를 반영하지 못할 수 있습니다.\n\n\nsc.tl.rank_genes_groups(adata, \"louvain_0.6\", method=\"wilcoxon\", key_added=\"wilcoxon\")\nsc.pl.rank_genes_groups(\n    adata,\n    n_genes=25,\n    sharey=False,\n    key=\"wilcoxon\",\n    ncols=3,\n)"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#유전자-리스트-비교",
    "href": "posts/ipynb/scanpy_workshop_03.html#유전자-리스트-비교",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "4.4 유전자 리스트 비교",
    "text": "4.4 유전자 리스트 비교\n위에서 얻은 T-test, T-test_ov, Wilcox 결과를 비교해보죠. 클러스터 0에 대한 DEG 리스트를 가져와 벤다이어 그램을 그려봅니다.\n\n# cluster 0 DEG 비교, 기본적으로 상위 100개만 저장합니다.\nwc = sc.get.rank_genes_groups_df(adata, group=\"0\", key=\"wilcoxon\", pval_cutoff=0.01, log2fc_min=0)[\n    \"names\"\n]\ntt = sc.get.rank_genes_groups_df(adata, group=\"0\", key=\"t-test\", pval_cutoff=0.01, log2fc_min=0)[\n    \"names\"\n]\ntt_ov = sc.get.rank_genes_groups_df(\n    adata, group=\"0\", key=\"t-test_ov\", pval_cutoff=0.01, log2fc_min=0\n)[\"names\"]\n\n\nvenn3_unweighted([set(wc), set(tt), set(tt_ov)], (\"Wilcox\", \"T-test\", \"T-test_ov\"))\nplt.show()\n\n\n\n\n\n\n\n\n위 결과를에서 알수 있듯이 많은 DEG들은 중복됩니다. 특히 분산이 과대 추정된 T-test와 T-test 결과는 매우 유사합니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#특정-클러스터간-비교",
    "href": "posts/ipynb/scanpy_workshop_03.html#특정-클러스터간-비교",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "5.1 특정 클러스터간 비교",
    "text": "5.1 특정 클러스터간 비교\n개별 클러스터를 하나의 클러스터 또는 여러 클러스터에 대해 쌍으로 비교할 수도 있습니다. 예를 들어, 클러스터 1과 2를 비교하는 방법은 아래와 같습니다.\n\nsc.tl.rank_genes_groups(\n    adata,\n    \"louvain_0.6\",\n    groups=[\"1\"],\n    reference=\"2\",\n    method=\"wilcoxon\",\n)\nsc.pl.rank_genes_groups(adata, groups=[\"1\"], n_genes=20)\n\n\n\n\n\n\n\n\n이 두 그룹을 바이올린으로 플롯합니다.\n\n# 모든 데이터 집합에 걸쳐 바이올린과 동일한 유전자를 플롯.\nsc.pl.rank_genes_groups_violin(adata, groups=\"1\", n_genes=10)\n\n\n\n\n\n\n\n\n\n# numpy.recarray를 리스트로 변환.\nmynames = [x[0] for x in adata.uns[\"rank_genes_groups\"][\"names\"][:10]]\nsc.pl.stacked_violin(adata, mynames, groupby=\"louvain_0.6\")"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#조건에-따른-dge-분석",
    "href": "posts/ipynb/scanpy_workshop_03.html#조건에-따른-dge-분석",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "5.2 조건에 따른 DGE 분석",
    "text": "5.2 조건에 따른 DGE 분석\n하나의 클러스터 내에서 조건에 따라 어떤 유전자가 차등적으로 발현되는지에 대한 답을 구하는 방법을 알아보죠. 예를 들면 특정 세포 유형에서 환자와 대조군에서 어떤 유전자가 가장 많은 영향을 받는지 알고 싶다고 가정하는 것입니다.\n분석을 위해서는 먼저 원하는 세포 클러스터에 대한 데이터만 하위 집합하고 비교 변수(현재 우리의 경우 Covid과 Ctrl)로 type 값을 변경합니다.\n\ncl1 = adata[adata.obs[\"louvain_0.6\"] == \"4\", :]\ncl1.obs[\"type\"].value_counts()\n\nsc.tl.rank_genes_groups(cl1, \"type\", method=\"wilcoxon\", key_added=\"wilcoxon\")\nsc.pl.rank_genes_groups(cl1, n_genes=25, sharey=False, key=\"wilcoxon\")\n\n\n\n\n\n\n\n\n\nwith rc_context({\"figure.figsize\": (5, 3)}):\n    sc.pl.rank_genes_groups_violin(\n        cl1, n_genes=10, key=\"wilcoxon\", split=True, use_raw=False, size=0\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n또한 모든 클러스터에서 이러한 유전자를 유형별로 분할해 다른 세포에서도 유전자가 상향/하향 조절되는지 확인할 수 있습니다.\n\ngenes1 = sc.get.rank_genes_groups_df(cl1, group=\"Covid\", key=\"wilcoxon\")[\"names\"][:3]\ngenes2 = sc.get.rank_genes_groups_df(cl1, group=\"Ctrl\", key=\"wilcoxon\")[\"names\"][:3]\ngenes = genes1.tolist() + genes2.tolist()\ndf = sc.get.obs_df(adata, genes + [\"louvain_0.6\", \"type\"], use_raw=False)\ndf2 = df.melt(id_vars=[\"louvain_0.6\", \"type\"], value_vars=genes)\n\nwith sns.axes_style(\"white\"):\n    ax = sns.catplot(\n        x=\"louvain_0.6\",\n        y=\"value\",\n        hue=\"type\",\n        kind=\"violin\",\n        inner=None,\n        split=True,\n        col=\"variable\",\n        col_wrap=2,\n        data=df2,\n    )\n\nplt.show()\n\n\n\n\n\n\n\n\n위 결과를 보면 상위 DEG 중에는 성염색체 관련 유전자가 많이 있습니다. 이것은 시료의 성별 분포가 불균형했기 때문이며 코로나 바이러스 감염 여부와는 관련이 없습니다."
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#성염색체-유전자-데이터-제거",
    "href": "posts/ipynb/scanpy_workshop_03.html#성염색체-유전자-데이터-제거",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "5.3 성염색체 유전자 데이터 제거",
    "text": "5.3 성염색체 유전자 데이터 제거\n시료의 성별 불균형으로 인한 편향성을 제거하기 위해 성염색체 관련 유전자를 제거합니다. 다시 바이오마트(biomart)패키지를 사용해 알려진 성염색체의 목록을 불러오고 데이터에서 제외시킵니다.\n\nannot = sc.queries.biomart_annotations(\n    \"hsapiens\",\n    [\n        \"ensembl_gene_id\",\n        \"external_gene_name\",\n        \"start_position\",\n        \"end_position\",\n        \"chromosome_name\",\n    ],\n).set_index(\"external_gene_name\")\n\nchrY_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"Y\"])\nchrX_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"X\"])\n\nsex_genes = chrY_genes.union(chrX_genes)\nall_genes = cl1.var.index.tolist()\nkeep_genes = [x for x in all_genes if x not in sex_genes]\ncl1 = cl1[:, keep_genes].copy()\nprint(f\"전체 유전자의 수: {len(all_genes)}\")\nprint(f\"알려진 성염색체 유전자의 수: {len(sex_genes)}\")\nprint(f\"남겨진 유전자의 수: {len(keep_genes)}\")\n\n전체 유전자의 수: 19094\n알려진 성염색체 유전자의 수: 544\n남겨진 유전자의 수: 18550\n\n\n이제 다시 DEG 찾기를 실행합니다.\n\nsc.tl.rank_genes_groups(cl1, \"type\", method=\"wilcoxon\", key_added=\"wilcoxon\")\nsc.pl.rank_genes_groups(cl1, n_genes=25, sharey=False, key=\"wilcoxon\")"
  },
  {
    "objectID": "posts/ipynb/scanpy_workshop_03.html#샘플별-배치-효과",
    "href": "posts/ipynb/scanpy_workshop_03.html#샘플별-배치-효과",
    "title": "Scanpy로 scRNA-seq 분석 03",
    "section": "5.4 샘플별 배치 효과",
    "text": "5.4 샘플별 배치 효과\nCovid 과 대조군(Control) 사이의 DEG를 찾고 그룹 안에서 개인별로 상위 DGE가 어떻게 표현되는지 확인해봅니다.\n\ngenes1 = sc.get.rank_genes_groups_df(cl1, group=\"Covid\", key=\"wilcoxon\")[\"names\"][:4]\ngenes2 = sc.get.rank_genes_groups_df(cl1, group=\"Ctrl\", key=\"wilcoxon\")[\"names\"][:4]\n# genes = genes1.tolist() + genes2.tolist()\n\nprint(\"Covid vs Ctrl\")\nsc.pl.violin(cl1, genes1[:3], groupby=\"sample\", rotation=90)\nsc.pl.violin(cl1, genes2[:3], groupby=\"sample\", rotation=90)\n\nCovid vs Ctrl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n위 결과에서 보이듯이 Covid에서 DEG로 검출된 유전자가 한 명의 환자(covid_17)에게만 값이 높습니다. 닷플랏을 그려서 좀 더 확인해보죠.\n\nsc.tl.filter_rank_genes_groups(\n    cl1,\n    # min_in_group_fraction=0.2,\n    # max_out_group_fraction=0.2,\n    key=\"wilcoxon\",\n    key_added=\"wilcoxon_filtered\",\n)\n\nsc.pl.rank_genes_groups_dotplot(\n    cl1,\n    groupby=\"sample\",\n    standard_scale=\"var\",\n    n_genes=10,\n    key=\"wilcoxon_filtered\",\n    dendrogram=False,\n)\n\n\n\n\n\n\n\n\n닷플랏을 보면 분명 covid_17 샘플에서만 높게 발현되는 유전자의 패턴이 보입니다. 이상하군요. 각 샘플의 수를 확인해보죠.\n\ncl1.obs[\"sample\"].value_counts()\n\nsample\nctrl_14     146\nctrl_13     132\ncovid_17    101\nctrl_5       93\ncovid_15     70\ncovid_1      37\nctrl_19      34\ncovid_16      9\nName: count, dtype: int64\n\n\n위 출력을 보면 다른 환자에 비해 covid_17 환자의 샘플 수가 현저히 많아서 발생한 현상이라는 것을 알 수 있습니다.\n\n5.4.1 다운 샘플링\nDEG 분석을 할때 반드시 고려해야 할 사항은 샘플당 동일한 갯수의 세포를 사용해야 결과가 단일 샘플에 의해 영향받지 않는다는 것입니다. 따라서 이 경우에는 모든 샘플을 34개의 세포로 다운 샘플링합니다.\n\ntarget_cells = 37\n\ntmp = [cl1[cl1.obs[\"sample\"] == s] for s in cl1.obs[\"sample\"].cat.categories]\n\nfor dat in tmp:\n    if dat.n_obs &gt; target_cells:\n        sc.pp.subsample(dat, n_obs=target_cells)\n\ncl1_sub = tmp[0].concatenate(*tmp[1:])\n\ncl1_sub.obs[\"sample\"].value_counts()\n\nsample\ncovid_1     37\ncovid_15    37\ncovid_17    37\nctrl_5      37\nctrl_13     37\nctrl_14     37\nctrl_19     34\ncovid_16     9\nName: count, dtype: int64\n\n\n\nsc.tl.rank_genes_groups(cl1_sub, \"type\", method=\"wilcoxon\", key_added=\"wilcoxon\")\nsc.pl.rank_genes_groups(cl1_sub, n_genes=25, sharey=False, key=\"wilcoxon\")\n\n\n\n\n\n\n\n\n\nsc.tl.filter_rank_genes_groups(\n    cl1_sub,\n    key=\"wilcoxon\",\n    key_added=\"wilcoxon_filtered\",\n)\n\nsc.pl.rank_genes_groups_dotplot(\n    cl1_sub,\n    groupby=\"sample\",\n    standard_scale=\"var\",\n    n_genes=10,\n    key=\"wilcoxon_filtered\",\n    dendrogram=False,\n)\n\n\n\n\n\n\n\n\n훨씬 나아졌습니다. 하지만 여전히 한 환자에 의해 지배되는 일부 유전자가 있다는 것을 알 수 있습니다.\n이런 샘플의 배치 효과 문제를 해결하는 방법에는 여러 가지가 있지만 여기서는 다루지 않습니다. 자세한 것은 sc-best-practice을 참고하세요."
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html",
    "href": "posts/ipynb/causal_inference_rules.html",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "",
    "text": "현실 데이터에서 인과 관계를 관찰할 수 있는 연관성 및 상관관계에 대한 기본 규칙에 대해 알아봅니다. 각각의 규칙을 최대한 쉽게 이해할 수 있도록 간단하게 설명한다음 매우 간단한 파이썬 코드를 사용해 어떻게 작동하는지 살펴봅니다.\n먼저 네 가지 기본 인과 관계 구조를 살펴보고 이후에 기본 규칙에 대해서 알아보겠습니다. 실제로 데이터를 분석할때에는 여기서 살펴본 규칙들을 복합적으로 사용해야 합니다. 따라서 인과 관계 추론에 하여 배우려면 Hernán과 Robins의 책 과 HarvardX 의 인과 관계 추론 과정을 배우는 것을 추천드립니다."
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#완전한-독립",
    "href": "posts/ipynb/causal_inference_rules.html#완전한-독립",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "1.1 완전한 독립",
    "text": "1.1 완전한 독립\nA 와 B 사이에는 경로를 추적할 수 없습니다.\n\n\n\n\n\nflowchart LR\n    A~~~B"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#체인-chain",
    "href": "posts/ipynb/causal_inference_rules.html#체인-chain",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "1.2 2. 체인; Chain",
    "text": "1.2 2. 체인; Chain\n인과 관계 사슬에서 모든 화살표가 A 에서 B 를 가리키는 방향 경로를 추적할 수 있습니다. 사슬 경로는 “열린 경로”라고도 하며, 이러한 유형의 그래프는 A 와 B 사이의 상관 관계를 전달합니다 (규칙 2 참조). 체인에 세 개 이상의 변수가 포함된 경우, A 와 B 를 연결하는 변수 M 을 매개변수라고 부르기도 합니다.\n\n\n\n\n\nflowchart LR\n    A--&gt;B"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#포크-fork",
    "href": "posts/ipynb/causal_inference_rules.html#포크-fork",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "1.3 3. 포크; Fork",
    "text": "1.3 3. 포크; Fork\n인과관계 포크에서 방향이 없는 경로 (모든 화살표가 같은 방향으로 달리지 않음) 는 공통의 인과관계 조상인 C 를 통해 A 에서 B 로 추적할 수 있으며, C 를 종종 교란 변수라고 부릅니다. 포크 경로는 “개방형”이며 A 와 B 사이의 상관관계를 전달합니다 (규칙 3 참조).\n\n\n\n\n\nflowchart LR\n    C--&gt;A\n    C--&gt;B\n    A~~~B"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#콜라이더-collider",
    "href": "posts/ipynb/causal_inference_rules.html#콜라이더-collider",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "1.4 4. 콜라이더; Collider",
    "text": "1.4 4. 콜라이더; Collider\n인과관계 콜라이더에서는 방향이 지정되지 않은 경로 (모든 화살표가 같은 방향으로 달리는 것은 아님) 가 인과관계의 하위 항목 D 를 통해 A 에서 B 로 추적될 수 있으며, D 를 흔히 콜라이더 변수라고 부릅니다. 콜라이더 경로는 “닫힌” 경로이며 A 와 B 사이의 상관관계를 전달하지 않습니다 (규칙 1 참조).\n\n\n\n\n\nflowchart LR\n    A~~~B\n    B--&gt;D\n    A--&gt;D"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#중요한-가정",
    "href": "posts/ipynb/causal_inference_rules.html#중요한-가정",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.1 중요한 가정",
    "text": "2.1 중요한 가정\n\n위의 규칙은 몇 가지 중요한 가정이 충족되는 경우에만 적용되며, 아래에 나열하지만 자세히 설명하지는 않겠습니다\n허위 상관관계가 없습니다: 상관관계는 무작위적인 우연에 의해 발생하는 것이 아닙니다. 큰 수의 법칙에 따르면 데이터가 많을수록 이 가정은 더 신뢰할 수 있습니다.\n일관성: 사용자가 보는 A의 값은 A의 실제 값, 즉 비교 대상인 치료의 값이 데이터의 치료 버전과 일치하는 잘 정의된 개입에 해당합니다.\n교환 가능성: 연구자가 결정하지는 않았지만 모든 치료 가치를 받을 수 있는 조건부 확률은 측정된 공변인에 의해서만 달라집니다.\n양성: L에 대한 조건부 치료의 모든 값을 받을 확률은 0보다 큰 양수입니다.\n충실성: 인과 효과는 데이터에서 평균을 0으로 만드는 방식으로 그룹에 따라 달라지지 않습니다. A가 50%는 긍정적인 영향을 미치고 50%는 똑같이 강력한 부정적인 영향을 미치지 않으며, 이 경우 전체 인구에서 평균적으로 0의 영향을 미칩니다.\n\n이러한 가정 중 하나가 충족되지 않으면 이러한 규칙에서 설명하는 인과 효과와 관찰 데이터 간의 관계가 잠재적으로 깨질 수 있습니다."
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#규칙-1-독립-변수는-상관관계가-없음",
    "href": "posts/ipynb/causal_inference_rules.html#규칙-1-독립-변수는-상관관계가-없음",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.2 규칙 1: 독립 변수는 상관관계가 없음",
    "text": "2.2 규칙 1: 독립 변수는 상관관계가 없음\n\n\n\n\n\nflowchart LR\n    A~~~B\n\n\n\n\n\n\nA 와 B 가 인과적으로 독립적이라면 데이터에서 서로 연관되지 않습니다.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\n\nplt.style.use(\"ggplot\")\n\n# Rule 1\nn = 10000  # Number of data points\na = np.random.normal(0, 1, n)  # A is a random variable\nb = np.random.normal(0, 1, n)  # B is a random variable\n\n# Calculate correlation\ncorrelation = np.corrcoef(a, b)[0, 1]\n\n# Create scatter plot\nplt.figure(figsize=(5, 5))\nplt.scatter(a, b, s=2, alpha=0.5)\n\n# Add title and labels\nplt.xlabel(\"A\")\nplt.ylabel(\"B\")\n\n# Add correlation text to the plot\nplt.text(0.05, 0.95, f\"Correlation: {correlation:.5f}\", \n         transform=plt.gca().transAxes, \n         verticalalignment=\"top\",\n         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5))\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A~~~B\n    B--&gt;D\n    A--&gt;D\n\n\n\n\n\n\nA 와 B 는 인과적 자손 (인과적 충돌자) 을 공유하더라도 인과적으로 독립적이며, D. 공유된 자손의 두 독립적 원인은 서로 상관관계가 없습니다 (규칙 7 은은 제외)."
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#규칙-2-인과적-영향은-상관관계를-생성합니다",
    "href": "posts/ipynb/causal_inference_rules.html#규칙-2-인과적-영향은-상관관계를-생성합니다",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.3 규칙 2: 인과적 영향은 상관관계를 생성합니다",
    "text": "2.3 규칙 2: 인과적 영향은 상관관계를 생성합니다\n\n\n\n\n\nflowchart LR\n    A--&gt;B\n\n\n\n\n\n\nA 가 B 의 원인인 경우 또는 B 가 A 의 원인인 경우 데이터에서 A 와 B 는 상관관계가 있습니다.\n\n# Rule 2\n\nn = 10000  # Number of data points\na = np.random.normal(0, 1, n)  # A is a random variable\nb = a + np.random.normal(0, 1, n)  # B is a random variable\n\n# Calculate correlation\ncorrelation = np.corrcoef(a, b)[0, 1]\n\n# Create scatter plot\nplt.figure(figsize=(5, 5))\nplt.scatter(a, b, s=2, alpha=0.5)\n\n# Add title and labels\nplt.xlabel(\"A\")\nplt.ylabel(\"B\")\n\n# Add correlation text to the plot\nplt.text(0.05, 0.95, f\"Correlation: {correlation:.5f}\", \n         transform=plt.gca().transAxes, \n         verticalalignment=\"top\",\n         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5))\n\nplt.show()\n\n\n\n\n\n\n\n\n이런 인과관계는 A가 M을 유발하고 M이 다시 B를 유발하는 (중계하는)경우에도 적용됩니다.\n\n\n\n\n\nflowchart LR\n    A--&gt;M\n    M--&gt;B\n\n\n\n\n\n\n\n\n# Rule 2 (mediation)\n\nn = 10000  # Number of data points\na = np.random.normal(0, 1, n)  # A is a random variable\nm = a + np.random.normal(0, 1, n) # M is a function of A\nb = m + np.random.normal(0, 1, n)  # B is a function of M\n\n# Calculate correlation\ncorrelation = np.corrcoef(a, b)[0, 1]\n\n# Create scatter plot\nplt.figure(figsize=(5, 5))\nplt.scatter(a, b, s=2, alpha=0.5)\n\n# Add title and labels\nplt.xlabel(\"A\")\nplt.ylabel(\"B\")\n\n# Add correlation text to the plot\nplt.text(0.05, 0.95, f\"Correlation: {correlation:.5f}\", \n         transform=plt.gca().transAxes, \n         verticalalignment=\"top\",\n         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5))\n\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#규칙-3-교락은-상관-관계를-만듭니다.",
    "href": "posts/ipynb/causal_inference_rules.html#규칙-3-교락은-상관-관계를-만듭니다.",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.4 규칙 3: 교락은 상관 관계를 만듭니다.",
    "text": "2.4 규칙 3: 교락은 상관 관계를 만듭니다.\nA와 B가 공통 조상 C(인과적 포크)를 공유하는 경우, A와 B는 데이터에서 상관관계가 생깁니다. 이러한 현상을 흔히 교락 또는 ’제3의 변수 문제’라고 합니다.\n\n\n\n\n\nflowchart LR\n    C--&gt;A\n    A~~~B\n    C--&gt;B\n\n\n\n\n\n\n\n\n# Rule 3\nn = 10000  # Number of data points\nc = np.random.normal(0, 1, n)  #  C is a random variable\na = c + np.random.normal(0, 1, n) # A is a function of C\nb = c + np.random.normal(0, 1, n)  # B is a function of C\n\n# Calculate correlation\ncorrelation = np.corrcoef(a, b)[0, 1]\n\n# Create scatter plot\nplt.figure(figsize=(5, 5))\nplt.scatter(a, b, s=2, alpha=0.5)\n\n# Add title and labels\nplt.xlabel(\"A\")\nplt.ylabel(\"B\")\n\n# Add correlation text to the plot\nplt.text(0.05, 0.95, f\"Correlation: {correlation:.5f}\", \n         transform=plt.gca().transAxes, \n         verticalalignment=\"top\",\n         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5))\n\nplt.show()\n\n\n\n\n\n\n\n\n이 규칙은 C가 A 또는 B에 미치는 영향이 다른 변수를 통해 매개되는 경우에도 적용됩니다.\n\n\n\n\n\nflowchart LR\n    C--&gt;M\n    M--&gt;A\n    A~~~N\n    C--&gt;N\n    N--&gt;B"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#규칙-4-무작위-조작은-인과적-영향으로부터-변수를-보호합니다.",
    "href": "posts/ipynb/causal_inference_rules.html#규칙-4-무작위-조작은-인과적-영향으로부터-변수를-보호합니다.",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.5 규칙 4: 무작위 조작은 인과적 영향으로부터 변수를 보호합니다.",
    "text": "2.5 규칙 4: 무작위 조작은 인과적 영향으로부터 변수를 보호합니다.\nA가 조작 변수인 무작위 대조 실험에서처럼 A의 값을 무작위로 할당할 수 있는 경우 다른 변수가 A에 영향을 줄 수 없습니다.\n\n\n\n\n\nflowchart LR\n    C1--xA[\"do(A)\"]\n    C2--xA\n    C3--xA\n\n\n\n\n\n\n\ndo(A) 표기는 A의 값을 무작위화하는 것을 의미합니다. 달리 말하면, 완전한 실험 통제와 무작위화를 통해 어떤 변수도 A의 값에 영향을 미치지 못하도록 하는 것입니다."
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#규칙-5-교란-요인에-대한-통제는-해당-교란-요인으로-인해-발생하는-상관관계를-차단합니다.",
    "href": "posts/ipynb/causal_inference_rules.html#규칙-5-교란-요인에-대한-통제는-해당-교란-요인으로-인해-발생하는-상관관계를-차단합니다.",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.6 규칙 5: 교란 요인에 대한 통제는 해당 교란 요인으로 인해 발생하는 상관관계를 차단합니다.",
    "text": "2.6 규칙 5: 교란 요인에 대한 통제는 해당 교란 요인으로 인해 발생하는 상관관계를 차단합니다.\nA와 B가 공통 조상 C(인과적 포크)를 공유하는 경우, C를 제어하면 C에 의해 생성된 A와 B 간의 교란 상관관계(규칙 3)가 제거됩니다.\n\n\n\n\n\nflowchart LR\n    C{\"if(C)\"}--&gt;A\n    A~~~B\n    C--&gt;B\n\n\n\n\n\n\n\n# Rule 5\nn = 10000  # Number of data points\nc = np.random.normal(0, 1, n)  # C is a random variable\na = c + np.random.normal(0, 1, n)  # A is a function of C\nb = c + np.random.normal(0, 1, n)  # B is a function of C\n\n# Perform linear regression\nslope_a, intercept_a, _, _, _ = stats.linregress(c, a)\nslope_b, intercept_b, _, _, _ = stats.linregress(c, b)\n\n# Calculate residuals\nresiduals_a = a - (slope_a * c + intercept_a)\nresiduals_b = b - (slope_b * c + intercept_b)\n\n# Create scatter plot of residuals\nplt.figure(figsize=(5, 5))\nplt.scatter(residuals_a, residuals_b, s=2, alpha=0.5)\nplt.title(\"Scatter Plot of Residuals(Rule 5)\")\nplt.xlabel(\"Residuals of A ~ C\")\nplt.ylabel(\"Residuals of B ~ C\")\n\n# Calculate and display correlation of residuals\ncorrelation = np.corrcoef(residuals_a, residuals_b)[0, 1]\nplt.text(0.05, 0.95, f\"Correlation of Residuals: {correlation:.4f}\", \n         transform=plt.gca().transAxes, \n         verticalalignment=\"top\",\n         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5))\n\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#규칙-6-매개자를-통제하면-매개된-인과-효과로-인해-발생하는-상관관계가-차단됩니다.",
    "href": "posts/ipynb/causal_inference_rules.html#규칙-6-매개자를-통제하면-매개된-인과-효과로-인해-발생하는-상관관계가-차단됩니다.",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.7 규칙 6: 매개자를 통제하면 매개된 인과 효과로 인해 발생하는 상관관계가 차단됩니다.",
    "text": "2.7 규칙 6: 매개자를 통제하면 매개된 인과 효과로 인해 발생하는 상관관계가 차단됩니다.\n\n\n\n\n\nflowchart LR\n    A--&gt;M\n    M--&gt;B \n\n\n\n\n\n\nA가 M의 원인이고 M이 B의 원인인 경우, M을 통제하면 매개 인과 효과(규칙 2)에 의해 생성된 A와 B 간의 상관관계가 제거됩니다.\n\n# Rule 6\nn = 10000  # Number of data points\na = np.random.normal(0, 1, n)  # A is a random variable\nm = a + np.random.normal(0, 1, n)  # M is a function of A\nb = m + np.random.normal(0, 1, n)  # B is a function of M\n\n# Perform linear regression\nslope_a, intercept_a, _, _, _ = stats.linregress(m, a)\nslope_b, intercept_b, _, _, _ = stats.linregress(m, b)\n\n# Calculate residuals\nresiduals_a = a - (slope_a * m + intercept_a)\nresiduals_b = b - (slope_b * m + intercept_b)\n\n# Create scatter plot of residuals\nplt.figure(figsize=(5, 5))\nplt.scatter(residuals_a, residuals_b, s=2, alpha=0.5)\nplt.title(\"Scatter Plot of Residuals (Rule 6)\")\nplt.xlabel(\"Residuals of A ~ M\")\nplt.ylabel(\"Residuals of B ~ M\")\n\n# Calculate and display correlation of residuals\ncorrelation = np.corrcoef(residuals_a, residuals_b)[0, 1]\nplt.text(0.05, 0.95, f\"Correlation of Residuals: {correlation:.4f}\", \n         transform=plt.gca().transAxes, \n         verticalalignment=\"top\",\n         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5))\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#규칙-7-콜라이더를-제어하면-상관-관계로-이어짐",
    "href": "posts/ipynb/causal_inference_rules.html#규칙-7-콜라이더를-제어하면-상관-관계로-이어짐",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.8 규칙 7: 콜라이더를 제어하면 상관 관계로 이어짐",
    "text": "2.8 규칙 7: 콜라이더를 제어하면 상관 관계로 이어짐\n\n\n\n\n\nflowchart LR\n    A~~~B\n    B--&gt;D\n    A--&gt;D \n\n\n\n\n\n\nA와 B가 인과 관계의 하위 항목(콜라이더) D를 공유하고 D를 제어하면 데이터에서 A와 B가 상관 관계를 갖게 됩니다. 이를 흔히 “콜라이더에 대한 조건부” 또는 콜라이더 편향이라고 합니다.\n\n# Rule 7\nn = 10000  # Number of data points\na = np.random.normal(0, 1, n)  # A is a random variable\nb = np.random.normal(0, 1, n)  # B is a random variable\nd = a + b + np.random.normal(0, 1, n)  # D is a function of A and B\n\n# Perform linear regression\nslope_a, intercept_a, _, _, _ = stats.linregress(d, a)\nslope_b, intercept_b, _, _, _ = stats.linregress(d, b)\n\n# Calculate residuals\nresiduals_a = a - (slope_a * d + intercept_a)\nresiduals_b = b - (slope_b * d + intercept_b)\n\n# Create scatter plot of residuals\nplt.figure(figsize=(5, 5))\nplt.scatter(residuals_a, residuals_b, s=2, alpha=0.5)\nplt.title(\"Scatter Plot of Residuals (Rule 7)\")\nplt.xlabel(\"Residuals of A ~ D\")\nplt.ylabel(\"Residuals of B ~ D\")\n\n# Calculate and display correlation of residuals\ncorrelation = np.corrcoef(residuals_a, residuals_b)[0, 1]\nplt.text(0.05, 0.95, f\"Correlation of Residuals: {correlation:.4f}\", \n         transform=plt.gca().transAxes, \n         verticalalignment=\"top\",\n         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5))\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/ipynb/causal_inference_rules.html#규칙-8-인과-관계에-있는-자손에-대한-제어는-조상에-대한-부분적-제어입니다.",
    "href": "posts/ipynb/causal_inference_rules.html#규칙-8-인과-관계에-있는-자손에-대한-제어는-조상에-대한-부분적-제어입니다.",
    "title": "인과 관계 추론을 위한 기본 규칙",
    "section": "2.9 규칙 8: 인과 관계에 있는 자손에 대한 제어는 조상에 대한 (부분적) 제어입니다.",
    "text": "2.9 규칙 8: 인과 관계에 있는 자손에 대한 제어는 조상에 대한 (부분적) 제어입니다.\nB가 A의 자손이고 B가 제어되는 경우, A도 (부분적으로) 제어됩니다.\n\n\n\n\n\nflowchart LR\n    A--&gt;B\n\n\n\n\n\n\nB를 통제할 때 A를 통제하는 정도는 일반적으로 A가 B를 얼마나 확실하게 유발하는지에 따라 달라집니다.\n아래 예에서 C는 A와 B의 교란 변수이지만 CM을 통제하면 교란 영향을 부분적으로 차단할 수 있습니다.\n\n\n\n\n\nflowchart LR\n    C--&gt;A\n    A~~~B\n    C--&gt;B\n    C--&gt;D[\"CM\"]\n\n\n\n\n\n\nCM이 C의 약간 신뢰할 수 있는 측정값인 경우 CM을 통제하면 A와 B 간의 상관관계가 일부 제거되지만 C를 통제할 때만큼은 아닙니다:\n\n# Rule ?\nn = 10000  # Number of data points\nc = np.random.normal(0, 1, n)  # C is a random variable\na = 2*c + np.random.normal(0, 1, n)  # A is a function of C\nb = 2*c + np.random.normal(0, 1, n)  # B is a function of C\ncm = 2*c + np.random.normal(0, 1, n)  # CM is a function of C\n\n# Perform linear regressions\nslope_ac, intercept_ac, _, _, _ = stats.linregress(c, a)\nslope_bc, intercept_bc, _, _, _ = stats.linregress(c, b)\nslope_acm, intercept_acm, _, _, _ = stats.linregress(cm, a)\nslope_bcm, intercept_bcm, _, _, _ = stats.linregress(cm, b)\n\n# Calculate residuals\nresiduals_ac = a - (slope_ac * c + intercept_ac)\nresiduals_bc = b - (slope_bc * c + intercept_bc)\nresiduals_acm = a - (slope_acm * cm + intercept_acm)\nresiduals_bcm = b - (slope_bcm * cm + intercept_bcm)\n\n# Create subplots\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n\n# Function to add correlation text to plot\ndef add_correlation_text(ax, x, y):\n    corr = np.corrcoef(x, y)[0, 1]\n    ax.text(0.05, 0.95, f\"Correlation: {corr:.4f}\", \n            transform=ax.transAxes, \n            verticalalignment=\"top\",\n            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.7))\n\n# Plot 1: No control\nax1.scatter(a, b, s=2, alpha=0.5)\nax1.set_title(\"No control\")\nax1.set_xlabel(\"A\")\nax1.set_ylabel(\"B\")\nadd_correlation_text(ax1, a, b)\n\n# Plot 2: Controlling for CM\nax2.scatter(residuals_acm, residuals_bcm, s=2, alpha=0.5)\nax2.set_title(\"Controlling for CM\")\nax2.set_xlabel(\"Residuals of A ~ CM\")\nax2.set_ylabel(\"Residuals of B ~ CM\")\nadd_correlation_text(ax2, residuals_acm, residuals_bcm)\n\n# Plot 3: Controlling for C\nax3.scatter(residuals_ac, residuals_bc, s=2, alpha=0.5)\nax3.set_title(\"Controlling for C\")\nax3.set_xlabel(\"Residuals of A ~ C\")\nax3.set_ylabel(\"Residuals of B ~ C\")\nadd_correlation_text(ax3, residuals_ac, residuals_bc)\n\n# Adjust layout and display\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n상관관계가 반드시 선형적 상관관계를 의미하는 것은 아닙니다.\n이 글에서는 “상관관계”라는 용어를 많이 사용합니다. 많은 사람들에게 상관관계라는 용어는 선형적 상관관계와 동의어입니다. 하지만 제가 의미하는 바는 그렇지 않습니다. 여기서 “상관관계”는 단순히 “상호 관계”, “연관성” 또는 “상호 정보”를 의미합니다. A와 B가 상관관계가 있다는 것은 A에 어떤 일이 발생하면 B에도 체계적으로 어떤 일이 발생한다는 의미일 뿐입니다.\n예를 들어, 규칙 2는 A가 B를 유발할 때 A와 B가 선형적으로 상관관계가 있다는 것을 의미하는 것이 아니라, A가 변하면 B가 어떤 식으로든 체계적으로 변한다는 것을 의미합니다.\n간단하게 설명하기 위해 모든 예제 R 코드에서 선형 상관관계를 사용했습니다. 그러나 실제 생활에서 우리가 기대하는 상관 관계/연관 관계/상호 정보의 패턴은 전적으로 관련된 인과 관계의 기능적 형태에 따라 달라집니다."
  },
  {
    "objectID": "posts/ipynb/python_TidyData.html#더-알아-보기",
    "href": "posts/ipynb/python_TidyData.html#더-알아-보기",
    "title": "깔끔하게 데이터 정리하기",
    "section": "1.1 더 알아 보기",
    "text": "1.1 더 알아 보기\n\n2014년도에 출판된 논문 입니다.\n데이터의 출처는 이곳 입니다."
  },
  {
    "objectID": "posts/ipynb/python_TidyData.html#깔끔한-데이터tidy-data의-특징",
    "href": "posts/ipynb/python_TidyData.html#깔끔한-데이터tidy-data의-특징",
    "title": "깔끔하게 데이터 정리하기",
    "section": "1.2 깔끔한 데이터(Tidy data)의 특징",
    "text": "1.2 깔끔한 데이터(Tidy data)의 특징\nJeff Leek가 쓴 책 The Elements of Data Analytic Style에서 정의한 깔끔한 데이터는 아래와 같은 특징을 가집니다.\n\n각 변수는 개별의 열(column)으로 존재한다.\n각 관측치는 행(row)를 구성한다.\n각 표는 단 하나의 관측기준에 의해서 조직된 데이터를 저장한다.\n만약 여러개의 표가 존재한다면, 적어도 하나이상의 열(column)이 공유되어야 한다.\n\n\n변수(Variable): 예를 들면 키, 몸무게, 성별\n값(Value): 예를 들자면 152 cm, 80 kg, 여성\n관측치(Observation): 값을 측정한 단위, 여기서는 각각의 사람\n\n너무 복잡하다고 생각되신다면 아래 예시를 확인하세요.\n\n1.2.1 지저분한 데이터의 예:\n\n\n\n\n\n\n\nTreatment A\n\n\nTreatment B\n\n\n\n\n\n\nJohn Smith\n\n\n-\n\n\n2\n\n\n\n\nJane Doe\n\n\n16\n\n\n11\n\n\n\n\nMary Johnson\n\n\n3\n\n\n1\n\n\n\n\n\n\n\n1.2.2 깔끔한 데이터(Tidy data)의 예:\n\n\n\n\n\nName\n\n\nTreatment\n\n\nResult\n\n\n\n\n\n\nJohn Smith\n\n\na\n\n\n-\n\n\n\n\nJane Doe\n\n\na\n\n\n16\n\n\n\n\nMary Johnson\n\n\na\n\n\n3\n\n\n\n\nJohn Smith\n\n\nb\n\n\n2\n\n\n\n\nJane Doe\n\n\nb\n\n\n11\n\n\n\n\nMary Johnson\n\n\nb\n\n\n1"
  },
  {
    "objectID": "posts/ipynb/python_TidyData.html#열-이름column-header이-변수-이름이-아니고-값인-경우",
    "href": "posts/ipynb/python_TidyData.html#열-이름column-header이-변수-이름이-아니고-값인-경우",
    "title": "깔끔하게 데이터 정리하기",
    "section": "2.1 1. 열 이름(Column header)이 변수 이름이 아니고 값인 경우",
    "text": "2.1 1. 열 이름(Column header)이 변수 이름이 아니고 값인 경우\n\n2.1.1 Pew Research Center Dataset\n종교에 따른 개인의 수입의 관한 데이터입니다. 먼저 pandas의 read_csv기능을 사용해 파일을 읽어옵니다.\n\ndf = pd.read_csv(\"./data/pew-raw.csv\")\ndf\n\n\n\n\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n\n\n\n\n0\nAgnostic\n27\n34\n60\n81\n76\n137\n\n\n1\nAtheist\n12\n27\n37\n52\n35\n70\n\n\n2\nBuddhist\n27\n21\n30\n34\n33\n58\n\n\n3\nCatholic\n418\n617\n732\n670\n638\n1116\n\n\n4\nDont know/refused\n15\n14\n15\n11\n10\n35\n\n\n5\nEvangelical Prot\n575\n869\n1064\n982\n881\n1486\n\n\n6\nHindu\n1\n9\n7\n9\n11\n34\n\n\n7\nHistorically Black Prot\n228\n244\n236\n238\n197\n223\n\n\n8\nJehovahs Witness\n20\n27\n24\n24\n21\n30\n\n\n9\nJewish\n19\n19\n25\n25\n30\n95\n\n\n\n\n\n\n\n\nEvangelical Prot는 기독교 종파중에 하나로 ’개신교’입니다.\n\n문제점: 이 데이터들의 문제는 열 이름(columns headers)이 개인소득의 범위로 되어 있다는 것이죠.\n\n다시 말해서 보기에는 좋아보일지는 몰라도 분석하기에는 어려운 형식입니다.\n\n이러한 데이터를 Tidy data 형태로 변환하기 위해서 pandas라이브러리에서는 아주 쉬운 기능을 제공해 줍니다. 바로 melt라는 기능이지요. pivot table의 반대되는 개념으로 행이 많은 데이터를 열이 많은 데이터로 바꿔줍니다. melt는 아주 유용하기 때문에 앞으로도 자주 언급 됩니다.\n\nformatted_df = pd.melt(df, [\"religion\"], var_name=\"income\", value_name=\"freq\")\nformatted_df = formatted_df.sort_values(by=[\"religion\"])  # 종교 이름순으로 정렬\nformatted_df.head(10)  # 너무 길기 때문에 윗쪽 10개만 보겠습니다.\n\n\n\n\n\n\n\nreligion\nincome\nfreq\n\n\n\n\n0\nAgnostic\n&lt;$10k\n27\n\n\n30\nAgnostic\n$30-40k\n81\n\n\n40\nAgnostic\n$40-50k\n76\n\n\n50\nAgnostic\n$50-75k\n137\n\n\n10\nAgnostic\n$10-20k\n34\n\n\n20\nAgnostic\n$20-30k\n60\n\n\n41\nAtheist\n$40-50k\n35\n\n\n21\nAtheist\n$20-30k\n37\n\n\n11\nAtheist\n$10-20k\n27\n\n\n31\nAtheist\n$30-40k\n52\n\n\n\n\n\n\n\n이것이 Pew Research Center Dataset 의 Tidy data 형태 입니다.\n\n\n2.1.2 Billboard Top 100 Dataset\n이 데이터는 아주 오래전 같은 1999년부터 2000년까지의 빌보드차트 주간 순위 변동을 포함하고 있는 데이터 입니다.\n\ndf = pd.read_csv(\"./data/billboard.csv\", encoding=\"mac_latin2\")\ndf.head(10)\n\n\n\n\n\n\n\nyear\nartist.inverted\ntrack\ntime\ngenre\ndate.entered\ndate.peaked\nx1st.week\nx2nd.week\nx3rd.week\n...\nx67th.week\nx68th.week\nx69th.week\nx70th.week\nx71st.week\nx72nd.week\nx73rd.week\nx74th.week\nx75th.week\nx76th.week\n\n\n\n\n0\n2000\nDestiny's Child\nIndependent Women Part I\n3:38\nRock\n2000-09-23\n2000-11-18\n78\n63.0\n49.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2000\nSantana\nMaria, Maria\n4:18\nRock\n2000-02-12\n2000-04-08\n15\n8.0\n6.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2000\nSavage Garden\nI Knew I Loved You\n4:07\nRock\n1999-10-23\n2000-01-29\n71\n48.0\n43.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2000\nMadonna\nMusic\n3:45\nRock\n2000-08-12\n2000-09-16\n41\n23.0\n18.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2000\nAguilera, Christina\nCome On Over Baby (All I Want Is You)\n3:38\nRock\n2000-08-05\n2000-10-14\n57\n47.0\n45.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\n2000\nJanet\nDoesn't Really Matter\n4:17\nRock\n2000-06-17\n2000-08-26\n59\n52.0\n43.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6\n2000\nDestiny's Child\nSay My Name\n4:31\nRock\n1999-12-25\n2000-03-18\n83\n83.0\n44.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\n2000\nIglesias, Enrique\nBe With You\n3:36\nLatin\n2000-04-01\n2000-06-24\n63\n45.0\n34.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8\n2000\nSisqo\nIncomplete\n3:52\nRock\n2000-06-24\n2000-08-12\n77\n66.0\n61.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9\n2000\nLonestar\nAmazed\n4:25\nCountry\n1999-06-05\n2000-03-04\n81\n54.0\n44.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows × 83 columns\n\n\n\n문제점:\n\n열 이름(columns headers)이 값으로 구성되어 있다: x1st.week, x2st.week 등등\n순위가 100위권 밖으로 밀려나게 되면 Nan 값을 가지고, 따라서 필요없는 부수적인 값이 많아진다.\n\n위 데이터의 깔끔한 데이터를 만들기 위해서는 다시 melt 기능을 사용하겠습니다. 각각의 열이 행이 되도록 하고, 순위가 100위 밖으로 밀려난 경우(Nan값을 갖는경우)에는 행을 삭제할게요.\n\n# Melting\nid_vars = [\n    \"year\",\n    \"artist.inverted\",\n    \"track\",\n    \"time\",\n    \"genre\",\n    \"date.entered\",\n    \"date.peaked\",\n]\ndf = pd.melt(frame=df, id_vars=id_vars, var_name=\"week\", value_name=\"rank\")\n\n# Formatting\ndf[\"week\"] = (\n    df[\"week\"].str.extract(\"(\\d+)\", expand=False).astype(int)\n)  # 정규식으로 x1st.week 에서 숫자 1만 추출\ndf[\"rank\"] = df[\"rank\"].astype(int)\n\n# 필요없는 행을 삭제합니다.\ndf = df.dropna()\n\n# Create \"date\" columns\ndf[\"date\"] = (\n    pd.to_datetime(df[\"date.entered\"])\n    + pd.to_timedelta(df[\"week\"], unit=\"w\")\n    - pd.DateOffset(weeks=1)\n)\n\ndf = df[[\"year\", \"artist.inverted\", \"track\", \"time\", \"genre\", \"week\", \"rank\", \"date\"]]\ndf = df.sort_values(ascending=True, by=[\"year\", \"artist.inverted\", \"track\", \"week\", \"rank\"])\n\n# Assigning the tidy dataset to a variable for future usage\nbillboard = df\n\ndf.head(10)\n\n\n\n\n\n\n\nyear\nartist.inverted\ntrack\ntime\ngenre\nweek\nrank\ndate\n\n\n\n\n246\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n1\n87\n2000-02-26\n\n\n563\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n2\n82\n2000-03-04\n\n\n880\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n3\n72\n2000-03-11\n\n\n1197\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n4\n77\n2000-03-18\n\n\n1514\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n5\n87\n2000-03-25\n\n\n1831\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n6\n94\n2000-04-01\n\n\n2148\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n7\n99\n2000-04-08\n\n\n287\n2000\n2Ge+her\nThe Hardest Part Of Breaking Up (Is Getting Ba...\n3:15\nR&B\n1\n91\n2000-09-02\n\n\n604\n2000\n2Ge+her\nThe Hardest Part Of Breaking Up (Is Getting Ba...\n3:15\nR&B\n2\n87\n2000-09-09\n\n\n921\n2000\n2Ge+her\nThe Hardest Part Of Breaking Up (Is Getting Ba...\n3:15\nR&B\n3\n92\n2000-09-16\n\n\n\n\n\n\n\n위와 같이 데이터를 깔끔하게 만들었습니다. 하지만 track, time, genre 열을 보시면 아주 많은 중복이 있는 것을 알 수 있습니다. 이러한 점을 해결 하는 방법은 다음 예제에서 다루어 보겠습니다."
  },
  {
    "objectID": "posts/ipynb/python_TidyData.html#하나의-표에-여러가지-타입",
    "href": "posts/ipynb/python_TidyData.html#하나의-표에-여러가지-타입",
    "title": "깔끔하게 데이터 정리하기",
    "section": "2.2 2. 하나의 표에 여러가지 타입",
    "text": "2.2 2. 하나의 표에 여러가지 타입\n위에서 다루었던 빌보드차트 데이터를 가지고 데이터가 반복되는 문제를 해결해 보겠습니다.\n문제점:\n\n다양한 관측 단위(observational units), 여기서는 song 과 rank가 하나의 표에 들어 있습니다. 이를 위해서는 표를 나눌 필요가 있습니다.\n\n먼저, 각각의 노래의 자세한 내용을 담고 있는 표를 만들어 보겠습니다. 그런다음 각각의 song_id를 부여합니다. 그런다음 순위 값을 가지고 있는 표를 song_id로 정리합니다.\n\nsongs_cols = [\"year\", \"artist.inverted\", \"track\", \"time\", \"genre\"]\nsongs = billboard[songs_cols].drop_duplicates()\nsongs = songs.reset_index(drop=True)\nsongs[\"song_id\"] = songs.index\nsongs.head(10)\n\n\n\n\n\n\n\nyear\nartist.inverted\ntrack\ntime\ngenre\nsong_id\n\n\n\n\n0\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n0\n\n\n1\n2000\n2Ge+her\nThe Hardest Part Of Breaking Up (Is Getting Ba...\n3:15\nR&B\n1\n\n\n2\n2000\n3 Doors Down\nKryptonite\n3:53\nRock\n2\n\n\n3\n2000\n3 Doors Down\nLoser\n4:24\nRock\n3\n\n\n4\n2000\n504 Boyz\nWobble Wobble\n3:35\nRap\n4\n\n\n5\n2000\n98°\nGive Me Just One Night (Una Noche)\n3:24\nRock\n5\n\n\n6\n2000\nA*Teens\nDancing Queen\n3:44\nPop\n6\n\n\n7\n2000\nAaliyah\nI Don't Wanna\n4:15\nRock\n7\n\n\n8\n2000\nAaliyah\nTry Again\n4:03\nRock\n8\n\n\n9\n2000\nAdams, Yolanda\nOpen My Heart\n5:30\nGospel\n9\n\n\n\n\n\n\n\n위 와같은 새로운 표를 분리하고, 아래와 같이 순위를 포함하고 있는 표를 새로 만들어줍니다. &gt; 두개의 표를 연결하기 위해 song_id열을 만드는 것을 주의하세요\n\nranks = pd.merge(billboard, songs, on=[\"year\", \"artist.inverted\", \"track\", \"time\", \"genre\"])\nranks = ranks[[\"song_id\", \"date\", \"rank\"]]\nranks.head(10)\n\n\n\n\n\n\n\nsong_id\ndate\nrank\n\n\n\n\n0\n0\n2000-02-26\n87\n\n\n1\n0\n2000-03-04\n82\n\n\n2\n0\n2000-03-11\n72\n\n\n3\n0\n2000-03-18\n77\n\n\n4\n0\n2000-03-25\n87\n\n\n5\n0\n2000-04-01\n94\n\n\n6\n0\n2000-04-08\n99\n\n\n7\n1\n2000-09-02\n91\n\n\n8\n1\n2000-09-09\n87\n\n\n9\n1\n2000-09-16\n92"
  },
  {
    "objectID": "posts/ipynb/python_TidyData.html#다양한-변수가-하나의-열에-있는-경우-multiple-variables-stored-in-one-column",
    "href": "posts/ipynb/python_TidyData.html#다양한-변수가-하나의-열에-있는-경우-multiple-variables-stored-in-one-column",
    "title": "깔끔하게 데이터 정리하기",
    "section": "2.3 3. 다양한 변수가 하나의 열에 있는 경우 Multiple variables stored in one column",
    "text": "2.3 3. 다양한 변수가 하나의 열에 있는 경우 Multiple variables stored in one column\n\n2.3.1 Tubercolosis Example\nWHO(World Health Organization)에서 수집한 결핵환자의 기록입니다. 이 데이터에는 확인된 결핵환자의 국가, 연도, 나이, 성별이 포함되어 있습니다.\n문제점:\n\n몇개의 열(columns)에 다양한 변수가 포함되어 있습니다.(성별과 나이)\n값이 존재하지 않는 곳에 NaN과 0 이 혼재되어 있습니다.\n\n미리 알아둘 점:\n\n열의 이름에 적혀있는 “m”이나 “f”는 성별을 뜻합니다.\n열의 이름에 적혀있는 숫자는 나이대(“0-14”,“15-24”, “25-34”, “45-54”, “55-64”, “65”, “unknown”)를 나타냅니다.\n\n\ndf = pd.read_csv(\"./data/tb-raw.csv\")\ndf\n\n\n\n\n\n\n\ncountry\nyear\nm014\nm1524\nm2534\nm3544\nm4554\nm5564\nm65\nmu\nf014\n\n\n\n\n0\nAD\n2000\n0.0\n0.0\n1.0\n0.0\n0\n0\n0.0\nNaN\nNaN\n\n\n1\nAE\n2000\n2.0\n4.0\n4.0\n6.0\n5\n12\n10.0\nNaN\n3.0\n\n\n2\nAF\n2000\n52.0\n228.0\n183.0\n149.0\n129\n94\n80.0\nNaN\n93.0\n\n\n3\nAG\n2000\n0.0\n0.0\n0.0\n0.0\n0\n0\n1.0\nNaN\n1.0\n\n\n4\nAL\n2000\n2.0\n19.0\n21.0\n14.0\n24\n19\n16.0\nNaN\n3.0\n\n\n5\nAM\n2000\n2.0\n152.0\n130.0\n131.0\n63\n26\n21.0\nNaN\n1.0\n\n\n6\nAN\n2000\n0.0\n0.0\n1.0\n2.0\n0\n0\n0.0\nNaN\n0.0\n\n\n7\nAO\n2000\n186.0\n999.0\n1003.0\n912.0\n482\n312\n194.0\nNaN\n247.0\n\n\n8\nAR\n2000\n97.0\n278.0\n594.0\n402.0\n419\n368\n330.0\nNaN\n121.0\n\n\n9\nAS\n2000\nNaN\nNaN\nNaN\nNaN\n1\n1\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n이 데이터를 정리하기 위해서는 먼저 melt를 이용해 sex + age group 를 합쳐서 하나의 행으로 만들겠습니다. 그런 다음에 다시 행을 sex, age로 구분해서 정리하도록 하죠.\n\ndf = pd.melt(df, id_vars=[\"country\", \"year\"], value_name=\"cases\", var_name=\"sex_and_age\")\n\n# Extract Sex, Age lower bound and Age upper bound group\ntmp_df = df[\"sex_and_age\"].str.extract(\"(\\D)(\\d+)(\\d{2})\", expand=False)\n\n# Name columns\ntmp_df.columns = [\"sex\", \"age_lower\", \"age_upper\"]\n\n# Create `age`column based on `age_lower` and `age_upper`\ntmp_df[\"age\"] = tmp_df[\"age_lower\"] + \"-\" + tmp_df[\"age_upper\"]\n\n# Merge\ndf = pd.concat([df, tmp_df], axis=1)\n\n# Drop unnecessary columns and rows\ndf = df.drop([\"sex_and_age\", \"age_lower\", \"age_upper\"], axis=1)\ndf = df.dropna()\ndf = df.sort_values(ascending=True, by=[\"country\", \"year\", \"sex\", \"age\"])\ndf.head(10)\n\n\n\n\n\n\n\ncountry\nyear\ncases\nsex\nage\n\n\n\n\n0\nAD\n2000\n0.0\nm\n0-14\n\n\n10\nAD\n2000\n0.0\nm\n15-24\n\n\n20\nAD\n2000\n1.0\nm\n25-34\n\n\n30\nAD\n2000\n0.0\nm\n35-44\n\n\n40\nAD\n2000\n0.0\nm\n45-54\n\n\n50\nAD\n2000\n0.0\nm\n55-64\n\n\n81\nAE\n2000\n3.0\nf\n0-14\n\n\n1\nAE\n2000\n2.0\nm\n0-14\n\n\n11\nAE\n2000\n4.0\nm\n15-24\n\n\n21\nAE\n2000\n4.0\nm\n25-34\n\n\n\n\n\n\n\n이것이 정리된 결과 입니다."
  },
  {
    "objectID": "posts/ipynb/python_TidyData.html#변수가-행과-열에rows-and-columns-모두-포함되어-있는-경우",
    "href": "posts/ipynb/python_TidyData.html#변수가-행과-열에rows-and-columns-모두-포함되어-있는-경우",
    "title": "깔끔하게 데이터 정리하기",
    "section": "2.4 4. 변수가 행과 열에(rows and columns) 모두 포함되어 있는 경우",
    "text": "2.4 4. 변수가 행과 열에(rows and columns) 모두 포함되어 있는 경우\n\n2.4.1 Global Historical Climatology Network Dataset\n이것은 2010년도 멕시코 기상청(MX17004)에서 5개월 동안 측정한 기상 데이터입니다.\n\ndf = pd.read_csv(\"./data/weather-raw.csv\")\n\n문제점:\n\n변수들이 행(tmin, tmax) 과 열(days)에 존재합니다.\n\n깔끔한 데이터를 만들기 위해 melt기능을 사용해 day_raw를 각각의 열로 만들겠습니다.\n\ndf = pd.melt(df, id_vars=[\"id\", \"year\", \"month\", \"element\"], var_name=\"day_raw\")\ndf.head(10)\n\n\n\n\n\n\n\nid\nyear\nmonth\nelement\nday_raw\nvalue\n\n\n\n\n0\nMX17004\n2010\n1\ntmax\nd1\nNaN\n\n\n1\nMX17004\n2010\n1\ntmin\nd1\nNaN\n\n\n2\nMX17004\n2010\n2\ntmax\nd1\nNaN\n\n\n3\nMX17004\n2010\n2\ntmin\nd1\nNaN\n\n\n4\nMX17004\n2010\n3\ntmax\nd1\nNaN\n\n\n5\nMX17004\n2010\n3\ntmin\nd1\nNaN\n\n\n6\nMX17004\n2010\n4\ntmax\nd1\nNaN\n\n\n7\nMX17004\n2010\n4\ntmin\nd1\nNaN\n\n\n8\nMX17004\n2010\n5\ntmax\nd1\nNaN\n\n\n9\nMX17004\n2010\n5\ntmin\nd1\nNaN\n\n\n\n\n\n\n\n그럼에도 아직 불필요한 것들이 보이는 군요. 좀 더 깔끔한 데이터를 만들기 위해 tmin, tmax를 각각의 열로 만들겠습니다. 그리고 날짜 정보들을 합쳐서 date로 통합하겠습니다.\n\n# Extracting day\ndf[\"day\"] = df[\"day_raw\"].str.extract(\"d(\\d+)\", expand=False)\ndf[\"id\"] = \"MX17004\"\n\n# To numeric values\ndf[[\"year\", \"month\", \"day\"]] = df[[\"year\", \"month\", \"day\"]].apply(\n    lambda x: pd.to_numeric(x, errors=\"ignore\")\n)\n\n\n# Creating a date from the different columns\ndef create_date_from_year_month_day(row):\n    return datetime.datetime(year=row[\"year\"], month=int(row[\"month\"]), day=row[\"day\"])\n\n\ndf[\"date\"] = df.apply(lambda row: create_date_from_year_month_day(row), axis=1)\ndf = df.drop([\"year\", \"month\", \"day\", \"day_raw\"], axis=1)\ndf = df.dropna()\n\n# Unmelting column \"element\"\ndf = df.pivot_table(index=[\"id\", \"date\"], columns=\"element\", values=\"value\")\ndf.reset_index(drop=False, inplace=True)\ndf\n\n\n\n\n\n\nelement\nid\ndate\ntmax\ntmin\n\n\n\n\n0\nMX17004\n2010-02-02\n27.3\n14.4\n\n\n1\nMX17004\n2010-02-03\n24.1\n14.4\n\n\n2\nMX17004\n2010-03-05\n32.1\n14.2\n\n\n\n\n\n\n\n충분히 깔끔한 모양새가 되었습니다."
  },
  {
    "objectID": "posts/ipynb/python_TidyData.html#하나의-관측-단위observational-units가-여러-파일로-나누어져-있는-경우",
    "href": "posts/ipynb/python_TidyData.html#하나의-관측-단위observational-units가-여러-파일로-나누어져-있는-경우",
    "title": "깔끔하게 데이터 정리하기",
    "section": "2.5 5. 하나의 관측 단위(observational units)가 여러 파일로 나누어져 있는 경우",
    "text": "2.5 5. 하나의 관측 단위(observational units)가 여러 파일로 나누어져 있는 경우\n\n2.5.1 Baby Names in Illinois\n2014, 2015년도 미국 일리노이 주(Illinois)의 신생아의 (남자)이름을 수집한 데이터 입니다.\n문제점:\n\n여러 표와 파일에 데이터가 흩어져 있다.\n연도(Year)”가 파일 이름에 적혀져 있다.\n\n서로 다른 파일에 데이터가 흩어져 있어 조금 복잡한 과정이 필요합니다. 먼저 아래의 코드로 파일 리스트를 만들고 거기에서 연도 값을 뽑아냅니다. 그런 다음 각각의 파일에서 표를 만들어내고 마지막으로 concat기능으로 사용해 하나의 표로 합치겠습니다.\n\ndef extract_year(string):\n    match = re.match(\".+(\\d{4})\", string)\n    if match is not None:\n        return match.group(1)\n\n\npath = \"./data\"\nallFiles = glob.glob(path + \"/201*-baby-names-illinois.csv\")\nframe = pd.DataFrame()\ndf_list = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_, index_col=None, header=0)\n    df.columns = map(str.lower, df.columns)\n    df[\"year\"] = extract_year(file_)\n    df_list.append(df)\n\ndf = pd.concat(df_list)\ndf.head(5)\n\n\n\n\n\n\n\nrank\nname\nfrequency\nsex\nyear\n\n\n\n\n0\n1\nNoah\n837\nMale\n2014\n\n\n1\n2\nAlexander\n747\nMale\n2014\n\n\n2\n3\nWilliam\n687\nMale\n2014\n\n\n3\n4\nMichael\n680\nMale\n2014\n\n\n4\n5\nLiam\n670\nMale\n2014"
  },
  {
    "objectID": "posts/ipynb/python_TidyData.html#마치며",
    "href": "posts/ipynb/python_TidyData.html#마치며",
    "title": "깔끔하게 데이터 정리하기",
    "section": "2.6 마치며",
    "text": "2.6 마치며\n이 글에서 가장 중점으로 둔것은 파이썬으로 지저분한 데이터를 깔끔하게 만드는 것이 었습니다. 그걸을 위해 Wickham의 논문에서 사용된 데이터를 살펴 보았죠. 깔끔한 데이터(Tidy data)의 최고의 장점은 시각화(Visualization)이 쉽다는 것에 있습니다. 그것은 다음에 다루어 보도록 하겠습니다.\n앞으로는 Tidy data를 고려해서 데이터를 수집하도록 하세요. 모두의 시간은 소중하니까요."
  },
  {
    "objectID": "posts/ipynb/python_GSE_data_fetch.html",
    "href": "posts/ipynb/python_GSE_data_fetch.html",
    "title": "GSE에서 sc-RNAseq 데이터 가져오기",
    "section": "",
    "text": "GSE 데이터베이스는 고처리량 유전자 발현 데이터, 특히 유전자 발현 옴니버스(Gene Expression Omnibus) 데이터의 저장소입니다. 이는 국립생물공학정보센터(NCBI)에서 관리하는 무료 온라인 데이터베이스로, 연구자와 과학자들이 유전자 발현 데이터를 공유하고 접근하는 데 널리 사용됩니다. 이 데이터베이스는 RNA 시퀀싱 실험을 포함한 다양한 출처의 방대한 데이터셋 컬렉션을 포함하고 있고 연구자들이 다운로드하여 추가 분석과 연구 목적으로 활용할 수 있습니다. 이는 생물정보학과 유전체학 연구에 매우 귀중한 자원입니다.\n\n1 예시 데이터\n예시 데이터로 GSE231559를 불러오는 방법을 다뤄 보겠습니다. GSE231559는 2023년 9월 25일에 공개된 데이터셋으로, 대장암(CRC)의 단일 세포 수준에서의 유전자 발현 프로파일을 다루고 있습니다. 이 연구는 MD Anderson 암센터의 CRC Moon Shot 프로젝트의 일환으로 수행되었으며, 단일 세포 RNA 시퀀싱(scRNA-seq) 기술을 사용하여 대장암의 다양성을 분석했습니다. 이 데이터셋은 Oncogenic KRAS drives lipo–fibrogenesis to promote angiogenesis and colon cancer progression 논문에 사용되었습니다.\n\n\n2 원시 데이터 살펴보기\nGSE 데이터베이스에서 파일을 다운로드해서 압축을 풀어보면 GSE231559가 아래와 같이 여러개의 파일로 되어 있다는 것을 확인 할 수 있습니다.\nGSM7290760_SC10_21N_barcodes.tsv.gz  GSM7290773_SC10_5_barcodes.tsv.gz\nGSM7290760_SC10_21N_features.tsv.gz  GSM7290773_SC10_5_features.tsv.gz\nGSM7290760_SC10_21N_matrix.mtx.gz    GSM7290773_SC10_5_matrix.mtx.gz\nGSM7290761_SC10_22T_barcodes.tsv.gz  GSM7290774_SC10_7_barcodes.tsv.gz\nGSM7290761_SC10_22T_features.tsv.gz  GSM7290774_SC10_7_features.tsv.gz\nGSM7290761_SC10_22T_matrix.mtx.gz    GSM7290774_SC10_7_matrix.mtx.gz\nGSM7290762_SC10_23N_barcodes.tsv.gz  GSM7290775_SC10_8_barcodes.tsv.gz\nGSM7290762_SC10_23N_features.tsv.gz  GSM7290775_SC10_8_features.tsv.gz\nGSM7290762_SC10_23N_matrix.mtx.gz    GSM7290775_SC10_8_matrix.mtx.gz\nGSM7290763_SC10_24T_barcodes.tsv.gz  GSM7290776_SC143_17_barcodes.tsv.gz\nGSM7290763_SC10_24T_features.tsv.gz  GSM7290776_SC143_17_features.tsv.gz\nGSM7290763_SC10_24T_matrix.mtx.gz    GSM7290776_SC143_17_matrix.mtx.gz\nGSM7290764_SC10_25N_barcodes.tsv.gz  GSM7290777_SC143_7_barcodes.tsv.gz\nGSM7290764_SC10_25N_features.tsv.gz  GSM7290777_SC143_7_features.tsv.gz\nGSM7290764_SC10_25N_matrix.mtx.gz    GSM7290777_SC143_7_matrix.mtx.gz\nGSM7290765_SC10_26N_barcodes.tsv.gz  GSM7290778_SC173_1_barcodes.tsv.gz\nGSM7290765_SC10_26N_features.tsv.gz  GSM7290778_SC173_1_features.tsv.gz\nGSM7290765_SC10_26N_matrix.mtx.gz    GSM7290778_SC173_1_matrix.mtx.gz\nGSM7290766_SC10_27N_barcodes.tsv.gz  GSM7290779_SC173_2_barcodes.tsv.gz\nGSM7290766_SC10_27N_features.tsv.gz  GSM7290779_SC173_2_features.tsv.gz\nGSM7290766_SC10_27N_matrix.mtx.gz    GSM7290779_SC173_2_matrix.mtx.gz\nGSM7290767_SC10_28T_barcodes.tsv.gz  GSM7290780_SC216_1_barcodes.tsv.gz\nGSM7290767_SC10_28T_features.tsv.gz  GSM7290780_SC216_1_features.tsv.gz\nGSM7290767_SC10_28T_matrix.mtx.gz    GSM7290780_SC216_1_matrix.mtx.gz\nGSM7290768_SC10_29N_barcodes.tsv.gz  GSM7290781_SC216_2_barcodes.tsv.gz\nGSM7290768_SC10_29N_features.tsv.gz  GSM7290781_SC216_2_features.tsv.gz\nGSM7290768_SC10_29N_matrix.mtx.gz    GSM7290781_SC216_2_matrix.mtx.gz\nGSM7290769_SC10_30T_barcodes.tsv.gz  GSM7290782_SC216_3_barcodes.tsv.gz\nGSM7290769_SC10_30T_features.tsv.gz  GSM7290782_SC216_3_features.tsv.gz\nGSM7290769_SC10_30T_matrix.mtx.gz    GSM7290782_SC216_3_matrix.mtx.gz\nGSM7290770_SC10_35N_barcodes.tsv.gz  GSM7290783_SC216_5_barcodes.tsv.gz\nGSM7290770_SC10_35N_features.tsv.gz  GSM7290783_SC216_5_features.tsv.gz\nGSM7290770_SC10_35N_matrix.mtx.gz    GSM7290783_SC216_5_matrix.mtx.gz\nGSM7290771_SC10_37N_barcodes.tsv.gz  GSM7290784_SC216_6_barcodes.tsv.gz\nGSM7290771_SC10_37N_features.tsv.gz  GSM7290784_SC216_6_features.tsv.gz\nGSM7290771_SC10_37N_matrix.mtx.gz    GSM7290784_SC216_6_matrix.mtx.gz\nGSM7290772_SC10_38T_barcodes.tsv.gz  GSM7290785_SC216_7_barcodes.tsv.gz\nGSM7290772_SC10_38T_features.tsv.gz  GSM7290785_SC216_7_features.tsv.gz\nGSM7290772_SC10_38T_matrix.mtx.gz    GSM7290785_SC216_7_matrix.mtx.gz\n골치가 벌써 아파지네요. 그래도 파일명에 일정한 패턴이 있기에 아래 파이썬 코드를 사용해 갹갹의 adata 객체를 만들어서 합치면 될 것 같습니다.\n\nimport os\nimport scanpy as sc\nimport pandas as pd\nimport numpy as np\nimport anndata as ad\nfrom scipy.io import mmread\nfrom scipy.sparse import csr_matrix\nfrom typing import List, Dict, Any, Optional\nimport matplotlib.pyplot as plt\n\ndef load_mtx_file(file_path: str) -&gt; csr_matrix:\n    return mmread(file_path).transpose().tocsr()\n\ndef load_tsv_file(file_path: str, header: Optional[int] = None, names: Optional[List[str]] = None) -&gt; pd.DataFrame:\n    return pd.read_csv(file_path, sep=\"\\t\", header=header, names=names)\n\ndef create_anndata(X: csr_matrix, obs_names: List[str], var_names: np.ndarray, var_gene_ids: np.ndarray, code_name: str) -&gt; ad.AnnData:\n    adata = ad.AnnData(X=X)\n    adata.obs_names = obs_names\n    adata.var_names = var_names\n    adata.var[\"gene_ids\"] = var_gene_ids\n    adata.obs['Code_name'] = code_name\n    return adata\n\ndef process_sample(data_dir: str, sample_name: str) -&gt; Optional[ad.AnnData]:\n    matrix_file = os.path.join(data_dir, f\"{sample_name}_matrix.mtx.gz\")\n    features_file = os.path.join(data_dir, f\"{sample_name}_features.tsv.gz\")\n    barcodes_file = os.path.join(data_dir, f\"{sample_name}_barcodes.tsv.gz\")\n    code_name = sample_name.split(\"_\")[0]\n\n    if not all(os.path.exists(f) for f in [matrix_file, features_file, barcodes_file]):\n        print(f\"Missing files for {sample_name}\")\n        return None\n\n    try:\n        X = load_mtx_file(matrix_file)\n        gene_names = load_tsv_file(features_file, names=[\"gene_ids\", \"gene_symbols\", \"Gene_expression\"])\n        barcodes = load_tsv_file(barcodes_file, names=[\"barcode\"])\n\n        obs_names = [f\"{code_name}_{b}\" for b in barcodes[\"barcode\"]]\n        var_names = gene_names[\"gene_symbols\"].values\n        var_gene_ids = gene_names[\"gene_ids\"].values\n\n        adata = create_anndata(X, obs_names, var_names, var_gene_ids, code_name)\n        print(f\"Successfully read {sample_name}\")\n        return adata\n    except Exception as e:\n        print(f\"Error reading {sample_name}: {str(e)}\")\n        return None\n\ndata_dir: str = \"../input/GSE231559\"\nadatas: List[ad.AnnData] = []\n\nfor file in os.listdir(data_dir):\n    if file.endswith(\"_matrix.mtx.gz\"):\n        sample_name: str = file.replace(\"_matrix.mtx.gz\", \"\")\n        adata: Optional[ad.AnnData] = process_sample(data_dir, sample_name)\n        if adata is not None:\n            adatas.append(adata)\n\nprint(f\"Total number of processed samples: {len(adatas)}\")\n\nSuccessfully read GSM7290779_SC173_2\nSuccessfully read GSM7290775_SC10_8\nSuccessfully read GSM7290761_SC10_22T\nSuccessfully read GSM7290770_SC10_35N\nSuccessfully read GSM7290768_SC10_29N\nSuccessfully read GSM7290777_SC143_7\nSuccessfully read GSM7290776_SC143_17\nSuccessfully read GSM7290766_SC10_27N\nSuccessfully read GSM7290774_SC10_7\nSuccessfully read GSM7290763_SC10_24T\nSuccessfully read GSM7290784_SC216_6\nSuccessfully read GSM7290767_SC10_28T\nSuccessfully read GSM7290764_SC10_25N\nSuccessfully read GSM7290782_SC216_3\nSuccessfully read GSM7290780_SC216_1\nSuccessfully read GSM7290783_SC216_5\nSuccessfully read GSM7290769_SC10_30T\nSuccessfully read GSM7290773_SC10_5\nSuccessfully read GSM7290771_SC10_37N\nSuccessfully read GSM7290772_SC10_38T\nSuccessfully read GSM7290765_SC10_26N\nSuccessfully read GSM7290760_SC10_21N\nSuccessfully read GSM7290762_SC10_23N\nSuccessfully read GSM7290781_SC216_2\nSuccessfully read GSM7290785_SC216_7\nSuccessfully read GSM7290778_SC173_1\nTotal number of processed samples: 26\n\n\n성공적으로 데이터를 불어왔는지 확인하기 첫번째 객체의 데이터를 확인해봅니다.\n\nadatas[0].obs.head()\n\n\n\n\n\n\n\n\nCode_name\n\n\n\n\nGSM7290779_AAACCCAAGCTAATCC-1\nGSM7290779\n\n\nGSM7290779_AAACGAAAGTCTAGCT-1\nGSM7290779\n\n\nGSM7290779_AAAGAACAGCTCAGAG-1\nGSM7290779\n\n\nGSM7290779_AAAGAACCATGAATAG-1\nGSM7290779\n\n\nGSM7290779_AAAGGATAGAGGCTGT-1\nGSM7290779\n\n\n\n\n\n\n\n\nadatas[0].var.head()\n\n\n\n\n\n\n\n\ngene_ids\n\n\n\n\nMIR1302-2HG\nENSG00000243485\n\n\nFAM138A\nENSG00000237613\n\n\nOR4F5\nENSG00000186092\n\n\nAL627309.1\nENSG00000238009\n\n\nAL627309.3\nENSG00000239945\n\n\n\n\n\n\n\n문제없이 불러온 것 같습니다.\n\n\n3 여러 데이터를 하나로 합치기\n이제 여러 데이터를 하나의 객체로 합쳐보겠습니다. 추가로 메타 데이터를 찾아서 맵핑을 통해 환자 정보와 질병 정보를 추가합니다.\n\n\ndef make_var_names_unique(adatas: List[ad.AnnData]) -&gt; List[ad.AnnData]:\n    for adata in adatas:\n        adata.var_names_make_unique()\n    return adatas\n\ndef merge_anndata(adatas: List[ad.AnnData]) -&gt; ad.AnnData:\n    adata_merged = ad.concat(adatas, join='outer', axis=0, index_unique=\"-\")\n    all_var = pd.concat([adata.var for adata in adatas], axis=0, join=\"outer\")\n    all_var = all_var.loc[~all_var.index.duplicated(keep='first')]\n    adata_merged.var = all_var.loc[adata_merged.var_names]\n    adata_merged.obs['Code_name'] = np.concatenate([np.repeat(adata.obs['Code_name'].iloc[0], adata.n_obs) for adata in adatas])\n    return adata_merged\n\ndef create_sample_dict() -&gt; Dict[str, str]:\n    return {\n        \"GSM7290760\": \"L1_N\", \"GSM7290761\": \"L1_T\", \"GSM7290762\": \"C1_N\",\n        \"GSM7290763\": \"C1_T\", \"GSM7290764\": \"L2_N\", \"GSM7290765\": \"L3_N\",\n        \"GSM7290766\": \"L4_N\", \"GSM7290767\": \"L4_T\", \"GSM7290768\": \"C2_N\",\n        \"GSM7290769\": \"C2_T\", \"GSM7290770\": \"L5_N\", \"GSM7290771\": \"C3_N\",\n        \"GSM7290772\": \"C3_T\", \"GSM7290773\": \"C4_T\", \"GSM7290774\": \"C5_T\",\n        \"GSM7290775\": \"L6_T\", \"GSM7290776\": \"L7_N\", \"GSM7290777\": \"C6_T\",\n        \"GSM7290778\": \"L8_T\", \"GSM7290779\": \"L8_T\", \"GSM7290780\": \"L9_N\",\n        \"GSM7290781\": \"L9_T\", \"GSM7290782\": \"L10_T\", \"GSM7290783\": \"L11_T\",\n        \"GSM7290784\": \"L11_N\", \"GSM7290785\": \"L12_T\"\n    }\n\ndef map_code_to_sample(code: str, sample_dict: Dict[str, str]) -&gt; str:\n    return sample_dict.get(code, None)\n\ndef create_metadata() -&gt; pd.DataFrame:\n    metadata = {\n        'Patient': ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'L1', 'L4', 'L6', 'L8', 'L9', 'L11'],\n        'Location': ['Colon', 'Colon', 'Colon', 'Colon', 'Colon', 'Colon', 'Liver', 'Liver', 'Liver', 'Liver', 'Liver', 'Liver'],\n        'Gender': ['M', 'M', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'M'],\n        'Current_age': [47, 39, 65, 85, 72, 62, 64, 70, 68, 54, 56, 60],\n        'Stage_at_collection': ['IV', 'IV', 'IV', 'II', 'IV', 'IV', 'IV', 'IV', 'IV', 'IV', 'IV', 'IV'],\n        'MS_status': ['MSS', 'MSS', 'MSS', 'MSI', 'MSS', 'MSS', 'MSS', 'MSS', 'MSS', 'MSS', 'MSS', 'MSS']\n    }\n    meta_df = pd.DataFrame(metadata)\n    meta_df.set_index('Patient', inplace=True)\n    return meta_df\n\ndef extract_patient_id(sample: str) -&gt; str:\n    return sample.split('_')[0]\n\ndef determine_tissue(sample: str) -&gt; str:\n    parts = sample.split('_')\n    if parts[-1].endswith('T'):\n        return 'tumor'\n    elif parts[-1].endswith('N'):\n        return 'normal'\n    else:\n        return 'unknown'\n\ndef add_metadata_to_anndata(adata: ad.AnnData, meta_df: pd.DataFrame) -&gt; ad.AnnData:\n    adata.obs['Patient'] = adata.obs['Sample'].apply(extract_patient_id)\n    for column in meta_df.columns:\n        adata.obs[column] = adata.obs['Patient'].map(meta_df[column])\n    adata.obs['Tissue_type'] = adata.obs['Sample'].apply(determine_tissue)\n    return adata\n\n# Main execution\nadatas = make_var_names_unique(adatas)\nadata_merged = merge_anndata(adatas)\n\nsample_dict = create_sample_dict()\nadata_merged.obs['Sample'] = adata_merged.obs['Code_name'].apply(lambda x: map_code_to_sample(x, sample_dict))\n\nmeta_df = create_metadata()\nadata_merged = add_metadata_to_anndata(adata_merged, meta_df)\n\n# Check the result\nadata_merged.obs.head()\n\n\n\n\n\n\n\n\nCode_name\nSample\nPatient\nLocation\nGender\nCurrent_age\nStage_at_collection\nMS_status\nTissue_type\n\n\n\n\nGSM7290779_AAACCCAAGCTAATCC-1-0\nGSM7290779\nL8_T\nL8\nLiver\nM\n54.0\nIV\nMSS\ntumor\n\n\nGSM7290779_AAACGAAAGTCTAGCT-1-0\nGSM7290779\nL8_T\nL8\nLiver\nM\n54.0\nIV\nMSS\ntumor\n\n\nGSM7290779_AAAGAACAGCTCAGAG-1-0\nGSM7290779\nL8_T\nL8\nLiver\nM\n54.0\nIV\nMSS\ntumor\n\n\nGSM7290779_AAAGAACCATGAATAG-1-0\nGSM7290779\nL8_T\nL8\nLiver\nM\n54.0\nIV\nMSS\ntumor\n\n\nGSM7290779_AAAGGATAGAGGCTGT-1-0\nGSM7290779\nL8_T\nL8\nLiver\nM\n54.0\nIV\nMSS\ntumor\n\n\n\n\n\n\n\n위의 출력을 보니 데이터가 정상적으로 합쳐진 것 같습니다. 데이터의 크기도 확인해보죠.\n\nadata_merged.obs.shape\n\n(39992, 15)\n\n\n\n\n4 데이터 저장하기\n이제 데이터 분석에 사용하기 위해 결과를 h5ad 파일로 저장합니다.\n\n# 결과 저장\nadata_merged.write('../output/241015_GSE231559_merged.h5ad', compression=\"gzip\")\n\n\n\n5 마치며\n이번 글에서 우리는 GSE231559 데이터셋을 불러오고 처리하는 과정을 살펴보았습니다. 이러한 과정은 생물정보학 연구에서 매우 중요한 단계이지만, 몇 가지 도전적인 측면이 있습니다. 특히 수동으로 데이터를 합치는 과정과 메타데이터를 추가하는 부분은 모든 데이터셋마다 통일되지 않아 작업자가 직접 작성해야 한다는 점이 번거로울 수 있습니다. 각 데이터셋의 고유한 특성과 구조로 인해 이 과정을 완전히 자동화하기는 어렵기 때문입니다.\n그럼에도 불구하고, 이러한 과정을 직접 수행해보는 것은 데이터의 구조와 특성을 깊이 이해하는 데 큰 도움이 됩니다. 또한, 이를 통해 데이터 처리 능력을 향상시킬 수 있으며, 향후 유사한 작업을 더 효율적으로 수행할 수 있게 될 것입니다.\n이 글을 통해 독자 여러분이 GSE231559와 같은 데이터셋을 직접 다루어보고, 생물정보학 데이터 처리의 실제적인 측면을 경험해보시기를 바랍니다. 이러한 경험은 여러분의 연구나 프로젝트에 큰 도움이 될 것입니다.\n\n\n6 참고\n\n이 데이터를 포함하고 있는 다른 CRC scRNA-seq 데이터 모음: https://onlinelibrary.wiley.com/doi/10.1002/tox.24157"
  }
]