[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Biologist, data scientist, and hobbyist coder\n\nmail • github • blog"
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\nProficient Python, R, Linux, \\(\\rm \\LaTeX\\), markdown\nFamiliar Docker, Tensorflow, pyTorch, Django, DOE"
  },
  {
    "objectID": "posts/learn_jupyter.html",
    "href": "posts/learn_jupyter.html",
    "title": "Jupyter notebook 소개",
    "section": "",
    "text": "원래 ipython는 파이썬을 위한 향상된 대화형 커맨드라인 콘솔입니다. Jupyter notebook은 그것에서 보다 발전된 형태로 코딩과 문서화를 동시에 해서 생산성을 극대화 하는 도구입니다.\n\nprint(\"Hello!\")\n\nHello!\n\n\n\n\n파일 시스템과 상호작용을 할 수 있는 명령어로 % 기호로 시작합니다. 현재 디렉토리 위치를 출력해 보겠습니다.\n\n%pwd\n\n'/home/partrita/Documents/blog/partrita.github.io/posts'\n\n\n더 많은 매직 명령어는 %lsmagic으로 확인 할 수 있고, 각 명령어에 ?를 추가하면 추가 정보를 보여줍니다.\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n\n\n\n\n\n자동완성은 믿을 수 없을 정도로 아주 유용한 기능입니다. 코딩을 하면서 모든 것을 타이핑하는것보다 tab키를 누르면 알아서 완성해주는 기능입니다."
  },
  {
    "objectID": "posts/learn_jupyter.html#매직-명령어magic-commands",
    "href": "posts/learn_jupyter.html#매직-명령어magic-commands",
    "title": "Jupyter notebook 소개",
    "section": "",
    "text": "파일 시스템과 상호작용을 할 수 있는 명령어로 % 기호로 시작합니다. 현재 디렉토리 위치를 출력해 보겠습니다.\n\n%pwd\n\n'/home/partrita/Documents/blog/partrita.github.io/posts'\n\n\n더 많은 매직 명령어는 %lsmagic으로 확인 할 수 있고, 각 명령어에 ?를 추가하면 추가 정보를 보여줍니다.\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics."
  },
  {
    "objectID": "posts/learn_jupyter.html#자동완성-tab-completion",
    "href": "posts/learn_jupyter.html#자동완성-tab-completion",
    "title": "Jupyter notebook 소개",
    "section": "",
    "text": "자동완성은 믿을 수 없을 정도로 아주 유용한 기능입니다. 코딩을 하면서 모든 것을 타이핑하는것보다 tab키를 누르면 알아서 완성해주는 기능입니다."
  },
  {
    "objectID": "posts/learn_jupyter.html#seaborn-시각화",
    "href": "posts/learn_jupyter.html#seaborn-시각화",
    "title": "Jupyter notebook 소개",
    "section": "Seaborn 시각화",
    "text": "Seaborn 시각화\nseaborn은 사용하기 쉬운 발전된 기능을 제공 합니다.\n\n# 예제에 사용될 데이터를 읽어오자\ndf = sns.load_dataset(\"iris\")\ndf.head()  # 데이터의 모양을 확인\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n사용한 데이터셋은 붓꽃(iris) 의 3가지 종(setosa, versicolor, virginica)에 대해 꽃받침(sepal)과 꽃잎(petal)의 넓이와 길이를 정리한 데이터입니다.\n\n위 그림을 참고하시면 이해가 되실 겁니다.\n\n# pair plot을 그려본다\nsns.pairplot(df, hue=\"species\", size=2.5)\n\n\n\n\n각각의 붓꽃종에 따라 꽃받침(sepal)과 꽃잎(petal)에 어떠한 연관성이 있는 지 확인 할 수 있습니다. 예를 들면 꽃잎의 길이가 길면 넓이도 넓어지는것은 모든종에서 연관관계가 있는 것을 볼수 있습니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html",
    "href": "posts/R_Seurat_tutorials.html",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "",
    "text": "scRNA-seq는 single-cell RNA sequencing의 줄임말로, 하나의 세포에서 mRNA를 측정하는 방법입니다. 이 기술은 기존 bulk RNA-seq 방법과는 달리 하나의 세포에서 RNA를 추출하여 분석합니다. 이를 통해, 개별 세포의 유전자 발현 패턴, 전사체 감지, 변형과 발현의 상호작용 등을 이해할 수 있습니다.\n10xGenomics는 scRNA-seq 분석에서 매우 인기있는 플랫폼으로 droplet-based 방법을 사용합니다. droplet-based 방법은 cell barcoding 및 unique molecular identifier(UMI)를 사용하여 RNA-seq 라이브러리를 생성하는 공정으로 사실상 현재 scRNA seq분야에 표준으로 사용됩니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#scrna-seq과-10xgenomics",
    "href": "posts/R_Seurat_tutorials.html#scrna-seq과-10xgenomics",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "",
    "text": "scRNA-seq는 single-cell RNA sequencing의 줄임말로, 하나의 세포에서 mRNA를 측정하는 방법입니다. 이 기술은 기존 bulk RNA-seq 방법과는 달리 하나의 세포에서 RNA를 추출하여 분석합니다. 이를 통해, 개별 세포의 유전자 발현 패턴, 전사체 감지, 변형과 발현의 상호작용 등을 이해할 수 있습니다.\n10xGenomics는 scRNA-seq 분석에서 매우 인기있는 플랫폼으로 droplet-based 방법을 사용합니다. droplet-based 방법은 cell barcoding 및 unique molecular identifier(UMI)를 사용하여 RNA-seq 라이브러리를 생성하는 공정으로 사실상 현재 scRNA seq분야에 표준으로 사용됩니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#시퀀싱-데이터-준비",
    "href": "posts/R_Seurat_tutorials.html#시퀀싱-데이터-준비",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "시퀀싱 데이터 준비",
    "text": "시퀀싱 데이터 준비\n실험을 통해 다음과 같은 fastq 파일을 가지고 있다고 간주합니다. 여기에서는 학습 목적으로 아주 작은 데이터셋으로 진행합니다만 실제 데이터는 훨씬 큽니다.\n.(pbmc_1k_v3_fastqs)\n├── pbmc_1k_v3_S1_L001_I1_001.fastq.gz\n├── pbmc_1k_v3_S1_L001_R1_001.fastq.gz\n├── pbmc_1k_v3_S1_L001_R2_001.fastq.gz\n├── pbmc_1k_v3_S1_L002_I1_001.fastq.gz\n├── pbmc_1k_v3_S1_L002_R1_001.fastq.gz\n└── pbmc_1k_v3_S1_L002_R2_001.fastq.gz"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#cell-ranger-설치",
    "href": "posts/R_Seurat_tutorials.html#cell-ranger-설치",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "cell ranger 설치",
    "text": "cell ranger 설치\n여기에서는 설치 방법은 생략하고 공식 홈페이지 링크를 참조하시기 바랍니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#cell-ranger-실행",
    "href": "posts/R_Seurat_tutorials.html#cell-ranger-실행",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "cell ranger 실행",
    "text": "cell ranger 실행\ncellranger count --id=run_count_1kpbmcs \\\n   --fastqs=./pbmc_1k_v3_fastqs \\\n   --sample=pbmc_1k_v3 \\\n   --transcriptome=./refdata-gex-GRCh38-2020-A\n   --nosecondary\n위의 명령어를 통해 cell ranger를 실행할 수 있습니다. --id 는 생성되는 결과의 폴더명이며, --fastqs는 fastq 파일이 있는 폴더의 위치, --sample은 metadata에 들어가는 샘플 정보, --transcriptome는 참조 전사체의 위치 입니다.\n저의 경우는 10x Genomics 사이트에서 다음의 명령어로 다운로드 받았습니다. 참고로 참조 전사체는 실험에 사용된 시료에 따라 다르게 사용해야 합니다.\nwget https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-2020-A.tar.gz\ntar -zxvf refdata-gex-GRCh38-2020-A.tar.gz"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#cell-ranger-결과",
    "href": "posts/R_Seurat_tutorials.html#cell-ranger-결과",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "cell ranger 결과",
    "text": "cell ranger 결과\ncell ranger가 문제없이 작동했다면 out 폴더에 다음과 같은 파일과 폴더가 생겨납니다.\n.(out)\n├── analysis\n├── cloupe.cloupe\n├── filtered_feature_bc_matrix\n├── filtered_feature_bc_matrix.h5\n├── metrics_summary.csv\n├── molecule_info.h5\n├── possorted_genome_bam.bam\n├── possorted_genome_bam.bam.bai\n├── raw_feature_bc_matrix\n├── raw_feature_bc_matrix.h5\n└── web_summary.html\n이중에 Seurat 패키지가 필요로 하는 것은 filtered_feature_bc_matrix 폴더 입니다. 폴더안에는 다음과 같은 파일이 들어있습니다.\n.(filtered_feature_bc_matrix)\n├── barcodes.tsv.gz\n├── features.tsv.gz\n└── matrix.mtx.gz\n이걸로 모든 사전 준비가 완료되었습니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#사용할-패키기-불러오기",
    "href": "posts/R_Seurat_tutorials.html#사용할-패키기-불러오기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "사용할 패키기 불러오기",
    "text": "사용할 패키기 불러오기\n\nSeurat\nSeurat은 R 프로그래밍 언어로 작성된 scRNA-seq 데이터 분석을 위한 유명한 패키지 중 하나입니다. Seurat은 높은 차원의 scRNA-seq 데이터에서 유전자 발현 패턴을 탐색하고 이를 이용하여 세포 및 클러스터의 식별과 분석, 특성 제시, 시각화 등 다양한 분석 작업을 수행할 수 있습니다. Seurat은 다양한 데이터 전처리 및 정규화 기능과 함께 차원 축소, 클러스터링, 시각화, 서브셋 생성, 유전자 발현 분석, 세포간 상호작용 분석 등 다양한 분석 도구를 제공합니다. Seurat은 현재까지 업데이트와 기능 추가가 활발하게 이루어지고 있으며, scRNA-seq 분석에 필수적인 유틸리티 패키지 중 하나입니다.\n\n\nscDblFinder\nscDblFinder는 단일 세포 RNA 시퀀싱 데이터에서 더블렛(두 개의 세포가 동시에 캡처되어 하나의 세포로 보이는 것) 현상을 탐지하고 제거하기 위한 R 패키지입니다. 이 패키지는 UMI(Unique Molecular Identifier)를 기반으로하여 두 개 이상의 세포에서 동시에 탐지된 UMIs를 찾아서 더블렛으로 추정하고, 더블렛으로 추정된 셀을 제거합니다. 이를 통해 scRNA-seq 데이터의 정확도와 해석력을 높일 수 있습니다. 또한, scDblFinder는 Seurat 및 SingleCellExperiment 형식의 데이터를 지원하며, 다양한 분석 옵션을 제공하여 사용자가 데이터에 맞게 조정할 수 있습니다.\n\n\ntidyverse\ntidyverse는 데이터 분석에 필요한 필수 R 패키지들의 모음으로 데이터 처리, 시각화, 모델링, 프로그래밍 등의 다양한 작업을 수행하는 데 사용됩니다. 주요 패키지로는 ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats 등이 있습니다.\n\noptions(verbose=FALSE) # Seurat 함수들이 실행될 때 로그 메시지를 표시하지 않습니다.\noptions(tidyverse.quiet=TRUE) # tidyverse 패키지의 로그 메시지가 출력되지 않습니다.\noptions(warn=-1)\noptions(future.rng.onMisuse=\"ignore\")\n\nlibrary(Seurat)\nlibrary(tidyverse)\nlibrary(scDblFinder)\nlibrary(future) # Enable parallelization\nplan(\"multicore\", workers=30) # cpu core에 맞게 조절합니다.\n# plan()\n\n\n\n패키지 버전 확인\n\n사용한 Seurat 패키지의 버전\n\npackageVersion(\"Seurat\")\n\n[1] ‘4.3.0’\n\n\n\n\n사용한 scDblFinder 패키지의 버전\n\npackageVersion(\"scDblFinder\")\n\n[1] ‘1.12.0’"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#count-행렬은-어떻게-생겼을까",
    "href": "posts/R_Seurat_tutorials.html#count-행렬은-어떻게-생겼을까",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "count 행렬은 어떻게 생겼을까?",
    "text": "count 행렬은 어떻게 생겼을까?\n\n# 처음 5개의 세포에 있는 몇 가지 유전자를 확인해봅니다.\nseurat_obj[[\"RNA\"]]@counts[c(\"CD3D\", \"TCL1A\", \"MS4A1\"), 1:3]\n\n3 x 3 sparse Matrix of class \"dgCMatrix\"\n      AAACCCAAGGAGAGTA-1 AAACGCTTCAGCCCAG-1 AAAGAACAGACGACTG-1\nCD3D                   .                  .                  6\nTCL1A                  .                  9                  .\nMS4A1                  .                  5                  ."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#seurat-개체의-메타데이터는-어디에-저장될까",
    "href": "posts/R_Seurat_tutorials.html#seurat-개체의-메타데이터는-어디에-저장될까",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "Seurat 개체의 메타데이터는 어디에 저장될까?",
    "text": "Seurat 개체의 메타데이터는 어디에 저장될까?\nseurat_obj@meta.data 혹은 seurat_obj[[]]을 통해 메타데이터를 확인할 수 있습니다.\n\nhead(seurat_obj@meta.data, 5)\n\n\nA data.frame: 5 × 3\n\n\n\norig.ident\nnCount_RNA\nnFeature_RNA\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\nAAACCCAAGGAGAGTA-1\npbmc1k\n12861\n3871\n\n\nAAACGCTTCAGCCCAG-1\npbmc1k\n9432\n3234\n\n\nAAAGAACAGACGACTG-1\npbmc1k\n6520\n2631\n\n\nAAAGAACCAATGGCAG-1\npbmc1k\n4362\n2121\n\n\nAAAGAACGTCTGCAAT-1\npbmc1k\n9905\n3157"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#서열-데이터-품질-관리",
    "href": "posts/R_Seurat_tutorials.html#서열-데이터-품질-관리",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "서열 데이터 품질 관리",
    "text": "서열 데이터 품질 관리\nscRNA seq 데이터의 분석의 신뢰성을 얻기 위해서 데이터의 품질 관리는 필수이빈다. Seurat을 사용하면 품질관리(QC) 지표를 쉽게 탐색하고 사용자 정의 기준에 따라 세포를 필터링할 수 있습니다. 일반적으로 사용되는 QC 기준은 다음 세가지 입니다.\n\n각 세포에서 검출된 고유 유전자의 수.\n\n품질이 낮은 세포는 종종 유전자가 매우 적습니다.\n이중 또는 다중의 세포가 들어간 droplet에는 비정상적으로 유전자 수가 높습니다.\n\n각 세포에서 검출된 총 서열의 수(고유 유전자의 수와 밀접한 상관관계가 있음)\n미토콘드리아 게놈에 매핑되는 서열의 비율\n\n품질이 낮거나 죽어가는 세포에는 미토콘드리아 유전자가 많이 발견됩니다.\n\n\n\nQC 지표 시각화하기\n\n바이올린 플랏\n\nseurat_obj[[\"percent.mt\"]] &lt;- PercentageFeatureSet(seurat_obj, pattern=\"^MT-\") \n# mouse 시료의 경우 pattern을 \"^mt-\"로 변경해야 합니다.\n\nplot &lt;- VlnPlot(seurat_obj, features=c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol=3)\n\noptions(repr.plot.width=6, repr.plot.height=6)\nplot\n\n\n\n\n\n\nScatter 플랏\n\nplot1 &lt;- FeatureScatter(seurat_obj, feature1=\"nCount_RNA\", feature2=\"percent.mt\")\nplot2 &lt;- FeatureScatter(seurat_obj, feature1=\"nCount_RNA\", feature2=\"nFeature_RNA\")\n\noptions(repr.plot.width=12, repr.plot.height=6)\nplot1 + plot2\n\n\n\n\n\n\n\nQC 및 추가 분석을 위한 세포 선택하기\n위의 결과를 토대로 nFeature_RNA가 200개에서 6000개 사이이고 percent.mt가 20 이하인 세포들만 선택합니다.\n\nseurat_obj &lt;- subset(seurat_obj, subset=nFeature_RNA &gt; 200 & nFeature_RNA &lt; 6000 & percent.mt &lt; 20)\n\n아직 모든 QC 과정이 끝난 것은 아닙니다. PCA를 진행하고나서 scDblFinder 패키지를 사용해 추가적인 더블렛 데이터를 제거하겠습니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#데이터-정규화하기",
    "href": "posts/R_Seurat_tutorials.html#데이터-정규화하기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "데이터 정규화하기",
    "text": "데이터 정규화하기\nQC를 통해 일부 데이터를 제거한 다음 단계는 데이터를 정규화하는 것입니다. 여기서는 각 세포의 발현 값을 전체 발현으로 나누고 스케일 계수(기본적으로 10,000)를 곱한 다음 로그 변환하는 LogNormalize방법을 사용합니다. 이렇게 정규화된 값은 seurat_obj[[\"RNA\"]]@data에 저장됩니다.\n\nseurat_obj &lt;- NormalizeData(seurat_obj, verbose=FALSE)\nseurat_obj &lt;- FindVariableFeatures(seurat_obj, verbose=FALSE)\nseurat_obj &lt;- ScaleData(seurat_obj, verbose=FALSE)\nseurat_obj &lt;- RunPCA(seurat_obj, verbose=FALSE)\n\n\nElbow 플랏 그리기\nSeurat에서 clustering을 수행하기 전에는 몇 개의 차원(dimension)을 사용할지 결정해야 합니다. 차원의 수는 PCA와 같은 차원 축소 기법을 사용하여 줄여진 차원의 수를 의미합니다. 그리고 이 차원의 수는 클러스터링 알고리즘에 사용됩니다.\n그러나 차원의 수가 너무 적거나 많으면 적절한 클러스터링이 어려울 수 있습니다. 차원이 적을 경우 정보 손실이 크게 발생하고, 차원이 많을 경우에는 불필요한 차원의 포함으로 인해 과적합(overfitting)이 발생할 가능성이 있습니다.\n따라서 적절한 차원의 수를 선택하기 위해 elbow plot을 사용합니다. elbow plot은 차원의 수를 x축으로, 해당 차원의 데이터를 잘 설명하는 정도(예: variance)를 y축으로 나타냅니다. 이 때, 차원의 수를 늘리면 y축 값은 점점 증가하게 됩니다. 그러나 어느 지점 이후로는 y값이 더 이상 크게 증가하지 않고 평평해지는 지점이 나타나는데, 이 지점이 elbow point입니다. 이 지점 이후로는 차원을 늘려도 데이터를 잘 설명하지 못하므로, elbow point를 기준으로 적절한 차원의 수를 선택합니다. 이를 통해 데이터의 차원을 축소할 때, 적절한 차원의 수를 선택하여 과적합을 방지하고 필요한 정보만을 추출할 수 있습니다.\n\nplot &lt;- ElbowPlot(seurat_obj)\n\noptions(repr.plot.width = 6, repr.plot.height = 6)\nplot"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#umap-그리기",
    "href": "posts/R_Seurat_tutorials.html#umap-그리기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "UMAP 그리기",
    "text": "UMAP 그리기\nUMAP은 Uniform Manifold Approximation and Projection의 약자로, scRNA-seq 데이터를 시각화하기 위한 비선형 차원 축소 방법 중 하나입니다. t-SNE와 유사한 기능을 가지고 있지만, 대규모 데이터셋에서 더욱 빠르고 정확한 임베딩을 제공합니다.\nUMAP은 데이터의 국부적인 구조를 보존하는데 초점을 둡니다. 즉, 비슷한 특성을 가진 데이터들이 서로 가깝게 묶이고, 서로 다른 특성을 가진 데이터들은 더 멀리 배치되도록 임베딩을 생성합니다. 이를 통해, scRNA-seq 데이터의 복잡한 구조를 파악하고 시각화할 수 있습니다.\n\nseurat_obj &lt;- FindNeighbors(seurat_obj, dims = 1:10, verbose = FALSE)\nseurat_obj &lt;- FindClusters(seurat_obj, resolution = 0.5, verbose = FALSE)\n# 일반적으로 resolution 값은 0.1 ~ 1.0 사이의 값을 많이 사용합니다. \n# 값이 작을수록 세분화된 군집을 얻을 수 있기 때문에\n# 세포의 종류나 상태 등을 더 세부적으로 파악하고자 할 때는 작은 값이 유용합니다. \n# 반면 큰 값은 대부분의 데이터를 하나의 군집으로 묶어줌으로써 전체적인 데이터 구조를 파악하는 데에 유용할 수 있습니다.\nseurat_obj &lt;- RunUMAP(seurat_obj, dims = 1:10, verbose = FALSE)\n\nplot &lt;- DimPlot(seurat_obj, reduction = \"umap\", label=TRUE)\n\noptions(repr.plot.width = 6, repr.plot.height = 6)\nplot\n\n\n\n\n위의 UMAP 플랏을 통해 총 10개의 cluster로 나누어 졌음을 알 수 있습니다. 이제 추가적인 QC를 진행해보겠습니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#scdblfinder를-사용해-doublet-제거-하기",
    "href": "posts/R_Seurat_tutorials.html#scdblfinder를-사용해-doublet-제거-하기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "scDblFinder를 사용해 doublet 제거 하기",
    "text": "scDblFinder를 사용해 doublet 제거 하기\nscDblFinder는 클러스터 정보를 기반으로 인공적으로 생성된 더블렛을 찾아냅니다. 아래 코드를 통해 더블렛을 찾고 UMAP 플랏에 표시해보겠습니다.\n\nsce &lt;- scDblFinder(GetAssayData(seurat_obj, slot = \"counts\"), clusters = Idents(seurat_obj))\n# scDblFinder 결과 점수를 다시 Seurat 객체로 옮깁니다.\nseurat_obj$scDblFinder.score &lt;- sce$scDblFinder.score\n\np &lt;- FeaturePlot(seurat_obj, \"scDblFinder.score\", pt.size = 0.1) \noptions(repr.plot.width = 6, repr.plot.height = 6)\np\n\nAssuming the input to be a matrix of counts or expected counts.\n\n11 clusters\n\nCreating ~5000 artificial doublets...\n\nas(&lt;dgeMatrix&gt;, \"dgCMatrix\") is deprecated since Matrix 1.5-0; do as(., \"CsparseMatrix\") instead\n\nDimensional reduction\n\nEvaluating kNN...\n\nTraining model...\n\niter=0, 32 cells excluded from training.\n\niter=1, 29 cells excluded from training.\n\niter=2, 30 cells excluded from training.\n\nThreshold found:0.519\n\n30 (2.6%) doublets called\n\n\n\n\n\n\nscDblFinder 결과에서 Threshold 값이 0.519 라는 것과 총 30(2.6%)개의 더블렛이 계산되었습니다. 다시 subset()함수를 사용해 scDblFinder 값이 0.519 이하인 세포만 고르는 QC 과정을 진행하겠습니다.\n\n더블렛 제거하기\n\n# 개체 메타 데이터의 값에 대한 하위 집합만들기\nseurat_obj &lt;- subset(x = seurat_obj, subset = scDblFinder.score &lt; 0.519 )\n\nDefaultAssay(seurat_obj) &lt;- \"RNA\"  # default assay is RNA\nseurat_obj &lt;- NormalizeData(seurat_obj, verbose = FALSE)\nseurat_obj &lt;- FindVariableFeatures(seurat_obj, verbose = FALSE)\nseurat_obj &lt;- ScaleData(seurat_obj, verbose = FALSE)\nseurat_obj &lt;- RunPCA(seurat_obj, verbose = FALSE)\nseurat_obj &lt;- FindNeighbors(seurat_obj, dims = 1:10, verbose = FALSE)\nseurat_obj &lt;- FindClusters(seurat_obj, resolution = 0.5, verbose = FALSE)\nseurat_obj &lt;- RunUMAP(seurat_obj, dims = 1:10, verbose = FALSE)\n\np &lt;- DimPlot(seurat_obj, label = TRUE)\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\np"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#marker-gene-찾기",
    "href": "posts/R_Seurat_tutorials.html#marker-gene-찾기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "Marker gene 찾기",
    "text": "Marker gene 찾기\n\nmarkers &lt;- FindAllMarkers(seurat_obj, only.pos = TRUE, verbose = FALSE)\nwrite_csv(markers, \"../output/pbmc1k_marker.csv\") # 결과를 csv 파일로 저장\nmarkers %&gt;% head()\n\n\nA data.frame: 6 × 7\n\n\n\np_val\navg_log2FC\npct.1\npct.2\np_val_adj\ncluster\ngene\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;chr&gt;\n\n\n\n\nS100A12\n4.929494e-211\n3.984628\n0.979\n0.047\n9.960536e-207\n0\nS100A12\n\n\nVCAN\n3.023544e-202\n4.236786\n1.000\n0.086\n6.109374e-198\n0\nVCAN\n\n\nCD14\n3.015410e-195\n2.387003\n0.954\n0.058\n6.092936e-191\n0\nCD14\n\n\nCSF3R\n2.588854e-192\n2.561836\n0.979\n0.080\n5.231038e-188\n0\nCSF3R\n\n\nMNDA\n1.513641e-189\n3.044325\n0.993\n0.104\n3.058462e-185\n0\nMNDA\n\n\nS100A8\n1.971209e-188\n5.905527\n0.996\n0.142\n3.983026e-184\n0\nS100A8\n\n\n\n\n\nFindAllMarkers() 결과는 데이터프레임입니다. avg_log2FC는 다른 클러스터와 비교해 발현량이 얼마나 차이나는지를 의미합니다. 해당 열을 가지고 각 클러스터당 상위 5개의 유전자 마커를 추려서 heatmap 을 그려봅니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#marker-gene-heatmap",
    "href": "posts/R_Seurat_tutorials.html#marker-gene-heatmap",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "Marker gene heatmap",
    "text": "Marker gene heatmap\n\ntop5 &lt;- markers %&gt;% group_by(cluster) %&gt;% top_n(n = 5, wt = avg_log2FC)\np &lt;- DoHeatmap(seurat_obj, features = top5$gene) + NoLegend()\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\np\n\n\n\n\n위의 heatmap은 멋져보이기는 하지만 그렇게 큰 정보를 제공해주지는 않습니다. 일반적으로 클러스터가 어떤 세포인지 알아내는 작업이 scRNA seq 분석방법에서 가장 중요한 부분이며 여러가지 접근법이 있지만 가장 덜 자동화된 부분이기도 합니다.\n가장 일반적인 방법은 FindAllMarkers() 함수를 이용해 얻은 각 클러스터의 유전자 마커와 참조 데이터 세트를 비교 하는 것입니다. 참조 데이터 세트란, 이미 잘 정의된 세포 유형들의 scRNA-seq 데이터로 현재 분석 중인 데이터의 클러스터들을 참조 데이터 세트와 비교하여 유사한 패턴을 가진 세포 유형을 찾아내는 것입니다. 이 작업은 시간이 아주 많이 필요하며 작업자에 따라 다른 결과가 나옵니다. 그래서 이번에는 ChatGPT를 사용하는 방법으로 해보겠습니다."
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#chatgpt-사용하기",
    "href": "posts/R_Seurat_tutorials.html#chatgpt-사용하기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "ChatGPT 사용하기",
    "text": "ChatGPT 사용하기\n\nChatGPT에 대한 자세한 설명은 생략합니다.\n\n프롬프터에 클러스터의 유전자 마커를 쉽게 입력하기 위해 다음의 코드를 사용합니다.\n\n# 빈 리스트 생성\nmarker_list &lt;- list()\ngene_number &lt;- 10 # 10개의 유전자만 찾아낼때\n\n# for loop으로 리스트에 값 추가\nfor (i in unique(Idents(seurat_obj))) {\n  marker_list[[paste0(\"cluster\",i)]] &lt;- markers %&gt;% group_by(cluster) %&gt;% top_n(gene_number, avg_log2FC) %&gt;%\n      ungroup() %&gt;% arrange(cluster, desc(avg_log2FC)) %&gt;% filter(cluster == i) %&gt;% .$gene\n}\n\n# 결과 출력\nmarker_list\n\n\n    $cluster0\n        \n'S100A8''S100A9''LYZ''VCAN''S100A12''FCN1''MNDA''CTSS''NAMPT''PLXDC2'\n\n    $cluster3\n        \n'IGHM''AFF3''IGHD''IGLC2''FCRL1''BANK1''TCL1A''BACH2''CD79A''LINC00926'\n\n    $cluster7\n        \n'GZMH''NKG7''CCL5''SGCD''SAMD3''CST7''GZMK''TOX''GZMA''KLRG1'\n\n    $cluster1\n        \n'INPP4B''IL7R''ANK3''CDC14A''SERINC5''BCL11B''IL32''RORA''CAMK4''TTC39C'\n\n    $cluster6\n        \n'IGKC''IGHA1''JCHAIN''IGHG3''IGHGP''IGHG1''IGHG2''BANK1''OSBPL10''MS4A1'\n\n    $cluster2\n        \n'LEF1''NELL2''TSHZ2''FHIT''CAMK4''PRKCA''BCL11B''PDE3B''TXK''TRABD2A'\n\n    $cluster5\n        \n'KLRB1''SLC4A10''IL4I1''GZMK''COLQ''ZBTB16''ADAM12''IL32''AL136456.1''AGAP1'\n\n    $cluster4\n        \n'TCF7L2''CST3''HLA-DPA1''FCGR3A''LST1''HLA-DPB1''AIF1''HLA-DRB1''HLA-DRA''IFI30'\n\n    $cluster8\n        \n'GNLY''NKG7''PRF1''KLRD1''GZMA''KLRF1''SPON2''FGFBP2''CST7''GZMB'\n\n    $cluster9\n        \n'PPBP''GP1BB''NRGN''PF4''CAVIN2''TUBB1''CLU''GNG11''RGS18''HIST1H2AC'\n\n\n\n\n위 코드의 출력을 가지고 ChatGPT에 입력했더니 다음과 같이 말했습니다.\n주어진 유전자 목록만으로는 각각의 클러스터의 정확한 세포 유형을 알 수 없습니다. 유전자는 여러 종류의 세포에서 발현될 수 있기 때문입니다. 유전자의 발현 패턴을 분석하거나 추가적인 실험 결과와 함께 고려해야만 각 클러스터의 세포 유형을 추론할 수 있습니다.\n\n그러나 일반적으로 각 클러스터에 대한 예상 세포 유형은 다음과 같습니다:\n\n$cluster0: 이 클러스터는 PBMC에서 neutrophils을 나타내는 것으로 추정됩니다.\n$cluster1: 이 클러스터는 PBMC에서 CD4 T cells을 나타내는 것으로 추정됩니다.\n$cluster2: 이 클러스터는 PBMC에서 B cells을 나타내는 것으로 추정됩니다.\n$cluster3: 이 클러스터는 PBMC에서 B cells을 나타내는 것으로 추정됩니다.\n$cluster4: 이 클러스터는 PBMC에서 monocytes을 나타내는 것으로 추정됩니다.\n$cluster5: 이 클러스터는 PBMC에서 NK cells을 나타내는 것으로 추정됩니다.\n$cluster6: 이 클러스터는 PBMC에서 B cells을 나타내는 것으로 추정됩니다.\n$cluster7: 이 클러스터는 PBMC에서 CD8 T cells을 나타내는 것으로 추정됩니다.\n$cluster8: 이 클러스터는 PBMC에서 NK cells을 나타내는 것으로 추정됩니다.\n$cluster9: 이 클러스터는 PBMC에서 platelets을 나타내는 것으로 추정됩니다.\n위의 결과를 통해 대부분의 클러스터 타입을 알 수 있지만, B cell이 3개 NK cell이 2개로 중복됩니다. 좀 더 세분화하기 위해 B cell의 유전자 마커를 물어봅니다.\nB cell은 다양한 sub type으로 분류될 수 있습니다. 일부 sub type들과 그에 따른 대표 마커들은 다음과 같습니다.\n\n- Naive B cell\n    CD19, CD20, CD27-, IgD+, IgM+\n- Memory B cell\n    CD19, CD20, CD27+, IgD-, IgM+/-\n- Plasma cell\n    CD19, CD20-, CD38++, CD138+, MUM1+, IRF4+, XBP1+\n- Germinal center B cell\n    CD19, CD20, CD77+, CD10+, BCL6+, AID+\n- Regulatory B cell (Breg)\n    CD19, CD20, CD24++, CD38++, CD25+, IL-10+\n- B1 B cell\n    CD19, CD20, CD27+, IgM+, CD43+\n- Marginal zone B cell\n    CD19, CD20, CD27-, CD43-, IgM+, IgD+, CD23-, CD21+\n\n각 sub type마다 대표적인 마커들이 있지만, 이것들만으로 모든 sub type을 완벽하게 분류하는 것은 어렵습니다. 따라서, sub type 분류는 보통 여러 마커를 조합하여 결정됩니다.\n이번에는 NK cell 세분화를 위한 유전자 마커를 물어봅니다.\nNK 세포는 크게 두 가지 subset으로 나눌 수 있습니다: CD56bright NK 세포와 CD56dim NK 세포입니다. CD56bright NK 세포는 주로 면역 조절에 관여하며, cytokine을 분비하는 역할을 합니다. 반면 CD56dim NK 세포는 주로 세포 독성에 관여하며, 인체 내에서 암세포와 바이러스에 대한 첫 번째 방어선 역할을 합니다.\n\nCD56bright NK 세포를 구별하는 유전자 마커로는 CD56(bright) (NCAM1)과 CD16 (FCGR3A)이 있습니다. 반면 CD56dim NK 세포를 구별하는 유전자 마커로는 CD56(dim) (NCAM1)과 KIR (Killer-cell immunoglobulin-like receptors)가 있습니다. KIR은 NK 세포의 분화 상태, 활성 상태, 대상 세포와의 상호 작용 등을 조절합니다.\n위의 결과와 pbmc1k_marker.csv 파일을 비교하면서 다음과 같이 클러스터의 이름을 지정합니다.\n\nChatGPT를 사용한 방법은 정확하지 않고 힌트정도만 알 수 있습니다. 실제 실험결과라면 SingleR 패키지 혹은 문헌 검색을 통해 직접 찾아보시기 바랍니다.\n\n그리고 UMAP 플랏을 그려서 결과를 확인합니다.\n\n# 참조를 위해 이전 ID 클래스(클러스터 레이블)를 저장합니다.\nseurat_obj[[\"old.ident\"]] &lt;- Idents(object = seurat_obj)\n\n# 레이블 변경하기\nseurat_obj &lt;- RenameIdents(\n    object = seurat_obj,\n    `0` = \"neutrophils\",\n    `1` = \"CD4+ T cells\",\n    `2` = \"naive B cells\",\n    `3` = \"Plasma cells\",\n    `4` = \"monocytes\",\n    `5` = \"CD56bright NK cells\",\n    `6` = \"memory  B cells\",\n    `7` = \"CD8 T cells\",\n    `8` = \"CD56dim NK cells\",\n    `9` = \"platelet\"\n    )\n\n\np &lt;- DimPlot(seurat_obj, reduction = \"umap\", label = TRUE, pt.size = 0.5) + NoLegend()\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\np"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#클러스터당-세포-수-확인",
    "href": "posts/R_Seurat_tutorials.html#클러스터당-세포-수-확인",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "클러스터당 세포 수 확인",
    "text": "클러스터당 세포 수 확인\n\ntable(Idents(seurat_obj))\n\n\n        neutrophils        CD4+ T cells       naive B cells        Plasma cells \n                284                 204                 169                 130 \n          monocytes CD56bright NK cells     memory  B cells         CD8 T cells \n                 87                  74                  60                  54 \n   CD56dim NK cells            platelet \n                 53                  15"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#클러스터당-세포-비율-확인",
    "href": "posts/R_Seurat_tutorials.html#클러스터당-세포-비율-확인",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "클러스터당 세포 비율 확인",
    "text": "클러스터당 세포 비율 확인\n\nprop.table(table(Idents(seurat_obj)))\n\n\n        neutrophils        CD4+ T cells       naive B cells        Plasma cells \n         0.25132743          0.18053097          0.14955752          0.11504425 \n          monocytes CD56bright NK cells     memory  B cells         CD8 T cells \n         0.07699115          0.06548673          0.05309735          0.04778761 \n   CD56dim NK cells            platelet \n         0.04690265          0.01327434"
  },
  {
    "objectID": "posts/R_Seurat_tutorials.html#rds-파일로-저장하기",
    "href": "posts/R_Seurat_tutorials.html#rds-파일로-저장하기",
    "title": "Seurat으로 scRNA seq데이터 분석하기",
    "section": "RDS 파일로 저장하기",
    "text": "RDS 파일로 저장하기\nRDS 파일을 저장해두면 추후 분석에 위의 과정을 반복할 필요가 없습니다.\n\nsaveRDS(seurat_obj, file = \"../output/pbmc1k_final.rds\")\n\n지금 까지 살펴본 내용을 요약해보면 다음과 같습니다.\n\nPre-processing: 데이터 전처리를 통해 불필요한 변수 제거, 정규화 등을 수행합니다.\nDimensionality reduction: 차원 축소 기법을 사용해 데이터의 주요 구조를 파악합니다.\nClustering: 유사한 특성을 가진 데이터들을 그룹화합니다.\nCell type identification: 각 클러스터에 대해 유전자 발현 패턴 등을 비교하여 cell type을 추론합니다.\n\n이후 진행되는 scRNA-seq Downstream analysis는 여기서 얻은 결과를 기반으로 합니다. 따라서 여기서의 결과가 부정확하거나 신뢰성이 떨어지면 추가 분석에서 얻은 결과 또한 의미가 없습니다. 그러므로 분석에서 사용된 데이터의 품질, 분석 방법의 적절성, 도구의 성능 등을 철저히 검토하고 확실한 기준에 따라 분석을 수행하세요. 또한, 추후에 데이터나 분석 방법이 변경되는 경우 이전의 결과와의 비교를 통해 신뢰성을 유지할 수 있도록 관리하는 것도 잊지 말아야 합니다."
  },
  {
    "objectID": "posts/miniforge/index.html#miniforge",
    "href": "posts/miniforge/index.html#miniforge",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "miniforge",
    "text": "miniforge\nminiforge는 덩치큰 아나콘다를 아주 날씬하게 만드는 프로젝트입니다. 그래서 아래와 같은 특징을 갖습니다.\n\n기본(그리고 유일한) 채널로 conda-forge를 사용.\n표준 Python 인터프리터(일명 “CPython”) 대신 PyPy에 대한 지원.\n콘다보다 더 빠른 맘바(mamba)도 지원.\n다양한 CPU 아키텍처(x86_64, ppc64le, Apple M1을 포함한 aarch64)지원."
  },
  {
    "objectID": "posts/miniforge/index.html#맘바mamba는-또-뭐죠",
    "href": "posts/miniforge/index.html#맘바mamba는-또-뭐죠",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "맘바(mamba)는 또 뭐죠?",
    "text": "맘바(mamba)는 또 뭐죠?\n콘다의 다른 단점에 라이브러리 설치 속도가 느리다는 점이 었습니다. 그래서 C++으로 작성된 맘바라는 도구가 새로 나오게 되었죠.\n\n파이썬 생태계는 이런 것이 특징입니다. 항상 새로운 도구가 우후죽순 나옵니다."
  },
  {
    "objectID": "posts/miniforge/index.html#설치하기",
    "href": "posts/miniforge/index.html#설치하기",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "설치하기",
    "text": "설치하기\n공식문서에서 각각의 OS에 맞는 설치 방법을 찾아 볼 수 있습니다. 저는 리눅스를 사용하기에 아래 명령어로 설치 하였습니다.\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\nbash Mambaforge-$(uname)-$(uname -m).sh"
  },
  {
    "objectID": "posts/miniforge/index.html#개발-환경-설정",
    "href": "posts/miniforge/index.html#개발-환경-설정",
    "title": "miniforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구",
    "section": "개발 환경 설정",
    "text": "개발 환경 설정\n\n가상환경 만들기\n맘바포지의 (base) 환경에 왠만하면 아무런 패키지를 설치하지 않을 것을 권장합니다. 따라서 새로운 가상환경인 ipynb를 만들어 주피터랩을 사용해 보겠습니다.\nmamba create -n ipynb python=3.11 r-base r-essentials jupyterlab\n콘다를 써본적 있으시다면 바로 아시겠지만 conda를 mamba로 바꾸기만 하면 됩니다. 결과는 아래와 같습니다.\n                  __    __    __    __\n                 /  \\  /  \\  /  \\  /  \\\n                /    \\/    \\/    \\/    \\\n███████████████/  /██/  /██/  /██/  /████████████████████████\n              /  / \\   / \\   / \\   / \\  \\____\n             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n            / _/                       \\_____/  `\n            |/\n        ███╗   ███╗ █████╗ ███╗   ███╗██████╗  █████╗\n        ████╗ ████║██╔══██╗████╗ ████║██╔══██╗██╔══██╗\n        ██╔████╔██║███████║██╔████╔██║██████╔╝███████║\n        ██║╚██╔╝██║██╔══██║██║╚██╔╝██║██╔══██╗██╔══██║\n        ██║ ╚═╝ ██║██║  ██║██║ ╚═╝ ██║██████╔╝██║  ██║\n        ╚═╝     ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚═════╝ ╚═╝  ╚═╝\n\n        mamba (1.1.0) supported by @QuantStack\n\n        GitHub:  https://github.com/mamba-org/mamba\n        Twitter: https://twitter.com/QuantStack\n\n█████████████████████████████████████████████████████████████\n\n\nLooking for: ['python=3.11', 'r-base', 'r-essentials', 'jupyterlab']\nconda-forge/noarch                                 @   3.7MB/s  3.2s\nconda-forge/linux-64                              30.2MB @   4.2MB/s  7.3s\n\n\n  All requested packages already installed\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\nTo activate this environment, use\n\n     $ mamba activate ipynb\n\nTo deactivate an active environment, use\n\n     $ mamba deactivate\n\n\n가상환경 활성화 하기\nmamba activate ipynb\n\n\n파이썬 라이브러리 설치하기\n아래 명령어를 통해 쉽게 설치할 수 있습니다.\nmamba install pandas\n\n\nR 라이브러리 설치하기\nR의 경우 보통 라이브러리 이름 앞에 r-을 붙이면 됩니다만, 확실하지 않기 때문에 다음 search 명령어를 통해 미리 확인하고 설치하면 좋습니다.\nmamba search r-tidyverse\n위 명령어를 통해 해당 라이브러리가 존재한다는 것을 알 수 있습니다.\nmamba install r-tidyverse -y\n\n\n라이브러리 제거하기\n설치한 라이브러리를 제거하고 싶다면 아래 명령어를 사용합니다.\nmamba remove r-tidyverse\n\n\n의존성 파일로 저장하기\n개발을 하다보면 배포 혹은 프로젝트간의 전환을 위해 의존성을 파일로 저장해야하는 경우가 생깁니다. 그럴 때에는 아래와 같이 하면 됩니다.\nmamba env export &gt; env.yaml\n\n의존성 파일로 부터 가상환경 만들기\n위 명령어로 생성된 파일을 가지고 다음 명령어를 사용해 새로운 가상환경을 만들 수 있습니다.\nmamba env create -f env.yaml\n\n\n\n가상환경 비활성화하기\n일반적으로는 그냥 터미널을 꺼버리고는 합니다만, 실수를 방지하기 위해 다음 명령어를 습관적으로 써주는 것이 좋습니다.\nmamba deactivate"
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#왜-이름이-miniprep-인가요",
    "href": "posts/lab_protocols/lab_protocols.html#왜-이름이-miniprep-인가요",
    "title": "각종 실험 방법 정리",
    "section": "왜 이름이 miniprep 인가요?",
    "text": "왜 이름이 miniprep 인가요?\n다양한 회사에서 plasmid DNSA를 정제하는 하는 키트를 생산하고 있는데, 시작하는 E.coli 배양액의 양에 따라서 다음과 같이 miniprep, midiprep, maxiprep라고 공통적으로 부르고 있습니다. 이름에서 알 수 있듯 양이 적으면 mini 많으면 maxi입니다.\nminiprep은 (상대적으로) 빠르고 작은 규모로 plasmid DNA를 뽑아 낼 수 있기에 많이 사용되고 있습니다. 원리는 alkaline lysis method 를 사용하고 있는데 간단히 이야기 하면 염기성 용액으로 셀을 깨주고, 빠르게 중화시켜 Plasmid DNA만 선택해 내는 방법 입니다. 보통의 경우 miniprep으로 약 50µg의 plasmid DNA를 얻을 수 있습니다.\n\nminiprep에 사용되는 다양한 키트가 있지만, 여기에서는 Qiagen Spin Miniprep 키트 기준으로 설명합니다."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#metarials",
    "href": "posts/lab_protocols/lab_protocols.html#metarials",
    "title": "각종 실험 방법 정리",
    "section": "Metarials:",
    "text": "Metarials:\n\n하룻밤 키운 E.coli 배양액 (1-5ml)\n차가운 P1 버퍼 (50 mM Tris-HCl pH 8.0, 10 mM EDTA, 100 µg/ml RNaseA) RNaseA가 들어있습니다, 항상 냉장 보관하세요.\nP2 버퍼 (200 mM NaOH, 1% SDS)\nN3 버퍼 (4.2 M Gu-HCl, 0.9 M potassium acetate, pH 4.8)\nPB 버퍼 (5 M Gu-HCl, 30% ethanol)\nPE 버퍼 (10 mM Tris-HCl pH 7.5, 80% ethanol)\nEB 버퍼 (10 mM Tris·Cl, pH 8.5; DW로 대체하셔도 됩니다.)\nQIAprep spin column (키트에 포함되어 있음)\nCentrifuge\n멸균된 1.5-ml microcentrifuge 튜브"
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#methods",
    "href": "posts/lab_protocols/lab_protocols.html#methods",
    "title": "각종 실험 방법 정리",
    "section": "Methods:",
    "text": "Methods:\n\nmicrocentrifuge 튜브에 1.5 ml 의 배양액을 넣어 줍니다.\n최대 속도로 Centrifuge for 1 min at room temperature, gently aspirate out the supernatant and discard it.\n충분한 크기의 cell pellet이 생길때 까지 반복합니다.\n250 µl 의 차가운 Buffer P1 를 넣어주고, Resuspend pelleted bacterial cells\nAdd 250 μl Buffer P2 and 조심스럽게 invert the tube 4–6 times to mix.\nAdd 300 µl of Buffer N3. NOTE: Proceed to the next step within immediately !\nClose the tube tightly and invert the tube 4 - 6 times . The solution should become cloudy.\nCentrifuge for 10 min at 13,000 rpm (~17,900 x g) in a table-top microcentrifuge. A compact white pellet will form. Apply the supernatants from step 4 to the QIAprep spin column by decanting or pipetting.\nCentrifuge for 30–60 s. Discard the flow-through. Spinning for 60 seconds produces good results.\nWash QIAprep spin column by adding 0.75 ml Buffer PE and centrifuging for 30–60 s.\nDiscard the flow-through, and centrifuge for an additional 1 min to remove residual wash buffer. IMPORTANT: Residual wash buffer will inhibit subsequent enzymatic reactions.\nPlace the QIAprep column in a clean 1.5 ml microcentrifuge tube. To elute DNA, add 50 μl Buffer EB (10 mM Tris·Cl, pH 8.5) or water to the center of each QIAprep spin column, let stand for 1 min, and centrifuge for 1 min. 만약 높은 농도로 elution 받고 싶다면 add 30 μL의 DW를 컬럼 가운데에 넣고, incubate at room temperature on the bench for 5 mins and then centrifuge for 1 min.\n\n\n\n노트\n\n한번에 10개 이상의 miniprep을 한다면 vacuum manifold 방법을 사용하는게 빠릅니다.\ncell lysate를 column에 두번 거치면 수율이 약 20% 증가됩니다.\n시퀀싱 경과가 안나오는 이유는 대체로 염에 의한 오염으로, Washing 하는 과정을 충분하게 해줍니다."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#원리",
    "href": "posts/lab_protocols/lab_protocols.html#원리",
    "title": "각종 실험 방법 정리",
    "section": "원리",
    "text": "원리\n이름에서 알 수 있듯이 효소가 사용됩니다. 항원의 농도는 기질 전환 정도에 달라집니다 항체나 항원이 고체상에 흡착되어있으며 이렇게 해야지만 결합하지 않은 free항원들을 세척으로 없앨 수 있습니다. 실험동물을 immunization 한 후 얻은 serum이나 fusion을 통해 얻은 하이브리도마의 culture supernatant안에 원하는 항체가 생성되어있는지 확인할 수 있습니다. 방사능을 사용하지 않으면서 검사할 수 있고 샌드위치와 경쟁적 ELISA 방법이 가장 많이 사용됩니다. Dirent ELISA도 사용되는데 항원이 고체상에 고정됩니다. 이 방법은 항원특이적 항체 검출시에 용이합니다. 효소로는 간단한 기질을 넣어주었을때 색이 나는 반응을 이용합니다. 대표적으로는 alkaline phosphatase와 HRP(horseradish peroxidase)가 사용됩니다."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#종류",
    "href": "posts/lab_protocols/lab_protocols.html#종류",
    "title": "각종 실험 방법 정리",
    "section": "종류",
    "text": "종류\n\nDirect ELISA: 항원과 반응하는 항체에 바로 효소를 결합시킨다.\nIndirect ELISA: 항원과 결합하는 항체(1차항체)에는 효소가 없고, 그 항체와 결합하는 항체(2차항체)에는 효소가 결합되어있다. 일반적으로 isotype에 대한 항체에 효소가 결합된 형태로 판매되고 있다. 자신이 이용하는 일차항체의 isotype에 맞는 효소결합항체를 구입하여 사용하면된다.항체를 정량 및 정성적으로 분석 할때 사용한다.\nSandwich ELISA: 항원에 대한 항체를 먼저 well에 결합시키고 그 항체에 대한 항원(시료)을 결합시킨다. 그 후 직접적이나 간접법으로 조사한다. 항원을 정성 및 정량적으로 분석한다."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#sds-page-gel-쉽게-만들기",
    "href": "posts/lab_protocols/lab_protocols.html#sds-page-gel-쉽게-만들기",
    "title": "각종 실험 방법 정리",
    "section": "SDS-PAGE Gel 쉽게 만들기",
    "text": "SDS-PAGE Gel 쉽게 만들기\n준비물 :\n2X running gel buffer : 750 mM Tris-HCl, pH8.8 상온보관\n1x stacking gel solution :  62.5mM Tris-HCl, pH6.8, 4% acrylamide 냉장보관\n30% acrylamide stock solution (29:1): 냉장보관\n10% Ammonium persulfate : 냉장보관\nAcrylamide의 quality는 해상도에 영향을 줍니다. 따라서 high quality를 사용하시는 편이 좋고 가급적 빠른 시간내 소비하는 것이 좋습니다.\n\nGel 만들기 : 총 10ml\n\n2x running gel buffer 5ml + 20% acrylamide stock 5ml + 10% APS 100ul 섞어 줍니다. 2. TEMED 10 ul 넣고 천천히 섞어줍니다.\nGel cast에 기포가 생기지 않게 조심히 부어 줍니다.\n100% Ethanol을 위에 조심스럽게 넣습니다. 양은 500ul정도\n30분후 Gel이 굳은 걸 확인하고 DW로 gel 윗 부분을 한번 씻어줍니다.\n1x stacking gel solution 1ml + 10% APS 10ul를 섞어 줍니다.\nTEMED 1ul를 마지막으로 넣어 천천히 섞어 줍니다.\n5번 과정을 끝낸 running gel 위에 붇고 comb을 꼽아 줍니다.\n20분후 comb을 뽑고 실험에 사용합니다.\n\nGel이 굳는 시간은 전적으로 APS의 첨가량에 따라 달라지며 시간이 촉박한 경우 APS의 양을 늘려 줍니다. Gel이 완전히 굳지 않았을 경우에 해상도는 떨어집니다. 이러한 점에서 오랜 시간 굳히는 방법보다는 APS의 양을 늘려 빨리 굳히는 편이 좋습니다.\n\n\nNote\nSDS는 sample buffer와 running buffer에 들어있으므로 gel에 넣지 않아도 해상도에 문제가 없습니다."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#standard-protocol-insert-vector-dna-ligation",
    "href": "posts/lab_protocols/lab_protocols.html#standard-protocol-insert-vector-dna-ligation",
    "title": "각종 실험 방법 정리",
    "section": "Standard Protocol: Insert + Vector DNA Ligation",
    "text": "Standard Protocol: Insert + Vector DNA Ligation\n대부분의 경우 3 insert : 1 vector 비율을 추천한다. 또한 ligation 반응 시 총 DNA의 양은 100ng 정도가 권장 된다.\n\nCombine the following in a PCR or Eppendorf tube:\n\n25ng Vector DNA\n75ng Insert DNA\nLigase Buffer (1μL/10μL reaction for 10X buffer, and 2μL/10μL reaction for 5X buffer) 0.5-1μL T4 DNA Ligase\nDW 를 넣어 total 10μL\n만약 DNA 농도가 너무 낮다면 total volume을 증가 시켜서 진행한다.\n항상 Vector만 넣은 control실험을 하고 다양한 vector : insert 비율을 시도 한다.\n\nRT에서 2시간 혹은 16°C에서 overnight 반응 시킨다.\n\n“high concentration” ligase를 사용하는 경우 RT 5분이면 충분하다.\n\ntransformation을 진행한다."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#vectorinsert-비율",
    "href": "posts/lab_protocols/lab_protocols.html#vectorinsert-비율",
    "title": "각종 실험 방법 정리",
    "section": "Vector:Insert 비율",
    "text": "Vector:Insert 비율\n보통 3:1 비율로 넣어줄 경우 충분하지만 안될 경우 vector: insert 비율을 조절 해 줄 필요가 있다. 자동으로 계산해주는 도구 이 있으니 참고한다."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#tips-and-faq",
    "href": "posts/lab_protocols/lab_protocols.html#tips-and-faq",
    "title": "각종 실험 방법 정리",
    "section": "Tips and FAQ",
    "text": "Tips and FAQ\nligation 단계는 실험의 성공 여부를 알기 힘들기 때문에 항상 컨트롤을 같이 해야 한다.\n\n\n\n\n\n\n\n\nControl\nLigase\nInterpretation\n\n\n\n\nUncut vector\n-\nChecks viability of competent cells and verifies the antibiotic resistance of the plasmid\n\n\nCut vector\n-\nBackground due to uncut vector\n\n\nCut vector\n+\nBackground due to vector re-circularization - most useful for phosphatase treated vector\n\n\nInsert or water\n+\nAny colonies indicate contamination of intact plasmid in ligation or transformation reagents"
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#pcr-개요",
    "href": "posts/lab_protocols/lab_protocols.html#pcr-개요",
    "title": "각종 실험 방법 정리",
    "section": "PCR 개요",
    "text": "PCR 개요\n1983년 Kary Mullis에 의해 고안. DNA 중합효소를 이용하여 DNA, RNA의 특정영역을 시험관 내에 대량으로 증폭 80년대 제한효소(restriction enzyme)의 발견에 의한 gene cloning법 이후, 90년대 생명공 학 연구의 혁명적인 사건 연구하고 싶은 유전자는 무엇이든 대량 생산이 가능."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#pcr-목적",
    "href": "posts/lab_protocols/lab_protocols.html#pcr-목적",
    "title": "각종 실험 방법 정리",
    "section": "PCR 목적",
    "text": "PCR 목적\n복잡한 전체 genome 중에 연구하고자 하는 유전자가 희귀유전자를 분석하고 연구하는데 가장 큰 문제점이였다. PCR은 특정 DNA sequence의 copy 수를 기하급수적으로 증폭시킴으로써 증폭된 DNA를 여러 가지 실험에 이용할 수 있고, 실험 결과를 토대로 분자생물학, 의학, 이학, 농학, 수의학, 식품과학, 환경과학 연구에 응용할 수 있음"
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#pcr-구성-요소",
    "href": "posts/lab_protocols/lab_protocols.html#pcr-구성-요소",
    "title": "각종 실험 방법 정리",
    "section": "PCR 구성 요소",
    "text": "PCR 구성 요소\n\nDNA, RNA template:\n\n증폭 대상이 되는 DNA, RNA\n\nPCR Primers:\n\n증폭할 부분을 잡는 짧은 염기서열.\n\nTaq polymerase:\n\n열에 특별히 강한 유전자 합성효소 (Taq polymerase: Thermus aquaticus 라는 온천에 사는 세균의DNA polymerase, 72℃가 최적온도, 94℃에서도 안정함)\n\ndNTP (dATP, dCTP, dGTP, dTTP):\n\n유전자를 합성하는 재료가 되는 각 nucleotide\n\nMgCl+2:\n\nMgCl2+은 dNTP와 복합체를 형성하여 효소활성, primer annealing 등에 관여"
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#pcr-의-3-단계",
    "href": "posts/lab_protocols/lab_protocols.html#pcr-의-3-단계",
    "title": "각종 실험 방법 정리",
    "section": "PCR 의 3 단계",
    "text": "PCR 의 3 단계\n\nDNA 의 변성(denaturation):\n\n90℃∼96℃로 가열하여 두가닥 DNA를 단일가닥 DNA로 분리.\n일반적으로 94℃사용: 높은 온도일수록 단일가닥 DNA로 잘 이행되지만 온도가 너무 높으면 Taq DNA polymerase 역시 activity(활성)가 낮아짐.\n첫 Cycle에서는 확실한 변성을 위하여 약 5분간 지속시킴.\n이 후의 cycle에서는 약 1분간 변성시킴.\n\n\n\nPrimer 의 결합(annealing):\n\n50℃∼65℃에서 진행.\n30sec~1min.\n염기 간의 결합은 G, C의경우 세군데 에서 수소결합이 일어나고 A, T는 두군데에서 결합이 일어나므로 G+C 비율에 따라 결합 온도 결정.\nPrimer design시에 Annealing temperature를 고려해야 함.\n일반적으로 GC content가 50%가 되는 primer 쌍을 이용하는 것이 바람직.\n\n\n\nDNA의 합성(polymerization, extension):\n\n70℃∼74℃에서 시행.\n1min ~ 1min 30sec.\nTaq DNA polymerase의 합성 속도: 2,000∼4,000 nucleotides/min, 1 kb마다 1분 정도의 시간을 배당.\n원하는 PCR 산물의 크기가 크 거나 반응요소의 농도가 낮을 때에는 시간을 연장할 수 있음.\nCycle이 계속되면서 효소 활성이 감소할 수 있고 DNA 산물은 점점 많이 존재하게 되므로 cycle 후반부에는 반응시간을 조금씩 늘려가는 것도 좋은 방법의 하나이며 마지막 cycle 에는 약 10분 정도 시간을 충분히 주어서 효소의 활성이 충분히 발휘되도록 함."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#pcr-실험-시-유의하여야-할-사항",
    "href": "posts/lab_protocols/lab_protocols.html#pcr-실험-시-유의하여야-할-사항",
    "title": "각종 실험 방법 정리",
    "section": "PCR 실험 시 유의하여야 할 사항",
    "text": "PCR 실험 시 유의하여야 할 사항\n\nPipetting & DNA template:\n여러 component를 혼합할 때에는 시료간에 오염이 되지 않도록 주의하여야 하며 가능하면 공기를 통한 오염을 방지할 수 있는 tip을 사용하는 것이 좋습니다. 반응물 혼합 시에는 tube를 ice상에 두고서 혼합하여야 상온에서의 잘못 primer annealing에 의한 extension을 방지할 수 있습니다. 이론적으로 Taq DNA polymerase는 최적 온도 이하에서도 반응이 어느 정도 진행됨 으로 상온 등에서 정확하게 annealing되지 않은 primer에 의한 임의의 반응이 진행됨으로써 원하는 size의 product 이외의 non-specific product가 만들어질 수 있습니다.\n\n\nSetting up the Laboratory:\nPCR은 민감도가 뛰어난 실험이기 때문에 아주 적은 양의 DNA가 오염되더라도 실험에 큰 영향을 미칠 수 있습니다. 그러므로 PCR을 위한 template를 준비하는 곳과 PCR 반응을 하는 곳, 그리고 PCR 후 전기영동 및 분석을 하는 곳은 격리시키는 것이 좋으며, DNase 와 RNase free PCR tube를 사용하는 것이 좋습니다. 모든 시약류는 반드시 autoclave와 filteration을 거친 후 사용하여야 합니다.\n\n\nPCR cycling program:\nPCR cycling 조건은 PCR의 종류와 주형 DNA, primer 그리고 PCR 기기등에 따라 달라 져야 합니다.\n\nInitial denaturation:\n\ntemplate DNA의 완전한 denaturation이 중요한데 94℃∼95℃에서 2∼3 min 정도로 충분하지만 대부분 5 min 정도 초기 변성 시간을 주는 것이 좋습니다. denaturation 이 충분하지 않으면 primer의 annealing 과 extension이 방해받아 정확한 반응물이 생기지 않을 수 도 있습니다.\n\nDenaturation step during cycling:\n\n보통 94℃∼95℃에서 20∼30 sec 정도이지만 PCR 기기와 tube 등에 따라 시간을 늘리기도 합니다. Template의 GC함량이 높으면 높은 온도와 긴 시간을 사용하기도 하지만 필요 이상으로 변성 온도가 높거나 길면 Taq DNA polymerase의 활성 이 감소됩니다.\n\nPrimer annealing:\n\n대개의 경우 annealing 온도는 primer의 Tm 값에 따라 결정됩니다. 온도가 너무 높으면 primer가 annealing 되지 않아 PCR product가 생기지 않게 되고, 온도가 너무 낮으면 non-specific annealing 이 일어나 정확한 PCR product가 생기지 않습니다.\n\nExtension (polymerization):\n\nTaq polymerase의 경우 72℃에서 1초당 약 60개의 염기를 중합시키기 때문에 1 kb 까지는 45 sec정도면 충분합니다. 하지만 대부분의 경우 1kb당 1분정도의 시간이 필요합니다.\n\n\nCycle number\n\n대부분의 경우 25∼35 cycles을 진행하고, Template 분자가 10개 이하인 경우에는 40 cycles 정도 진행하면 product을 관찰할 수 있습니다. 그러나 cycle의 수를 무작 정 늘린다고 해서 product의 양이 급격히 늘지는 않으며 오히려 비특이적 밴드가 늘어날 수 있습니다.\n\n\n\n\n\nEquipment (Thermocyclers & PCR tubes):\nPCR 기기는 기본적으로 PCR 반응을 구성하는 세 가지 온도를 최소한의 시간에 정확하 고 재현성있게 유지할 수 있어야 합니다. 또한 반응 tube에 따라 열전도율의 차이가 있기 때문에 가능하면 thin-walled reaction tube를 사용하는 것이 좋으며 thermal cycler의 block에 꼭 맞는 크기를 사용하여야 합니다.\n\n\nPCR reaction components:\n\nDNA template: Template 양과 질은 PCR에 절대적인 영향을 미칩니다. template가 적을수록 product의 양 역시 비례적으로 감소하게 되며, RNA의 오염은 Mg2+ 이온을 잡아먹어 yield를 낮추게 되고 불 순한 template에는 반응저해제들을 많이 포함 하고 있어 반응의 효율을 떨어뜨립니다.\nPCR Primers: PCR의 많은 요소들 중에서도 primer의 염기서열과 농도는 전체 반응의 성패에 가장 큰 영향을 미치는 요인 중 하나로 다음과 같은 사항들을 고려하여 설계하는 것이 좋습니다. 길이는 18∼24mer가 적당하며 두 primer의 Tm 값의 차이는 5℃ 이내로 하고 가급적 2차 구조가 형성되지 않도록하며 G+C 값은 40∼60%로하여 두 primer의 3′ 사이에 상보결합이 없어야 합니다.\nChoice of DNA polymerase: PCR 반응에 사용하는 Taq DNA polymerase는 0.5∼2.5U/20∼50ul volume 정도가 적당합니다. 비율적으로 너무 많은 효소가 들어가게 되면 높은 glycerol 농도로 인하여 product가 끌리는 현상이나 특이성이 떨어져 불균형적인 결과를 초래하게 되며, 너무 적은 양의 효소를 사용하면 생성물의 양이 부족하게 됩니다.\nDeoxynucleotide triphosphate (dNTP): 항상 dNTP의 4가지 요소들은 동일 농도로 사용하여야 합니다. dNTP mixture의 불균 형은 Taq polymerase의 fidelity를 감소시켜 error rate가 증가될 수 있습니다. 또한 dNTP stock은 thawing/freezing에 민감하여 3∼5차례만 반복하여도 활성이 감소하여 올바른 결과를 기대할 수 없습니다. 그러므로 stock은 사용량에 맞게끔 적절하게 배분해놓는 것이 좋습니다. 만일 dNTP의 농도를 증가시키려면 반드시 Mg2+의 농도 역시 증가시켜 주어야 합니다. 높은 dNTP 농도는 free Mg2+을 감소시켜 효소의 반응을 방해하고 primer의 annealing을 감소시키게 됩니다. 일반적으로 사용되는 dNTP의 최종 농도는 각 200∼250uM 입니다.\nMgCl2 concentration: Mg2+은 dNTP와 복합체를 형성하여 효소의 실질적인 substrate로 이용됩니다. free Mg2+의 농도는 dNTP, free pyrophosphate 그리 고 EDTA 같은 ion 결합 물질의 농도에 영향을 받게 됩니다. 최적 의 실험결과를 위해선 적절한 MgCl2의 농도를 사용하여야 하는데 가장 일반적인 농도는 1.5mM (dNTP 각 200uM 일 때)입니다. Mg2+은 효소 활성에 영향을 미치고 double-strand DNA의 Tm 값 을 증가시키는 효과가 있습니다. 과다한 Mg2+은 primer의 비 특이 적인 결합과 background를 증가시키게 됩니다.\nReaction overlay: PCR 반응을 하는 동안 mixture가 증발되는 것을 방지하기 위하여 mineral oil을 넣어주어야 합니다. 하지만 PCR 기기의 두껑에 히터가 달려있다면 mineral oil을 넣어 줄 필요 없습니다."
  },
  {
    "objectID": "posts/lab_protocols/lab_protocols.html#pcr의-종류",
    "href": "posts/lab_protocols/lab_protocols.html#pcr의-종류",
    "title": "각종 실험 방법 정리",
    "section": "PCR의 종류",
    "text": "PCR의 종류\n\nRT-PCR\nRT-PCR((Reverse Transcriptase Polymerase Chain Reaction)이란 P.Seeburg(1986)에 의해 RNA를 찾고 분석하는데 도입된 방법으로 mRNA(messenger RNA)로부터 reverse transcription 과정을 통해 얻어진 cDNA(complementary DNA)를 PCR로 증폭하는 방법이다. 이러한 방법은 RNA 검사의 sensitivity를 높이고 소량의 RNA로부터 염기서열을 분석할 수 있게 하였다. 이 방법은 Northern blot hybridization과 같은 방법을 통해 가능하던 RNA 분석보다 실험방법이 더욱 간단할 뿐 아니라 유전자의 염기서열 결정이 가능하기 때문에 주로 mRNA의 염기서열 및 전사량을 연구할 때 크게 도움을 준다. 염기서열이 알려진 유전자의 경우 RT-PCR을 통해서 전체 길이의 cDNA를 간단하게 합성하여 cloning 할 수 있다.\n\n\nRT-PCR의 세가지 과정:\n\nRNA 분리 과정(이 과정은 Northern Blot을 하기 전에 시행해야 하는 동일한 과정이다)\ncDNA 합성 과정(reverse transcription)\nPCR amplification (이 과정은 Genomic DNA로부터 특정 유전자 부위를 증폭시키는 과정과 같다)으로 진행된다.\n\nmRNA로부터 reverse transcriptase를 이용하여 cDNA를 제조하는 방법에는 어떤 oligonucleotide를 primer로 사용하는가에 따라 세가지 방법\n\nAntisense primer(3’쪽 유전자에 특이성을 지닌 primer)를 이용하여 특정부위 cDNA 제조\nRandom hexamer를 이용하여 전체 mRNA에 상보적인 cDNA 제조\nOligo dT primer를 이용하여 전체 mRNA에 상보적인 cDNA 제조가 있다.\n\n\n\nSSCP(Single Strand Conformation Polymorphism)\npoint mutation 찾기. 유전자내의 변이, 특히 point mutation을 발견하는데 가장 간단하고 신속한 방법이다. PCR 수행시 각하는 방향과 반대 방향으로 PCR을 실시하는 방법이다. 변이가 있을 것으로 예상되는 DNA 특정부위 양쪽으로 적당히 덜어져 있는 부위에 PCR을 실시하기 위한 primer를 제조한 후 PCR로 이 DNA를 증폭시킨다. 이 증폭된 DNA를 검출하기 위해서는 전기영동을 실시해야 하는데 전기영동에 영향을 주는 인자들에는 여러 가지가 있다. 그 중 SSCP에 이용되는 인자는 입자들의 형태에 따라 전기영동시 이동하는 속도가 달라진다는 점을 이용하는 것이다. 한 개의 point mutation만 있는 경우에도 나타날 수 있는 미세한 이동 속도의 차이도 검출해야 하므로 SSCP를 위해서는 agarose gel대신 acrylamide gel이 이용된다. PCR 산물을 전기영동하기 전에 DNA를 변성시킬 수 있도록 NaOH를 가한 후 urea가 포함된 denaturing acrylamide gel에 전기영동하면 변성된 DNA double strand는 두 개의 single strand DNA로 분리되어 전기영동하게 되며 이 때 point mutation이 생긴 시료는 다른 정상적인 시료에서 얻어진 두 개의 DNA band와 다른 위치에 band가 나타나게 된다. 일반적인 염색만으로 변이 여부를 판정할 수 있는 경우도 있으나, 워낙 미세한 차이를 감지해야 하므로 민감도가 높은 silver nitrate 염색을 하는 편이 결과를 판정하기에 편리하다. 과거에는 아주 시료를 변성시킨 후 non-denatured acrylamide에 전기영동하고, 이를 nylon membrane에 transfer한 후 방사성 동위 원소(radioisotope)로 표지된 probe를 붙이는 복잡한 실험 방법이었으나 최근에는 아주 간단한 방법으로 개량되어 널리 쓰이고 있으며, 민감도를 높이기 위해 방사성 동위 원소를 사용할 필요 없이 silver nitrate를 이용하여 건강을 헤치지 않고 간단히 결과를 판독할 수 있게 되었다.\n\n\nRACE(Rapid Amplification of cDNA Ends):\ncDNA를 cloning하기 위해서는 cDNA library를 screening하는 방법이 현재 가장 일반적이라 할 수있지만, 이 방법으로 처음 screening을 하여 찾아낸 clone은 대개 전체 cDNA의 일부이며 계속 반복하여 screening을 하여야만 완전한 cDNA를 얻어낼 수 있다. 그러나 이런 과정은 대단히 시간과 노력이 소모되는 작업이며 유전자 자체가 cDNA library에 적은 양으로 존재하는 경우는 더더욱 힘든 일이 될 것이다. 또한 initiation codon부터 termination codon까지 open reading frame(ORF)을 완전히 결정한 경우에도 cDNA의 5’과 3′-끝의 non-coding region 일부는 library screening에서 얻기가 어렵다. 이러한 문제를 해결하고자, 1988년 Frohmann 등은 다음과 같은 방법을 소개하고, 이를 RACE(rapid amplification of cDNA ends)라 이름하였다. 즉, cDNA의 일부 염기서열을 알고 있으면, 이 부분에서 gene specific primer를 합성하고 PCR reaction을 통해 5′ 혹은 3′-end 까지의 DNA를 증폭하는 것이다. 3′-RACE에서는 mRNA의 3′-end에 존재하는 poly(A) tail을 이용할 수 있으므로, down stream primer로 oligo-(dT) primer를 쓴다. 그러나, 5′-RACE의 경우는 gene specific primer로 합성한 1st single strand cDNA의 끝에 TdT(terminal deoxynucleotidyl transferase)를 사용하여 poly(A) 혹은 poly(C) tail을 인위적으로 만들어 주어야만 한다.\n\n\nISPCR(In Situ Polymerase Chain Reaction):\nPCR은 원하는 DNA를 대량으로 증폭시키는 방법이고, in situ hybidization (ISH)은 세포나 조직에 존재하는 극미량의 DNA 및 RNA를 찾아낼 수 있음은 물론 원하는 유전자들의 위치까지 확인할 수 있는 방법이다. ISPCR은 이 두 가지 방법의 장점을 혼합하여 응용한 방법으로서 PCR의 sensitivity와 ISH의 specificity를 고루 갖추고 있다. ISPCR의 실험원리는 일반적인 PCR 방법과 같으나, slide glass 등의 사용에 적합한 ISPCR용 기구가 필요하며 사용하는 기구에 따라 반응시키는 방법들이 다양하게 제시되어 있다. ISPCR은 세포내의 target sequence를 증폭시키는 것으로부터 반응이 시작되며 세포막을 통해 여러 물질(예: PCR 용액에 들어 있는 salt 등)들이 쉽게 이동할 수 있도록 세포막을 HCl, proteinase K 또는 Triton X-100 등으로 처리해 주는 과정을 거쳐야 한다. 이 과정에 이상이 생기면 세포가 손상되거나 파괴되는 수가 있으므로 PCR이 끝난 후에 세포 안에서 증폭된 PCR 산물이 세포 밖으로 빠져나오는 원인이 되기도 한다. 그러므로 적당한 세제의 적절한 선택과 사용이 무엇보다 중요하며 PCR 산물이 세포 밖으로 유출되는 것을 막기 위해서는 single primer pair with complementary tail, biotinylated dNTPs, multiple overlapping primer pair 등을 이용한 방법이 소개되어 있다.현재까지 ISPCR 방법의 효율은 그다지 높지 못한 것으로 알려져 있으며 specificity를 증가시키기 위해서는 DNA probe를 이용한 in situ hybridization을 시행하거나 Southern blot hybridization을 시행하는 것이 좋은 방법이 된다. 효율이 별로 높지 못한 방법임에도 불구하고 ISPCR에 대한 관심이 최근 크게 증가하고 있는 것은 여러 가지 질병들의 조기 발견에 큰 도움을 받을 수 있을 것으로 기대되기 때문이며, 지금도 여러 회사에서 ISPCR에 유용한 기구들을 제작, 판매하고 있으나 더 좋은 기구의 개발이 이루어져 효율을 더욱 높일 수 있다면 ISPCR이 더욱 유용하게 사용될 수 있을 것으로 기대된다.\n\n\nDDRT-PCR(Differential Display Reverse Transcriptase PCR):\n일반적으로 고등동물에서는 약 100,000가지 정도의 서로 다른 유전자가 발현되고 있으며 각각의 세포 한 개에서는 이중 약 15%만이 발현되고 있다. 발생, 분화, homeostasis, 세포주기 조절, 노화, 발암과정 및 세포의 퇴화 등의 과정에서 표현되어 나타나는 유전자들은 전체 유전자들 중에서 일부가 선택되어 나타나는 것이라고 할 수가 있다. 특정 세포에서 발현되는 특정 유전자들을 찾아내기 위하여 주로 사용되는 방법은 subtractive hybridization 또는 differential hybridization 방법이었으나 최근 PCR을 이용하여 유전자를 찾아내는 방법(DDRT-PCR)이 개발되었다. DDRT-PCR이란 서로 다른 세포로부터 RNA를 분리한 후 특정하게 발현되는 mRNA를 찾아내기 위하여 T 염기 10개와 비특정염기 2개가 연결된 oligonucleotide를 primer로 이용하여 cDNA를 합성한 후 이 primer와 비특정 염기서열을 지닌 oligonucleotide를 primer로 이용하여 PCR 하였을 때 나타나는 여러가지 PCR 산물을 비교함으로써 서로 다른 세포로부터 증폭된 PCR 산물에 차이가 있는지 없는지를 확인하는 방법이다. 서로 다르게 증폭된 PCR 산물을 선택하여 이와 같이 증폭된 DNA가 특정 세포에서 특징적으로 발현되는 유전자인지를 보는 것으로 subtractive hybridization보다 방법이 훨씬 간단한 것이 장점이지만, 실험의 sensitivity와 specificity가 낮은 것이 단점이다. 최근 이 방법에 대한 연구가 많이 진행되면서 새로운 더 좋은 방법들이 계속 발표되고 있으므로 최신 논문을 참고로 적절한 조건을 찾아내어 실험을 시행하면 좋은 결과를 얻을 수 있을 것으로 기대된다.\n\n특징:\n\n서로 다르게 증폭된 PCR 산물을 선택하여 이와 같이 증폭된 DNA가 특정 세포에서 특징적으로 발현되는 유전자인지를 보는 것으로 subtractive hybridization보다 방법이 훨씬 간단한 것이 장점\n\n\n\n단점:\n\n실험의 sensitivity와 specificity가 낮은 것이 단점\n\n\n\n\nHot start PCR:\nTaq1 polymerase는 37℃에서도 그 활성이 좋기 때문에 간혹 가다가 첫 번째 denaturation 과정이 완전히 진행되기도 전에 primer가 DNA molecule에 붙어서 extension이 일어나는 경우가 있다. 이렇게 낮은 온도에서 annealing이 일어날 경우 primer가 mismatch할 확률이 높고, 따라서 결과적으로 정확도가 떨어지는 band를 얻게 된다. 이것을 막는 방법이 Hot-start PCR를 이용하는 것이다. Hot start PCR에서는 primer가 정확하게 원하는 DNA site에만 붙을 수 있도록 충분한 온도가 된 후에 PCR이 일어나도록 필요한 물질(ex. polymerase, MgCl2, dNTP)을 넣어준다. (물론 그 전에는 안 넣어준다는 말이다.) 이렇게 하면 위에서 말한 것과 같은 mismatch를 피할 수 있어, 상대적으로 clear한 band를 얻을 수 있다.\n\n\nRAPD PCR:\nRAPD (random amplified polymorphic DNA) PCR은 일반적으로 두 생명체가 계통 유전학적으로 얼마나 유사성이 있는가를 판별할 때 사용한다. PCR할 때 primer가 짧을 경우 genome 상에 여러 군데 달라 붙어서 여러 fragment를 만든다. 이렇게 해서 생겨난 여러 fragment를 전기영동을 사용해서 확인하면 생명체마다 각각 독특한 band를 형성하는데 이것을 가지고 생명체간의 유사성을 판별할 수 있다. 만약 두 생명체가 비슷한 종일 경우에는 band의 모양이 비슷할 것이며, 그렇지 않을 경우에는 서로 많은 차이를 나타낼 것이다. 이 RAPD PCR은 방법이 비교적 간단하고 쉬워 genome sequencing을 하기 전에 대략적인 종간의 관계를 나타내기 위해서 많이 사용된다.\n\n\nTouchdown PCR:\n방법은 PCR할 때 Tm(melting temperature)을 알기 위해서 하는 PCR 방법이다. PCR 과정에서 매 2 cycle 마다 annealing temperature을 1℃씩 낮춘다. 일반적으로 annealing이 Tm에서 1℃만큼 차이가 난 온도에서 일어날 때 한 cycle에 product의 양이 두배 정도 차이가 난다. Touchdown PCR에서는 2 cycle 마다 온도를 낮췄으므로 1℃에 product의 양이 4배가 차이나는 셈이다. 이것을 이용해서 Tm을 알 수 있다. (즉 온도가 1도 내려갔을 때 product의 양이 4배 증가한 과정에서의 temperature가 Tm이 된다.) 특징적으로 Gene-specific product 를 많이 생성하므로, non-specific amplification을 줄일 수 있는 PCR 방법\n\n\nLong Accurate PCR:\n일반적으로 PCR로는 많아야 3kb, 좀 더 이상적일려면 1kb 미만의 크기의 DNA를 증폭할 수 있으나 이 방법을 사용하면 5-40kb의 DNA도 증폭할 수 있다. 이 long accurate PCR에서는 polymerase를 두 개 사용하는데, 그중 하나가 minor-proofreading을 할 수 있어서, 긴 DNA를 증폭할 때 error rate를 감소시켜 준다.\n\n\nAsymmetric PCR:\nSingle strand를 얻을 목적으로 이용하는 PCR방법이다. Primer를 두 개 사용하는데 두 primer를 농도가 100:1로 섞어 사용한다. PCR의 초기에 한쪽의 primer는 소비되어 버리고 과잉의 primer에서 single strand DNA가 생성된다\n\n\nNest PCR:\n한번 PCR로 증폭한 DNA 단편을 한번 더 내부의 primer를 사용하여 증폭하는 방법이다. 이는 비 특이적 반응을 감소 시키며, PCR을 2회 실시하므로 감도가 상승하는 효과를 얻을 수 있다.\n\n\nInverse PCR:\nInverse PCR은 기존에 서열을 알고 있는 DNA의 양 옆에 서열을 알지 못하는 region이 있고, 이것을 알고자 할 때 많이 이용된다.\n\n\nGradient PCR:\n하나의 목적 유전자를 증폭하는 반응조건을 최적화하기 위하여 여러 번의 실험을 수행해야 하는 번거로움이 있었음. 각 단계의 온도 조건에서 시료 block의 첫 lane과 마지막 lane 사이에 최대 20 ℃의 온도 폭 설정이 가능한 gradient 기능을 이용하여 한번의 실험으로 반응조건을 최적화하는 PCR 방법\n효과:\n\nAnnealing temp. 를 단계별로 설정하여 최적의 melting temp.를 찾을 수 있음\nDenaturation 단계뿐만 아니라 extension 단계에도 적용 가능\n\n\n\nReal-Time PCR:\nThermal Cycler와 분광 형광 광도계가 일체화된 장치를 이용하여 PCR 증폭 산물의 생성 과정을 real time으로 모니터링하여 해석하는 방법. 지금까지 단순히 DNA 증폭을 목적으로 하는 PCR에 비해 증폭량을 real time으로 모니터 하면서 PCR 하는 방법입니다. Real time PCR은 지금까지의 PCR에 비해 (1)전기영동이 필요없고, (2)반응 사이클 도중에 증폭산물을 확인할 수 있으며, (3)정량적인 결과를 얻을 수 있는 이점을 가지고 있습니다.\n특징:\n\n신속성: 전기영동이 필요 없음\n정량성: 증폭이 지수함수적으로 일어나는 영역에서 증폭산물량을 비교할 수 있음(보다 정확한 정량이 가능)\n역전사효소를 이용하여 total RNA나 mRNA에서 cDNA를 합성한 후, PCR로 목적 cDNA를 증폭하는 RT(Reverse Transcription)-PCR 방법은, 미량의 RNA 시료에서도 분석이 가능하여 RNA 실험에서 중요하게 대두되고 있음\n\n\n\nPCR Annealing 온도\nTm은 DNA 두 가닥이 반쯤 풀어졌을때 또는 두 가닥 ssDNA가 반쯤 annealing됬을떄의 온도를 말합니다. “반 쯤” 이라는 표현은 “반쯤 풀어졌을때” 와 “반쯤 붙었을때” 가 같은 의미로 사용됩니다. 긴 template와 고농도의 짧은 primer들이 같이 있을때 프라이머가 적당한 온도가 되면 서서히 유사한 서열에 붙기 시작합니다. 하지만 Tm에서는 반 밖에 붙지 못하므로, 특히 프라이머의 3’ 쪽이 붙지 못한 상태라면 PCR 이 일어나지 않습니다. 그래서 온도를 좀 더 내려서 프라이머가 template에 더 붙도록 합니다. DNA의 Tm 값은 다음과 같이 계산할 수 있습니다.\n\\[ Tm = 4*(G+C)+2*(A+T) \\]\nPrimer를 디자인할때 가능하면 같은 Tm 값의 forward,reverse primer를 주문하는 하는것이 좋습니다. 그렇지 못한 경우 낮은 Tm값의 primer를 기준으로 약 5도 정도 낮은 온도에서 PCR을 수행합니다. 만약 원하는 사이즈의 PCR 밴드와 잡밴드가 같이 뜨면 annealing 온도를 2도씩 올려 가면서 PCR 실행합니다.\n만약 실험실에 gradient PCR기가 있으면 낮은 Tm-5도와 높은 Tm-5도로해서 한번에 하는 방법도 있습니다. 보통 20~30 mer사이는 보통 55도 근처에서 하면 잘 나오는경우가 많습니다.\n\n\n잘 안되는 경우\nprimer의 길이가 50~60mer를 넘어간다던가 GC비율이 현저하게 높거나 낮을때에는 annealing temperature를 찾아야 하는 경우가 많습니다. 그럴때는 바로 gradient PCR을 최저 온도(보통 48~50도)에서 윗쪽으로 해보는것이 좋습니다. 잡밴드를 감수하고 원하는 밴드를 PCR 하기 위함입니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Log.42",
    "section": "",
    "text": "파이썬 statsmodels로 통계분석\n\n\n\n\n\n\n\npython\n\n\nstatstics\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nTaeyoon kim\n\n\n\n\n\n\n  \n\n\n\n\n파이썬 통계분석하기\n\n\n\n\n\n\n\npython\n\n\nstatstics\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nTaeyoon kim\n\n\n\n\n\n\n  \n\n\n\n\nSeurat으로 scRNA seq데이터 분석하기\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\nseurat\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\nTaeyoon kim\n\n\n\n\n\n\n  \n\n\n\n\nminiforge: 파이썬과 R의 패키지 및 개발 환경 관리 도구\n\n\n\n\n\n\n\nnews\n\n\npython\n\n\nminiforge\n\n\nmamba\n\n\nanaconda\n\n\nconda\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nTaeyoon kim\n\n\n\n\n\n\n  \n\n\n\n\nR로 논문급 도표 그리기\n\n\n\n\n\n\n\nnews\n\n\nR\n\n\ndata science\n\n\nggpubr\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nTaeyoon kim\n\n\n\n\n\n\n  \n\n\n\n\n각종 실험 방법 정리\n\n\n\n\n\n\n\nlab\n\n\nbiology\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nTaeyoon kim\n\n\n\n\n\n\n  \n\n\n\n\nJupyter notebook 소개\n\n\n\n\n\n\n\nnews\n\n\nqmd\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nTaeyoon kim\n\n\n\n\n\n\n  \n\n\n\n\n깔끔하게 데이터 정리하기\n\n\n\n\n\n\n\nnews\n\n\npython\n\n\ndata science\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nTaeyoon kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "이 블로그는 제가 생물학자 겸 개발자 로서 흥미롭고 유용하다고 생각되는 것을 올리고 있어요. 과학자로서의 삶 은 흥미로운 것들로 가득찬 모험이어야 합니다. 그래서 뒤죽박죽 엉망처럼 보일 수 있지만 나름 공을 들여 정리한 것들 입니다.\n**‘과학자는 유용(有用)하게 쓸 수 있다고 해서 연구하는 것이 아니다. 그 속에서 희열을 느끼기 때문이다.’** \n-- Henri Poincaré"
  },
  {
    "objectID": "about.html#짧은-소개",
    "href": "about.html#짧은-소개",
    "title": "About",
    "section": "",
    "text": "이 블로그는 제가 생물학자 겸 개발자 로서 흥미롭고 유용하다고 생각되는 것을 올리고 있어요. 과학자로서의 삶 은 흥미로운 것들로 가득찬 모험이어야 합니다. 그래서 뒤죽박죽 엉망처럼 보일 수 있지만 나름 공을 들여 정리한 것들 입니다.\n**‘과학자는 유용(有用)하게 쓸 수 있다고 해서 연구하는 것이 아니다. 그 속에서 희열을 느끼기 때문이다.’** \n-- Henri Poincaré"
  },
  {
    "objectID": "about.html#무계획이-계획",
    "href": "about.html#무계획이-계획",
    "title": "About",
    "section": "무계획이 계획",
    "text": "무계획이 계획\n습관적으로 일을 미루는 나의 행동을 정당하기 위한 변명입니다. 언젠가 하겠다는 생각은 늘해왔지만 글을 쓰는 일은 하기 싫은 일입니다. 아무리 적어도 다시 읽어보면 내용이 형편 없기 때문입니다. 그래도 조금이나마 글쓰는 힘을 키우기 위해 이 블로그를 만들었습니다. 그래서 큰 목표와 목적 없이 일벌레와 게으름뱅이 과학자 사이에서 내 마음대로 조금 유익한 일을 하려고 합니다."
  },
  {
    "objectID": "about.html#작업-환경",
    "href": "about.html#작업-환경",
    "title": "About",
    "section": "작업 환경",
    "text": "작업 환경\n제가 사용하는 환경에 대해서 간략히 적어볼게요.\n\n사용하는 OS는 Windows [#]_ 와 Linux [#]_ 입니다. Linux 중 에서도 Ubuntu를 좀 더 선호했었죠. 그런데 최근에는 더 이쁜 Solus &lt;https://getsol.us/home/&gt;_ 를 쓰고 있습니다.\n코딩할 때는 Visual studio code &lt;https://code.visualstudio.com/&gt;_ 를 쓰고, 파이펫팅 할 때는 Eppendorf 파이펫 &lt;https://www.pipette.com/eppendorfpipettes&gt;_ 을 씁니다, 가볍거든요.\n\n주로 쓰는 프로그래밍 언어는 Python &lt;https://www.python.org/&gt;_ 이고 R &lt;https://www.r-project.org/&gt;_ 과 Julia &lt;https://julialang.org/&gt;_ 를 공부하고 있습니다.\n이 블로그는 Github 에서 무료 호스팅되고, Static site generator 인 Nikola 를 사용했습니다."
  },
  {
    "objectID": "about.html#왜-블로그를-하는가",
    "href": "about.html#왜-블로그를-하는가",
    "title": "About",
    "section": "왜 블로그를 하는가?",
    "text": "왜 블로그를 하는가?\n오래전부터(아마도 2006년) 존재감 없는 블로그를 만들어왔습니다. 제가 기억하는 선에서 한번 나열해보죠.\n\nhttp://netsphere.codex.kr/ : 호스팅 업체가 망해버렸습니다.\nhttp://partrita.posterous.com/ : 서비스가 종료되었습니다.\nhttps://partrita.blogspot.kr/ : 구글검색어 노출에 유리 [#]_ 했지만, 제약이 많아서 워드프레스로 이사\nhttps://partrita.wordpress.com/ : 처음에는 설치형을 사용하다. 무료호스팅을 제공하는 wordpress.com 으로 이사했습니다.\nhttps://partrita.github.io/ : PHP(워드프레스)는 너무 올드한 것 같고 핫한 깃헙으로 이전했습니다.\n\n여러번 이전을 하면서 과거의 글은 소실(나중에 보니 부끄러워 삭제)되었네요. 뭐 괜찮습니다. 제가 블로그를 하는 이유도 명확하지 않거든요. 그냥 재미로 하는 겁니다."
  },
  {
    "objectID": "about.html#nikola-블로그",
    "href": "about.html#nikola-블로그",
    "title": "About",
    "section": "Nikola 블로그",
    "text": "Nikola 블로그\nStatic site generator 중에 가장 유명한건 Jekyll &lt;https://jekyllrb-ko.github.io/&gt;_ 입니다. 저도 처음에는 Jekyll을 사용했습니다. 그러다 Nikola &lt;https://getnikola.com/&gt;_ 를 발견했고 Jupyter notebook 포스팅을 제공 한다는 사실을 알았죠. 저는 주로 Jupyter notebook에 코딩을 하기 때문에 바로 Nikola로 갈아 탔습니다.\n장점: - Jupyter notebook 포맷(ipynb) 변경이 필요하지 않습니다. - Python으로 작성되어 있어 가독성이 좋고 (내가) 편집이 쉽죠. - 여러가지 편리한 명령어를 지원합니다. 예를 들면 github_deploy 로 한방에 올리기!\n단점: - 사용자 수가 적어 정보가 적고 앞으로도 늘어날 것 같지는 않습니다. - 테마는 많이 부족합니다.\n저처럼 Jupyter notebook으로 블로그 포스트를 작성하고 싶으신 분들은 설치 가이드 &lt;http://partrita.github.io/posts/nikola-for-jupyer-blog/&gt;_ 를 참고 하세요."
  },
  {
    "objectID": "about.html#라즈베리파이-_",
    "href": "about.html#라즈베리파이-_",
    "title": "About",
    "section": "라즈베리파이 [#]_",
    "text": "라즈베리파이 [#]_\n최근에 웹 프로그래밍에 관심이 생겨서 개인프로젝트용으로 서버로 사용하고 있습니다. 접속 주소는 http://partrita.iptime.org &lt;http://partrita.iptime.org/&gt;_ 입니다.\n\n라즈베리안 OS를 사용 중입니다.\n간이 NAS로 더 유용하게 쓰고 있습니다.\n\n라즈베리에 대해서도 정리해서 올려놓도록 하겠습니다."
  },
  {
    "objectID": "about.html#저작권에-대해",
    "href": "about.html#저작권에-대해",
    "title": "About",
    "section": "저작권에 대해",
    "text": "저작권에 대해\n이 곳에 올리는 글의 대부분은 저의 독창 적인 내용이 아닙니다. 제가 다른 곳(주로 인터넷)에서 얻은 정보들을 제멋대로 편집 해놓은 것이지요. 그래서 제가 쓴 글에 저작권 보호를 받는 자료가 포함되어 있을 수도 있어요. 만약 저작권을 침해한 것이 있다면 이메일 [#]_ 으로 연락 주시면 바로 처리하겠습니다.\n\n.. [#] 거의 반강제적으로 사용하는거라 좋아하진 않습니다. .. [#] 리누스 토발즈가 개발한 컴퓨터 운영 체제. .. [#] 블로거 서비스가 구글에 인수되었습니다. .. [#] 영국 라즈베리 파이(Raspberry Pi) 재단에서 만든 초소형/초저가 PC .. [#] partrita@gmail.com"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018\n\n\nC.V.\nBrief | Long"
  },
  {
    "objectID": "posts/python_TidyData.html#더-알아-보기",
    "href": "posts/python_TidyData.html#더-알아-보기",
    "title": "깔끔하게 데이터 정리하기",
    "section": "더 알아 보기",
    "text": "더 알아 보기\n\n2014년도에 출판된 논문 입니다.\n데이터의 출처는 이곳 입니다."
  },
  {
    "objectID": "posts/python_TidyData.html#깔끔한-데이터tidy-data의-특징",
    "href": "posts/python_TidyData.html#깔끔한-데이터tidy-data의-특징",
    "title": "깔끔하게 데이터 정리하기",
    "section": "깔끔한 데이터(Tidy data)의 특징",
    "text": "깔끔한 데이터(Tidy data)의 특징\nJeff Leek가 쓴 책 The Elements of Data Analytic Style에서 정의한 깔끔한 데이터는 아래와 같은 특징을 가집니다.\n\n각 변수는 개별의 열(column)으로 존재한다.\n각 관측치는 행(row)를 구성한다.\n각 표는 단 하나의 관측기준에 의해서 조직된 데이터를 저장한다.\n만약 여러개의 표가 존재한다면, 적어도 하나이상의 열(column)이 공유되어야 한다.\n\n\n변수(Variable): 예를 들면 키, 몸무게, 성별\n값(Value): 예를 들자면 152 cm, 80 kg, 여성\n관측치(Observation): 값을 측정한 단위, 여기서는 각각의 사람\n\n너무 복잡하다고 생각되신다면 아래 예시를 확인하세요.\n\n지저분한 데이터의 예:\n\n\n\n\n\n\n\n\nTreatment A\n\n\nTreatment B\n\n\n\n\n\n\nJohn Smith\n\n\n-\n\n\n2\n\n\n\n\nJane Doe\n\n\n16\n\n\n11\n\n\n\n\nMary Johnson\n\n\n3\n\n\n1\n\n\n\n\n\n\n\n\n깔끔한 데이터(Tidy data)의 예:\n\n\n\n\n\n\nName\n\n\nTreatment\n\n\nResult\n\n\n\n\n\n\nJohn Smith\n\n\na\n\n\n-\n\n\n\n\nJane Doe\n\n\na\n\n\n16\n\n\n\n\nMary Johnson\n\n\na\n\n\n3\n\n\n\n\nJohn Smith\n\n\nb\n\n\n2\n\n\n\n\nJane Doe\n\n\nb\n\n\n11\n\n\n\n\nMary Johnson\n\n\nb\n\n\n1"
  },
  {
    "objectID": "posts/python_TidyData.html#열-이름column-header이-변수-이름이-아니고-값인-경우",
    "href": "posts/python_TidyData.html#열-이름column-header이-변수-이름이-아니고-값인-경우",
    "title": "깔끔하게 데이터 정리하기",
    "section": "1. 열 이름(Column header)이 변수 이름이 아니고 값인 경우",
    "text": "1. 열 이름(Column header)이 변수 이름이 아니고 값인 경우\n\nPew Research Center Dataset\n종교에 따른 개인의 수입의 관한 데이터입니다. 먼저 pandas의 read_csv기능을 사용해 파일을 읽어옵니다.\n\ndf = pd.read_csv(\"./data/pew-raw.csv\")\ndf\n\n\n\n\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n\n\n\n\n0\nAgnostic\n27\n34\n60\n81\n76\n137\n\n\n1\nAtheist\n12\n27\n37\n52\n35\n70\n\n\n2\nBuddhist\n27\n21\n30\n34\n33\n58\n\n\n3\nCatholic\n418\n617\n732\n670\n638\n1116\n\n\n4\nDont know/refused\n15\n14\n15\n11\n10\n35\n\n\n5\nEvangelical Prot\n575\n869\n1064\n982\n881\n1486\n\n\n6\nHindu\n1\n9\n7\n9\n11\n34\n\n\n7\nHistorically Black Prot\n228\n244\n236\n238\n197\n223\n\n\n8\nJehovahs Witness\n20\n27\n24\n24\n21\n30\n\n\n9\nJewish\n19\n19\n25\n25\n30\n95\n\n\n\n\n\n\n\n\nEvangelical Prot는 기독교 종파중에 하나로 ’개신교’입니다.\n\n문제점: 이 데이터들의 문제는 열 이름(columns headers)이 개인소득의 범위로 되어 있다는 것이죠.\n\n다시 말해서 보기에는 좋아보일지는 몰라도 분석하기에는 어려운 형식입니다.\n\n이러한 데이터를 Tidy data 형태로 변환하기 위해서 pandas라이브러리에서는 아주 쉬운 기능을 제공해 줍니다. 바로 melt라는 기능이지요. pivot table의 반대되는 개념으로 행이 많은 데이터를 열이 많은 데이터로 바꿔줍니다. melt는 아주 유용하기 때문에 앞으로도 자주 언급 됩니다.\n\nformatted_df = pd.melt(df, [\"religion\"], var_name=\"income\", value_name=\"freq\")\nformatted_df = formatted_df.sort_values(by=[\"religion\"])  # 종교 이름순으로 정렬\nformatted_df.head(10)  # 너무 길기 때문에 윗쪽 10개만 보겠습니다.\n\n\n\n\n\n\n\nreligion\nincome\nfreq\n\n\n\n\n0\nAgnostic\n&lt;$10k\n27\n\n\n30\nAgnostic\n$30-40k\n81\n\n\n40\nAgnostic\n$40-50k\n76\n\n\n50\nAgnostic\n$50-75k\n137\n\n\n10\nAgnostic\n$10-20k\n34\n\n\n20\nAgnostic\n$20-30k\n60\n\n\n41\nAtheist\n$40-50k\n35\n\n\n21\nAtheist\n$20-30k\n37\n\n\n11\nAtheist\n$10-20k\n27\n\n\n31\nAtheist\n$30-40k\n52\n\n\n\n\n\n\n\n이것이 Pew Research Center Dataset 의 Tidy data 형태 입니다.\n\n\nBillboard Top 100 Dataset\n이 데이터는 아주 오래전 같은 1999년부터 2000년까지의 빌보드차트 주간 순위 변동을 포함하고 있는 데이터 입니다.\n\ndf = pd.read_csv(\"./data/billboard.csv\", encoding=\"mac_latin2\")\ndf.head(10)\n\n\n\n\n\n\n\nyear\nartist.inverted\ntrack\ntime\ngenre\ndate.entered\ndate.peaked\nx1st.week\nx2nd.week\nx3rd.week\n...\nx67th.week\nx68th.week\nx69th.week\nx70th.week\nx71st.week\nx72nd.week\nx73rd.week\nx74th.week\nx75th.week\nx76th.week\n\n\n\n\n0\n2000\nDestiny's Child\nIndependent Women Part I\n3:38\nRock\n2000-09-23\n2000-11-18\n78\n63.0\n49.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2000\nSantana\nMaria, Maria\n4:18\nRock\n2000-02-12\n2000-04-08\n15\n8.0\n6.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2000\nSavage Garden\nI Knew I Loved You\n4:07\nRock\n1999-10-23\n2000-01-29\n71\n48.0\n43.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2000\nMadonna\nMusic\n3:45\nRock\n2000-08-12\n2000-09-16\n41\n23.0\n18.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2000\nAguilera, Christina\nCome On Over Baby (All I Want Is You)\n3:38\nRock\n2000-08-05\n2000-10-14\n57\n47.0\n45.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\n2000\nJanet\nDoesn't Really Matter\n4:17\nRock\n2000-06-17\n2000-08-26\n59\n52.0\n43.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6\n2000\nDestiny's Child\nSay My Name\n4:31\nRock\n1999-12-25\n2000-03-18\n83\n83.0\n44.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\n2000\nIglesias, Enrique\nBe With You\n3:36\nLatin\n2000-04-01\n2000-06-24\n63\n45.0\n34.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8\n2000\nSisqo\nIncomplete\n3:52\nRock\n2000-06-24\n2000-08-12\n77\n66.0\n61.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9\n2000\nLonestar\nAmazed\n4:25\nCountry\n1999-06-05\n2000-03-04\n81\n54.0\n44.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows × 83 columns\n\n\n\n문제점:\n\n열 이름(columns headers)이 값으로 구성되어 있다: x1st.week, x2st.week 등등\n순위가 100위권 밖으로 밀려나게 되면 Nan 값을 가지고, 따라서 필요없는 부수적인 값이 많아진다.\n\n위 데이터의 깔끔한 데이터를 만들기 위해서는 다시 melt 기능을 사용하겠습니다. 각각의 열이 행이 되도록 하고, 순위가 100위 밖으로 밀려난 경우(Nan값을 갖는경우)에는 행을 삭제할게요.\n\n# Melting\nid_vars = [\n    \"year\",\n    \"artist.inverted\",\n    \"track\",\n    \"time\",\n    \"genre\",\n    \"date.entered\",\n    \"date.peaked\",\n]\ndf = pd.melt(frame=df, id_vars=id_vars, var_name=\"week\", value_name=\"rank\")\n\n# Formatting\ndf[\"week\"] = (\n    df[\"week\"].str.extract(\"(\\d+)\", expand=False).astype(int)\n)  # 정규식으로 x1st.week 에서 숫자 1만 추출\ndf[\"rank\"] = df[\"rank\"].astype(int)\n\n# 필요없는 행을 삭제합니다.\ndf = df.dropna()\n\n# Create \"date\" columns\ndf[\"date\"] = (\n    pd.to_datetime(df[\"date.entered\"])\n    + pd.to_timedelta(df[\"week\"], unit=\"w\")\n    - pd.DateOffset(weeks=1)\n)\n\ndf = df[[\"year\", \"artist.inverted\", \"track\", \"time\", \"genre\", \"week\", \"rank\", \"date\"]]\ndf = df.sort_values(\n    ascending=True, by=[\"year\", \"artist.inverted\", \"track\", \"week\", \"rank\"]\n)\n\n# Assigning the tidy dataset to a variable for future usage\nbillboard = df\n\ndf.head(10)\n\n\n\n\n\n\n\nyear\nartist.inverted\ntrack\ntime\ngenre\nweek\nrank\ndate\n\n\n\n\n246\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n1\n87\n2000-02-26\n\n\n563\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n2\n82\n2000-03-04\n\n\n880\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n3\n72\n2000-03-11\n\n\n1197\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n4\n77\n2000-03-18\n\n\n1514\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n5\n87\n2000-03-25\n\n\n1831\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n6\n94\n2000-04-01\n\n\n2148\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n7\n99\n2000-04-08\n\n\n287\n2000\n2Ge+her\nThe Hardest Part Of Breaking Up (Is Getting Ba...\n3:15\nR&B\n1\n91\n2000-09-02\n\n\n604\n2000\n2Ge+her\nThe Hardest Part Of Breaking Up (Is Getting Ba...\n3:15\nR&B\n2\n87\n2000-09-09\n\n\n921\n2000\n2Ge+her\nThe Hardest Part Of Breaking Up (Is Getting Ba...\n3:15\nR&B\n3\n92\n2000-09-16\n\n\n\n\n\n\n\n위와 같이 데이터를 깔끔하게 만들었습니다. 하지만 track, time, genre 열을 보시면 아주 많은 중복이 있는 것을 알 수 있습니다. 이러한 점을 해결 하는 방법은 다음 예제에서 다루어 보겠습니다."
  },
  {
    "objectID": "posts/python_TidyData.html#하나의-표에-여러가지-타입",
    "href": "posts/python_TidyData.html#하나의-표에-여러가지-타입",
    "title": "깔끔하게 데이터 정리하기",
    "section": "2. 하나의 표에 여러가지 타입",
    "text": "2. 하나의 표에 여러가지 타입\n위에서 다루었던 빌보드차트 데이터를 가지고 데이터가 반복되는 문제를 해결해 보겠습니다.\n문제점:\n\n다양한 관측 단위(observational units), 여기서는 song 과 rank가 하나의 표에 들어 있습니다. 이를 위해서는 표를 나눌 필요가 있습니다.\n\n먼저, 각각의 노래의 자세한 내용을 담고 있는 표를 만들어 보겠습니다. 그런다음 각각의 song_id를 부여합니다. 그런다음 순위 값을 가지고 있는 표를 song_id로 정리합니다.\n\nsongs_cols = [\"year\", \"artist.inverted\", \"track\", \"time\", \"genre\"]\nsongs = billboard[songs_cols].drop_duplicates()\nsongs = songs.reset_index(drop=True)\nsongs[\"song_id\"] = songs.index\nsongs.head(10)\n\n\n\n\n\n\n\nyear\nartist.inverted\ntrack\ntime\ngenre\nsong_id\n\n\n\n\n0\n2000\n2 Pac\nBaby Don't Cry (Keep Ya Head Up II)\n4:22\nRap\n0\n\n\n1\n2000\n2Ge+her\nThe Hardest Part Of Breaking Up (Is Getting Ba...\n3:15\nR&B\n1\n\n\n2\n2000\n3 Doors Down\nKryptonite\n3:53\nRock\n2\n\n\n3\n2000\n3 Doors Down\nLoser\n4:24\nRock\n3\n\n\n4\n2000\n504 Boyz\nWobble Wobble\n3:35\nRap\n4\n\n\n5\n2000\n98°\nGive Me Just One Night (Una Noche)\n3:24\nRock\n5\n\n\n6\n2000\nA*Teens\nDancing Queen\n3:44\nPop\n6\n\n\n7\n2000\nAaliyah\nI Don't Wanna\n4:15\nRock\n7\n\n\n8\n2000\nAaliyah\nTry Again\n4:03\nRock\n8\n\n\n9\n2000\nAdams, Yolanda\nOpen My Heart\n5:30\nGospel\n9\n\n\n\n\n\n\n\n위 와같은 새로운 표를 분리하고, 아래와 같이 순위를 포함하고 있는 표를 새로 만들어줍니다. &gt; 두개의 표를 연결하기 위해 song_id열을 만드는 것을 주의하세요\n\nranks = pd.merge(\n    billboard, songs, on=[\"year\", \"artist.inverted\", \"track\", \"time\", \"genre\"]\n)\nranks = ranks[[\"song_id\", \"date\", \"rank\"]]\nranks.head(10)\n\n\n\n\n\n\n\nsong_id\ndate\nrank\n\n\n\n\n0\n0\n2000-02-26\n87\n\n\n1\n0\n2000-03-04\n82\n\n\n2\n0\n2000-03-11\n72\n\n\n3\n0\n2000-03-18\n77\n\n\n4\n0\n2000-03-25\n87\n\n\n5\n0\n2000-04-01\n94\n\n\n6\n0\n2000-04-08\n99\n\n\n7\n1\n2000-09-02\n91\n\n\n8\n1\n2000-09-09\n87\n\n\n9\n1\n2000-09-16\n92"
  },
  {
    "objectID": "posts/python_TidyData.html#다양한-변수가-하나의-열에-있는-경우-multiple-variables-stored-in-one-column",
    "href": "posts/python_TidyData.html#다양한-변수가-하나의-열에-있는-경우-multiple-variables-stored-in-one-column",
    "title": "깔끔하게 데이터 정리하기",
    "section": "3. 다양한 변수가 하나의 열에 있는 경우 Multiple variables stored in one column",
    "text": "3. 다양한 변수가 하나의 열에 있는 경우 Multiple variables stored in one column\n\nTubercolosis Example\nWHO(World Health Organization)에서 수집한 결핵환자의 기록입니다. 이 데이터에는 확인된 결핵환자의 국가, 연도, 나이, 성별이 포함되어 있습니다.\n문제점:\n\n몇개의 열(columns)에 다양한 변수가 포함되어 있습니다.(성별과 나이)\n값이 존재하지 않는 곳에 NaN과 0 이 혼재되어 있습니다.\n\n미리 알아둘 점:\n\n열의 이름에 적혀있는 “m”이나 “f”는 성별을 뜻합니다.\n열의 이름에 적혀있는 숫자는 나이대(“0-14”,“15-24”, “25-34”, “45-54”, “55-64”, “65”, “unknown”)를 나타냅니다.\n\n\ndf = pd.read_csv(\"./data/tb-raw.csv\")\ndf\n\n\n\n\n\n\n\ncountry\nyear\nm014\nm1524\nm2534\nm3544\nm4554\nm5564\nm65\nmu\nf014\n\n\n\n\n0\nAD\n2000\n0.0\n0.0\n1.0\n0.0\n0\n0\n0.0\nNaN\nNaN\n\n\n1\nAE\n2000\n2.0\n4.0\n4.0\n6.0\n5\n12\n10.0\nNaN\n3.0\n\n\n2\nAF\n2000\n52.0\n228.0\n183.0\n149.0\n129\n94\n80.0\nNaN\n93.0\n\n\n3\nAG\n2000\n0.0\n0.0\n0.0\n0.0\n0\n0\n1.0\nNaN\n1.0\n\n\n4\nAL\n2000\n2.0\n19.0\n21.0\n14.0\n24\n19\n16.0\nNaN\n3.0\n\n\n5\nAM\n2000\n2.0\n152.0\n130.0\n131.0\n63\n26\n21.0\nNaN\n1.0\n\n\n6\nAN\n2000\n0.0\n0.0\n1.0\n2.0\n0\n0\n0.0\nNaN\n0.0\n\n\n7\nAO\n2000\n186.0\n999.0\n1003.0\n912.0\n482\n312\n194.0\nNaN\n247.0\n\n\n8\nAR\n2000\n97.0\n278.0\n594.0\n402.0\n419\n368\n330.0\nNaN\n121.0\n\n\n9\nAS\n2000\nNaN\nNaN\nNaN\nNaN\n1\n1\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n이 데이터를 정리하기 위해서는 먼저 melt를 이용해 sex + age group 를 합쳐서 하나의 행으로 만들겠습니다. 그런 다음에 다시 행을 sex, age로 구분해서 정리하도록 하죠.\n\ndf = pd.melt(\n    df, id_vars=[\"country\", \"year\"], value_name=\"cases\", var_name=\"sex_and_age\"\n)\n\n# Extract Sex, Age lower bound and Age upper bound group\ntmp_df = df[\"sex_and_age\"].str.extract(\"(\\D)(\\d+)(\\d{2})\", expand=False)\n\n# Name columns\ntmp_df.columns = [\"sex\", \"age_lower\", \"age_upper\"]\n\n# Create `age`column based on `age_lower` and `age_upper`\ntmp_df[\"age\"] = tmp_df[\"age_lower\"] + \"-\" + tmp_df[\"age_upper\"]\n\n# Merge\ndf = pd.concat([df, tmp_df], axis=1)\n\n# Drop unnecessary columns and rows\ndf = df.drop([\"sex_and_age\", \"age_lower\", \"age_upper\"], axis=1)\ndf = df.dropna()\ndf = df.sort_values(ascending=True, by=[\"country\", \"year\", \"sex\", \"age\"])\ndf.head(10)\n\n\n\n\n\n\n\ncountry\nyear\ncases\nsex\nage\n\n\n\n\n0\nAD\n2000\n0.0\nm\n0-14\n\n\n10\nAD\n2000\n0.0\nm\n15-24\n\n\n20\nAD\n2000\n1.0\nm\n25-34\n\n\n30\nAD\n2000\n0.0\nm\n35-44\n\n\n40\nAD\n2000\n0.0\nm\n45-54\n\n\n50\nAD\n2000\n0.0\nm\n55-64\n\n\n81\nAE\n2000\n3.0\nf\n0-14\n\n\n1\nAE\n2000\n2.0\nm\n0-14\n\n\n11\nAE\n2000\n4.0\nm\n15-24\n\n\n21\nAE\n2000\n4.0\nm\n25-34\n\n\n\n\n\n\n\n이것이 정리된 결과 입니다."
  },
  {
    "objectID": "posts/python_TidyData.html#변수가-행과-열에rows-and-columns-모두-포함되어-있는-경우",
    "href": "posts/python_TidyData.html#변수가-행과-열에rows-and-columns-모두-포함되어-있는-경우",
    "title": "깔끔하게 데이터 정리하기",
    "section": "4. 변수가 행과 열에(rows and columns) 모두 포함되어 있는 경우",
    "text": "4. 변수가 행과 열에(rows and columns) 모두 포함되어 있는 경우\n\nGlobal Historical Climatology Network Dataset\n이것은 2010년도 멕시코 기상청(MX17004)에서 5개월 동안 측정한 기상 데이터입니다.\n\ndf = pd.read_csv(\"./data/weather-raw.csv\")\n\n문제점:\n\n변수들이 행(tmin, tmax) 과 열(days)에 존재합니다.\n\n깔끔한 데이터를 만들기 위해 melt기능을 사용해 day_raw를 각각의 열로 만들겠습니다.\n\ndf = pd.melt(df, id_vars=[\"id\", \"year\", \"month\", \"element\"], var_name=\"day_raw\")\ndf.head(10)\n\n\n\n\n\n\n\nid\nyear\nmonth\nelement\nday_raw\nvalue\n\n\n\n\n0\nMX17004\n2010\n1\ntmax\nd1\nNaN\n\n\n1\nMX17004\n2010\n1\ntmin\nd1\nNaN\n\n\n2\nMX17004\n2010\n2\ntmax\nd1\nNaN\n\n\n3\nMX17004\n2010\n2\ntmin\nd1\nNaN\n\n\n4\nMX17004\n2010\n3\ntmax\nd1\nNaN\n\n\n5\nMX17004\n2010\n3\ntmin\nd1\nNaN\n\n\n6\nMX17004\n2010\n4\ntmax\nd1\nNaN\n\n\n7\nMX17004\n2010\n4\ntmin\nd1\nNaN\n\n\n8\nMX17004\n2010\n5\ntmax\nd1\nNaN\n\n\n9\nMX17004\n2010\n5\ntmin\nd1\nNaN\n\n\n\n\n\n\n\n그럼에도 아직 불필요한 것들이 보이는 군요. 좀 더 깔끔한 데이터를 만들기 위해 tmin, tmax를 각각의 열로 만들겠습니다. 그리고 날짜 정보들을 합쳐서 date로 통합하겠습니다.\n\n# Extracting day\ndf[\"day\"] = df[\"day_raw\"].str.extract(\"d(\\d+)\", expand=False)\ndf[\"id\"] = \"MX17004\"\n\n# To numeric values\ndf[[\"year\", \"month\", \"day\"]] = df[[\"year\", \"month\", \"day\"]].apply(\n    lambda x: pd.to_numeric(x, errors=\"ignore\")\n)\n\n\n# Creating a date from the different columns\ndef create_date_from_year_month_day(row):\n    return datetime.datetime(year=row[\"year\"], month=int(row[\"month\"]), day=row[\"day\"])\n\n\ndf[\"date\"] = df.apply(lambda row: create_date_from_year_month_day(row), axis=1)\ndf = df.drop([\"year\", \"month\", \"day\", \"day_raw\"], axis=1)\ndf = df.dropna()\n\n# Unmelting column \"element\"\ndf = df.pivot_table(index=[\"id\", \"date\"], columns=\"element\", values=\"value\")\ndf.reset_index(drop=False, inplace=True)\ndf\n\n\n\n\n\n\nelement\nid\ndate\ntmax\ntmin\n\n\n\n\n0\nMX17004\n2010-02-02\n27.3\n14.4\n\n\n1\nMX17004\n2010-02-03\n24.1\n14.4\n\n\n2\nMX17004\n2010-03-05\n32.1\n14.2\n\n\n\n\n\n\n\n충분히 깔끔한 모양새가 되었습니다."
  },
  {
    "objectID": "posts/python_TidyData.html#하나의-관측-단위observational-units가-여러-파일로-나누어져-있는-경우",
    "href": "posts/python_TidyData.html#하나의-관측-단위observational-units가-여러-파일로-나누어져-있는-경우",
    "title": "깔끔하게 데이터 정리하기",
    "section": "5. 하나의 관측 단위(observational units)가 여러 파일로 나누어져 있는 경우",
    "text": "5. 하나의 관측 단위(observational units)가 여러 파일로 나누어져 있는 경우\n\nBaby Names in Illinois\n2014, 2015년도 미국 일리노이 주(Illinois)의 신생아의 (남자)이름을 수집한 데이터 입니다.\n문제점:\n\n여러 표와 파일에 데이터가 흩어져 있다.\n연도(Year)”가 파일 이름에 적혀져 있다.\n\n서로 다른 파일에 데이터가 흩어져 있어 조금 복잡한 과정이 필요합니다. 먼저 아래의 코드로 파일 리스트를 만들고 거기에서 연도 값을 뽑아냅니다. 그런 다음 각각의 파일에서 표를 만들어내고 마지막으로 concat기능으로 사용해 하나의 표로 합치겠습니다.\n\ndef extract_year(string):\n    match = re.match(\".+(\\d{4})\", string)\n    if match is not None:\n        return match.group(1)\n\n\npath = \"./data\"\nallFiles = glob.glob(path + \"/201*-baby-names-illinois.csv\")\nframe = pd.DataFrame()\ndf_list = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_, index_col=None, header=0)\n    df.columns = map(str.lower, df.columns)\n    df[\"year\"] = extract_year(file_)\n    df_list.append(df)\n\ndf = pd.concat(df_list)\ndf.head(5)\n\n\n\n\n\n\n\nrank\nname\nfrequency\nsex\nyear\n\n\n\n\n0\n1\nNoah\n837\nMale\n2014\n\n\n1\n2\nAlexander\n747\nMale\n2014\n\n\n2\n3\nWilliam\n687\nMale\n2014\n\n\n3\n4\nMichael\n680\nMale\n2014\n\n\n4\n5\nLiam\n670\nMale\n2014"
  },
  {
    "objectID": "posts/python_TidyData.html#마치며",
    "href": "posts/python_TidyData.html#마치며",
    "title": "깔끔하게 데이터 정리하기",
    "section": "마치며",
    "text": "마치며\n이 글에서 가장 중점으로 둔것은 파이썬으로 지저분한 데이터를 깔끔하게 만드는 것이 었습니다. 그걸을 위해 Wickham의 논문에서 사용된 데이터를 살펴 보았죠. 깔끔한 데이터(Tidy data)의 최고의 장점은 시각화(Visualization)이 쉽다는 것에 있습니다. 그것은 다음에 다루어 보도록 하겠습니다.\n앞으로는 Tidy data를 고려해서 데이터를 수집하도록 하세요. 모두의 시간은 소중하니까요."
  },
  {
    "objectID": "posts/python_StatisticalTesting.html",
    "href": "posts/python_StatisticalTesting.html",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "",
    "text": "통계적 추론이라는 것은 제한된 실험 데이터에서 얻은 결과를 모집단에도 적용하려는 것입니다. 이번 포스트에서는 통계적 추론에 사용되는 검정법을 배워봅니다."
  },
  {
    "objectID": "posts/python_StatisticalTesting.html#순열-검정을-통한-ab-검정",
    "href": "posts/python_StatisticalTesting.html#순열-검정을-통한-ab-검정",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "1.1. 순열 검정을 통한 A/B 검정",
    "text": "1.1. 순열 검정을 통한 A/B 검정\n\n순열검정(permutaion test): 두 개 이상의 표본을 함께 결합하여 관측값들을 무작위로 재표본으로 추출하는 과정을 말한다.\n\n파이썬에서 순열 검정을 구현하기 위해 아래와 같이 perm_fun 함수를 정의합니다.\n\n# Permutation test example with stickiness\ndef perm_fun(x, nA, nB):\n    n = nA + nB\n    idx_B = set(random.sample(range(n), nB))\n    idx_A = set(range(n)) - idx_B\n    return x.loc[idx_B].mean() - x.loc[idx_A].mean()\n\n\nnA = session_times[session_times.Page == \"Page A\"].shape[0]\nnB = session_times[session_times.Page == \"Page B\"].shape[0]\nprint(perm_fun(session_times.Time, nA, nB))\n\n24.23809523809524\n\n\n단 한번의 계산을 통해서 24초라는 차이가 발생하였습니다. 계산을 반복해서 페이지A와 페이지B 사이의 시간 차이에 대한 도수 분포표를 그려봅시다.\n\nrandom.seed(1)\nperm_diffs = [perm_fun(session_times.Time, nA, nB) for _ in range(1000)]\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.hist(perm_diffs, bins=11, rwidth=0.9)\nax.axvline(x=mean_b - mean_a, color=\"black\", lw=2)\nax.text(50, 190, \"Observed\\ndifference\", bbox={\"facecolor\": \"white\"})\nax.set_xlabel(\"Session time differences (in seconds)\")\nax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n위 그림에서 수직선은 관측된 차이입니다. 이것을 통해 순열 검정에서 가끔 실제 관찰된 차이를 넘어가는 것을 알 수 있습니다. 그렇다면 어느정도의 확률로 그런 일이 벌어질까요?\n\nprint(np.mean(perm_diffs &gt; mean_b - mean_a))\n\n0.121\n\n\n답은 12.1% 입니다. 이것을 통해 페이지A와 페이지B의 차이인 36초가 통계적으로 봤을때는 차이가 없어도 약 12% 확률로 발생할 수 있다는 결론을 얻었습니다."
  },
  {
    "objectID": "posts/python_StatisticalTesting.html#t-test를-사용한-ab-검정",
    "href": "posts/python_StatisticalTesting.html#t-test를-사용한-ab-검정",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "1.1.2. t-Test를 사용한 A/B 검정",
    "text": "1.1.2. t-Test를 사용한 A/B 검정\n\nt-테스트(t-test) 또는 t-검정 또는 스튜던트 t-테스트(Student’s t-test)는 검정통계량이 귀무가설 하에서 t-분포를 따르는 통계적 가설 검정법이다. t-테스트는 일반적으로 검정통계량이 정규 분포를 따르며 분포와 관련된 스케일링 변숫값들이 알려진 경우에 사용한다. 이때 모집단의 분산과 같은 스케일링 항을 알 수 없으나, 이를 데이터를 기반으로 한 추정값으로 대체하면 검정통계량은 t-분포를 따른다. 예를 들어 t-테스트를 사용하여 두 데이터 세트(집단)의 평균이 서로 유의하게 다른지 여부를 판별할 수 있다. -wiki\n\n파이썬에서는 ttest_ind 함수를 사용하면 손쉽게 구해볼 수 있습니다.\n\nres = stats.ttest_ind(\n    session_times[session_times.Page == \"Page A\"].Time,\n    session_times[session_times.Page == \"Page B\"].Time,\n    equal_var=False,\n)\nprint(f\"p-value for single sided test: {res.pvalue / 2:.4f}\")\n\np-value for single sided test: 0.1408\n\n\n결과 값이 앞서 구한 순열 검정의 확률인 12%과 유사한 수치인 14%임을 확인 할 수 있습니다. 컴퓨터가 보급되기 전에 순열 검정은 실용적이지 않았고 그래서 통계학자들에게 t-Test가 널리 사용되었습니다.\n\ntstat, pvalue, df = sm.stats.ttest_ind(\n    session_times[session_times.Page == \"Page A\"].Time,\n    session_times[session_times.Page == \"Page B\"].Time,\n    usevar=\"unequal\",\n    alternative=\"smaller\",\n)\nprint(f\"p-value: {pvalue:.4f}\")\n\np-value: 0.1408"
  },
  {
    "objectID": "posts/python_StatisticalTesting.html#p-value-구하기",
    "href": "posts/python_StatisticalTesting.html#p-value-구하기",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "2.1. P-Value 구하기",
    "text": "2.1. P-Value 구하기\np-value는 순열 검정에서 얻은 결과 중에 관찰된 차이와 같거나 더 큰 차이를 보이는 경우의 비율이라고 할 수 있기에 다음과 같이 추정할 수 있습니다.\n\nprint(np.mean([diff &gt; obs_pct_diff for diff in perm_diffs]))\n\n0.332\n\n\n예상한 것처럼 30%의 확률로 우연에 의해서 나타날 수 있는 차이였습니다. 따라서 페이지A와 페이지B의 차이는 통계적으로 유의미하지 않다고 말할 수 있겠습니다."
  },
  {
    "objectID": "posts/python_StatisticalTesting.html#순열검정을-통한-one-way-anova",
    "href": "posts/python_StatisticalTesting.html#순열검정을-통한-one-way-anova",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "3.1. 순열검정을 통한 one-way ANOVA",
    "text": "3.1. 순열검정을 통한 one-way ANOVA\n파이썬에서는 다음 코드를 사용해 순열 검정을 통해 ANOVA 분석을 진행할 수 있습니다.\n\nobserved_variance = four_sessions.groupby(\"Page\").mean().var()[0]\nprint(\"Observed means:\", four_sessions.groupby(\"Page\").mean().values.ravel())\nprint(\"Variance:\", observed_variance)\n\n\n# Permutation test example with stickiness\ndef perm_test(df):\n    df = df.copy()\n    df[\"Time\"] = np.random.permutation(df[\"Time\"].values)\n    return df.groupby(\"Page\").mean().var()[0]\n\n\nprint(perm_test(four_sessions))\n\nObserved means: [172.8 182.6 175.6 164.6]\nVariance: 55.426666666666655\n57.02666666666678\n\n\n\nrandom.seed(1)\nperm_variance = [perm_test(four_sessions) for _ in range(3000)]\nprint(\"Pr(Prob)\", np.mean([var &gt; observed_variance for var in perm_variance]))\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.hist(perm_variance, bins=11, rwidth=0.9)\nax.axvline(x=observed_variance, color=\"black\", lw=2)\nax.text(60, 200, \"Observed\\nvariance\", bbox={\"facecolor\": \"white\"})\nax.set_xlabel(\"Variance\")\nax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\nPr(Prob) 0.07766666666666666\n\n\n\n\n\nPr(Prob)의 값은 p-value이며 결과는 0.07입니다. 통상적인 임계 p-value 값인 0.05이상임으로 네 페이지간의 차이가 우연히 발생할 수 있다고 결론 내릴 수 있습니다."
  },
  {
    "objectID": "posts/python_StatisticalTesting.html#f-통계량을-통한-one-way-anova",
    "href": "posts/python_StatisticalTesting.html#f-통계량을-통한-one-way-anova",
    "title": "파이썬 statsmodels로 통계분석",
    "section": "3.2. F-통계량을 통한 one-way ANOVA",
    "text": "3.2. F-통계량을 통한 one-way ANOVA\nF-통계량은 잔차 오류에 인한 분산과 그룹 평균의 분산에 대한 비율을 기초로 합니다. 비율이 높으면 통계적으로 유의미 하다고 할 수 있고 이를 토대로 p-value를 계산할 수 있습니다.\n\nmodel = smf.ols(\"Time ~ Page\", data=four_sessions).fit()\n\naov_table = sm.stats.anova_lm(model)\nprint(aov_table)\n\n            df  sum_sq     mean_sq         F    PR(&gt;F)\nPage       3.0   831.4  277.133333  2.739825  0.077586\nResidual  16.0  1618.4  101.150000       NaN       NaN\n\n\ndf는 자유도, sum_sq는 제곱합, mean_sq는 평균제곱, F는 F-통계량을 나타냅니다.\n\nres = stats.f_oneway(\n    four_sessions[four_sessions.Page == \"Page 1\"].Time,\n    four_sessions[four_sessions.Page == \"Page 2\"].Time,\n    four_sessions[four_sessions.Page == \"Page 3\"].Time,\n    four_sessions[four_sessions.Page == \"Page 4\"].Time,\n)\nprint(f\"F-Statistic: {res.statistic / 2:.4f}\")\nprint(f\"p-value: {res.pvalue / 2:.4f}\")\n\nF-Statistic: 1.3699\np-value: 0.0388\n\n\nF-통계량을 사용한 방법은 p-value 값이 더 적게나와 임계값인 0.05 이하입니다. 그러나 ANOVA 분석의 p-value가 낮게 나왔다고 해서 모든 그룹에서 통계적으로 차이가 있다고 할 수는 없습니다. 추가적인 Ad hoc 분석을 진행해 어떤 그룹에서 차이가 있는지 확인해보아야 합니다."
  },
  {
    "objectID": "posts/R_ggpubr.html",
    "href": "posts/R_ggpubr.html",
    "title": "R로 논문급 도표 그리기",
    "section": "",
    "text": "모든 내용은 공식문서에서 간추린 것입니다. 자세한것은 공식문서를 읽어주세요.\nggpubr은 ggplot2에 기반한 R 패키지입니다. 연구자들이 쉽게 높은 질의 도표를 그리는 것을 목표로 하고 있는 시각화 패키지죠. 주요 특징은 다음과 같습니다:\n\nggplot2 패키지를 기반으로해서 좀 더 명확한 문법으로 보다 쉽게 사용할 수 있습니다.\nR 언어를 잘 모르더라도 높은 질의 도표를 만들수 있습니다.\n자동으로 p-values 나 통계적 유의성을 표시할 할 수 있습니다.\n여러 도표를 한 페이지에 배열 할 수 있는 기능을 가지고 있습니다.\n레이블이나 색상을 쉽게 변경할 수 있습니다.\n\n먼저 ggpubr 로 시각화를 하는 간단한 방법을 살펴보고, 이후에 다양한 예시 도표를 보여드리겠습니다."
  },
  {
    "objectID": "posts/R_ggpubr.html#데이터-불러오기",
    "href": "posts/R_ggpubr.html#데이터-불러오기",
    "title": "R로 논문급 도표 그리기",
    "section": "3.1. 데이터 불러오기",
    "text": "3.1. 데이터 불러오기\n\n# 필요한  패키지 불러오기\nlibrary(\"dplyr\") \nlibrary(\"ggpubr\")\noptions(warn=-1) # 경고메세지 무시하기\n\ndata(\"ToothGrowth\") # 예제 데이터 불러오기\nhead(ToothGrowth,4) # 데이터 테이블 확인\n\n\n\n\nlen\nsupp\ndose\n\n\n\n\n4.2\nVC\n0.5\n\n\n11.5\nVC\n0.5\n\n\n7.3\nVC\n0.5\n\n\n5.8\nVC\n0.5"
  },
  {
    "objectID": "posts/R_ggpubr.html#시각화-설정하기",
    "href": "posts/R_ggpubr.html#시각화-설정하기",
    "title": "R로 논문급 도표 그리기",
    "section": "3.2. 시각화 설정하기",
    "text": "3.2. 시각화 설정하기\n\nggline(ToothGrowth, x = \"dose\", y = \"len\", add = \"mean_se\", # 각각의 축설정 \n      color = \"supp\", palette = \"npg\")+  # 색상 설정하기\n      stat_compare_means(aes(group = supp), label = \"p.signif\", label.y = c(16, 25, 29)) + # 통계적 유의성 표시\n      labs(list(x = 'Dose', y = 'Length', fill = 'Supp')) # 레이블 변경"
  },
  {
    "objectID": "posts/R_ggpubr.html#한페이지에-여러-도표-넣기",
    "href": "posts/R_ggpubr.html#한페이지에-여러-도표-넣기",
    "title": "R로 논문급 도표 그리기",
    "section": "3.3. 한페이지에 여러 도표 넣기",
    "text": "3.3. 한페이지에 여러 도표 넣기\n여러 도표를 한페이지에 넣는 기능은 ggarrange()입니다. cowplot의 plot_grid()함수에 기반하고 있죠. 그래서 사용법도 동일합니다. 아래의 예시 코드를 확인하세요.\nggarrange(a, b, c ,  \n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/R_ggpubr.html#분포distribution-시각화",
    "href": "posts/R_ggpubr.html#분포distribution-시각화",
    "title": "R로 논문급 도표 그리기",
    "section": "4.1. 분포(Distribution) 시각화",
    "text": "4.1. 분포(Distribution) 시각화\n\n# 예제 데이터 만들기\nset.seed(1234)\nwdata = data.frame(\n   sex = factor(rep(c(\"F\", \"M\"), each=200)),\n   weight = c(rnorm(200, 55), rnorm(200, 58)))\nhead(wdata, 4)\n\n\n\n\nsex\nweight\n\n\n\n\nF\n53.79293\n\n\nF\n55.27743\n\n\nF\n56.08444\n\n\nF\n52.65430\n\n\n\n\n\n\na1 &lt;- ggdensity(wdata, x = \"weight\",\n   add = \"mean\", rug = TRUE, # Density plot with mean lines and marginal rug\n   color = \"sex\", fill = \"sex\",  # Change outline and fill colors by groups (\"sex\")\n   palette = c(\"#00AFBB\", \"#E7B800\")) # Use custom palette\n\na2 &lt;- gghistogram(wdata, x = \"weight\",\n   add = \"mean\", rug = TRUE,\n   color = \"sex\", fill = \"sex\",\n   palette = c(\"#00AFBB\", \"#E7B800\"))\n\na3 &lt;- ggdensity(wdata, x = \"weight\",\n   add = \"mean\", rug = TRUE,\n   fill = \"lightgray\")\n\n# Combine histogram and density plots\na4 &lt;-  gghistogram(wdata, x = \"weight\",\n   add = \"mean\", rug = FALSE,\n   fill = \"sex\", palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\n\n# 한페이지에 넣기\nggarrange(a1, a2, a3 , a4,\n          labels = c(\"A\", \"B\", \"C\", \"D\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/R_ggpubr.html#막대-그래프bar-plots",
    "href": "posts/R_ggpubr.html#막대-그래프bar-plots",
    "title": "R로 논문급 도표 그리기",
    "section": "4.3. 막대 그래프(Bar plots)",
    "text": "4.3. 막대 그래프(Bar plots)\n\n4.3.1 간단한 막대 그래프\n\n# example Data\ndf &lt;- data.frame(dose=c(\"D0.5\", \"D1\", \"D2\"),\n   len=c(4.2, 10, 29.5))\ndf2 &lt;- data.frame(supp=rep(c(\"VC\", \"OJ\"), each=3),\n   dose=rep(c(\"D0.5\", \"D1\", \"D2\"),2),\n   len=c(6.8, 15, 33, 4.2, 10, 29.5))\ndf3 &lt;- ToothGrowth\n\n# Change position: Interleaved (dodged) bar plot\np1 &lt;- ggbarplot(df2, \"dose\", \"len\",\n        fill = \"supp\", color = \"supp\", palette = \"Paired\",\n        position = position_dodge(0.8))\n\n# Change fill and outline color add labels inside bars\np2 &lt;- ggbarplot(df, \"dose\", \"len\",\n        fill = \"dose\", color = \"dose\",\n        palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n        label = TRUE, lab.pos = \"in\", lab.col = \"white\")\n\n# Add jitter points and errors (mean_se)\np3 &lt;- ggbarplot(df3, x = \"dose\", y = \"len\",\n        add = c(\"mean_se\", \"jitter\"))\n\n# Multiple groups with error bars and jitter point\np4 &lt;- ggbarplot(df3, x = \"dose\", y = \"len\", color = \"supp\",\n         add = \"mean_se\", palette = c(\"#00AFBB\", \"#E7B800\"),\n         position = position_dodge(0.8))\n\nggarrange(p1, p2, p3, p4,\n          labels = c(\"A\", \"B\", \"C\", \"D\"),\n          ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n\n\n4.3.2 정돈된(Ordered) 바 그래프\ncyl에 따라서 그룹화하고, 전체적으로 정렬한 그래프(A)와 그룹별로 정렬한 그래프(B)의 시각화입니다.\n\n# 샘플 데이터 불러오기\ndata(\"mtcars\")\ndfm &lt;- mtcars\ndfm$cyl &lt;- as.factor(dfm$cyl) # Convert the cyl variable to a factor\ndfm$name &lt;- rownames(dfm) # Add the name colums\nhead(dfm[, c(\"name\", \"wt\", \"mpg\", \"cyl\")]) # 데이터 살펴보기\n\n\n\n\n\nname\nwt\nmpg\ncyl\n\n\n\n\nMazda RX4\nMazda RX4\n2.620\n21.0\n6\n\n\nMazda RX4 Wag\nMazda RX4 Wag\n2.875\n21.0\n6\n\n\nDatsun 710\nDatsun 710\n2.320\n22.8\n4\n\n\nHornet 4 Drive\nHornet 4 Drive\n3.215\n21.4\n6\n\n\nHornet Sportabout\nHornet Sportabout\n3.440\n18.7\n8\n\n\nValiant\nValiant\n3.460\n18.1\n6\n\n\n\n\n\n\na1 &lt;- ggbarplot(dfm, x = \"name\", y = \"mpg\",\n          fill = \"cyl\",               # change fill color by cyl\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"desc\",          # Sort the value in dscending order\n          sort.by.groups = FALSE,     # Don't sort inside each group\n          x.text.angle = 90)           # Rotate vertically x axis texts\n\na2 &lt;- ggbarplot(dfm, x = \"name\", y = \"mpg\",\n          fill = \"cyl\",               # change fill color by cyl\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"asc\",           # Sort the value in dscending order\n          sort.by.groups = TRUE,      # Sort inside each group\n          x.text.angle = 90)           # Rotate vertically x axis texts\n\nggarrange(a1, a2,\n          labels = c(\"A\", \"B\"),\n          ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\n4.3.3. 편차(Deviation) 그래프\n편차(deviation) 그래프는 각각의 값들이 평균값 대비 얼마나 차이가 나는지를 시각화 합니다. 여기서는 연비 평균값에 비교해서 각 차량의 편차가 얼마인지 계산해(Z-score) 도표를 그려보겠습니다.\n\n# Calculate the z-score of the mpg data\ndfm$mpg_z &lt;- (dfm$mpg -mean(dfm$mpg))/sd(dfm$mpg)\ndfm$mpg_grp &lt;- factor(ifelse(dfm$mpg_z &lt; 0, \"low\", \"high\"), \n                     levels = c(\"low\", \"high\"))\n# Inspect the data\nhead(dfm[, c(\"name\", \"wt\", \"mpg\", \"mpg_z\", \"mpg_grp\", \"cyl\")])\n\n\n\n\n\nname\nwt\nmpg\nmpg_z\nmpg_grp\ncyl\n\n\n\n\nMazda RX4\nMazda RX4\n2.620\n21.0\n0.1508848\nhigh\n6\n\n\nMazda RX4 Wag\nMazda RX4 Wag\n2.875\n21.0\n0.1508848\nhigh\n6\n\n\nDatsun 710\nDatsun 710\n2.320\n22.8\n0.4495434\nhigh\n4\n\n\nHornet 4 Drive\nHornet 4 Drive\n3.215\n21.4\n0.2172534\nhigh\n6\n\n\nHornet Sportabout\nHornet Sportabout\n3.440\n18.7\n-0.2307345\nlow\n8\n\n\nValiant\nValiant\n3.460\n18.1\n-0.3302874\nlow\n6\n\n\n\n\n\n\n# Create an ordered bar plot, colored according to the level of mpg:\nggbarplot(dfm, x = \"name\", y = \"mpg_z\",\n          fill = \"mpg_grp\",           # change fill color by mpg_level\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"desc\",          # Sort the value in descending order\n          sort.by.groups = FALSE,     # Don't sort inside each group\n          x.text.angle = 90,          # Rotate vertically x axis texts\n          ylab = \"MPG z-score\",\n          legend.title = \"MPG Group\",\n          rotate = TRUE,\n          ggtheme = theme_minimal())"
  },
  {
    "objectID": "posts/R_ggpubr.html#막대사탕lollipop-plot",
    "href": "posts/R_ggpubr.html#막대사탕lollipop-plot",
    "title": "R로 논문급 도표 그리기",
    "section": "4.4.1 막대사탕(Lollipop) plot",
    "text": "4.4.1 막대사탕(Lollipop) plot\n막대사탕 그래프는 많은 양의 데이터를 시각화하는데 적합합니다. 아래 예시에서는 cyl 그룹에 맞춰서 색상을 구분하였습니다.\n\nggdotchart(dfm, x = \"name\", y = \"mpg\",\n           color = \"cyl\",                                # Color by groups\n           palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), # Custom color palette\n           sorting = \"descending\",                       # Sort value in descending order\n           add = \"segments\",                             # Add segments from y = 0 to dots\n           rotate = TRUE,                                # Rotate vertically\n           group = \"cyl\",                                # Order by groups\n           dot.size = 6,                                 # Large dot size\n           label = round(dfm$mpg),                        # Add mpg values as dot labels\n           font.label = list(color = \"white\", size = 9, \n           vjust = 0.5),               # Adjust label parameters\n           ggtheme = theme_pubr())                        # ggplot2 theme"
  },
  {
    "objectID": "posts/R_ggpubr.html#도표에-설명figure-legend-넣기",
    "href": "posts/R_ggpubr.html#도표에-설명figure-legend-넣기",
    "title": "R로 논문급 도표 그리기",
    "section": "4.5. 도표에 설명(figure legend) 넣기",
    "text": "4.5. 도표에 설명(figure legend) 넣기\n도표 밑에 설명을 넣는 방법입니다. 한줄단위로 내용을 끊어서 작성해야, 산출물에서 줄이 잘 맞게 할 수 있습니다. 아래의 예시 코드를 확인하세요.\nggparagraph(text, color = NULL, size = NULL, face = NULL, family = NULL,\n  lineheight = NULL)\n# S3 method for splitText\ndrawDetails(x, recording)\n\n# Density plot\ndensity.p &lt;- ggdensity(iris, x = \"Sepal.Length\",\n                      fill = \"Species\", palette = \"jco\")\n# Text plot\ntext &lt;- paste(\"Iris data set gives the measurements in cm\",\n             \"of the variables sepal length and width\",\n             \"and petal length and width, respectively,\",\n             \"for 50 flowers from each of 3 species of iris.\",\n             \"The species are Iris setosa, versicolor, and virginica.\", sep = \" \")\ntext.p &lt;- ggparagraph(text, face = \"italic\", size = 12)\n\n# Arrange the plots on the same page\nggarrange(density.p, text.p,\n         ncol = 1, nrow = 2,\n         heights = c(1, 0.3))"
  },
  {
    "objectID": "posts/R_ggpubr.html#선-그래프",
    "href": "posts/R_ggpubr.html#선-그래프",
    "title": "R로 논문급 도표 그리기",
    "section": "4.6. 선 그래프",
    "text": "4.6. 선 그래프\n\n# Data: ToothGrowth data set we'll be used.\ndf3 &lt;- ToothGrowth\n# Add error bars: mean_se\n# (other values include: mean_sd, mean_ci, median_iqr, ....)\n# Add labels\np1 &lt;- ggline(df3, x = \"dose\", y = \"len\", add = \"mean_se\")\n# Add jitter points and errors (mean_se)\np2 &lt;- ggline(df3, x = \"dose\", y = \"len\",\n add = c(\"mean_se\",'jitter'))\n# Multiple groups with error bars\np3 &lt;- ggline(df3, x = \"dose\", y = \"len\", color = \"supp\",\n add = \"mean_se\", palette = c(\"#00AFBB\", \"#FC4E07\"))\n\nggarrange(p1, p2, p3,\n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/R_ggpubr.html#히스토그램과-산포도scatter-plot-with-histograms",
    "href": "posts/R_ggpubr.html#히스토그램과-산포도scatter-plot-with-histograms",
    "title": "R로 논문급 도표 그리기",
    "section": "4.7. 히스토그램과 산포도(Scatter Plot with Histograms)",
    "text": "4.7. 히스토그램과 산포도(Scatter Plot with Histograms)\n히스토그램과 산포도를 하나의 도표에 합쳐서 그려보도록 하겠습니다.\n\n# Grouped data\nggscatterhist(\n iris, x = \"Sepal.Length\", y = \"Sepal.Width\",\n color = \"Species\", size = 3, alpha = 0.6,\n palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n margin.params = list(fill = \"Species\", color = \"black\", size = 0.2))"
  },
  {
    "objectID": "posts/R_ggpubr.html#상관분석correlation-coefficients과-p-values-추가하기",
    "href": "posts/R_ggpubr.html#상관분석correlation-coefficients과-p-values-추가하기",
    "title": "R로 논문급 도표 그리기",
    "section": "4.8. 상관분석(Correlation Coefficients)과 P-values 추가하기",
    "text": "4.8. 상관분석(Correlation Coefficients)과 P-values 추가하기\n산포도에 상관분석과 p-values를 추가하는 방법입니다.\n\n# Load data\ndata(\"mtcars\")\ndf &lt;- mtcars\ndf$cyl &lt;- as.factor(df$cyl)\n\n# Scatter plot with correlation coefficient\nsp &lt;- ggscatter(df, x = \"wt\", y = \"mpg\",\n   add = \"reg.line\",  # Add regressin line\n   add.params = list(color = \"blue\", fill = \"lightgray\"), # Customize reg. line\n   conf.int = TRUE) # Add confidence interval\n# Add correlation coefficient\np1 &lt;- sp + stat_cor(method = \"pearson\", label.x = 3, label.y = 30)\n# Color by groups and facet\nsp &lt;- ggscatter(df, x = \"wt\", y = \"mpg\",\n   color = \"cyl\", palette = \"jco\",\n   add = \"reg.line\", conf.int = TRUE)\np2 &lt;- sp + stat_cor(aes(color = cyl), label.x = 3)\n# Scatter plot with ellipses and group mean points\np3 &lt;- ggscatter(df, x = \"wt\", y = \"mpg\",\n   color = \"cyl\", shape = \"cyl\",\n   mean.point = TRUE, ellipse = TRUE)+\n   stat_stars(aes(color = cyl))\n\nggarrange(p1, p2, p3,\n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/R_ggpubr.html#plot-paired-data",
    "href": "posts/R_ggpubr.html#plot-paired-data",
    "title": "R로 논문급 도표 그리기",
    "section": "4.9. Plot Paired Data",
    "text": "4.9. Plot Paired Data\n\n# Example data\nbefore &lt;-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\nafter &lt;-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\n\nd &lt;- data.frame(before = before, after = after)\np1 &lt;- ggpaired(d, cond1 = \"before\", cond2 = \"after\", width = 0.0,\n    line.color = \"gray\", line.size = 0.4, palette = \"npg\")\np2 &lt;- ggpaired(d, cond1 = \"before\", cond2 = \"after\", width = 0.2,\n    line.color = \"gray\", line.size = 0.4, palette = \"aaas\")\np3 &lt;- ggpaired(d, cond1 = \"before\", cond2 = \"after\", width = 0.2,\n    line.color = \"gray\", line.size = 0.4, fill = \"condition\",palette = \"npg\")\nggarrange(p1, p2, p3,\n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/R_ggpubr.html#p-values-를-박스-그래프에-추가하기",
    "href": "posts/R_ggpubr.html#p-values-를-박스-그래프에-추가하기",
    "title": "R로 논문급 도표 그리기",
    "section": "4.10. P-values 를 박스 그래프에 추가하기",
    "text": "4.10. P-values 를 박스 그래프에 추가하기\n\n# Load data\ndata(\"ToothGrowth\")\nhead(ToothGrowth)\n\n\n\n\nlen\nsupp\ndose\n\n\n\n\n4.2\nVC\n0.5\n\n\n11.5\nVC\n0.5\n\n\n7.3\nVC\n0.5\n\n\n5.8\nVC\n0.5\n\n\n6.4\nVC\n0.5\n\n\n10.0\nVC\n0.5\n\n\n\n\n\n\n# Two independent groups\np &lt;- ggboxplot(ToothGrowth, x = \"supp\", y = \"len\",\n    color = \"supp\", palette = \"npg\", add = \"jitter\")\n\n#  Add p-value\np1 &lt;- p + stat_compare_means(method = \"t.test\")\n\n# Paired samples\np2 &lt;- ggpaired(ToothGrowth, x = \"supp\", y = \"len\",\n    color = \"supp\", line.color = \"gray\", line.size = 0.4,\n    palette = \"npg\")+\n    stat_compare_means(paired = TRUE, method = \"t.test\")\n\n# More than two groups, Pairwise comparisons: Specify the comparisons you want\nmy_comparisons &lt;- list( c(\"0.5\", \"1\"), c(\"1\", \"2\"), c(\"0.5\", \"2\") )\np3 &lt;- ggboxplot(ToothGrowth, x = \"dose\", y = \"len\",\n          color = \"dose\", palette = \"npg\")+\n# Add pairwise comparisons p-value\n    stat_compare_means(comparisons = my_comparisons, label.y = c(29, 35, 40))+\n    stat_compare_means(label.y = 45)     # Add global Anova p-value\n\n# Multiple pairwise test against a reference group\np4 &lt;- ggboxplot(ToothGrowth, x = \"dose\", y = \"len\",\n    color = \"dose\", palette = \"npg\")+\n    stat_compare_means(method = \"anova\", label.y = 40)+ # Add global p-value\n    stat_compare_means(aes(label = ..p.signif..),\n                      method = \"t.test\", ref.group = \"0.5\")\n\nggarrange(p1, p2, p3, p4,  ncol = 2, nrow = 2,\n          labels = c(\"A\", \"B\",\"C\",\"D\"))\n\n\n\n\n\n\n\n\n# Multiple grouping variables\np &lt;- ggboxplot(ToothGrowth, x = \"supp\", y = \"len\",\n              color = \"supp\", palette = \"npg\",\n              add = \"jitter\",\n              facet.by = \"dose\", short.panel.labs = FALSE)\n# Use only p.format as label. Remove method name.\np5 &lt;- p + stat_compare_means(aes(label = paste0(\"p = \", ..p.format..)))\np5"
  },
  {
    "objectID": "posts/python_Statistics.html",
    "href": "posts/python_Statistics.html",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "통계 계산을 위한 프로그래밍 언어에 R 프로그래밍 언어(줄여서 R)가 있는데, 왜 파이썬을 써야 할까요?\nR은 문법 자체부터 통계에 특화되어 있고 여러가지 통계분석을 할 수 있습니다. 그럼에도 불구하고 제가 파이썬을 통계분석에 사용하는 이유는 간단합니다. 파이썬은 보다 범용적인 언어이고 라이브러리가 풍부해서 제가 원하는 기능은 거의 이미 다 있기 때문이죠.\n\n\n여기, brain_size 라는 데이터를 살펴 보겠습니다.\n\n# 필요한 라이브러리를 불러옵니다.\n%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\nbrain_size 데이터는 Willerman이 1991년에 사람의 뇌 크기와 무게, 그리고 IQ에 대하여 측정한 값입니다. 범주형의 데이터와 수치형 데이터로 구성 되어 있죠.\nPandas의 read_csv 기능을 이용해 데이터프레임을 만들어 보겠습니다.\n\ndf = pd.read_csv(\n    \"http://www.scipy-lectures.org/_downloads/brain_size.csv\",\n    sep=\";\",\n    index_col=0,\n    na_values=\".\",\n)\ndf.head()  # 상단의 5개의 데이터 확인하기\n\n\n\n\n\n\n\n\nGender\nFSIQ\nVIQ\nPIQ\nWeight\nHeight\nMRI_Count\n\n\n\n\n1\nFemale\n133\n132\n124\n118.0\n64.5\n816932\n\n\n2\nMale\n140\n150\n124\nNaN\n72.5\n1001121\n\n\n3\nMale\n139\n123\n150\n143.0\n73.3\n1038437\n\n\n4\nMale\n133\n129\n128\n172.0\n68.8\n965353\n\n\n5\nFemale\n137\n132\n134\n147.0\n65.0\n951545\n\n\n\n\n\n\n\n간단히 살펴보면, 총 40명의 사람들의 성별, IQ, 몸무게, 키 그리고 MRI_count(total pixel Count from the 18 MRI scans) 값이 측정되어 있습니다. IQ의 경우 3종류로 세분화 되어있는데 각각을 알아 보면 아래와 같습니다.\n\nFull Scale Intelligence Quotient (FSIQ) : VIQ와 PIQ의 종합적인 수치입니다\nVerbal IQ (VIQ) : 언어적인 측면을 측정합니다.\nPerformance IQ (PIQ) : 논리, 계산적인 측면을 측정\n\n\n\n\npandas에서는 간단하게 평균값과 표준편차등을 계산해주는 기능이 있습니다.\ndescribe() 함수를 사용하면 모든 열에 대한 설명통계값을 보여줍니다.\n\n# padas 에서 제공하는 설명 통계\ndf.describe()\n\n\n\n\n\n\n\n\nFSIQ\nVIQ\nPIQ\nWeight\nHeight\nMRI_Count\n\n\n\n\ncount\n40.000000\n40.000000\n40.00000\n38.000000\n39.000000\n4.000000e+01\n\n\nmean\n113.450000\n112.350000\n111.02500\n151.052632\n68.525641\n9.087550e+05\n\n\nstd\n24.082071\n23.616107\n22.47105\n23.478509\n3.994649\n7.228205e+04\n\n\nmin\n77.000000\n71.000000\n72.00000\n106.000000\n62.000000\n7.906190e+05\n\n\n25%\n89.750000\n90.000000\n88.25000\n135.250000\n66.000000\n8.559185e+05\n\n\n50%\n116.500000\n113.000000\n115.00000\n146.500000\n68.000000\n9.053990e+05\n\n\n75%\n135.500000\n129.750000\n128.00000\n172.000000\n70.500000\n9.500780e+05\n\n\nmax\n144.000000\n150.000000\n150.00000\n192.000000\n77.000000\n1.079549e+06\n\n\n\n\n\n\n\nIQ의 평균값은 113이군요. 몸무게는 kg으로 변환하면 약 70kg쯤 됩니다.\n\n\n\n전체적인 데이터의 양상을 보기에는 시각화가 중요합니다. 파이썬에서는 간단하게 산포 행렬(sactter matrix)를 그려 볼 수 있습니다.\n먼저 키와 몸무게, MRI_count 간의 상관관계를 보겠습니다.\n\n# Plotting data\nfrom pandas.plotting import scatter_matrix\n\n# 키와 몸무게, MRI_count\nscatter_matrix(df[[\"Weight\", \"Height\", \"MRI_Count\"]])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A1D2B70&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A443668&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A47E588&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A4B9588&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A56D4A8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A56D4E0&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A5DC978&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A614EB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A643B70&gt;]], dtype=object)\n\n\n\n\n\n키와 몸무계는 서로 연관이 있는듯 하고 나머지는 그다지 서로 연관이 없어 보입니다.\n그 다음으로는 여러 IQ 수치간에 상관관계를 알아 보죠.\n\nscatter_matrix(df[[\"PIQ\", \"VIQ\", \"FSIQ\"]])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A9AD6A0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A9F34A8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AA2E3C8&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AA64358&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AAA0358&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AAA0390&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB01EB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB3DEB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB72EB8&gt;]], dtype=object)\n\n\n\n\n\n당연하지만, FSIQ는 VIQ, PIQ 각각과 연관성이 있어 보입니다. VIQ와 PIQ간에는 애매하게 연관성이 없어 보이네요. FSIQ의 히스토그램에서는 100 - 125 사이에는 데이터가 없는 것을 확인 할 수 있습니다."
  },
  {
    "objectID": "posts/python_Statistics.html#예를-들어봅시다.",
    "href": "posts/python_Statistics.html#예를-들어봅시다.",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "여기, brain_size 라는 데이터를 살펴 보겠습니다.\n\n# 필요한 라이브러리를 불러옵니다.\n%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/python_Statistics.html#csv-파일-읽어오기",
    "href": "posts/python_Statistics.html#csv-파일-읽어오기",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "brain_size 데이터는 Willerman이 1991년에 사람의 뇌 크기와 무게, 그리고 IQ에 대하여 측정한 값입니다. 범주형의 데이터와 수치형 데이터로 구성 되어 있죠.\nPandas의 read_csv 기능을 이용해 데이터프레임을 만들어 보겠습니다.\n\ndf = pd.read_csv(\n    \"http://www.scipy-lectures.org/_downloads/brain_size.csv\",\n    sep=\";\",\n    index_col=0,\n    na_values=\".\",\n)\ndf.head()  # 상단의 5개의 데이터 확인하기\n\n\n\n\n\n\n\n\nGender\nFSIQ\nVIQ\nPIQ\nWeight\nHeight\nMRI_Count\n\n\n\n\n1\nFemale\n133\n132\n124\n118.0\n64.5\n816932\n\n\n2\nMale\n140\n150\n124\nNaN\n72.5\n1001121\n\n\n3\nMale\n139\n123\n150\n143.0\n73.3\n1038437\n\n\n4\nMale\n133\n129\n128\n172.0\n68.8\n965353\n\n\n5\nFemale\n137\n132\n134\n147.0\n65.0\n951545\n\n\n\n\n\n\n\n간단히 살펴보면, 총 40명의 사람들의 성별, IQ, 몸무게, 키 그리고 MRI_count(total pixel Count from the 18 MRI scans) 값이 측정되어 있습니다. IQ의 경우 3종류로 세분화 되어있는데 각각을 알아 보면 아래와 같습니다.\n\nFull Scale Intelligence Quotient (FSIQ) : VIQ와 PIQ의 종합적인 수치입니다\nVerbal IQ (VIQ) : 언어적인 측면을 측정합니다.\nPerformance IQ (PIQ) : 논리, 계산적인 측면을 측정"
  },
  {
    "objectID": "posts/python_Statistics.html#pandas-설명통계",
    "href": "posts/python_Statistics.html#pandas-설명통계",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "pandas에서는 간단하게 평균값과 표준편차등을 계산해주는 기능이 있습니다.\ndescribe() 함수를 사용하면 모든 열에 대한 설명통계값을 보여줍니다.\n\n# padas 에서 제공하는 설명 통계\ndf.describe()\n\n\n\n\n\n\n\n\nFSIQ\nVIQ\nPIQ\nWeight\nHeight\nMRI_Count\n\n\n\n\ncount\n40.000000\n40.000000\n40.00000\n38.000000\n39.000000\n4.000000e+01\n\n\nmean\n113.450000\n112.350000\n111.02500\n151.052632\n68.525641\n9.087550e+05\n\n\nstd\n24.082071\n23.616107\n22.47105\n23.478509\n3.994649\n7.228205e+04\n\n\nmin\n77.000000\n71.000000\n72.00000\n106.000000\n62.000000\n7.906190e+05\n\n\n25%\n89.750000\n90.000000\n88.25000\n135.250000\n66.000000\n8.559185e+05\n\n\n50%\n116.500000\n113.000000\n115.00000\n146.500000\n68.000000\n9.053990e+05\n\n\n75%\n135.500000\n129.750000\n128.00000\n172.000000\n70.500000\n9.500780e+05\n\n\nmax\n144.000000\n150.000000\n150.00000\n192.000000\n77.000000\n1.079549e+06\n\n\n\n\n\n\n\nIQ의 평균값은 113이군요. 몸무게는 kg으로 변환하면 약 70kg쯤 됩니다."
  },
  {
    "objectID": "posts/python_Statistics.html#산포-행렬을-그려보겠습니다.",
    "href": "posts/python_Statistics.html#산포-행렬을-그려보겠습니다.",
    "title": "파이썬 통계분석하기",
    "section": "",
    "text": "전체적인 데이터의 양상을 보기에는 시각화가 중요합니다. 파이썬에서는 간단하게 산포 행렬(sactter matrix)를 그려 볼 수 있습니다.\n먼저 키와 몸무게, MRI_count 간의 상관관계를 보겠습니다.\n\n# Plotting data\nfrom pandas.plotting import scatter_matrix\n\n# 키와 몸무게, MRI_count\nscatter_matrix(df[[\"Weight\", \"Height\", \"MRI_Count\"]])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A1D2B70&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A443668&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A47E588&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A4B9588&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A56D4A8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A56D4E0&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A5DC978&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A614EB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A643B70&gt;]], dtype=object)\n\n\n\n\n\n키와 몸무계는 서로 연관이 있는듯 하고 나머지는 그다지 서로 연관이 없어 보입니다.\n그 다음으로는 여러 IQ 수치간에 상관관계를 알아 보죠.\n\nscatter_matrix(df[[\"PIQ\", \"VIQ\", \"FSIQ\"]])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A9AD6A0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000A9F34A8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AA2E3C8&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AA64358&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AAA0358&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AAA0390&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB01EB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB3DEB8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000000AB72EB8&gt;]], dtype=object)\n\n\n\n\n\n당연하지만, FSIQ는 VIQ, PIQ 각각과 연관성이 있어 보입니다. VIQ와 PIQ간에는 애매하게 연관성이 없어 보이네요. FSIQ의 히스토그램에서는 100 - 125 사이에는 데이터가 없는 것을 확인 할 수 있습니다."
  },
  {
    "objectID": "posts/python_Statistics.html#sample-t-test",
    "href": "posts/python_Statistics.html#sample-t-test",
    "title": "파이썬 통계분석하기",
    "section": "1-sample T-test",
    "text": "1-sample T-test\n하나의 집단의 평균이 특정 기준보다 유의미하게 다른지 를 알아보는 분석 방법입니다. Student T-test이라고도 하는 1-sample T-test 를 사용하려면 scipy.stats.ttest_1samp() 함수를 사용하면 됩니다.\n\nfrom scipy import stats\n\n\n## Student’s t-test: the simplest statistical test\nstats.ttest_1samp(df[\"VIQ\"], 0)\n# VIQ의 평균값이 0과 통계적으로 유의미하게 다른지 알아 보겠습니다.\n\nTtest_1sampResult(statistic=30.088099970849328, pvalue=1.3289196468728067e-28)\n\n\n간단하게 결론만 말하자면, p-value가 아주 낮음(10의 -28제곱) 으로 VIQ의 평균은 0이 아니라고 말할 수 있습니다."
  },
  {
    "objectID": "posts/python_Statistics.html#sample-t-test-1",
    "href": "posts/python_Statistics.html#sample-t-test-1",
    "title": "파이썬 통계분석하기",
    "section": "2-sample t-test",
    "text": "2-sample t-test\n서로 다른 두개의 그룹 간 평균의 차이가 유의미 한지 여부를 판단하기 위해 시행합니다. 2-sample t-test 는 scipy.stats.ttest_ind(): 함수를 사용합니다.\n예를 들어 여자의 VIQ와 남자의 VIQ의 평균은 통계적으로 차이가 있는지 알아 보겠습니다.\n\n# 여자의 VIQ\nfemale_viq = df[df[\"Gender\"] == \"Female\"][\"VIQ\"]\n# 남자의 VIQ\nmale_viq = df[df[\"Gender\"] == \"Male\"][\"VIQ\"]\n# 두개의 리스트를 가지고 t-test실행\nstats.ttest_ind(female_viq, male_viq)\n\nTtest_indResult(statistic=-0.77261617232750113, pvalue=0.44452876778583217)\n\n\np-value가 0.44로 아주 높게 나왔습니다. 따라서 기무가설이었던 남자와 여자의 VIQ 평균에는 차이가 있다. 는 기각되고 차이가 없다 라고 결론을 낼 수 있습니다."
  },
  {
    "objectID": "posts/python_Statistics.html#paired-tests",
    "href": "posts/python_Statistics.html#paired-tests",
    "title": "파이썬 통계분석하기",
    "section": "Paired tests:",
    "text": "Paired tests:\nPaired t-test는 동일한 집단에서의 반복적인 측정에 의한 차이를 비교하기 위해 사용됩니다. 예를 들면 커피가 수면시간에 미치는 영향을 보기 위해 커피를 마시지 않고 측정하고 커피를 마시고 측정한 데이터를 수집하여 사용합니다. &gt; 전제조건을 충족하기 위해서는 실험이 길어지는 단점이 있습니다"
  },
  {
    "objectID": "posts/python_Statistics.html#f-test",
    "href": "posts/python_Statistics.html#f-test",
    "title": "파이썬 통계분석하기",
    "section": "F-test",
    "text": "F-test\nF-test는 두 표본의 분산에 대한 차이가 통계적으로 유의한가를 판별하는 검정기법입니다. 다른 이름으로 var-test로도 불립니다.\n\nfrom statsmodels.formula.api import ols\n\nmodel = ols(\"VIQ ~ Gender + MRI_Count + Height\", df).fit()\nprint(model.summary())\nprint(model.f_test([0, 1, 0, 0]))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    VIQ   R-squared:                       0.246\nModel:                            OLS   Adj. R-squared:                  0.181\nMethod:                 Least Squares   F-statistic:                     3.809\nDate:                Thu, 21 Dec 2017   Prob (F-statistic):             0.0184\nTime:                        15:34:54   Log-Likelihood:                -172.34\nNo. Observations:                  39   AIC:                             352.7\nDf Residuals:                      35   BIC:                             359.3\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept        166.6258     88.824      1.876      0.069     -13.696     346.948\nGender[T.Male]     8.8524     10.710      0.827      0.414     -12.890      30.595\nMRI_Count          0.0002   6.46e-05      2.615      0.013    3.78e-05       0.000\nHeight            -3.0837      1.276     -2.417      0.021      -5.674      -0.494\n==============================================================================\nOmnibus:                        7.373   Durbin-Watson:                   2.109\nProb(Omnibus):                  0.025   Jarque-Bera (JB):                2.252\nSkew:                           0.005   Prob(JB):                        0.324\nKurtosis:                       1.823   Cond. No.                     2.40e+07\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.4e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n&lt;F test: F=array([[ 0.68319608]]), p=0.4140878441244722, df_denom=35, df_num=1&gt;\n\n\nF test 결과, p-value 가 0.41로 아주 높게 나왔습니다. 따라서 기무가설을 기각하지 못합니다. 다시 말해 성별에 의한 VIQ 차이는 없다 라고 할 수 있습니다."
  },
  {
    "objectID": "posts/python_Statistics.html#시각화",
    "href": "posts/python_Statistics.html#시각화",
    "title": "파이썬 통계분석하기",
    "section": "시각화",
    "text": "시각화\n시각화를 하면 통계분석에 사용된 변수간의 상관관계를 손쉽게 확인할 수 있습니다. 아래 코드는 scatter_matrix를 그리는 방법입니다.\n\n# This plotting is useful to get an intuitions on the relationships between\n# our different variables\n\n# Fill in the missing values for Height for plotting\ndf[\"Height\"].fillna(method=\"pad\", inplace=True)\n\n# The parameter 'c' is passed to plt.scatter and will control the color\n# The same holds for parameters 'marker', 'alpha' and 'cmap', that\n# control respectively the type of marker used, their transparency and\n# the colormap\nscatter_matrix(\n    df[[\"VIQ\", \"MRI_Count\", \"Height\"]],\n    c=(df[\"Gender\"] == \"Female\"),\n    marker=\"o\",\n    alpha=0.7,\n)\n\nfig = plt.gcf()\nfig.suptitle(\"purple: male, yellow: female\", size=13)\n\nplt.show()"
  },
  {
    "objectID": "book.html",
    "href": "book.html",
    "title": "Book",
    "section": "",
    "text": "생명정보학 알고리즘\n파이썬으로 구현하는 생명정보학 알고리즘\n\n\n\n목차\n\n1장. 서문\n\n1.1 들어가며\n1.2 생명정보학이란?\n1.3 책의 구성\n\n2장. 파이썬 소개\n\n2.1 파이썬의 특징\n2.2 변수와 미리 정의된 함수\n2.3 파이썬 코드 작성하기\n2.4 파이썬 프로그램 개발\n2.5 객체지향 프로그래밍\n2.6 사전 정의된 클래스 및 메서드\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n3장. 세포 및 분자생물학의 기초\n\n3.1 세포: 생명의 기본 단위\n3.2 유전자 정보: 핵산\n3.3 유전자: 유전 정보의 이산 단위\n3.4 인간 유전체\n3.5 생물 자원 및 데이터베이스\n참고 문헌과 추가 자료\n연습 문제\n\n4장. 생물학적 서열의 기본적 처리\n\n4.1 생물학적 서열: 표현과 기본 알고리즘\n4.2 전사와 역상보\n4.3 번역\n4.4 가능성 있는 유전자 찾기: 오픈 리딩 프레임\n4.5 하나로 합체\n4.6 생물학 서열의 클래스\n4.7 바이오파이썬으로 서열 처리\n4.8 바이오파이썬의 서열 주석 객체\n연습 문제와 프로그래밍 프로젝트\n\n5장. 서열 데이터에서 패턴 찾기\n\n5.1 소개: 생명정보학에서 패턴 찾기의 중요성\n5.2 고정된 패턴을 찾는 단순한 알고리즘\n5.3 휴리스틱 알고리즘: 보이어-무어\n5.4 결정적 유한 오토마타\n5.5 정규표현식으로 유연한 패턴 찾기\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n6장. 쌍 서열 정렬\n\n6.1 소개: 서열 비교와 서열 정렬\n6.2 시각화 정렬: 점 도표\n6.3 서열 정렬의 최적화 문제\n6.4 전역 정렬을 위한 동적 프로그래밍 알고리즘\n6.5 지역 정렬을 위한 동적 프로그래밍 알고리즘\n6.6 서열 정렬의 특별한 경우\n6.7 바이오파이썬을 활용한 쌍 서열 정렬\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n7장. 데이터베이스에서 유사한 서열 찾기\n\n7.1 소개\n7.2 BLAST 알고리즘과 프로그램\n7.3 구현한 BLAST 이식\n7.4 바이오파이썬을 통한 BLAST 사용\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n8장. 다중 서열 정렬\n\n8.1 소개: 문제 정의와 복잡도\n8.2 다중 서열 정렬의 알고리즘 최적화 클래스\n8.3 점진적 정렬을 파이썬에서 구현\n8.4 바이오파이썬으로 정렬 다루기\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n9장. 계통학 분석\n\n9.1 소개: 문제 정의 및 연관성\n9.2 계통학적 분석을 위한 알고리즘 클래스\n9.3 파이썬으로 거리 기반 알고리즘 구현\n9.4 계통학 분석을 위한 바이오파이썬\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n10장. 모티프 발견 알고리즘\n\n10.1 소개: 문제 정의와 관련성\n10.2 브루트 포스 알고리즘: 완전 탐색\n10.3 분기 및 경계 알고리즘\n10.4 휴리스틱 알고리즘\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n11장. 확률적 모티프와 알고리즘\n\n11.1 확률 모티프 표현 및 검색\n11.2 확률 알고리즘: 기댓값 최대화\n11.3 모티프 발견을 위한 깁스 샘플링\n11.4 바이오파이썬의 확률 모티프\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n12장. 은닉 마르코프 모델\n\n12.1 소개: 은닉 마르코프 모델이란 무엇인가?\n12.2 파이썬으로 알고리즘 구현\n12.3 데이터베이스 검색을 위한 은닉 마르코프 모델\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n13장. 그래프: 개념과 알고리즘\n\n13.1 그래프: 정의와 표현\n13.2 파이썬 클래스 그래프\n13.3 인접 노드와 차수\n13.4 경로, 탐색, 거리\n13.5 사이클\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n14장. 그래프와 생물학적 네트워크\n\n14.1 소개\n14.2 네트워크를 그래프로 표현\n14.3 네트워크 위상 분석\n14.4 대사작용 가능성 평가\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n15장. 게놈으로 리드 어셈블리: 그래프 기반 알고리즘\n\n15.1 게놈 어셈블리 소개 및 관련한 도전들\n15.2 오버랩 그래프와 해밀턴 사이클\n15.3 드브루인 그래프와 오일러 경로\n15.4 실제 게놈 어셈블리\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n16장. 참조 유전자 서열에 리드 어셈블리\n\n16.1 소개: 서열 일치 문제의 정의와 응용법\n16.2 패턴 전처리: 트라이\n16.3 서열의 전처리: 접미사 트리\n16.4 버로우즈 휠러 변환\n참고 문헌과 추가 자료\n연습 문제와 프로그래밍 프로젝트\n\n17장. 더 읽을거리\n\n17.1 추천하는 생명정보학 서적\n17.2 논문 및 학회\n17.3 정규 교육 과정\n17.4 온라인 교육 자료\n\n\n\n\n\n생명과학을 위한 딥러닝\n생물학, 유전체학, 신약 개발에 적용하는 실무 딥러닝\n\n\n분자 데이터에 머신러닝을 적용하는 방법\n딥러닝으로 유전학/유전체학 분석하기\n딥러닝으로 생물물리학 시스템 이해\nDeepChem 라이브러리 소개\n딥러닝을 사용한 현미경 이미지 분석\n딥러닝을 사용한 의료 이미지 분석\nVAE와 GAN 모델\n머신러닝 모델의 작동 원리 해석\n\n\n\n목차\n\n1장. 왜 생명과학인가?\n\n딥러닝은 왜 필요한가?\n현대 생명과학은 빅데이터를 다룬다\n무엇을 배우는가?\n\n2장. 딥러닝 소개\n\n선형 모델\n다층 퍼셉트론\n모델 학습하기\n검증하기\n정규화\n하이퍼파라미터 최적화\n다른 유형의 모델들\n\n합성곱 신경망\n순환 신경망\n\n\n3장. DeepChem을 이용한 머신러닝\n\nDeepChem의 기본 데이터셋\n독성 분자 예측 모델 만들기\nMNIST 데이터셋으로 필기 인식 모델 만들기\n\nMNIST 필기 인식 데이터셋\n합성곱 신경망으로 필기 인식하기\n\n소프트맥스와 소프트맥스 교차 엔트로피\n\n4장. 분자 수준 데이터 다루기\n\n분자란 무엇인가?\n\n분자 간 결합\n분자 그래프\n분자 구조\n분자 카이랄성\n\n분자 데이터 피처화\n\nSMILES 문자열과 RDKit\n확장 연결 지문\n분자 표현자\n\n그래프 합성곱\n용해도 예측 모델\nMoleculeNet\nSMARTS 문자열\n\n5장. 생물물리학과 머신러닝\n\n단백질의 구조\n\n단백질 서열\n\n단백질 3차원 구조를 예측할 수 있을까?\n\n단백질-리간드 결합\n\n생물물리학적 피처화\n\n그리드 피처화\n원자 피처화\n\n생물물리학 데이터 사례 연구\n\nPDBBind 데이터셋\nPDBBind 데이터셋 피처화\n\n\n6장. 유전학과 딥러닝\n\nDNA, RNA, 단백질\n실제 세포 내에서 일어나는 일\n전사인자의 결합\n\n전사인자의 결합을 예측하는 합성곱 모델\n\n염색질 접근성\nRNA 간섭\n\n7장. 현미경을 위한 딥러닝\n\n현미경에 대한 간략한 소개\n\n현대의 광학현미경\n\n회절 한계\n\n전자현미경과 원자현미경\n초고해상도 현미경\n딥러닝과 회절 한계\n\n현미경을 위한 시료 준비\n\n시료 염색하기\n시료 고정\n\n시료 절편 가공\n\n형광현미경\n시료 준비 과정의 영향\n\n딥러닝 활용법\n\n세포수 측정\n\n세포주란 무엇인가?\n\n세포 구별하기\n머신러닝과 실험\n\n\n8장. 의료 체계를 위한 딥러닝\n\n컴퓨터 지원 질병 진단\n베이즈 네트워크를 이용한 불확실성 예측\n전자 건강 기록\nICD-10 코드\n비지도 학습이란 무엇인가?\n\n거대 전자 건강 기록 데이터베이스의 위험성\n\n방사선학을 위한 딥러닝\n\nX선 촬영과 CT 촬영\n조직학\nMRI 촬영\n\n치료법으로서의 머신러닝\n당뇨망막병증\n\n9장. 생성 모델\n\nVAE\nGAN\n생명과학에 생성 모델 응용하기\n\n신약 후보 물질 찾기\n단백질 엔지니어링\n과학적 발견을 위한 도구\n\n생성 모델의 미래\n생성 모델 사용하기\n\n생성 모델 결과 분석\n\n\n10장. 딥러닝 모델의 해석\n\n예측값 설명하기\n입력값 최적화하기\n불확실성 예측하기\n해석 가능성, 설명 가능성, 실제 결과\n\n11장. 가상 선별검사\n\n예측 모델을 위한 데이터셋 준비\n머신러닝 모델 학습하기\n예측을 위한 데이터셋 준비하기\n예측 모델 적용하기\n\n12장. 딥러닝의 미래와 전망\n\n질병 진단\n맞춤 의학\n신약 개발\n생물학 연구\n\n\n\n\n구입처\n\n알라딘\nYES24\n교보문고\n인터파크\n반디앤루니스\n\n\n\n\n파이썬을 활용한 생명정보학 2/e\n\n\n지은이: 티아구 안타오\n옮긴이: 김태윤\n출판사: 에이콘 출판\n\n원제 : Bioinformatics with Python Cookbook - Second Edition\n생명정보학 데이터를 파이썬 프로그래밍 기법과 프레임워크를 사용해 처리한다. 차세대 염기서열 분석, 유전체학, 메타지노믹스(metagenomics), 집단 유전학, 계통 발생학, 프로테오믹스(proteomics)의 내용을 다룬다. 다양한 파이썬 도구와 라이브러리로 데이터를 변환, 분석, 시각화하는 최신 프로그래밍 기법을 배운다. 차세대 염기서열 분석 데이터의 필터링(filtering) 기술과 병렬처리 프레임워크(framework)인 대스크(Dask)와 스파크(Spark)도 소개한다.\n\n\n목차\n\n1장. 파이썬과 주변 생태계\n\n소개\n아나콘다를 사용한 필요 소프트웨어 설치\n도커를 사용한 필요 소프트웨어 설치\nrpy2를 통해 R과 인터페이스 만들기\n주피터 노트북에서 R 매직 명령어 사용하기\n\n2장. 차세대 염기서열 분석\n\n소개\nNCBI와 진뱅크 데이터베이스 둘러보기\n염기서열 분석의 기초\n배우기\nFASTQ 파일 다루기\n정렬 데이터 다루기\nVCF 파일 데이터 분석하기\n게놈 접근성과 SNP 데이터 필터하기\nHTSeq로 NGS 데이터 처리하기\n\n3장. 게놈 데이터 다루기\n\n소개\n좋은 품질의 참조 게놈 다루기\n낮은 품질의 참조 게놈 다루기\n게놈 주석 살펴보기\n게놈 주석으로 원하는 유전자 추출하기\nEnsembl REST API로 오소로그검색\nEnsembl REST API로 유전자 온톨로지 정보 검색\n\n4장. 집단유전학\n\n소개\nPLINK 형식 데이터셋 관리하기\nGenepop 파일 형식 소개\nBio.PopGen으로 데이터셋 탐색하기\nF - 통계 계산하기\n주성분 분석하기\nADMIXTURE 프로그램으로 집단 구조 조사하기\n\n5장. 집단유전학 시뮬레이션\n\n소개\n순방향 시뮬레이터 소개\n선택 시뮬레이션\n섬 모델과 디딤돌 모델을 사용한 시뮬레이션\n복잡한 집단 통계 모델 만들기\n\n6장. 계통 발생학\n\n소개\n계통 발생학 분석을 위한 데이터셋 준비\n유전자와 게놈 데이터 정렬\n서열 데이터 비교하기\n계통수 그리기\n재귀적으로 계통수 다루기\n계통수 시각화하기\n\n7장. 단백질 데이터 뱅크 사용하기\n\n소개\n데이터베이스에서 단백질 정보 찾기\nBio.PDB 소개\nPDB 파일에서 더 많은 정보 추출하기\nPDB 파일에서 분자간 거리 계산\n기하학적 계산하기\nPyMOL로 애니메이션 만들기\nBiopython을 사용해 mmCIF 파일 파싱하기\n\n8장. 생명정보학 파이프라인\n\n소개\n갤럭시 서버 소개\nAPI를 사용해 갤럭시 사용하기\n갤럭시 도구 개발\n일반적인 파이프라인 사용법\nAirflow를 사용해 유전변이 분석 파이프라인 만들기\n\n9장. 파이썬으로 유전체 빅데이터 다루기\n\n소개\nHDF5 데이터 형식\n대스크 라이브러리로 병렬분산처리\n파케이 데이터 형식\n스파크 라이브러리로 병렬분산처리\n사이썬과 눔바로 코드 최적화\n\n10장. 생명정보학의 다른 주제들\n\n소개\nQIIME2로 메타지노믹스 분석하기\n생식세포계열로 공통 염색체 찾기\nREST API로 GBIF 데이터베이스 사용하기\nGBIF의 지리 참조 데이터 다루기\n사이토스케이프로 단백질 네트워크 시각화\n\n11장. 고급 차세대 염기서열 분석\n\n소개\n분석을 위한 데이터셋 준비하기\n멘델리언 오류로 데이터 품질 관리\n의사 결정 나무를 사용한 데이터 탐색\n표준 통계로 데이터 탐색\n주석 데이터로 생물학적 특성 찾기\n\n\n\n\n구입처\n\n알라딘\nYES24\n교보문고\n인터파크\n반디앤루니스"
  }
]