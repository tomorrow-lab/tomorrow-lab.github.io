{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"LLM ì‚¬ì „ í•™ìŠµì— ëŒ€í•œ ì´í•´\"\n",
    "author: \"Taeyoon Kim\"\n",
    "categories: [LLM, Machine Learning, Python]\n",
    "draft: false\n",
    "date: \"2025-03-13\"\n",
    "date-modified: last-modified\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ê¸€ì€ [deeplearning.ai](https://learn.deeplearning.ai/courses/pretraining-llms)ì˜ Pre-training LLM ê°•ì˜ë¥¼ ë“£ê³  ë‚˜ë¦„ëŒ€ë¡œ ì •ë¦¬ë¥¼ í•´ë³¸ ê¸€ì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ê°•ì˜ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”. LLM ì‚¬ì „ í•™ìŠµì€ ì»´í“¨íŒ… íŒŒì›Œê°€ ë§ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ ê°œì¸ì´ ìˆ˜í–‰í•˜ê¸°ì—ëŠ” ì–´ë ¤ìš´ ì‘ì—…ì´ì§€ë§Œ, ì´ ê¸€ì„ í†µí•´ LLM ì‚¬ì „ í•™ìŠµì— ëŒ€í•œ ì „ë°˜ì ì¸ ì´í•´ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "# LLM ì‚¬ì „ í•™ìŠµì´ë€?\n",
    "\n",
    "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ê²ƒì€ ë§‰ëŒ€í•œ ê³„ì‚° ìì›ê³¼ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ë¯¸ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í™œìš©í•´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ê°€ì ¸ì™€ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì†Œê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì¸ ìì—°ì–´ ì²˜ë¦¬(NLP) ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ê¸€ì—ì„œëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì¤€ë¹„í•˜ëŠ” ì²«ë²ˆì§¸ ë‹¨ê³„ë¶€í„° LLM ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµ ì‹œí‚¤ëŠ” ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ì— ëŒ€í•´ ë°°ì›Œë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "```{mermaid}\n",
    "graph LR;\n",
    "    A[ë°ì´í„° ìˆ˜ì§‘] --> B[ë°ì´í„° íŒ¨í‚¤ì§•]\n",
    "    B --> C[ëª¨ë¸ í•™ìŠµ]\n",
    "    C --> D[ëª¨ë¸ ì‚¬ìš©]\n",
    "    D --> E[ëª¨ë¸ í‰ê°€]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¼ë°˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "ì—¬ê¸°ì„œ ì˜ˆì‹œë¡œ ë‹¤ë£¨ëŠ” `TinySolar-248m-4k` ëª¨ë¸ì€ 248M ë§¤ê°œë³€ìˆ˜(GPT2ì™€ ë¹„ìŠ·í•œ ê·œëª¨)ì™€ 4096 í† í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ ê°€ì§„ ì‘ì€ ë””ì½”ë” ì „ìš© ì†Œí˜• ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ Hugging Face ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ [ë§í¬](https://huggingface.co/upstage/TinySolar-248m-4k) í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •ì€ ë‹¤ìŒ ì„¸ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤:\n",
    "1. Hugging Face ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ëª¨ë¸ ê²½ë¡œ ì§€ì •í•˜ê¸°\n",
    "2. `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `AutoModelforCausalLM`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "3. ê°™ì€ ëª¨ë¸ ê²½ë¡œì—ì„œ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "ì–¸ì–´ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ í’ˆì§ˆì€ ë§¤ìš° ì¤‘ìš”í•˜ë©° ì‚¬ì „ í•™ìŠµ(Pre-training)ì—ì„œ í•„ìš”í•œ ë°ì´í„°ì™€ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)ì—ì„œì˜ ë°ì´í„°ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì„±ì§ˆì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ì „ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë” ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ êµ¬ì¡°í™”ê°€ ëœ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´ì— ë¯¸ì„¸ ì¡°ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” íŠ¹ì • ì‘ì—…ì— ë§ê²Œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¹„ìœ í•˜ìë©´ ì‚¬ì „ í•™ìŠµì€ ë§ì€ ì±…ì„ ì½ëŠ” ê²ƒê³¼ ê°™ê³ , ë¯¸ì„¸ ì¡°ì •ì€ ì˜ˆë¹„ ê³ ì‚¬ë¥¼ ì¹˜ë¥´ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì—¬ê¸°ì—ì„œëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ í™•ë³´í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. Hugging Faceì—ì„œ ê¸°ì¡´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "2. ì§ì ‘ ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "\n",
    "ë‘ ê²½ìš° ëª¨ë‘ ê²°ê³¼ëŠ” `Datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¼ë¶€ì¸ Hugging Face `Dataset` ê°ì²´ê°€ ë©ë‹ˆë‹¤. Datasetì˜ ì†ì„±ê³¼ ì‚¬ìš© ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [Hugging Face ì›¹ì‚¬ì´íŠ¸](https://huggingface.co/docs/datasets/en/index)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## Hugging Faceì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "ì—¬ê¸°ì„œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” `upstage/Pretraining_Dataset` ë°ì´í„°ì…‹ì€ **Red Pajama**ë¼ëŠ” í›¨ì”¬ ë” í°(1ì¡° í† í° ê·œëª¨) ë°ì´í„°ì…‹ì˜ ì„œë¸Œì…‹ì…ë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì…‹ì€ Hugging Faceì˜ [ì´ ë§í¬](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pretraining_dataset = load_dataset(\"upstage/Pretraining_Dataset\", split=\"train\")\n",
    "pretraining_dataset = pretraining_dataset.select_columns([\"text\"])\n",
    "print(pretraining_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pretraining_dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚¬ì „ í•™ìŠµ ë° ë¯¸ì„¸ ì¡°ì • ë°ì´í„° ì„¸íŠ¸ ë¹„êµ  \n",
    "\n",
    "ë‹¤ìŒ ì…€ì—ì„œëŠ” ìœ„ì—ì„œ ë¡œë“œí•œ ì‚¬ì „ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì™€ ëŒ€ì¡°í•  ë¯¸ì„¸ ì¡°ì • ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. Alpaca ëª¨ë¸ ë° ëª…ë ¹ì–´ íŠœë‹ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://crfm.stanford.edu/2023/03/13/alpaca.html)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "instruction_dataset = load_dataset(\"c-s-ale/alpaca-gpt4-data\", split=\"train\")\n",
    "\n",
    "i = 0\n",
    "\n",
    "print(f\"\"\"Instruction: {instruction_dataset[i][\"instruction\"]}\n",
    "Input: {instruction_dataset[i][\"input\"]}\n",
    "Output: {instruction_dataset[i][\"output\"]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ì „ í•™ìŠµ ë°ì´í„°ê°€ ë‹¨ìˆœí•œ ì›ì‹œ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ê²ƒê³¼ ë‹¬ë¦¬ ë¯¸ì„¸ ì¡°ì •ìš© ë°ì´í„° ì„¸íŠ¸ëŠ” ì§ˆë¬¸-ë‹µë³€ ìŒì´ë‚˜ ëª…ë ¹ì–´-ì‘ë‹µ í˜•íƒœë¡œ êµ¬ì¡°í™”ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì…ë ¥ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ì—¬ê¸°ì—ì„œëŠ” ì•ìœ¼ë¡œëŠ” ë¹„êµ¬ì¡°í™”ëœ ì‚¬ì „ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ë§Œ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "## ì§ì ‘ ë°ì´í„° ìŠ¤í¬ë©í•´ì˜¤ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "# ìŠ¤í¬ë©ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "scrap_text = []\n",
    "source_dir = Path(\"../data/input/scraped_text\")\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ì½ê¸°\n",
    "for file_path in source_dir.iterdir():\n",
    "    if file_path.is_file():  # íŒŒì¼ì¸ì§€ í™•ì¸\n",
    "        with file_path.open(\"r\", encoding=\"utf-8\") as file:\n",
    "            scrap_text.append({\"text\": file.read()})\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ë¥¼ Dataset ê°ì²´ë¡œ ë³€í™˜\n",
    "scrap_text_dataset = Dataset.from_list(scrap_text)\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°ì…‹ê³¼ ìƒˆë¡œ ë§Œë“  ë°ì´í„°ì…‹ ê²°í•©\n",
    "dataset = concatenate_datasets([pretraining_dataset, scrap_text_dataset])\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì •ë¦¬\n",
    "\n",
    "ì´ì œ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„° ì •ë¦¬ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:  \n",
    "\n",
    "1. ë„ˆë¬´ ì§§ì€ ë°ì´í„° í•„í„°ë§  \n",
    "2. í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ë°˜ë³µëœ ë¶€ë¶„ ì œê±°  \n",
    "3. ì¤‘ë³µëœ ë¬¸ì„œ ì œê±°  \n",
    "4. ë¹„ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•˜ëŠ” í’ˆì§ˆ í•„í„° ì ìš©  \n",
    "\n",
    "### ë„ˆë¬´ ì§§ì€ ë°ì´í„° ì œê±°\n",
    "\n",
    "ì§§ì€ ë°ì´í„°ëŠ” ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë° ë„ì›€ì´ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë²ˆ ë‹¨ê³„ì—ì„œëŠ” ë„ˆë¬´ ì§§ì€ ë°ì´í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 3ê°œ ì´í•˜ì˜ í† í°ì„ ê°€ì§„ ë°ì´í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import re\n",
    "\n",
    "\n",
    "def paragraph_length_filter(example):\n",
    "    \"\"\"í˜ì´ì§€ì˜ ì¤„ ìˆ˜ê°€ ë„ˆë¬´ ì ê±°ë‚˜ ì¤„ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    lines = example[\"text\"].split(\"\\n\")\n",
    "    # ê°€ì¥ ê¸´ 3ê°œì˜ ì¤„ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ê³ , ê·¸ ì¤‘ ìµœì†Œ ê¸¸ì´ê°€ 3ë³´ë‹¤ ì‘ì€ ê²½ìš° False ë°˜í™˜\n",
    "    if len(lines) < 3 or min(heapq.nlargest(3, [len(line) for line in lines])) < 3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "print(f\"ë°ì´í„° ì •ë¦¬ ì „ ë°ì´í„°ì˜ ìˆ˜: {dataset.num_rows}\")\n",
    "\n",
    "# í•„í„°ë§ ì‘ì—… ìˆ˜í–‰\n",
    "dataset = dataset.filter(paragraph_length_filter, load_from_cache_file=False)\n",
    "\n",
    "print(f\"ë°ì´í„° ì •ë¦¬ í›„ ë°ì´í„°ì˜ ìˆ˜: {dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í›ˆë ¨ ìƒ˜í”Œ ë‚´ ë°˜ë³µëœ í…ìŠ¤íŠ¸ ì œê±°  \n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ê° ìƒ˜í”Œ ë‚´ì—ì„œ ë°˜ë³µëœ í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë°˜ë³µëœ í…ìŠ¤íŠ¸ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(paragraphs: list[str]) -> tuple[int, int]:\n",
    "    \"\"\"ì¤‘ë³µëœ ë‹¨ë½ì˜ ìˆ˜ì™€ ë¬¸ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
    "    unique_paragraphs = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph in unique_paragraphs:\n",
    "            duplicate_chars += len(paragraph)\n",
    "            duplicate_count += 1\n",
    "        else:\n",
    "            unique_paragraphs.add(paragraph)\n",
    "\n",
    "    return duplicate_count, duplicate_chars\n",
    "\n",
    "\n",
    "def paragraph_repetition_filter(example: dict[str, str]) -> bool:\n",
    "    \"\"\"í˜ì´ì§€ì— ì¤‘ë³µì´ ë„ˆë¬´ ë§ìœ¼ë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    text = example[\"text\"]\n",
    "    paragraphs = re.compile(r\"\\n{2,}\").split(text.strip())\n",
    "\n",
    "    duplicate_count, duplicate_chars = find_duplicates(paragraphs)\n",
    "\n",
    "    if duplicate_count / len(paragraphs) > 0.3:\n",
    "        return False\n",
    "    if duplicate_chars / len(text) > 0.2:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# ë°ì´í„°ì…‹ì— í•„í„° ì ìš©\n",
    "filtered_dataset = dataset.filter(paragraph_repetition_filter, load_from_cache_file=False)\n",
    "print(f\"í•„í„°ë§ í›„ ë°ì´í„°ì…‹ í¬ê¸°: {filtered_dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¤‘ë³µ ì œê±°  \n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì¤‘ë³µëœ ìƒ˜í”Œì„ ì œê±°í•©ë‹ˆë‹¤. (ì´ì „ ë‹¨ê³„ì—ì„œëŠ” ê° ìƒ˜í”Œ ë‚´ì—ì„œ ë°˜ë³µëœ í…ìŠ¤íŠ¸ë§Œ ì œê±°í–ˆìŠµë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplication(dataset: Dataset) -> Dataset:\n",
    "    unique_texts = set()\n",
    "\n",
    "    def dedup_func(example: dict[str, Any]) -> bool:\n",
    "        \"\"\"ì¤‘ë³µëœ í…ìŠ¤íŠ¸ í•­ëª©ì„ ì œê±°í•©ë‹ˆë‹¤.\"\"\"\n",
    "        if example[\"text\"] in unique_texts:\n",
    "            return False\n",
    "        unique_texts.add(example[\"text\"])\n",
    "        return True\n",
    "\n",
    "    deduplicated_dataset = dataset.filter(dedup_func, load_from_cache_file=False, num_proc=1)\n",
    "\n",
    "    print(f\"ì›ë˜ ë°ì´í„°ì…‹ í¬ê¸°: {dataset.num_rows}\")\n",
    "    print(f\"ì¤‘ë³µ ì œê±° í›„ ë°ì´í„°ì…‹ í¬ê¸°: {deduplicated_dataset.num_rows}\")\n",
    "\n",
    "    return deduplicated_dataset\n",
    "\n",
    "\n",
    "dataset = deduplication(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì–¸ì–´ í•„í„°ë§\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ì˜ì–´ê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ ìƒ˜í”Œì„ ì œê±°í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ `lingua-py`ë¼ëŠ” ì–¸ì–´ ê°ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://github.com/pemistahl/lingua-py)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "# ì˜ì–´ì™€ í•œêµ­ì–´ ê°ì§€ë¥¼ ìœ„í•œ ì–¸ì–´ ê°ì§€ê¸° ìƒì„±\n",
    "detector = LanguageDetectorBuilder.from_languages(Language.ENGLISH, Language.KOREAN).build()\n",
    "\n",
    "# ë°ì´í„°ì…‹ì—ì„œ ì˜ì–´ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§\n",
    "dataset = dataset.filter(\n",
    "    lambda x: detector.detect_language_of(x[\"text\"].replace(\"\\n\", \"\")) == Language.ENGLISH,\n",
    "    load_from_cache_file=False,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "# í•„í„°ë§ í›„ ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥\n",
    "print(f\"ì œê±° í›„ ë°ì´í„°ì…‹ í¬ê¸°: {dataset.num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì„¸íŠ¸ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥  \n",
    "\n",
    "Parquet ë°ì´í„° í˜•ì‹ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://parquet.apache.org/)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../data/output\")\n",
    "file_name = \"preprocessed_dataset.parquet\"\n",
    "file_path = output_dir / file_name\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íŒŒì¼€ì´ íŒŒì¼ë¡œ ì €ì¥\n",
    "dataset.to_parquet(file_path)\n",
    "\n",
    "print(f\"ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ íŒ¨í‚¤ì§•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.  \n",
    "\n",
    "# ë°ì´í„° íŒ¨í‚¤ì§•  \n",
    "\n",
    "ë°ì´í„° íŒ¨í‚¤ì§•ì€ í† í°í™”(Tokenizing)ì™€ íŒ¨í‚¹(Packing) ê³¼ì •ì„ í¬í•¨í•˜ë©° ê°ê°ì˜ ì—­í• ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- **í† í°í™”(Tokenizing)**: ê° í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ì‘ì€ ë‹¨ìœ„(í† í°)ë¡œ ë¶„í• í•˜ëŠ” ê³¼ì •  \n",
    "- **íŒ¨í‚¹(Packing)**: í›ˆë ¨ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ í† í°ì„ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë§ê²Œ ì •ë¦¬í•˜ëŠ” ê³¼ì •  \n",
    "\n",
    "## í† í°í™” ë° input_ids ë§Œë“¤ê¸°  \n",
    "\n",
    "ì´ì „ ë‹¨ê³„ì—ì„œ ì €ì¥í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"parquet\", data_files=file_path.as_posix(), split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face `Dataset` ê°ì²´ì˜ `shard` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ 10ê°œì˜ ë” ì‘ì€ ì¡°ê°(*shards*)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤. (*shard*ëŠ” ê¹¨ì§„ ìœ ë¦¬ ì¡°ê°ì²˜ëŸ¼ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê°œë…ì…ë‹ˆë‹¤.) Shardingì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://huggingface.co/docs/datasets/en/process#shard)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shard(num_shards=10, index=0)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , `input_ids`ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì€ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³  í† í°ì„ `input_ids`ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path_or_name = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    use_fast=False,  # ì°¸ê³ : ê¸´ í…ìŠ¤íŠ¸ ìƒ˜í”Œì´ ë•Œë•Œë¡œ ë©ˆì¶”ëŠ” ê²½í–¥ì´ ìˆì–´ ë¹ ë¥¸ í† í°í™”ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. ëŒ€ì‹  ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ map í•¨ìˆ˜ì™€ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
    ")\n",
    "tokenizer.tokenize(\"I'm a short sentence\")\n",
    "\n",
    "\n",
    "# í—¬í¼ í•¨ìˆ˜ ìƒì„±:\n",
    "def tokenization(example):\n",
    "    # í† í°í™”\n",
    "    tokens = tokenizer.tokenize(example[\"text\"])\n",
    "    # í† í°ì„ IDë¡œ ë³€í™˜\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # <bos>, <eos> í† í°ì„ token_idsì˜ ì•ê³¼ ë’¤ì— ì¶”ê°€\n",
    "    # bos: ì‹œí€€ìŠ¤ ì‹œì‘, eos: ì‹œí€€ìŠ¤ ë\n",
    "    token_ids = [tokenizer.bos_token_id] + token_ids + [tokenizer.eos_token_id]\n",
    "    example[\"input_ids\"] = token_ids\n",
    "\n",
    "    # ìµœì¢… ë°ì´í„°ì…‹ì˜ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì´ ì—´ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "    example[\"num_tokens\"] = len(token_ids)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ì „ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì˜ ëª¨ë“  ì˜ˆì œë¥¼ í† í°í™” í•©ë‹ˆë‹¤. \n",
    "\n",
    "> ì´ ê³¼ì •ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ì˜ ê° ì˜ˆì œì— tokenization í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "# load_from_cache_file=False ì˜µì…˜ì€ ìºì‹œëœ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•­ìƒ ìƒˆë¡œ ê³„ì‚°í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "dataset = dataset.map(tokenization, load_from_cache_file=False)\n",
    "\n",
    "# ë³€í™˜ëœ ë°ì´í„°ì…‹ì˜ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[3]  # ë°ì´í„°ì…‹ì—ì„œ ë„¤ ë²ˆì§¸ ìƒ˜í”Œì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "\n",
    "print(\"text\", sample[\"text\"][:30])  # ìƒ˜í”Œ í…ìŠ¤íŠ¸ì˜ ì²˜ìŒ 30ìë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(\"\\ninput_ids\", sample[\"input_ids\"][:30])  # í† í°í™”ëœ ì…ë ¥ IDì˜ ì²˜ìŒ 30ê°œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(\"\\nnum_tokens\", sample[\"num_tokens\"])  # ìƒ˜í”Œì˜ ì´ í† í° ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ì…‹ì˜ ì´ í† í° ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_tokens = np.sum(dataset[\"num_tokens\"])\n",
    "print(f\"ì´ í† í° ìˆ˜: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° íŒ¨í‚¹\n",
    "\n",
    "ë°ì´í„° íŒ¨í‚¹ì€ ì—¬ëŸ¬ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ë°ì´í„° ë¸”ë¡ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì—ì„œ ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. ì•„ë˜ ë‹¤ì´ì–´ê·¸ë¨ì€ ì¼ë°˜ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "```{mermaid}\n",
    "graph TD;\n",
    "    B[Load Dataset];\n",
    "    B --> C[Tokenize Each Example];\n",
    "    C --> D[Create input_ids];\n",
    "    D --> E[Pad Sequences to Max Length];\n",
    "    E --> F[Pack Tokens into Batches];\n",
    "```\n",
    "\n",
    "1. ë°ì´í„°ì…‹ ë¡œë“œ: ì›ì‹œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "2. í† í°í™”: ê° ì˜ˆì œ í…ìŠ¤íŠ¸ë¥¼ ê°œë³„ í† í°ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¨ì–´, í•˜ìœ„ ë‹¨ì–´ ë˜ëŠ” ë¬¸ì ìˆ˜ì¤€ì—ì„œ ìˆ˜í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "3. input_ids ìƒì„±: í† í°ì„ í•´ë‹¹í•˜ëŠ” ì •ìˆ˜ IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ì…ë‹ˆë‹¤.\n",
    "4. ì‹œí€€ìŠ¤ íŒ¨ë”©: ë°°ì¹˜ ë‚´ì˜ ëª¨ë“  ì‹œí€€ìŠ¤ê°€ ë™ì¼í•œ ê¸¸ì´ë¥¼ ê°–ë„ë¡ ì§§ì€ ì‹œí€€ìŠ¤ì— íŒ¨ë”©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "5. ë°°ì¹˜ë¡œ í† í° íŒ¨í‚¹: ì—¬ëŸ¬ ì˜ˆì œì˜ í† í°ì„ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ ëª¨ë“  ì˜ˆì œì˜ input_idsë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì—°ê²°í•˜ëŠ” ê²ƒì€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì²˜ë¦¬ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ì¤‘ìš”í•œ ìµœì í™” ê¸°ë²•ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ í•œ ë²ˆì— ì—¬ëŸ¬ ì˜ˆì œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŒ¨í‚¹ ê¸°ë²•ì€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•˜ê³  ë³‘ë ¬ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìˆì–´ ì „ì²´ì ì¸ í›ˆë ¨ ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetì˜ \"input_ids\" ë°°ì—´ì„ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "input_ids = np.concatenate(dataset[\"input_ids\"])\n",
    "\n",
    "# ì—°ê²°ëœ ë°°ì—´ì˜ ê¸¸ì´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(f\"ì—°ê²°ëœ ë°°ì—´ì˜ ê¸¸ì´ {len(input_ids)}\")\n",
    "\n",
    "# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "max_seq_length = 32\n",
    "\n",
    "# ì´ ê¸¸ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. input_idsì˜ ê¸¸ì´ì—ì„œ max_seq_lengthë¡œ ë‚˜ëˆ„ì–´ ë‚˜ë¨¸ì§€ë¥¼ ëº€ ê°’ì…ë‹ˆë‹¤.\n",
    "total_length = len(input_ids) - len(input_ids) % max_seq_length\n",
    "\n",
    "# ê³„ì‚°ëœ ì´ ê¸¸ì´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(f\"ê³„ì‚°ëœ ì´ ê¸¸ì´: {total_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¦¬ìŠ¤íŠ¸ ëì—ì„œ ì¶”ê°€ í† í°ì„ ë²„ë ¤ì„œ í† í°ì˜ ìˆ˜ê°€ `max_seq_length`ë¡œ ì •í™•í•˜ê²Œ ë‚˜ëˆ„ì–´ì§€ë„ë¡ í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids ë°°ì—´ì„ total_length ê¸¸ì´ë§Œí¼ ìë¦…ë‹ˆë‹¤.\n",
    "input_ids = input_ids[:total_length]\n",
    "\n",
    "# ì˜ë¦° ë°°ì—´ì˜ shape(í˜•ìƒ)ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(f\"input_idsì˜ shape: {input_ids.shape}\")\n",
    "\n",
    "# input_ids ë°°ì—´ì„ (í–‰: -1, ì—´: max_seq_length) í˜•íƒœë¡œ ì¬êµ¬ì¡°í™”í•˜ê³ , ë°ì´í„° íƒ€ì…ì„ int32ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)\n",
    "\n",
    "# ì¬êµ¬ì¡°í™”ëœ ë°°ì—´ì˜ shape(í˜•ìƒ)ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(f\"input_ids_reshapedì˜ shape: {input_ids_reshaped.shape}\")\n",
    "\n",
    "# ì¬êµ¬ì¡°í™”ëœ ë°°ì—´ì˜ ë°ì´í„° íƒ€ì…ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(f\"input_ids_reshapedì˜ ë°ì´í„° íƒ€ì…: {type(input_ids_reshaped)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face ë°ì´í„° ì„¸íŠ¸ë¡œ ë³€í™˜í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_reshapedë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "input_ids_list = input_ids_reshaped.tolist()\n",
    "\n",
    "# ë³€í™˜ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ Dataset ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "packaged_pretrain_dataset = datasets.Dataset.from_dict({\"input_ids\": input_ids_list})\n",
    "\n",
    "# ìƒì„±ëœ Dataset ê°ì²´ì˜ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(f\"ìƒì„±ëœ Dataset ê°ì²´: {packaged_pretrain_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## íŒ¨í‚¹ëœ ë°ì´í„° ì„¸íŠ¸ ë””ìŠ¤í¬ì— ì €ì¥\n",
    "\n",
    "Hugging Face Dataset ê°ì²´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n",
    "\n",
    "> `to_parquet()` ë©”ì„œë“œëŠ” ë°ì´í„°ë¥¼ Parquet íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ParquetëŠ” íš¨ìœ¨ì ì¸ ì»¬ëŸ¼ ê¸°ë°˜ ì €ì¥ í˜•ì‹ìœ¼ë¡œ, ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë° ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Parquet í˜•ì‹ì€ ë‹¤ë¥¸ ë°ì´í„° ë¶„ì„ íˆ´(ì˜ˆ: pandas, Apache Spark ë“±)ê³¼ ì‰½ê²Œ í˜¸í™˜ë©ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œ ë˜ëŠ” ë‹¤ë¥¸ íˆ´ê³¼ ê³µìœ í•  ë•Œ ìœ ìš©í•˜ë©° ì••ì¶•ê³¼ ì„±ëŠ¥ ìµœì í™” ì¸¡ë©´ì—ì„œ íš¨ìœ¨ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"packaged_pretrain_dataset.parquet\"\n",
    "file_path = output_dir / file_name\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íŒŒì¼€ì´ íŒŒì¼ë¡œ ì €ì¥\n",
    "packaged_pretrain_dataset.to_parquet(file_path)\n",
    "\n",
    "print(f\"ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packaged_pretrain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(packaged_pretrain_dataset[0][\"input_ids\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í›ˆë ¨ ì¤€ë¹„í•˜ê¸°\n",
    "\n",
    "## ëª¨ë¸ êµ¬ì„±\n",
    "\n",
    "Metaì˜ Llama ëª¨ë¸ ê³„ì—´ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ êµ¬ì„±í•  ê²ƒì…ë‹ˆë‹¤. transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ëŠ” ì´ ëª¨ë¸ë“¤ê³¼ í•¨ê»˜ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ë„êµ¬ê°€ ìˆìœ¼ë©°, ì´ì— ëŒ€í•´ [ì—¬ê¸°](https://huggingface.co/docs/transformers/main/en/model_doc/llama)ì—ì„œ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘ì€ `LlamaConfig` ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "\n",
    "def print_nparams(model):\n",
    "    \"\"\"ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜: {nparams}\")\n",
    "\n",
    "\n",
    "# LlamaConfig ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "config = LlamaConfig()\n",
    "print(f\"ê¸°ë³¸ ì„¤ì •: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ë³€ê²½í•˜ê¸° ìœ„í•´ ì„¤ì • ê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:\n",
    "config.num_hidden_layers = 12  # ê¸°ë³¸ê°’ 32ì—ì„œ 12ë¡œ ê°ì†Œ\n",
    "config.hidden_size = 1024  # ê¸°ë³¸ê°’ 4096ì—ì„œ 1024ë¡œ ê°ì†Œ (1/4 ì¶•ì†Œ)\n",
    "config.intermediate_size = 4096  # ê¸°ë³¸ê°’ 11008ì—ì„œ 4096ìœ¼ë¡œ ê°ì†Œ (MLP í‘œí˜„ ì°¨ì›, ì•½ 1/3 ì¶•ì†Œ)\n",
    "config.num_key_value_heads = 8  # ê¸°ë³¸ê°’ num_attention_heads=32ì—ì„œ 8ë¡œ ê°ì†Œ (1/4 ì¶•ì†Œ)\n",
    "config.torch_dtype = \"bfloat16\"  # ì •ë°€ë„ë¥¼ ê°ì†Œ\n",
    "config.use_cache = False  # `True`ëŠ” gradient checkpointingê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŒ\n",
    "print(f\"ì—…ë°ì´íŠ¸ëœ ì„¤ì •: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "\n",
    "ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë°©ë²•ì—ëŠ” ë‹¤ìŒ 4ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ê°ê°ì„ ê°„ëµíˆ ì„¤ëª…í•˜ê³  4ë²ˆì§¸ ë°©ë²•ì¸ depth upscaling ë°©ë²•ì„ ì‚¬ìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "  \n",
    "### ê°€ì¤‘ì¹˜ ëœë¤ ì´ˆê¸°í™”\n",
    "\n",
    "ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ëœë¤ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ëª¨ë“  ê°€ì¤‘ì¹˜ëŠ” í‰ê· ì´ 0ì´ê³  í‘œì¤€ í¸ì°¨ê°€ 0.02ì¸ ì ˆë‹¨ëœ ì •ê·œ ë¶„í¬(truncated normal distribution)ì—ì„œ ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤. í‰ê· ì—ì„œ 2ì‹œê·¸ë§ˆ(2Ïƒ)ë¥¼ ì´ˆê³¼í•˜ëŠ” ê°’ì€ 0ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì¥ì :\n",
    "\n",
    "- ëŒ€ì¹­ì„±ì„ ê¹¨ëœ¨ë ¤ ë‰´ëŸ°ì´ ì„œë¡œ ë‹¤ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì›€.\n",
    "- ë‹¤ì–‘í•œ ì´ˆê¸°ê°’ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ê³µê°„ íƒìƒ‰ ê°€ëŠ¥.\n",
    "\n",
    "ë‹¨ì :\n",
    "- ì´ˆê¸°ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ê¸°ìš¸ê¸° í­ì£¼(exploding gradient) ë˜ëŠ” ì†Œì‹¤(vanishing gradient) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.\n",
    "- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "### ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€ ì‚¬ì „ í›ˆë ¨\n",
    "\n",
    "ê¸°ì¡´ ê³µê°œëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì¥ì :\n",
    "\n",
    "- ê¸°ì¡´ ëª¨ë¸ì˜ ê°•ì ì„ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ ë°ì´í„°ì— ë§ê²Œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥.\n",
    "- í•™ìŠµ ì‹œê°„ì´ ë‹¨ì¶•ë  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "ë‹¨ì :\n",
    "\n",
    "- ê¸°ì¡´ ëª¨ë¸ì´ ìƒˆë¡œìš´ ë°ì´í„°ì™€ ì¶©ë¶„íˆ ìœ ì‚¬í•˜ì§€ ì•Šë‹¤ë©´ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥.\n",
    "- ì¶”ê°€ í›ˆë ¨ ì‹œ ê³¼ì í•©(overfitting) ìœ„í—˜ ë°œìƒ ê°€ëŠ¥.\n",
    "\n",
    "### ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ì¶•ì†Œ\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ë©´ `tinySolar-248m-4k` ëª¨ë¸ì„ 12ê°œ ë ˆì´ì–´ì—ì„œ 10ê°œ ë ˆì´ì–´ë¡œ ì¶•ì†Œí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¥ì :\n",
    "- ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì—¬ ê³„ì‚° ë¹„ìš© ê°ì†Œ.\n",
    "- ê°„ë‹¨í•œ ì‘ì—…ì— ë” ì í•©í•˜ê²Œ ì¡°ì • ê°€ëŠ¥.\n",
    "\n",
    "ë‹¨ì :\n",
    "- ë³µì¡í•œ ë¬¸ì œì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥.\n",
    "- ì¤‘ìš”í•œ ì •ë³´ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "### ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ í™•ì¥\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ë©´ `tinySolar-248m-4k` ëª¨ë¸ì„ 12ê°œ ë ˆì´ì–´ì—ì„œ 16ê°œ ë ˆì´ì–´ë¡œ í™•ì¥í•  ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ë†’ì´ëŠ” ë° íš¨ê³¼ì ì´ë©°, ê¸°ì¡´ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•´ íš¨ìœ¨ì ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì¥ì :\n",
    "- ë” ë³µì¡í•œ ë¬¸ì œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ í–¥ìƒ.\n",
    "- ê¸°ì¡´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¥í•˜ì—¬ ë” ë§ì€ ë°ì´í„°ë¥¼ í™œìš© ê°€ëŠ¥.\n",
    "\n",
    "ë‹¨ì :\n",
    "- ê³„ì‚° ë¹„ìš© ì¦ê°€ ë° í•™ìŠµ ì‹œê°„ ì—°ì¥.\n",
    "- ë ˆì´ì–´ë¥¼ ì˜ëª» ì¶”ê°€í•˜ë©´ ê³¼ì í•© ìœ„í—˜ ì¦ê°€.\n",
    "\n",
    "#### ì‹¤ìŠµ\n",
    "\n",
    "ì´ì œ 12ê°œ ë ˆì´ì–´ì˜ `tinySolar-248m-4k` ëª¨ë¸ì„ 16ê°œ ë ˆì´ì–´ë¡œ í™•ì¥í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë ˆì´ì–´ ì„ íƒ ì „ëµ: ì›ë³¸ 12ê°œ ë ˆì´ì–´ ì¤‘ í•˜ìœ„ 8ê°œ(ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ) + ìƒìœ„ 8ê°œ(ê³ ìˆ˜ì¤€ ì¶”ìƒí™”)ë¥¼ ì¤‘ë³µ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê³„ì‚° íš¨ìœ¨ì„±(ì „ì²´ ë ˆì´ì–´ ì¬í•™ìŠµ ëŒ€ì‹  ê¸°ì¡´ ë ˆì´ì–´ ì¬í™œìš©ìœ¼ë¡œ í•™ìŠµ ì‹œê°„ ë‹¨ì¶•)ê³¼ í˜¸í™˜ì„±ì´ ë³´ì¥(ì„ë² ë”©/ë¶„ë¥˜ ë ˆì´ì–´ ìœ ì§€ë¡œ ì…ë ¥-ì¶œë ¥ êµ¬ì¡° ì¼ê´€ì„± í™•ë³´)ë©ë‹ˆë‹¤. ìˆ˜í–‰í•  ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. 16ê°œ ë ˆì´ì–´ ëª¨ë¸ êµ¬ì„± ë° ëœë¤ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "    - 16ê°œ ë ˆì´ì–´ êµ¬ì¡°ì˜ ìƒˆ ëª¨ë¸ì„ ìƒì„±í•˜ê³  ëœë¤ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "2. 12ê°œ ë ˆì´ì–´ë¥¼ ê°€ì§„ `tinySolar-248m-4k` ëª¨ë¸ ë©”ëª¨ë¦¬ ë¡œë“œ\n",
    "    - ê¸°ì¡´ 12ê°œ ë ˆì´ì–´ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "3. ë ˆì´ì–´ ë³µì œ ë° ê°€ì¤‘ì¹˜ ë®ì–´ì“°ê¸°\n",
    "    - ì›ë³¸ 12ê°œ ë ˆì´ì–´ ëª¨ë¸ì—ì„œ í•˜ìœ„ 8ê°œ ë ˆì´ì–´ì™€ ìƒìœ„ 8ê°œ ë ˆì´ì–´ë¥¼ ë³µì‚¬í•˜ì—¬ 16ê°œ ë ˆì´ì–´ ëª¨ë¸ì˜ ëœë¤ ê°€ì¤‘ì¹˜ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "4. ì„ë² ë”©/ë¶„ë¥˜ ë ˆì´ì–´ ë³µì œ\n",
    "    - ì›ë³¸ ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´(embedding layers)ì™€ ë¶„ë¥˜ ë ˆì´ì–´(classifying layers)ë¥¼ ìƒˆ ëª¨ë¸ì˜ ëœë¤ ì´ˆê¸°í™”ëœ í•´ë‹¹ ë ˆì´ì–´ì— ë³µì‚¬í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaConfig, TextStreamer\n",
    "\n",
    "# LLaMA ëª¨ë¸ êµ¬ì„± ì„¤ì •\n",
    "config = LlamaConfig(\n",
    "    num_hidden_layers=16,  # ìµœì¢…ì ìœ¼ë¡œ 16ê°œì˜ ë ˆì´ì–´ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ì›í•¨\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=4096,\n",
    "    num_attention_heads=32,\n",
    "    num_key_value_heads=8,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    use_cache=False,\n",
    ")\n",
    "print(config)\n",
    "\n",
    "# ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„± ë° bfloat16ìœ¼ë¡œ ë³€í™˜\n",
    "model = LlamaForCausalLM(config)\n",
    "model = model.to(dtype=torch.bfloat16)\n",
    "print_nparams(model)  # 308839424 => 308M\n",
    "\n",
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print_nparams(pretrained_model)  # 248013824 => 248M\n",
    "\n",
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ë ˆì´ì–´ë¥¼ ìƒˆ ëª¨ë¸ë¡œ ë³µì‚¬\n",
    "model.model.layers = deepcopy(pretrained_model.model.layers[:-4]) + deepcopy(\n",
    "    pretrained_model.model.layers[4:]\n",
    ")\n",
    "\n",
    "# ì„ë² ë”© ë ˆì´ì–´ ë³µì‚¬\n",
    "model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n",
    "\n",
    "# ì–¸ì–´ ëª¨ë¸ í—¤ë“œ ë³µì‚¬\n",
    "model.lm_head = deepcopy(pretrained_model.lm_head)\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# ê°„ë‹¨í•œ ì¶”ë¡  ì‹¤í–‰ìœ¼ë¡œ í•™ìŠµë˜ì§€ ì•Šì€ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, streamer=streamer, use_cache=True, max_new_tokens=128, do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì˜ ê²°ê³¼ë¥¼ í†µí•´ í•™ìŠµë˜ì§€ ì•Šì€ ëª¨ë¸ì€ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ê°™ì€ ë§ì„ ë°˜ë³µ) ì¼ë‹¨ì€ í•´ë‹¹ ëª¨ë¸ì„ ë””ìŠ¤í¬ì— ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤. ìƒˆ ëª¨ë¸ ì´ë¦„ì€ í™•ì¥ëœ 3ì–µ 8ë°±ë§Œ ê°œ ë§¤ê°œë³€ìˆ˜(308M)ë¥¼ ë°˜ì˜í•´ `TinySolar-308m-4k-init`ë¡œ ì§€ì •í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"TinySolar-308m-4k-init\"\n",
    "file_path = output_dir / file_name\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 308M íŒŒë¼ë¯¸í„° ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(file_path)\n",
    "\n",
    "# ì°¸ê³ : ë©”ëª¨ë¦¬ ì œí•œ í™˜ê²½ì—ì„œ ëŒ€ê·œëª¨ ëª¨ë¸ ì‹¤í–‰ ì‹œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ë¬¸ì œ ë°œìƒ ì‹œ ì‹¤í–‰)\n",
    "import gc\n",
    "\n",
    "del model  # ëª¨ë¸ ê°ì²´ ì‚­ì œ\n",
    "gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "ì‚¬ì „ í•™ìŠµ(pretraining)ì€ ì»´í“¨íŒ… ìì›ì´ ë§¤ìš° ë§ì´ ë“­ë‹ˆë‹¤! ë”°ë¼ì„œ ì‚¬ì „ í•™ìŠµ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ê¸° ì „ì— ë¹„ìš©ì´ ì–¼ë§ˆë‚˜ ë“¤ì§€ í™•ì¸í•´ë³´ëŠ”ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ğŸ¤— Hugging Faceì˜ [ë¹„ìš© ê³„ì‚°ê¸°](https://huggingface.co/training-cluster)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì‘ì—… ë¹„ìš©ì„ ëŒ€ëµì ìœ¼ë¡œ ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AWSë‚˜ Google Cloudì™€ ê°™ì€ ë‹¤ë¥¸ ì¸í”„ë¼ì—ì„œ í•™ìŠµí•  ê²½ìš°, í•´ë‹¹ ì œê³µì—…ì²´ì˜ ìµœì‹  ë¹„ìš© ì¶”ì •ì¹˜ë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµí•  ëª¨ë¸ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "ì´ì „ì— ë§Œë“¤ì—ˆë˜ í™•ì¥(upscale)í•œ ëª¨ë¸ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    file_path.as_posix(),\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")\n",
    "pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ ì¶œë ¥ ê²°ê³¼ë¥¼ í†µí•´ ë ˆì´ì–´ê°€ 16ê°œì¸ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "\n",
    "`Dataset` ê°ì²´ì˜ ë‘ ê°€ì§€ ë©”ì„œë“œë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ íŠ¸ë ˆì´ë„ˆì™€ ì¸í„°í˜ì´ìŠ¤í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ ë©”ì„œë“œë“¤ì€ ìƒì„±í•œ ë°ì´í„°ì…‹ì„ í•™ìŠµ ë°ì´í„°ë¡œ ì§€ì •í•  ë•Œ í•„ìš” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, split=\"train\"):\n",
    "        \"\"\"ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ê°ì²´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\"\"\"\n",
    "        self.args = args\n",
    "        self.dataset = datasets.load_dataset(\"parquet\", data_files=args.dataset_name, split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        ì§€ì •ëœ ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„°ì…‹ì˜ ë‹¨ì¼ ë°ì´í„° ìƒ˜í”Œì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        # ë¦¬ìŠ¤íŠ¸ë¥¼ PyTorchìš© LongTensorë¡œ ë³€í™˜\n",
    "        input_ids = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "        labels = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "\n",
    "        # ìƒ˜í”Œì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ íŒŒë¼ë¯¸í„° êµ¬ì„±\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ë‹¤ì–‘í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ì „ì— ì¤€ë¹„í•œ ë°ì´í„°ì…‹ì„ í•™ìŠµ ê³¼ì •ì— ì—°ê²°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "import transformers\n",
    "\n",
    "output_dir = Path(\"../data/output\")\n",
    "dataset_name = \"packaged_pretrain_dataset.parquet\"\n",
    "dataset_path = output_dir / dataset_name\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomArguments(transformers.TrainingArguments):\n",
    "    # ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "    dataset_name: str = field(  # ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "        default=dataset_path.as_posix()\n",
    "    )\n",
    "    num_proc: int = field(default=1)  # ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ ìˆ˜\n",
    "    max_seq_length: int = field(default=32)  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "\n",
    "    # í•µì‹¬ í•™ìŠµ ì„¤ì •\n",
    "    seed: int = field(default=0)  # ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ëœë¤ ì‹œë“œ, ì¬í˜„ì„±ì„ ë³´ì¥\n",
    "    optim: str = field(default=\"adamw_torch\")  # ì˜µí‹°ë§ˆì´ì € ì„¤ì •, ì—¬ê¸°ì„œëŠ” PyTorchì˜ AdamW ì‚¬ìš©\n",
    "    max_steps: int = field(default=10000)  # ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜\n",
    "    per_device_train_batch_size: int = field(default=2)  # ê° ë””ë°”ì´ìŠ¤ì—ì„œ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë°°ì¹˜ í¬ê¸°\n",
    "\n",
    "    # ê¸°íƒ€ í•™ìŠµ ì„¤ì •\n",
    "    learning_rate: float = field(default=5e-5)  # ì˜µí‹°ë§ˆì´ì €ì˜ ì´ˆê¸° í•™ìŠµë¥  ì„¤ì •\n",
    "    weight_decay: float = field(default=0)  # ê°€ì¤‘ì¹˜ ê°ì†Œìœ¨ ì„¤ì •\n",
    "    warmup_steps: int = field(default=10)  # í•™ìŠµë¥  ì›Œë°ì—… ë‹¨ê³„ ìˆ˜ ì„¤ì •\n",
    "    lr_scheduler_type: str = field(default=\"linear\")  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìœ í˜• ì„¤ì •\n",
    "    gradient_checkpointing: bool = field(\n",
    "        default=True\n",
    "    )  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŠ¸ í™œì„±í™”\n",
    "    dataloader_num_workers: int = field(default=2)  # ë°ì´í„° ë¡œë”©ì„ ìœ„í•œ ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì„¤ì •\n",
    "    bf16: bool = field(default=True)  # ì§€ì›ë˜ëŠ” í•˜ë“œì›¨ì–´ì—ì„œ bfloat16 ì •ë°€ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ìˆ˜í–‰\n",
    "    gradient_accumulation_steps: int = field(\n",
    "        default=1\n",
    "    )  # ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ì „ì— ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ëŠ” ë‹¨ê³„ ìˆ˜\n",
    "\n",
    "    # ë¡œê¹… êµ¬ì„±\n",
    "    logging_steps: int = field(default=1000)  # í•™ìŠµ ì •ë³´ë¥¼ ë¡œê¹…í•˜ëŠ” ë¹ˆë„(ìŠ¤í… ë‹¨ìœ„)\n",
    "    report_to: str = field(default=\"none\")  # ë¡œê¹… ëŒ€ìƒ(e.g., WandB, TensorBoard)\n",
    "\n",
    "    # ì €ì¥ êµ¬ì„±\n",
    "    save_strategy: str = field(default=\"steps\")  # \"epoch\"ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥ (ì €ì¥ ì „ëµ)\n",
    "    save_steps: int = field(default=1000)  # í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ëŠ” ë¹ˆë„(ìŠ¤í… ë‹¨ìœ„)\n",
    "    save_total_limit: int = field(default=2)  # ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ì˜ ìµœëŒ€ ê°œìˆ˜ ì œí•œ\n",
    "\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ ì¸ìë¥¼ íŒŒì‹±í•˜ê³  ëª¨ë¸ ì €ì¥ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤:\n",
    "parser = transformers.HfArgumentParser(CustomArguments)\n",
    "(args,) = parser.parse_args_into_dataclasses(args=[f\"--output_dir={output_dir.as_posix()}\"])\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì…‹ì„ ì„¤ì •í•©ë‹ˆë‹¤:\n",
    "train_dataset = CustomDataset(args=args)\n",
    "\n",
    "# ë°ì´í„°ì…‹ì˜ í˜•íƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤:\n",
    "print(\"Input shape: \", train_dataset[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## íŠ¸ë ˆì´ë„ˆ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "ë¨¼ì €, í•™ìŠµ ì¤‘ ì†ì‹¤ ê°’ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ ì½œë°±ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ì½œë°±ì€ í•™ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì†ì‹¤ ê°’ì„ ì¶”ì í•˜ê³  ê¸°ë¡í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "\n",
    "> ì†ì‹¤ ê°’ì˜ ì¶”ì´ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ë©´ í•™ìŠµ ê³¼ì •ì„ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸‰ê²©í•œ ì†ì‹¤ ë³€í™”ë‚˜ ì´ìƒì¹˜ê°€ ìˆëŠ”ì§€ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ì„¸ìš”. ì´ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œì˜ ë¬¸ì œë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback\n",
    "\n",
    "\n",
    "# ì†ì‹¤ ê°’ ê¸°ë¡ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± ì •ì˜\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"í•™ìŠµ ì¤‘ ë¡œê·¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤\"\"\"\n",
    "        if logs is not None:\n",
    "            self.logs.append(logs)\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"ì½œë°± ì´ˆê¸°í™” ë° ë¡œê·¸ ì €ì¥ì†Œ ìƒì„±\"\"\"\n",
    "        self.logs = []\n",
    "\n",
    "\n",
    "# ì½œë°± ê°ì²´ ìƒì„±\n",
    "loss_logging_callback = LossLoggingCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ Hugging Face `Trainer` ê°ì²´ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. íŠ¸ë ˆì´ë„ˆì˜ `train()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=pretrained_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    callbacks=[loss_logging_callback],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹œê°í™”ë¥¼ í†µí•´ Training lossê°€ ì–´ë–»ê²Œ ì¤„ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ggplot ìŠ¤íƒ€ì¼ ì ìš©\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "logs = loss_logging_callback.logs\n",
    "# loss_logging_callbackì—ì„œ ê¸°ë¡í•œ ì†ì‹¤ ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "losses = [log[\"loss\"] for log in logs if \"loss\" in log]  # 'loss' í‚¤ê°€ ìˆëŠ” ë¡œê·¸ë§Œ ì¶”ì¶œ\n",
    "\n",
    "plt.figure(figsize=(6, 4))  # ë°°ê²½ìƒ‰ ì„¤ì •\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì„ ê³„ì†í•  ìˆ˜ë¡ ì†ì‹¤ ê°’ì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë” ì˜ ì´í•´í•˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì†ì‹¤ ê°’ì´ ê°ì†Œí•˜ëŠ” ì†ë„ê°€ ëŠë ¤ì§€ë©´ í•™ìŠµì´ ìˆ˜ë ´(convergence)í•˜ê³  ìˆìŒì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° í•™ìŠµë¥ ì„ ì¡°ì •í•˜ê±°ë‚˜ ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì œ í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•´ë³´ì£ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ë¦„ ë° ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "model_name = \"TinySolar-308m-4k-finetune\"  # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì´ë¦„\n",
    "save_path = output_dir / model_name  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ìƒì„±\n",
    "\n",
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "pretrained_model.save_pretrained(save_path.as_posix())  # ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ êµ¬ì„± íŒŒì¼ ì €ì¥\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "tokenizer.save_pretrained(save_path.as_posix())  # í† í¬ë‚˜ì´ì € ê´€ë ¨ íŒŒì¼(vocab ë“±) ì €ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ì˜ ì„±ëŠ¥ í™•ì¸\n",
    "\n",
    "ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•˜ë©´ì„œ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸(ì„ì‹œ ì €ì¥)ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•´ë³´ëŠ” ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ëŠ” ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Solar í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³  `TextStreamer` ê°ì²´ë¥¼ ì„¤ì •í•˜ì—¬ ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "# 1. í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 2. ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint_path = \"../data/output/checkpoint-10000\"\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 3. í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "# 4. ì…ë ¥ í† í°í™”\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model2.device)\n",
    "\n",
    "# 5. í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì •\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# 6. í…ìŠ¤íŠ¸ ìƒì„±\n",
    "outputs = model2.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ ì²˜ìŒ ì‹œì‘í–ˆë˜ ëª¨ë¸ê³¼ ë¹„êµí•˜ë©´ í›¨ì”¬ ë” ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ê°€ ìƒì„±ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ë§¥ë½ì€ ì´í•´í•˜ê¸° ì–´ë µë„¤ìš”. \n",
    "\n",
    "# ëª¨ë¸ í‰ê°€\n",
    "\n",
    "LLM ëª¨ë¸ í‰ê°€ëŠ” ì „í†µì ì¸ ê¸°ê³„í•™ìŠµ ëª¨ë¸ê³¼ ë‹¬ë¦¬ ë³µì¡í•˜ê³  ë‹¤ë©´ì ì¸ ê³¼ì •ì…ë‹ˆë‹¤. ì£¼ìš” í‰ê°€ ë°©ë²•ìœ¼ë¡œëŠ” ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì™€ ë¦¬ë”ë³´ë“œ í‰ê°€ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ë§Œ ì‹¤ì œ êµ¬í˜„í•˜ëŠ” ì½”ë“œëŠ” ì´ ê¸€ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ê¸°ì— ìƒëµí•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ë²¤ì¹˜ë§ˆí¬ í‰ê°€ë²•\n",
    "\n",
    "ë²¤ì¹˜ë§ˆí¬ í‰ê°€ëŠ” í‘œì¤€í™”ëœ ë°ì´í„°ì…‹ê³¼ íƒœìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- LM Evaluation Harness: EleutherAIì—ì„œ ê°œë°œí•œ ë„êµ¬ë¡œ, ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ íƒœìŠ¤í¬ì— ëŒ€í•´ LLMì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ [GitHub ë¦¬í¬ì§€í† ë¦¬](https://github.com/EleutherAI/lm-evaluation-harness)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- TruthfulQA: ëª¨ë¸ì˜ ì§„ì‹¤ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë¡œ, 817ê°œì˜ ì§ˆë¬¸ì„ í¬í•¨í•˜ë©° ê±´ê°•, ë²•ë¥ , ê¸ˆìœµ ë“± 38ê°œ ì£¼ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. TruthfulQA ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì´ ë…¼ë¬¸](https://arxiv.org/abs/2109.07958)ì—ì„œ ì½ì„ ìˆ˜ ìˆìœ¼ë©° êµ¬í˜„ ì½”ë“œëŠ” ë‹¤ìŒ [GitHub ë¦¬í¬ì§€í† ë¦¬](https://github.com/sylinrl/TruthfulQA)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ë¦¬ë”ë³´ë“œ í‰ê°€ë²•\n",
    "\n",
    "ë¦¬ë”ë³´ë“œëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•  ìˆ˜ ìˆëŠ” í”Œë«í¼ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "- Hugging Face [ë¦¬ë”ë³´ë“œ](https://huggingface.co/open-llm-leaderboard): ë‹¤ì–‘í•œ LLM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•  ìˆ˜ ìˆëŠ” í”Œë«í¼ì…ë‹ˆë‹¤. ì „ë¬¸ê°€ë‚˜ ì¼ë°˜ ì‚¬ìš©ìê°€ ì§ì ‘ ëª¨ë¸ì˜ ì¶œë ¥ì„ í‰ê°€í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë§ˆì¹˜ë©°\n",
    "\n",
    "ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” LLM ì‚¬ì „ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘, ì •ì œ, íŒ¨í‚¤ì§•, ëª¨ë¸ êµ¬ì„±, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”, í•™ìŠµì˜ ê³¼ì •ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ LLM ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì œ ì—¬ëŸ¬ë¶„ë„ LLM ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í‰ê°€í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ì…¨ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ë‹¤ì–‘í•œ ì—…ë¬´ì— LLM ëª¨ë¸ì„ ì ìš©í•˜ì—¬ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "\n",
    "> AIëŠ” ìƒˆë¡œìš´ ì „ê¸°ì™€ ê°™ì•„ì„œ ì‚¶ì˜ ê±°ì˜ ëª¨ë“  ì˜ì—­ì„ ë³€í™”ì‹œí‚¤ê³  ê°œì„ í•  ê²ƒì…ë‹ˆë‹¤. - [Andrew Ng](https://www.andrewng.org/)\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [Pretraining LLMs](https://learn.deeplearning.ai/courses/pretraining-llms/lesson/xg5n5/why-pre-training)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
