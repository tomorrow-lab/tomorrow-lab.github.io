{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1e692b83-8519-4d6a-a25c-c405fa494654",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"직접 구현하는 대규모 언어 모델\"\n",
    "author: \"Taeyoon Kim\"\n",
    "categories: [LLM, Machine Learning, Python]\n",
    "draft: true\n",
    "date: \"2025-03-10\"\n",
    "date-modified: last-modified\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61066932",
   "metadata": {
    "papermill": {
     "duration": 0.01656,
     "end_time": "2024-09-06T09:15:41.885911",
     "exception": false,
     "start_time": "2024-09-06T09:15:41.869351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "이 글은 책 \"(처음부터)대규모 언어 모델 구축하기\"에서 배운 내용을 하나의 파일로 정리해서 대규모 언어 모델에 대한 이해를 높이기 위해 작성되었습니다. 모든 단계의 세부 사항을 알고 싶다면 공식 [리포지토리](https://github.com/rasbt/LLMs-from-scratch)에서 확인 할 수 있습니다.\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "시작하기 전에 딥러닝에 대한 기본 지식, 자연어 처리의 기초, 파이썬 프로그래밍 능력, 그리고 파이토치(PyTorch) 프레임워크 사용법을 어느 정도 알고 계시면 좋습니다. 이 정도 배경 지식이 있으면 내용을 더 쉽게 이해하실 수 있습니다.\n",
    "\n",
    ":::\n",
    "\n",
    "# 대규모 언어 모델이란?\n",
    "\n",
    "대규모 언어 모델(LLM)은 자연어 처리 분야에 혁명을 일으켰습니다. 여기서 \"대규모\"라는 말은 모델의 매개변수 수와 훈련에 사용된 거대한 데이터셋을 모두 가리킵니다. 이런 모델들은 보통 수십억에서 수백억 개의 매개변수를 가지고 있어요. 이 매개변수들은 문장의 다음 단어를 예측하기 위해 훈련 과정에서 조정되는 네트워크의 가중치들이에요. 다음 단어를 예측하는 이 방식은 언어의 순차적인 특성을 잘 활용해서, 모델이 텍스트 속의 맥락, 구조, 관계를 이해하도록 만드는 데 아주 적합합니다.\n",
    "\n",
    "대규모 언어 모델의 학습은 보통 두 단계로 이루어집니다. 먼저, 엄청난 양의 레이블 없는 텍스트 데이터를 사용해 다음 단어를 예측하는 사전 학습을 합니다. 그 다음 특정 작업을 위해 더 작은 규모의 레이블이 있는 데이터로 미세 조정을 거칩니다. \n",
    "\n",
    "여기에서는 GPT의 기본 아이디어를 청사진으로 삼아 세 단계로 재현해 봅니다.\n",
    "\n",
    "## 환경 설정\n",
    "\n",
    "먼저 라이브러리를 설치하고 가져온 다음 프로젝트 전반에서 사용할 수 있는 모든 변수를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e8ebb",
   "metadata": {
    "papermill": {
     "duration": 3.331245,
     "end_time": "2024-09-06T09:15:59.974725",
     "exception": false,
     "start_time": "2024-09-06T09:15:56.64348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 라이브러리 가져오기\n",
    "import os\n",
    "import urllib.request\n",
    "from dataclasses import asdict, dataclass, replace\n",
    "\n",
    "import numpy as np\n",
    "import tiktoken as ttk\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 공식 리포지토리에서 작업할 원시 텍스트 가져오기\n",
    "if not os.path.exists(\"../data/input/the-verdict.txt\"):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    file_path = \"../data/input/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257\n",
    "    context_length: int = 512\n",
    "    emb_dim: int = 768\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    drop_rate: float = 0.1\n",
    "    qkv_bias: bool = False\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        config_dict = self.to_dict()\n",
    "        formatted_items = [f'\"{key}\": {repr(value)}' for key, value in config_dict.items()]\n",
    "        return \"GPT_CONFIG_124M = {\\n    \" + \",\\n    \".join(formatted_items) + \"\\n}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    dataPath: str = \"../data/input/the-verdict.txt\"\n",
    "    max_length: int = GPTConfig.context_length\n",
    "    batch_size: int = 64\n",
    "    train_ratio: float = 0.9\n",
    "    stride: int = GPTConfig.context_length\n",
    "\n",
    "\n",
    "DataConfig = DataConfig()\n",
    "GPTConfig = GPTConfig()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def read_txt(path: str) -> str:\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        return raw_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: {path}에서 파일을 찾을 수 없습니다\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"오류가 발생했습니다: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # 배치 차원 추가\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # 배치 차원 제거\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "class LLMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    언어 모델링을 위한 입력 및 목표 시퀀스로 텍스트 데이터를 처리하는 사용자 정의 Dataset입니다.\n",
    "\n",
    "    인자:\n",
    "        txt (str): 토큰화 및 처리될 입력 텍스트입니다.\n",
    "        tokenizer (Tokenizer): 텍스트 인코딩에 사용될 토크나이저입니다.\n",
    "        max_length (int): 각 입력 시퀀스의 최대 길이입니다.\n",
    "        stride (int): 시퀀스 간 건너뛸 토큰의 수입니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, txt, tokenizer, max_length: int, stride: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i in tqdm(range(0, len(token_ids) - max_length, stride)):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 샘플 수를 반환합니다.\n",
    "        \"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        지정된 인덱스의 입력 및 목표 시퀀스를 검색합니다.\n",
    "\n",
    "        인자:\n",
    "            idx (int): 검색할 시퀀스의 인덱스입니다.\n",
    "\n",
    "        반환:\n",
    "            tuple: (input_ids, target_ids) 두 텐서를 포함하는 튜플입니다.\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def LLM_DataLoader(\n",
    "    txt,\n",
    "    tokenizer,\n",
    "    batch_size: int,\n",
    "    max_length: int,\n",
    "    stride: int,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    LLMDataset을 위한 DataLoader를 생성합니다.\n",
    "\n",
    "    인자:\n",
    "        txt (str): 토큰화 및 처리될 입력 텍스트입니다.\n",
    "        tokenizer (Tokenizer): 텍스트 인코딩에 사용될 토크나이저입니다.\n",
    "        batch_size (int): 로드할 배치당 샘플 수입니다.\n",
    "        max_length (int): 각 입력 시퀀스의 최대 길이입니다.\n",
    "        stride (int): 시퀀스 간 건너뛸 토큰의 수입니다.\n",
    "        shuffle (bool, 선택적): 매 에폭마다 데이터를 섞을지 여부입니다. 기본값은 True입니다.\n",
    "        drop_last (bool, 선택적): 마지막 불완전한 배치를 버릴지 여부입니다. 기본값은 True입니다.\n",
    "\n",
    "    반환:\n",
    "        DataLoader: LLMDataset을 위한 DataLoader 인스턴스입니다.\n",
    "    \"\"\"\n",
    "    llmdataset = LLMDataset(txt, tokenizer, max_length, stride)\n",
    "    llmdataloader = DataLoader(\n",
    "        llmdataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "    return llmdataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb640601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T12:20:13.752538Z",
     "iopub.status.busy": "2024-08-26T12:20:13.750863Z",
     "iopub.status.idle": "2024-08-26T12:20:13.765879Z",
     "shell.execute_reply": "2024-08-26T12:20:13.764098Z",
     "shell.execute_reply.started": "2024-08-26T12:20:13.752459Z"
    },
    "papermill": {
     "duration": 0.016775,
     "end_time": "2024-09-06T09:16:00.64489",
     "exception": false,
     "start_time": "2024-09-06T09:16:00.628115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1단계\n",
    "\n",
    "이 장에서는 대규모 언어 모델(LLM) 훈련을 위한 입력 텍스트 준비에 필요한 핵심 단계를 살펴봅니다. 먼저, 텍스트를 개별 단어와 하위 단어 토큰으로 분해하는 방법을 배우고, 이를 LLM 입력에 적합한 벡터 표현으로 변환하는 과정을 학습합니다. 또한 GPT와 같은 주요 모델에서 사용되는 바이트 페어 인코딩(BPE)과 같은 고급 토큰화 기법을 깊이 있게 다룹니다. 이어서 슬라이딩 윈도우를 사용한 샘플링 전략을 구현하여 훈련에 필요한 입력-출력 쌍을 생성하는 방법을 다룹니다. 주요 주제로는 단어 임베딩의 이해, 텍스트 토큰화, 토큰을 토큰 ID로 변환, 특수 컨텍스트 토큰의 역할 등이 포함됩니다. 또한 바이트 페어 인코딩이 토큰화를 개선하는 방법과 LLM에서 토큰 임베딩 및 위치 인코딩이 생성되고 사용되는 방식을 살펴봅니다.\n",
    "\n",
    "## 데이터 준비 및 샘플링\n",
    "\n",
    "### 이 책의 이 장에서는 무엇을 다루나요?\n",
    "\n",
    "1. 단어 임베딩 이해하기:\n",
    "   - 다양한 형태의 임베딩이 있습니다\n",
    "   - LLM은 고차원 공간(즉, 수천 차원)에서 임베딩을 다룹니다\n",
    "   - 우리는 그런 고차원 공간을 시각화할 수 없기 때문에(인간은 1, 2, 3차원으로 생각합니다), 아래 그림은 2차원 임베딩 공간을 보여줍니다\n",
    "\n",
    "2. 텍스트 토큰화:\n",
    "   - 텍스트를 개별 단어와 문장부호 같은 더 작은 단위로 나누는 것을 의미합니다\n",
    "   - 간단한 샘플 텍스트를 기반으로 간단한 토크나이저를 개발하여 나중에 위의 텍스트에 적용할 수 있습니다\n",
    "\n",
    "3. 토큰을 토큰 ID로 변환:\n",
    "   - 텍스트 토큰을 나중에 임베딩 레이어로 처리할 수 있는 토큰 ID로 변환합니다\n",
    "   - 이 토큰들로부터 모든 고유 토큰으로 구성된 어휘를 만들 수 있습니다\n",
    "\n",
    "4. 특수 컨텍스트 토큰 추가:\n",
    "   - 알 수 없는 단어와 텍스트의 끝을 나타내는 \"특수\" 토큰\n",
    "\n",
    "5. 바이트페어 인코딩:\n",
    "   - GPT-2는 토크나이저로 바이트페어 인코딩(BPE)을 사용했습니다\n",
    "   - 이를 통해 모델은 미리 정의된 어휘에 없는 단어를 더 작은 하위 단어 단위나 개별 문자로 분해할 수 있어, 어휘 외 단어를 처리할 수 있습니다\n",
    "   - 예를 들어, GPT-2의 어휘에 \"unfamiliarword\"가 없다면, 훈련된 BPE 병합에 따라 [\"unfam\", \"iliar\", \"word\"] 등으로 토큰화할 수 있습니다\n",
    "   - 원래의 BPE 토크나이저는 여기에서 찾을 수 있습니다: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "\n",
    "> 이 프로젝트에서는 OpenAI의 오픈소스 tiktoken 라이브러리의 BPE 토크나이저를 사용합니다.\n",
    "\n",
    "6. 데이터 샘플링:\n",
    "   - LLM을 한 번에 한 단어씩 생성하도록 훈련시키므로, 시퀀스의 다음 단어가 예측할 대상을 나타내는 방식으로 훈련 데이터를 준비하고자 합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508507fa",
   "metadata": {
    "papermill": {
     "duration": 4.995273,
     "end_time": "2024-09-06T09:16:05.738383",
     "exception": false,
     "start_time": "2024-09-06T09:16:00.74311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the-verdict.txt를 로드하고 tiktoken의 인스턴스를 생성합니다\n",
    "raw_data = read_txt(DataConfig.dataPath)\n",
    "tokenizer = ttk.get_encoding(\"gpt2\")\n",
    "\n",
    "total_token = len(tokenizer.encode(raw_data))\n",
    "print(f\"-> 문자 수: {len(raw_data)}\\n-> 토큰 수: {total_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f826fca",
   "metadata": {
    "papermill": {
     "duration": 0.027206,
     "end_time": "2024-09-06T09:16:05.783225",
     "exception": false,
     "start_time": "2024-09-06T09:16:05.756019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 다음으로, 데이터를 훈련용과 검증용으로 분할합니다\n",
    "train_ratio = DataConfig.train_ratio\n",
    "split_idxs = int(train_ratio * len(raw_data))\n",
    "train_data = raw_data[:split_idxs]\n",
    "val_data = raw_data[split_idxs:]\n",
    "print(f\"-> 훈련 데이터 길이: {len(train_data)}\\n-> 검증 데이터 길이: {len(val_data)}\")\n",
    "\n",
    "\n",
    "# 정상 확인\n",
    "if total_token * (train_ratio) < GPTConfig.context_length:\n",
    "    print(\n",
    "        \"훈련 로더를 위한 토큰이 충분하지 않습니다. \"\n",
    "        \"`GPTConfig.context_length`를 낮추거나 \"\n",
    "        \"`training_ratio`를 높여보세요\"\n",
    "    )\n",
    "\n",
    "if total_token * (1 - train_ratio) < GPTConfig.context_length:\n",
    "    print(\n",
    "        \"검증 로더를 위한 토큰이 충분하지 않습니다. \"\n",
    "        \"`GPTConfig.context_length`를 낮추거나 \"\n",
    "        \"`training_ratio`를 낮춰보세요\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b70473",
   "metadata": {
    "papermill": {
     "duration": 0.103755,
     "end_time": "2024-09-06T09:16:05.904088",
     "exception": false,
     "start_time": "2024-09-06T09:16:05.800333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM의 입력으로 사용할 수 있도록 데이터를 처리합니다\n",
    "train_dataloader = LLM_DataLoader(\n",
    "    txt=train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=DataConfig.max_length,\n",
    "    batch_size=DataConfig.batch_size,\n",
    "    stride=DataConfig.stride,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"예시 보기:\")\n",
    "dataiter = iter(train_dataloader)\n",
    "firstbatch = next(dataiter)\n",
    "print(f\"입력: \\n{firstbatch[0]} \\n목표: \\n{firstbatch[1]}\")\n",
    "firstbatch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd845f23",
   "metadata": {
    "papermill": {
     "duration": 0.032614,
     "end_time": "2024-09-06T09:16:05.954289",
     "exception": false,
     "start_time": "2024-09-06T09:16:05.921675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataloader = LLM_DataLoader(\n",
    "    txt=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=DataConfig.max_length,\n",
    "    batch_size=DataConfig.batch_size,\n",
    "    stride=DataConfig.stride,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "dataiter = iter(val_dataloader)\n",
    "firstbatch = next(dataiter)\n",
    "firstbatch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001e181",
   "metadata": {
    "papermill": {
     "duration": 0.018061,
     "end_time": "2024-09-06T09:16:06.063081",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.04502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "이제 데이터가 훈련을 위해 준비되었습니다.\n",
    "\n",
    "## 어텐션 메커니즘\n",
    "\n",
    "**이 장에서 다루는 내용**\n",
    "\n",
    "- 신경망에서 어텐션 메커니즘을 사용하는 이유 탐구\n",
    "- 기본적인 셀프 어텐션 프레임워크 소개 및 향상된 셀프 어텐션 메커니즘으로의 발전\n",
    "- LLM이 한 번에 하나의 토큰을 생성할 수 있게 하는 인과적 어텐션 모듈 구현\n",
    "- 과적합을 줄이기 위해 무작위로 선택된 어텐션 가중치를 드롭아웃으로 마스킹\n",
    "- 여러 인과적 어텐션 모듈을 멀티헤드 어텐션 모듈로 쌓기\n",
    "\n",
    "### 어텐션에 대한 요약:\n",
    "어텐션 메커니즘은 현대 딥러닝, 특히 자연어 처리(NLP)와 컴퓨터 비전 분야에서 핵심적인 개념입니다. 이는 모델이 입력 데이터의 특정 부분에 동적으로 집중할 수 있게 해주며, 마치 인간이 복잡한 작업을 처리할 때 관련 정보에 주의를 기울이는 것과 유사합니다.\n",
    "\n",
    "Bahdanau 등이 기계 번역 맥락에서 소개한 어텐션 메커니즘은 모델이 다른 입력 요소의 중요성을 가중치로 부여할 수 있게 하여, 시퀀스의 장거리 의존성을 더 잘 처리할 수 있게 합니다. 이 메커니즘은 현재 작업과 관련하여 입력의 각 부분의 관련성을 결정하는 어텐션 가중치 집합을 계산하여, 모델이 가장 적절한 정보에 주목할 수 있게 합니다.\n",
    "\n",
    "어텐션은 트랜스포머 아키텍처의 기반이 되는 셀프 어텐션을 포함한 다양한 형태로 발전했습니다. 셀프 어텐션은 모델이 시퀀스의 모든 부분을 동시에 고려할 수 있게 하여, 입력의 먼 요소 간의 관계를 포착하는 능력을 향상시킵니다. 이는 BERT, GPT, T5와 같은 모델에서 볼 수 있듯이 NLP에서 상당한 진전을 이끌었습니다.\n",
    "\n",
    "컴퓨터 비전에서는 어텐션 메커니즘이 이미지 분류, 객체 탐지, 이미지 생성과 같은 작업의 성능을 향상시키기 위해 적용되었습니다. 여기서 어텐션은 모델이 이미지 내의 관련 공간 영역에 집중할 수 있게 하여 정확도와 해석 가능성을 개선합니다.\n",
    "\n",
    "전반적으로, 어텐션 메커니즘은 모델이 복잡한 데이터를 처리하고 이해하는 방식을 혁신하여 다양한 AI 작업에서 높은 성능을 이끌어냈습니다.\n",
    "\n",
    "어텐션 메커니즘에는 아래 4가지 유형이 있습니다.\n",
    "\n",
    "1. Simplified self-attention\n",
    "2. Self-attention\n",
    "3. Causal attention\n",
    "4. Multi-head attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b757f3c7",
   "metadata": {
    "papermill": {
     "duration": 0.018112,
     "end_time": "2024-09-06T09:16:06.099304",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.081192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 멀티헤드 어텐션:\n",
    "\n",
    "멀티헤드 어텐션은 트랜스포머 아키텍처의 핵심 구성 요소로, 모델이 입력의 여러 부분에 동시에 집중할 수 있게 함으로써 어텐션 메커니즘의 능력을 향상시키도록 설계되었습니다. Vaswani 등이 \"Attention is All You Need\" 논문에서 소개한 멀티헤드 어텐션은 여러 어텐션 메커니즘을 병렬로 적용하여 데이터의 다양한 관계를 포착할 수 있게 합니다.\n",
    "\n",
    "단일 어텐션 메커니즘에서는 입력 시퀀스가 쿼리(Q), 키(K), 값(V) 세 가지 벡터로 변환됩니다. 어텐션 점수는 쿼리와 키 벡터 간의 내적으로 계산되며, 이 점수는 값 벡터에 가중치를 부여하여 입력의 가장 관련성 높은 부분을 강조합니다.\n",
    "\n",
    "멀티헤드 어텐션은 이 아이디어를 확장하여 각각 다른 \"헤드\"에 해당하는 여러 세트의 쿼리, 키, 값 벡터를 생성합니다. 이 헤드들은 서로 독립적이며 입력의 다른 측면에 집중하도록 학습될 수 있습니다. 예를 들어, 한 헤드는 지역적 맥락에 집중하고 다른 헤드는 장거리 의존성을 포착할 수 있습니다. 각 헤드에 대한 어텐션 점수를 계산한 후, 결과를 연결하고 선형 변환하여 최종 출력을 생성합니다.\n",
    "\n",
    "이 접근 방식은 다음과 같은 여러 장점을 제공합니다:\n",
    "- 병렬성: 여러 어텐션 헤드를 사용함으로써 모델은 입력의 다른 부분을 동시에 처리할 수 있어 더 풍부한 특징 표현이 가능합니다.\n",
    "- 다양화: 각 헤드가 입력의 다른 부분에 주목하도록 학습될 수 있어, 모델이 데이터 내의 더 넓은 범위의 관계를 포착할 수 있습니다.\n",
    "- 일반화 향상: 멀티헤드 어텐션은 학습 과정을 여러 헤드에 분산시켜 각 헤드가 데이터의 다른 측면에 집중하게 함으로써 과적합의 위험을 줄입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7178f",
   "metadata": {
    "papermill": {
     "duration": 0.036775,
     "end_time": "2024-09-06T09:16:06.154495",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.11772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    멀티헤드 어텐션 모듈.\n",
    "    인자:\n",
    "        d_in (int): 입력 차원.\n",
    "        d_out (int): 출력 차원.\n",
    "        context_length (int): 입력 시퀀스의 길이.\n",
    "        dropout (float): 드롭아웃 확률.\n",
    "        num_heads (int): 어텐션 헤드의 수.\n",
    "        qkv_bias (bool, 선택적): 쿼리, 키, 값 투영에 편향을 포함할지 여부. 기본값은 False.\n",
    "    속성:\n",
    "        d_out (int): 출력 차원.\n",
    "        num_heads (int): 어텐션 헤드의 수.\n",
    "        head_dim (int): 각 어텐션 헤드의 차원.\n",
    "        w_queries (nn.Linear): 쿼리를 위한 선형 투영.\n",
    "        w_keys (nn.Linear): 키를 위한 선형 투영.\n",
    "        w_values (nn.Linear): 값을 위한 선형 투영.\n",
    "        out_proj (nn.Linear): 출력을 위한 선형 투영.\n",
    "        dropout (nn.Dropout): 드롭아웃 레이어.\n",
    "        mask (torch.Tensor): 인과성을 보장하기 위한 하삼각 마스크.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out은 num_heads로 나누어 떨어져야 합니다\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.w_queries = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_keys = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_values = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(context_length, context_length)).unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batches, num_tokens, dim_in = x.shape\n",
    "\n",
    "        # 선형 투영\n",
    "        queries = self.w_queries(x)\n",
    "        keys = self.w_keys(x)\n",
    "        values = self.w_values(x)\n",
    "\n",
    "        # 멀티헤드 어텐션을 위한 reshape 및 transpose\n",
    "        queries = queries.view(batches, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(batches, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(batches, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 어텐션 점수 계산\n",
    "        attn_scores = (queries @ keys.transpose(2, 3)) / (self.head_dim**0.5)\n",
    "\n",
    "        # 마스크 적용: 배치와 헤드에 걸쳐 브로드캐스팅\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask[:, :, :num_tokens, :num_tokens] == 0, float(\"-inf\")\n",
    "        )\n",
    "\n",
    "        # 소프트맥스로 어텐션 가중치 얻기\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 컨텍스트 벡터 계산\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(batches, num_tokens, self.d_out)\n",
    "\n",
    "        # 최종 선형 투영\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2c431",
   "metadata": {
    "papermill": {
     "duration": 0.018039,
     "end_time": "2024-09-06T09:16:06.227432",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.209393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LLM 구축\n",
    "\n",
    "- **GPT2의 아키텍처:**\n",
    "\n",
    "```python\n",
    "124백만 파라미터 GPT-2 모델의 구성 세부사항:\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # 어휘 크기\n",
    "    \"context_length\": 1024, # 컨텍스트 길이\n",
    "    \"emb_dim\": 768,         # 임베딩 차원\n",
    "    \"n_heads\": 12,          # 어텐션 헤드 수\n",
    "    \"n_layers\": 12,         # 레이어 수\n",
    "    \"drop_rate\": 0.1,       # 드롭아웃 비율\n",
    "    \"qkv_bias\": False       # 쿼리-키-값 편향\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**그림에 표시된 번호가 매겨진 상자들은 최종 GPT 아키텍처를 코딩하는 데 필요한 개별 개념들을 다루는 순서를 보여줍니다**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[GPT Backbone] --> B[Layer Normalization]\n",
    "    B --> C[GELU Activation]\n",
    "    C --> D[Feed Forward Network]\n",
    "    D --> E[Shortcut Connections]\n",
    "    \n",
    "    subgraph \"Transformer Block\"\n",
    "        B\n",
    "        C\n",
    "        D\n",
    "        E\n",
    "    end\n",
    "    \n",
    "    E --> G[Final GPT Architecture]\n",
    "```\n",
    "\n",
    "### 레이어 정규화\n",
    "\n",
    "많은 레이어를 가진 깊은 신경망을 훈련시키는 것은 때때로 그래디언트 소실이나 폭발과 같은 문제로 인해 어려울 수 있습니다. 이러한 문제들은 불안정한 훈련 동역학을 야기하고 네트워크가 가중치를 효과적으로 조정하기 어렵게 만듭니다. 이는 학습 과정이 손실 함수를 최소화하는 신경망의 파라미터(가중치) 세트를 찾는 데 어려움을 겪는다는 것을 의미합니다. 다시 말해, 네트워크가 정확한 예측이나 결정을 내릴 수 있을 정도로 데이터의 기본 패턴을 학습하는 데 어려움을 겪습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab1ce3",
   "metadata": {
    "papermill": {
     "duration": 0.0285,
     "end_time": "2024-09-06T09:16:06.346611",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.318111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    레이어 정규화 모듈.\n",
    "    인자:\n",
    "        emb_dim (int): 입력 임베딩의 차원.\n",
    "    속성:\n",
    "        eps (float): 0으로 나누는 것을 방지하기 위한 작은 값.\n",
    "        scale (nn.Parameter): 학습 가능한 스케일 파라미터.\n",
    "        shift (nn.Parameter): 학습 가능한 이동 파라미터.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a1c3f",
   "metadata": {
    "papermill": {
     "duration": 0.018281,
     "end_time": "2024-09-06T09:16:06.383357",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.365076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### GELU 활성화 함수\n",
    "\n",
    "* 딥러닝에서는 간단하면서도 효과적인 ReLU(Rectified Linear Unit) 활성화 함수가 다양한 신경망 구조에서 널리 사용됩니다.\n",
    "\n",
    "* 대규모 언어 모델(LLM)에서는 전통적인 ReLU 외에도 다양한 종류의 활성화 함수가 사용됩니다. 그 중 주목할 만한 두 가지 예시로 GELU(Gaussian Error Linear Unit)와 SwiGLU(Swish-Gated Linear Unit)가 있습니다.\n",
    "\n",
    "* GELU와 SwiGLU는 ReLU의 단순한 구분적 선형 함수와는 달리, 더 복잡하고 부드러운 활성화 함수입니다. GELU는 가우시안 분포를, SwiGLU는 시그모이드 게이트가 있는 선형 유닛을 각각 활용하여 딥러닝 모델에서 더 나은 성능을 제공합니다.\n",
    "\n",
    "* GELU(Hendrycks와 Gimpel, 2016)는 여러 가지 방식으로 구현할 수 있습니다. 정확한 버전은 GELU(x)=x⋅Φ(x)로 정의되며, 여기서 Φ(x)는 표준 가우시안 분포의 누적 분포 함수입니다.\n",
    "\n",
    "* 실제로는 계산 비용이 적은 아래와 같은 근사식을 사용하는 것이 일반적입니다:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)\\right]\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c003c",
   "metadata": {
    "papermill": {
     "duration": 0.02719,
     "end_time": "2024-09-06T09:16:06.428692",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.401502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    가우시안 오차 선형 유닛(GELU) 활성화 함수.\n",
    "    이 구현은 다음의 근사식을 따릅니다:\n",
    "    GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x^3)))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1\n",
    "                + torch.tanh(\n",
    "                    torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8cbf32",
   "metadata": {
    "papermill": {
     "duration": 0.017784,
     "end_time": "2024-09-06T09:16:06.464727",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.446943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 순방향 신경망\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d73b7",
   "metadata": {
    "papermill": {
     "duration": 0.02701,
     "end_time": "2024-09-06T09:16:06.509575",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.482565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    GELU 활성화 함수를 사용한 순방향 신경망 층입니다.\n",
    "    인자:\n",
    "        cfg (dict): 'emb_dim'을 키로 가지는 설정 딕셔너리. 'emb_dim'은 임베딩 차원을 나타냅니다.\n",
    "    네트워크 구조:\n",
    "    - 입력을 'emb_dim'에서 4 * 'emb_dim'으로 투영하는 선형 층\n",
    "    - GELU 활성화 함수\n",
    "    - 4 * 'emb_dim'에서 다시 'emb_dim'으로 투영하는 선형 층\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        emb_dim = cfg.emb_dim\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597d1fa",
   "metadata": {
    "papermill": {
     "duration": 0.017715,
     "end_time": "2024-09-06T09:16:06.545394",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.527679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 지름길 연결 추가하기\n",
    "\n",
    "* 이제 지름길 연결(shortcut connections)에 대해 알아볼까요? 이는 건너뛰기 연결(skip connections) 또는 잔차 연결(residual connections)이라고도 불립니다.\n",
    "\n",
    "* 원래 지름길 연결은 컴퓨터 비전을 위한 깊은 신경망(잔차 네트워크)에서 제안되었습니다. 이는 기울기 소실 문제를 완화하기 위해서였죠.\n",
    "\n",
    "* 지름길 연결은 기울기가 네트워크를 통해 흐를 수 있는 더 짧은 대체 경로를 만듭니다.\n",
    "\n",
    "* 이는 한 층의 출력을 나중 층의 출력에 더하는 방식으로 구현됩니다. 보통 중간에 하나 이상의 층을 건너뛰게 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3073b",
   "metadata": {
    "papermill": {
     "duration": 0.017601,
     "end_time": "2024-09-06T09:16:06.580792",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.563191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 트랜스포머 블록\n",
    "\n",
    "이제 트랜스포머 블록을 구현해 볼 차례입니다. 이 블록은 GPT와 다른 대규모 언어 모델(LLM) 구조의 핵심 구성 요소입니다. 1억 2400만 개의 매개변수를 가진 GPT-2 구조에서는 이 블록이 열두 번 반복되는데요, 우리가 앞서 다룬 여러 개념들을 한데 모아놓은 것입니다. 여기에는 다중 헤드 어텐션, 층 정규화, 드롭아웃, 순방향 층, 그리고 GELU 활성화 함수가 포함됩니다. 이 트랜스포머 블록을 구현한 후에는 GPT 구조의 나머지 부분들과 연결할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f876f1",
   "metadata": {
    "papermill": {
     "duration": 0.028078,
     "end_time": "2024-09-06T09:16:06.626656",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.598578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 트랜스포머 구조 구축\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg.emb_dim,\n",
    "            d_out=cfg.emb_dim,\n",
    "            context_length=cfg.context_length,\n",
    "            num_heads=cfg.n_heads,\n",
    "            dropout=cfg.drop_rate,\n",
    "            qkv_bias=cfg.qkv_bias,\n",
    "        )\n",
    "        self.ff = FeedForwardGELU(cfg)\n",
    "        self.norm1 = LayerNorm(cfg.emb_dim)\n",
    "        self.norm2 = LayerNorm(cfg.emb_dim)\n",
    "        self.dropout = nn.Dropout(cfg.drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 첫 번째 잔차 연결\n",
    "        resid_conn = x\n",
    "        x = self.norm1(x)  # 사전 층 정규화\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + resid_conn\n",
    "\n",
    "        # 두 번째 잔차 연결\n",
    "        resid_conn = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + resid_conn\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea696fc",
   "metadata": {
    "papermill": {
     "duration": 0.017819,
     "end_time": "2024-09-06T09:16:06.697467",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.679648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 최종 GPT 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db048a2b",
   "metadata": {
    "papermill": {
     "duration": 0.029246,
     "end_time": "2024-09-06T09:16:06.744883",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.715637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 설정에 따라 GPT 아키텍처를 구축\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(cfg.context_length, cfg.emb_dim)\n",
    "        self.dropout_emb = nn.Dropout(cfg.drop_rate)\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg.emb_dim)\n",
    "        self.out_ff = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        batch_size, seq_len = idx.shape\n",
    "        tok_embeds = self.tok_emb(idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.dropout_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_ff(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453329f",
   "metadata": {
    "papermill": {
     "duration": 0.017782,
     "end_time": "2024-09-06T09:16:06.780614",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.762832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2단계\n",
    "\n",
    "학습 함수를 작성하기 전에 알아야 할 것들:\n",
    "\n",
    "1. GPT를 사용하여 텍스트를 생성하는 방법\n",
    "2. 학습 세트와 검증 세트의 손실을 계산하는 방법\n",
    "\n",
    "### 생성 함수\n",
    "\n",
    "- 더 독창적인 텍스트를 생성하기 위한 텍스트 생성 전략(디코딩 전략이라고도 함)에 대해 다룰 예정입니다.\n",
    "- 그 다음, 이 함수(탐욕적 방식)를 개선하기 위한 두 가지 기법인 *온도 스케일링*과 *상위-k 샘플링*에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051fb16",
   "metadata": {
    "papermill": {
     "duration": 0.034911,
     "end_time": "2024-09-06T09:16:06.86899",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.834079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx는 현재 컨텍스트의 인덱스를 나타내는 (B, T) 배열입니다\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 지원되는 컨텍스트 크기를 초과하면 현재 컨텍스트를 자릅니다\n",
    "        # 예: LLM이 5개의 토큰만 지원하고 컨텍스트 크기가 10이면\n",
    "        # 마지막 5개의 토큰만 컨텍스트로 사용됩니다\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # 예측 수행\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # 마지막 시간 단계에만 집중\n",
    "        # (batch, n_token, vocab_size)가 (batch, vocab_size)로 변환됩니다\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 가장 높은 logits 값을 가진 어휘 항목의 idx를 얻습니다\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # 샘플링된 인덱스를 실행 중인 시퀀스에 추가합니다\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    # 이전과 동일한 for 루프: logits를 얻고 마지막 시간 단계에만 집중합니다\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 새로운 기능: top_k 샘플링으로 logits 필터링\n",
    "        if top_k is not None:\n",
    "            # 상위 k개의 값만 유지\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits\n",
    "            )\n",
    "\n",
    "        # 새로운 기능: 온도 스케일링 적용\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # 소프트맥스를 적용하여 확률 얻기\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # 분포에서 샘플링\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # 그렇지 않으면 이전과 동일: 가장 높은 logits 값을 가진 어휘 항목의 idx를 얻습니다\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if (\n",
    "            eos_id is not None and idx_next == eos_id\n",
    "        ):  # eos_id가 지정되어 있고 시퀀스 종료 토큰이 발견되면 생성을 조기 종료합니다\n",
    "            break\n",
    "\n",
    "        # 이전과 동일: 샘플링된 인덱스를 실행 중인 시퀀스에 추가합니다\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context, temperature, top_k, eos_id):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            eos_id=eos_id,\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # 간결한 출력 형식\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59fb09",
   "metadata": {
    "papermill": {
     "duration": 0.017614,
     "end_time": "2024-09-06T09:16:06.90432",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.886706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 학습 세트와 검증 세트의 손실 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60ad08",
   "metadata": {
    "papermill": {
     "duration": 0.028038,
     "end_time": "2024-09-06T09:16:06.950275",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.922237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    # 입력과 타겟 배치를 지정된 장치로 이동\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    # 모델을 통해 로짓 계산\n",
    "    logits = model(input_batch)\n",
    "    # 크로스 엔트로피 손실 계산\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # num_batches가 데이터 로더의 배치 수를 초과하는 경우\n",
    "        # 데이터 로더의 총 배치 수에 맞춰 num_batches를 줄임\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05d5c1",
   "metadata": {
    "papermill": {
     "duration": 0.017586,
     "end_time": "2024-09-06T09:16:06.985656",
     "exception": false,
     "start_time": "2024-09-06T09:16:06.96807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 학습과 평가 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48595f65",
   "metadata": {
    "papermill": {
     "duration": 0.033926,
     "end_time": "2024-09-06T09:16:07.037329",
     "exception": false,
     "start_time": "2024-09-06T09:16:07.003403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    eval_freq,\n",
    "    eval_iter,\n",
    "    start_context,\n",
    "    tokenizer,\n",
    "    temperature,\n",
    "    top_k,\n",
    "    eos_id,\n",
    "):\n",
    "    # 손실과 처리된 토큰 수를 추적하기 위한 리스트 초기화\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 주요 학습 루프\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 모델을 학습 모드로 설정\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # 이전 배치 반복에서의 손실 그래디언트 초기화\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # 손실 그래디언트 계산\n",
    "            optimizer.step()  # 손실 그래디언트를 사용하여 모델 가중치 업데이트\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # 선택적 평가 단계\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"에폭 {epoch + 1} (스텝 {global_step:03d}): \"\n",
    "                    f\"학습 손실 {train_loss:.3f}, 검증 손실 {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # 각 에폭 후 샘플 텍스트 출력\n",
    "        print(\"예시: \")\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context, temperature, top_k, eos_id\n",
    "        )\n",
    "        print(\"-*-\" * 10)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3b370",
   "metadata": {
    "papermill": {
     "duration": 0.01767,
     "end_time": "2024-09-06T09:16:07.072805",
     "exception": false,
     "start_time": "2024-09-06T09:16:07.055135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16365976",
   "metadata": {
    "papermill": {
     "duration": 75.72999,
     "end_time": "2024-09-06T09:17:22.820601",
     "exception": false,
     "start_time": "2024-09-06T09:16:07.090611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # Xavier 균등 분포를 사용하여 가중치 초기화\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            # 편향을 0.01로 초기화\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "# GPT 모델 초기화\n",
    "model = GPTModel(GPTConfig)\n",
    "model.to(device)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# AdamW 옵티마이저 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00009, weight_decay=0.1)\n",
    "\n",
    "# 그래디언트 클리핑 적용\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# 학습 설정\n",
    "num_epochs = 50\n",
    "\n",
    "# 모델 학습 실행\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves\",\n",
    "    tokenizer=tokenizer,\n",
    "    top_k=10,\n",
    "    temperature=0.4,\n",
    "    eos_id=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae33265",
   "metadata": {
    "papermill": {
     "duration": 1.660388,
     "end_time": "2024-09-06T09:17:24.50727",
     "exception": false,
     "start_time": "2024-09-06T09:17:22.846882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ep 46 (Step 045): Train loss 5.668, Val loss 6.691 ->4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # 에폭에 대한 학습 및 검증 손실 그래프 그리기\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # x축에 정수 레이블만 표시\n",
    "    # 처리된 토큰 수에 대한 두 번째 x축 생성\n",
    "    ax2 = ax1.twiny()  # 같은 y축을 공유하는 두 번째 x축 생성\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # 눈금 정렬을 위한 보이지 않는 그래프\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()  # 레이아웃 조정\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9692a2",
   "metadata": {
    "papermill": {
     "duration": 3.45237,
     "end_time": "2024-09-06T09:17:27.986921",
     "exception": false,
     "start_time": "2024-09-06T09:17:24.534551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"quite insensible to the irony\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPTConfig.context_length,\n",
    "    top_k=5,\n",
    "    temperature=0.7,\n",
    "    eos_id=None,\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13164812",
   "metadata": {
    "papermill": {
     "duration": 0.026387,
     "end_time": "2024-09-06T09:17:28.040189",
     "exception": false,
     "start_time": "2024-09-06T09:17:28.013802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "🚩 학습 손실과 검증 손실 모두 첫 번째 에폭에서 개선되기 시작합니다. 하지만 두 번째 에폭을 지나면서 두 손실 값이 갈라지기 시작하죠. 이렇게 두 손실이 벌어지고, 검증 손실이 학습 손실보다 훨씬 크다는 사실은 모델이 학습 데이터에 과적합되고 있음을 나타냅니다. 우리는 생성된 텍스트 조각들을 검색해봄으로써 모델이 학습 데이터를 그대로 암기하고 있다는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24436d0",
   "metadata": {
    "papermill": {
     "duration": 0.025999,
     "end_time": "2024-09-06T09:17:28.145769",
     "exception": false,
     "start_time": "2024-09-06T09:17:28.11977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PyTorch에서 모델 가중치 저장하고 불러오기\n",
    "\n",
    "* 대규모 언어 모델(LLM)을 학습하는 데는 많은 계산 비용이 들기 때문에, LLM의 가중치를 저장하고 불러올 수 있는 능력이 매우 중요합니다.\n",
    "* PyTorch에서 권장하는 방법은 모델 가중치, 즉 'state_dict'라고 불리는 것을 저장하는 것입니다. 이는 .state_dict() 메서드에 torch.save 함수를 적용하여 수행합니다.\n",
    "* 적응형 옵티마이저들은 각 모델 가중치에 대한 추가 매개변수를 저장합니다. 따라서 나중에 사전 학습을 계속하려는 경우를 대비해 이들도 함께 저장하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a31a1",
   "metadata": {
    "papermill": {
     "duration": 2.82473,
     "end_time": "2024-09-06T09:17:30.996954",
     "exception": false,
     "start_time": "2024-09-06T09:17:28.172224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 가중치만 저장:\n",
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "# # 그런 다음 새로운 GPTModel 모델 인스턴스에 모델 가중치를 다음과 같이 불러올 수 있습니다:\n",
    "# model = GPTModel(GPT_CONFIG_124M)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "# model.eval();\n",
    "\n",
    "# 모델 가중치와 옵티마이저 저장하기\n",
    "print(\"모델과 옵티마이저 저장 중...\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\",\n",
    ")\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b474be3",
   "metadata": {
    "papermill": {
     "duration": 0.038594,
     "end_time": "2024-09-06T09:17:31.067264",
     "exception": false,
     "start_time": "2024-09-06T09:17:31.02867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 삭제\n",
    "del model\n",
    "\n",
    "print(\"그런 다음 메모리에서 모델을 삭제하여 로드한 모델을 사용하도록 합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39231cf2",
   "metadata": {
    "papermill": {
     "duration": 3.170716,
     "end_time": "2024-09-06T09:17:34.264945",
     "exception": false,
     "start_time": "2024-09-06T09:17:31.094229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델과 옵티마이저 불러오기\n",
    "print(\"불러오는 중...\")\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPTConfig)\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f863bc",
   "metadata": {
    "papermill": {
     "duration": 3.133285,
     "end_time": "2024-09-06T09:17:37.425552",
     "exception": false,
     "start_time": "2024-09-06T09:17:34.292267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 이제 다시 학습하거나 사용할 수 있습니다\n",
    "\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves \",\n",
    "    tokenizer=tokenizer,\n",
    "    top_k=10,\n",
    "    temperature=2.7,\n",
    "    eos_id=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49da928",
   "metadata": {
    "papermill": {
     "duration": 0.02961,
     "end_time": "2024-09-06T09:17:37.487301",
     "exception": false,
     "start_time": "2024-09-06T09:17:37.457691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## OpenAI의 사전 학습된 가중치 불러오기\n",
    "\n",
    "- 지금까지 우리는 교육 목적으로 아주 작은 단편 소설 책만을 사용해 소규모 GPT-2 모델을 학습해 봤습니다.\n",
    "- 다행히도 우리가 대규모 사전 학습 데이터셋으로 모델을 직접 학습하기 위해 수만에서 수십만 달러를 쓸 필요는 없습니다. 대신 OpenAI에서 제공하는 이미 학습된 가중치를 가져와 사용할 수 있습니다.\n",
    "- OpenAI의 GPT 모델을 불러오는 방법에는 두 가지가 있습니다:\n",
    "    * TensorFlow를 이용하는 방법\n",
    "    * HuggingFace의 Transformers 라이브러리를 사용하는 방법\n",
    "\n",
    "### Transformers를 이용해 OpenAI의 사전 학습된 가중치 불러오기\n",
    "\n",
    "이 부분에서는 HuggingFace의 Transformers 라이브러리를 사용하여 OpenAI의 사전 학습된 GPT 모델을 불러오는 방법에 대해 설명합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472713e5",
   "metadata": {
    "papermill": {
     "duration": 4.81137,
     "end_time": "2024-09-06T09:17:42.386303",
     "exception": false,
     "start_time": "2024-09-06T09:17:37.574933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "gpt2_small = \"openai-community/gpt2\"\n",
    "\n",
    "gpt_hf = GPT2Model.from_pretrained(gpt2_small, cache_dir=\"checkpoints\")\n",
    "gpt_hf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b61caf",
   "metadata": {
    "papermill": {
     "duration": 0.037933,
     "end_time": "2024-09-06T09:17:42.452699",
     "exception": false,
     "start_time": "2024-09-06T09:17:42.414766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "copyConfig = replace(GPTConfig)\n",
    "copyConfig.qkv_bias = True\n",
    "copyConfig.context_length = 1024\n",
    "copyConfig.drop_rate = 0.0\n",
    "copyConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b2dcf-6f71-4eef-8474-f41c8ca0055f",
   "metadata": {
    "papermill": {
     "duration": 0.047456,
     "end_time": "2024-09-06T09:17:42.598893",
     "exception": false,
     "start_time": "2024-09-06T09:17:42.551437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_check(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(right.clone().detach())\n",
    "\n",
    "\n",
    "def load_weights(gpt, gpt_hf):\n",
    "    d = gpt_hf.state_dict()\n",
    "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
    "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
    "\n",
    "    for b in range(copyConfig.n_layers):\n",
    "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
    "        gpt.transformer_blocks[b].att.w_queries.weight = assign_check(\n",
    "            gpt.transformer_blocks[b].att.w_queries.weight, q_w.T\n",
    "        )\n",
    "        gpt.transformer_blocks[b].att.w_keys.weight = assign_check(\n",
    "            gpt.transformer_blocks[b].att.w_keys.weight, k_w.T\n",
    "        )\n",
    "        gpt.transformer_blocks[b].att.w_values.weight = assign_check(\n",
    "            gpt.transformer_blocks[b].att.w_values.weight, v_w.T\n",
    "        )\n",
    "\n",
    "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
    "        gpt.transformer_blocks[b].att.w_queries.bias = assign_check(\n",
    "            gpt.transformer_blocks[b].att.w_queries.bias, q_b\n",
    "        )\n",
    "        gpt.transformer_blocks[b].att.w_keys.bias = assign_check(\n",
    "            gpt.transformer_blocks[b].att.w_keys.bias, k_b\n",
    "        )\n",
    "        gpt.transformer_blocks[b].att.w_values.bias = assign_check(\n",
    "            gpt.transformer_blocks[b].att.w_values.bias, v_b\n",
    "        )\n",
    "\n",
    "        gpt.transformer_blocks[b].att.out_proj.weight = assign_check(\n",
    "            gpt.transformer_blocks[b].att.out_proj.weight,\n",
    "            d[f\"h.{b}.attn.c_proj.weight\"].T,\n",
    "        )\n",
    "        gpt.transformer_blocks[b].att.out_proj.bias = assign_check(\n",
    "            gpt.transformer_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"]\n",
    "        )\n",
    "\n",
    "        gpt.transformer_blocks[b].ff.layers[0].weight = assign_check(\n",
    "            gpt.transformer_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T\n",
    "        )\n",
    "        gpt.transformer_blocks[b].ff.layers[0].bias = assign_check(\n",
    "            gpt.transformer_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"]\n",
    "        )\n",
    "        gpt.transformer_blocks[b].ff.layers[2].weight = assign_check(\n",
    "            gpt.transformer_blocks[b].ff.layers[2].weight,\n",
    "            d[f\"h.{b}.mlp.c_proj.weight\"].T,\n",
    "        )\n",
    "        gpt.transformer_blocks[b].ff.layers[2].bias = assign_check(\n",
    "            gpt.transformer_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"]\n",
    "        )\n",
    "\n",
    "        gpt.transformer_blocks[b].norm1.scale = assign_check(\n",
    "            gpt.transformer_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"]\n",
    "        )\n",
    "        gpt.transformer_blocks[b].norm1.shift = assign_check(\n",
    "            gpt.transformer_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"]\n",
    "        )\n",
    "        gpt.transformer_blocks[b].norm2.scale = assign_check(\n",
    "            gpt.transformer_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"]\n",
    "        )\n",
    "        gpt.transformer_blocks[b].norm2.shift = assign_check(\n",
    "            gpt.transformer_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"]\n",
    "        )\n",
    "\n",
    "        gpt.final_norm.scale = assign_check(gpt.final_norm.scale, d[\"ln_f.weight\"])\n",
    "        gpt.final_norm.shift = assign_check(gpt.final_norm.shift, d[\"ln_f.bias\"])\n",
    "        gpt.out_ff.weight = assign_check(gpt.out_ff.weight, d[\"wte.weight\"])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gpt = GPTModel(copyConfig)\n",
    "load_weights(gpt, gpt_hf)\n",
    "\n",
    "# test\n",
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=copyConfig.context_length,\n",
    "    top_k=1,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "# del gpt\n",
    "# print(\"delete gpt model loaded from huggingface.\")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afa591",
   "metadata": {
    "papermill": {
     "duration": 14.46985,
     "end_time": "2024-09-06T09:19:26.453966",
     "exception": false,
     "start_time": "2024-09-06T09:19:11.984116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    gpt,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves \",\n",
    "    tokenizer=tokenizer,\n",
    "    top_k=5,\n",
    "    temperature=0.7,\n",
    "    eos_id=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f9f49",
   "metadata": {
    "papermill": {
     "duration": 1.168987,
     "end_time": "2024-09-06T09:19:27.684217",
     "exception": false,
     "start_time": "2024-09-06T09:19:26.51523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996c431",
   "metadata": {
    "papermill": {
     "duration": 0.512468,
     "end_time": "2024-09-06T09:19:28.26057",
     "exception": false,
     "start_time": "2024-09-06T09:19:27.748102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(\"Every effort moves \", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=copyConfig.context_length,\n",
    "    top_k=1,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397b35d-42e8-40e0-996d-e3564271cdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cuda12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 232.844049,
   "end_time": "2024-09-06T09:19:31.87945",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-06T09:15:39.035401",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1288cda84b944d92ba9981f49da8b1d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1359f883f2bd4c6f886b85fdfb500323": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e50dfa1156c4ef4851479b0e784ffad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2925ea64aef54875bdfa1c3f07d6df0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8b6c2483ceb40af97a5d381fa933ab8",
       "placeholder": "​",
       "style": "IPY_MODEL_b5476961a6f04e4b8806d67e102587d2",
       "value": "model.safetensors: 100%"
      }
     },
     "2b3bbba10b3b437bb0cdef2cfd54ded7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3959ef94679145779e112115b6b44811": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "399793a50df44de893f21cf959f84eef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "490301ff8a544de1b13e3030adc8c2b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ba09336280d6411aa68033233eed3311",
        "IPY_MODEL_bc3b94b7ece14d8eb4a559bb275ab7b4",
        "IPY_MODEL_c9330950c9b24c9bbea544494c5e5cd8"
       ],
       "layout": "IPY_MODEL_4ba1ac0f15e942f29ce3e9541db1c938"
      }
     },
     "4ba1ac0f15e942f29ce3e9541db1c938": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d8bd60656f94c85adf3fa9a53d519cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "527fa0f3e1d34f768c2419420cc40cac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_399793a50df44de893f21cf959f84eef",
       "placeholder": "​",
       "style": "IPY_MODEL_dca8669445e540b3aac799fa7da3d911",
       "value": " 548M/548M [00:02&lt;00:00, 290MB/s]"
      }
     },
     "587ce8ba18084b4e807caf4bf7296871": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be61271995204a58832578c6e2288215",
       "max": 548105171,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2b3bbba10b3b437bb0cdef2cfd54ded7",
       "value": 548105171
      }
     },
     "a3ca62092dda4f99a9be41e94e2927aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a8b6c2483ceb40af97a5d381fa933ab8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5476961a6f04e4b8806d67e102587d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ba09336280d6411aa68033233eed3311": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e65918a49ee64233a2e9b1caa6427cc8",
       "placeholder": "​",
       "style": "IPY_MODEL_1e50dfa1156c4ef4851479b0e784ffad",
       "value": "config.json: 100%"
      }
     },
     "bc3b94b7ece14d8eb4a559bb275ab7b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1288cda84b944d92ba9981f49da8b1d5",
       "max": 665,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a3ca62092dda4f99a9be41e94e2927aa",
       "value": 665
      }
     },
     "be61271995204a58832578c6e2288215": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c9330950c9b24c9bbea544494c5e5cd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1359f883f2bd4c6f886b85fdfb500323",
       "placeholder": "​",
       "style": "IPY_MODEL_3959ef94679145779e112115b6b44811",
       "value": " 665/665 [00:00&lt;00:00, 54.9kB/s]"
      }
     },
     "dca8669445e540b3aac799fa7da3d911": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e358ef14e89248b9ba237118e8ef8838": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2925ea64aef54875bdfa1c3f07d6df0e",
        "IPY_MODEL_587ce8ba18084b4e807caf4bf7296871",
        "IPY_MODEL_527fa0f3e1d34f768c2419420cc40cac"
       ],
       "layout": "IPY_MODEL_4d8bd60656f94c85adf3fa9a53d519cf"
      }
     },
     "e65918a49ee64233a2e9b1caa6427cc8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
