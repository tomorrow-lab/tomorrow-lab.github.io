---
title: "생물학자를 위한 딥러닝 실전 팁"
author: "Taeyoon Kim"
categories: [Python, Deep learning, Tip, Biology]
draft: false
date: "2025-12-25"
date-modified: last-modified
---

![](guide_deeplearning_biology.png){width=50% fig-align="center"}

머신러닝 프로젝트를 수행할 때 첫 번째 시도나 심지어 열 번째 시도에서 모든 것이 완벽하게 작동하는 경우는 드뭅니다. 디버깅은 실패를 의미하는 신호가 아니라 개발 프로세스의 당연한 과정이므로 결코 낙담하지 마세요.

딥러닝 프로젝트에서 성과가 꾸준히 그리고 점진적으로 개선되기를 기대해서는 안 됩니다. 특히 생물학 데이터를 다루는 딥러닝 프로젝트는 진행 과정이 매우 비선형적일 때가 많습니다. 뚜렷한 진전 없이 몇 주 동안 디버깅에만 매달리다가도, 단 한 번의 작은 변화가 모든 문제를 한순간에 해결하기도 합니다. 이는 지극히 정상적인 현상이니 걱정할 필요가 없습니다.

> 이 글은 책 "Deep Learning for Biology: Harness AI to Solve Real-World Biology Problems"의 일부분을 번역하고 요약해 정리한 글입니다. 더 알고 싶으시다면 저자의 책을 한번 읽어보세요.

여기에 생물학 딥러닝의 여러 난관을 헤쳐 나가는 데 도움을 주었던 팁들을 이곳에 공유합니다. 뼈아픈 실수를 통해 배운 노하우도 있고, 책[^1]을 집필하며 새롭게 깨달은 통찰도 담았습니다. 이 목록이 모든 정답을 담고 있지는 않지만, 여러분이 작동하는 모델을 개발하는 시간을 단축해 줄 것입니다.

# 문제 단순화하기(Simplify)

상황이 이해되지 않을 때는 문제를 단순화해야 합니다. 데이터셋을 줄이거나, 모델을 얕게 만들거나, 손실 함수를 단순화하여 문제를 최소한으로 깎아내세요. 복잡한 파이프라인 속에서는 길을 잃기 쉽지만, 요소를 하나씩 격리하면 디버깅이 훨씬 수월해집니다.

## 모델 단순화하기

모델에 화려한 기능이 너무 많으면 어디가 잘못되었는지 정확히 짚어내기 어렵습니다. 가장 효과적인 디버깅 전략은 모델을 핵심 요소만 남기고 깎아내는 것입니다.

### 아키텍처 단순화

복잡한 아키텍처는 무엇이 문제인지 추론하기 어렵게 만듭니다. 이를 위해 다음 조치를 고려하세요.

* 불필요한 레이어 제거: 입력과 출력의 매핑에 직접 기여하지 않는 레이어(차원을 변경하지 않고 모델 용량만 늘리는 레이어 등)는 디버깅 중에 제거하는 것이 좋습니다.
* 기본 레이어 직접 호출: 커스텀 블록 대신 `nn.Conv`나 `nn.Dense` 같은 기본 레이어를 사용하세요. 커스텀 블록은 버그와 내부 동작을 가릴 수 있습니다.
* 깊이와 너비 축소: 레이어 수나 유닛 수를 줄이세요. 얕은 모델은 개발 초기 단계에서 이해하고 디버깅하기가 훨씬 쉽습니다.
* 잔차 연결(Residual connections) 제거: 잔차 연결은 레이어 간의 의존성을 만들어 디버깅을 복잡하게 하고, 연결된 레이어의 문제(잘못된 초기화나 기울기 문제 등)를 감추기도 합니다.

### 정규화 및 드롭아웃 같은 부가 기능 끄기

이러한 기능들은 초기 디버깅 단계에서 불필요한 복잡성을 더합니다. Flax에서는 모델 상태(예: 배치 정규화 통계)와 난수 생성기(RNG)를 명시적으로 관리해야 하므로 미묘한 버그가 발생하기 쉽습니다.

* 배치 정규화(Batch norm) 사용 중단: 배치 정규화는 세 가지 측면에서 복잡합니다. 첫째, 학습과 추론 시 다르게 동작합니다. 둘째, 표준 기울기 업데이트 외에 별도로 업데이트해야 하는 상태(이동 평균 및 분산)를 도입합니다. 셋째, '대부분의 레이어는 각 배치 요소에 독립적으로 작동한다'는 가정을 깨고 배치 전체의 통계량을 계산합니다. 이는 별도의 동기화 처리가 없다면 `vmap`이나 분산 학습 도구(`pmap`, `pjit`)와 호환되지 않을 수 있습니다.
* 드롭아웃 생략: 드롭아웃은 확률적 요소를 도입하므로, 저조한 성능이 무작위성 때문인지 아니면 더 근본적인 문제 때문인지 판단하기 어렵게 만듭니다.

### 옵티마이저 단순화

기본적인 동작이 확인될 때까지 다양한 옵티마이저나 학습률 스케줄을 실험하지 마세요. 학습률 `1e-3` 정도의 `Adam`과 같은 합리적인 기본 설정을 선택하고 더 근본적인 문제를 해결하는 데 집중하세요.

### 혼합 정밀도(Mixed precision) 피하기

`bfloat16`, `float16` 같은 낮은 정밀도의 데이터 타입은 성능과 메모리 효율을 높여주지만, 디버깅하기 매우 어려운 수치적 불안정성을 초래할 수 있습니다. JAX는 `float32` 입력을 받더라도 기본적으로 낮은 정밀도의 행렬 곱셈을 수행할 수 있으므로, 디버깅 중에는 `jax.config.update("jax_default_matmul_precision", "float32")`를 추가하여 전역적으로 정밀도를 강제하는 것이 좋습니다.

## 환경 단순화 및 제어하기

모델은 정상이지만 기술적 환경이 예상치 못한 문제를 일으킬 수 있습니다. 딥러닝의 실패처럼 보이는 많은 버그가 실제로는 환경적인 결함인 경우가 많습니다.

* 결정론(Determinism) 및 재현성 확보: 실험이 재현 가능해야 문제를 격리하기 쉽습니다. 드롭아웃을 끄는 것 외에 다음 사항을 고려하세요.
* 명시적인 난수 시드 설정: JAX에서는 모델 초기화와 모든 무작위 동작을 제어하기 위해 `jax.random.PRNGKey(...)`의 시드를 설정해야 합니다.
* 데이터셋 셔플링 중단: 훈련 데이터의 순서를 고정하여 실행 간 변동성을 제거하세요.
* 환경 상수화: 하드웨어, 라이브러리 버전, 설정을 동일하게 유지하세요.
* 훈련 루프 최소화: JAX와 Flax의 훈련 루프는 강력하지만 수동 제어가 많이 필요해 실수하기 쉽습니다.
* 단일 배치로 몇 단계만 훈련: 이것만으로도 주요 이슈를 파악하는 데 충분합니다.
* 로깅, 메트릭, 학습률 스케줄 끄기: 핵심 동작을 가릴 수 있는 요소들을 제거하세요.
* 고수준 추상화 잠시 피하기: `TrainState` 같은 도구 대신 기본 변수 업데이트 방식으로 코드를 작성하여 로직을 확인하세요.
* 코드의 독립성 확보: 코랩이나 주피터 노트북 환경에서는 이전 변수나 상태가 남기 쉽습니다. 커널을 재시작하고 코드를 독립적인 함수로 정리하세요.
* JIT 컴파일 끄기 (주의해서 사용): `@jax.jit`을 비활성화하면 스택 트레이스가 명확해져 디버깅이 쉬워지지만, 실행 속도가 매우 느려지고 메모리 사용량이 급증할 수 있습니다.
* 멀티 GPU 대신 단일 GPU 사용: 멀티 GPU 학습은 복잡성을 크게 증가시킵니다. 먼저 GPU 하나에서 작동하는지 확인하세요.
* NumPy 활용: `jax.numpy`는 NumPy와 유사하지만 원본 NumPy 라이브러리가 내부 구조가 더 단순하고 에러 메시지도 명확한 경우가 많습니다. 계산 로직을 검증할 때는 JAX 외부에서 NumPy만으로 테스트해 보는 것이 유용합니다.

## 데이터 및 문제 단순화하기

디버깅 속도를 높이기 위해 데이터셋과 예측 과제를 더 쉽게 만드세요.

* 개별 사례 시각화: 요약 수치만 보지 말고 실제 입력값과 라벨을 직접 출력하거나 그려보세요. 인코딩 오류나 라벨 불일치는 이 단계에서 쉽게 발견됩니다.
* 클래스 균형 확인: 불균형한 데이터셋에서는 모델이 다수 클래스만 찍어도 정확도가 높게 나올 수 있습니다. 디버깅 중에는 데이터를 리샘플링하여 균형을 맞추세요.
* 데이터 증강(Augmentation) 제거: 크롭, 플립, 노이즈 추가 등은 작업을 불필요하게 어렵게 만들 수 있습니다. 파이프라인이 확실히 작동할 때까지는 모두 끄세요.
* 출력 공간 단순화: 회귀 문제라면 이진 분류로 바꾸거나, 클래스 수를 줄여보세요. 복잡한 문제를 풀기 전에 파이프라인 자체를 검증하는 데 도움이 됩니다.
* 데이터셋 규모 축소: 큰 데이터셋은 모든 과정을 느리게 만듭니다. 핵심 구조를 담고 있는 작고 대표성 있는 하위 집합을 사용하세요.
* 라벨 누수(Label Leakage) 점검: 생물학 데이터에서는 환자 ID나 실험 날짜 같은 메타데이터가 타겟 정보를 암시하는 경우가 있습니다. 모델이 편법을 배우고 있지 않은지 확인하세요.
* 합성 데이터 활용: 실제 데이터의 특성을 모방한 합성 데이터를 만들어 테스트하면 디버깅이 훨씬 명확해집니다.

## 단일 배치 데이터에 과적합시키기

모델이 아무것도 배우지 못하는 것 같다면, 단 하나의 배치만 반복적으로 학습시켜 모델이 이를 '암기'할 수 있는지 확인해 보세요. 이 테스트는 훈련 루프, 손실 함수, 옵티마이저가 제대로 연결되었는지 확인하는 가장 빠른 방법입니다. 훈련 세트 전체를 돌리는 대신 단 하나의 배치만 뽑아 10~100회 업데이트를 반복했을 때 손실값이 급격히 떨어져야 합니다. 그렇지 않다면 다음을 확인하세요.

* 학습률 문제: 너무 높아서 발산하거나, 너무 낮아서 변화가 없을 수 있습니다.
* 가중치 고정 또는 잘못된 기울기: 파라미터가 업데이트 대상에서 누락되었거나, 기울기가 0 또는 `NaN`인지 확인하세요.
* 손실 함수 버그: 과제에 맞는 올바른 손실 함수를 사용하고 있는지 확인하세요.
* 형태 불일치 및 브로드캐스팅 에러: 코드가 멈추지 않더라도 텐서 형태가 맞지 않아 잘못된 계산이 수행될 수 있습니다.

## 기본으로 돌아가기

모든 노력이 수포로 돌아간다면, 이미 검증된 아주 단순한 예제로 돌아가세요.

* 기존 예제로 시작: MNIST나 단순한 CNN 예제처럼 확실히 작동하는 코드를 실행해 보세요.
* 자체 데이터로 교체: 베이스라인이 작동하면, 아주 점진적으로 여러분의 데이터셋을 하나씩 집어넣으며 어디서 문제가 발생하는지 관찰하세요.

## 모든 것을 기록하기

기록은 감에 의존하는 디버깅과 생산적인 디버깅의 차이를 만듭니다.

* 훈련 손실 및 주요 메트릭 기록: 최소한 손실과 정확도는 기록해야 합니다.
* 검증 성능 기록: 모델이 일반화되고 있는지, 혹은 훈련 성능과 동떨어지고 있는지 확인하세요.
* 입력, 예측, 오류 사례 저장: 모델이 어떤 데이터에서 구체적으로 틀리는지 아는 것이 중요합니다.
* 하이퍼파라미터 기록: 학습률, 배치 사이즈 등을 적어두세요. 인간은 반드시 잊어버립니다.

## 도움 요청하기

여전히 해결되지 않는다면 Stack Overflow나 GitHub Discussions 같은 커뮤니티, 혹은 동료에게 도움을 요청하세요. 때로는 누군가에게 문제를 소리 내어 설명하는 것만으로도 해결책이 떠오르기도 합니다. 또한 ChatGPT나 Gemini 같은 LLM을 활용해 코드를 검토할 수도 있지만, AI의 제안이 항상 정확한 것은 아니므로 반드시 다시 확인해야 합니다.

# 일반적인 데이터 문제

종종 모델 자체의 버그가 아니라 데이터가 문제일 때가 있습니다. 데이터셋에 숨어 있는 미묘한 이슈들은 학습 파이프라인을 조용히 망가뜨립니다. 근본 원인이 상위(upstream) 단계에 있음에도 불구하고 모델 설정을 디버깅하느라 몇 시간을 허비할 수 있습니다. 이번 섹션에서는 아키텍처를 뜯어고치기 전에 반드시 점검해야 할 일반적인 데이터 함정들을 다룹니다.

## 데이터 누수(Data Leakage)

모델이 실제로 실력을 발휘하는 것이 아니라 '커닝'을 하고 있음에도 성능이 잘 나오고 있다고 믿기 쉽습니다. 데이터 누수는 훈련 중에 숨겨져야 할 정보가 실수로 모델에 노출되어 성능 지표가 지나치게 낙관적으로 나타나는 현상을 말합니다.

명백한 사례:

* 훈련 세트 자체에서 모델을 평가하는 경우입니다. 다소 황당하게 들리겠지만, 캐글 노트북 같은 비공식적인 환경에서 놀라울 정도로 자주 발생합니다.
* 훈련 세트와 일부 샘플이 겹치는 검증 또는 테스트 세트에서 평가하는 경우입니다.

미묘한 사례:

* 전처리 과정의 누수: 예를 들어, 전체 데이터를 훈련/검증/테스트 세트로 나누기 전에 전체 데이터셋을 기준으로 정규화(Normalization)를 수행하는 경우입니다.
* 미래 정보 유출: 예측 시점에는 사용할 수 없지만 타겟(정답)과 상관관계가 높은 기능을 입력값으로 사용하는 경우입니다.

미묘한 누수의 실제 사례:
* 피부 병변 분류: 암 환자의 사진은 A 클리닉에서, 건강한 사람의 사진은 B 클리닉에서 촬영했다고 가정해 봅시다. 만약 한 클리닉의 조명이 더 밝다면, 모델은 암의 특징이 아니라 조명의 밝기를 암의 지표로 학습할 수 있습니다.
* 단백질 결합 예측: 단백질이 유전자에 결합하는지 예측하면서 유전자 발현 수치를 입력값으로 넣는 경우입니다. 결합 자체가 유전자 발현에 영향을 줄 수 있으므로, 모델은 의도했던 DNA 서열 특징이 아닌 이 수치에만 의존하게 됩니다.

데이터 누수를 방지하려면:

* 테스트 데이터가 훈련 파이프라인으로부터 완전히 격리되었는지 항상 확인하세요.
* 프로젝트 중간에 새로운 데이터를 추가할 때, 기존의 검증 또는 테스트 세트에 이미 포함된 데이터인지 체크하세요.
* "이 특징을 추론 시점에도 사용할 수 있는가?"라고 자문해 보세요. 아니라면 사용하지 마세요.
* 모델 해석 도구를 사용하여 모델이 예측 시 어떤 데이터에 의존하는지 확인하세요. 여러분의 예상과 일치하나요, 아니면 데이터의 인위적인 흔적(Artifacts)을 쫓고 있나요?

## 잘못된 데이터 라벨 (Incorrect Data Labels)

당연해 보이지만, 잘못 라벨링된 데이터는 모델 성능 저하의 가장 흔하고 고통스러운 원인 중 하나입니다. 다음과 같은 상황에서 위험이 큽니다.

* 입력값과 라벨의 분리 저장: 라벨이 파일 이름과 클래스가 적힌 별도의 CSV 파일에 있다면, 전처리 과정에서 순서가 뒤섞이거나 잘못 매칭될 수 있습니다.
* 입력과 라벨의 독립적 셔플링: 데이터와 라벨을 따로 셔플링하면 둘 사이의 연결이 조용히 끊어집니다.
* TensorFlow 데이터셋의 형태 불일치: `tf.data.Dataset`은 데이터(100, 10)와 라벨(43,)의 형태가 맞지 않아도 바로 경고를 주지 않을 수 있습니다. 이는 나중에야 드러나는 침묵의 실패로 이어집니다.
* 잘못된 테이블 병합: pandas의 `merge` 등을 사용할 때 정렬 상태를 확인하지 않고 데이터셋을 합치면 에러 없이 데이터 행이 잘못 매칭될 수 있습니다.
* 라벨을 손상시키는 데이터 증강: 증강 파이프라인이 입력 데이터만 변형해야 하는데 실수로 라벨까지 변형하거나 매칭을 깨뜨릴 수 있습니다.

라벨 문제를 해결하는 가장 좋은 방법은 원본 데이터 단계와 파이프라인의 각 지점에서 입력-라벨 쌍을 직접 검사하는 데 시간을 투자하는 것입니다. 수작업으로 몇 개의 배치를 직접 확인하고 예시를 그려보며 라벨을 검증하세요. 지루하게 느껴질 수 있지만, 보이지 않는 버그를 잡는 가장 빠른 길입니다.

## 클래스 불균형 (Imbalanced Classes)

생물학에서는 희귀 돌연변이 검출이나 특정 질환 세포 식별처럼 한 클래스가 다른 클래스보다 압도적으로 많은 경우가 흔합니다. 클래스 불균형 자체가 문제는 아니지만, 이런 데이터로 학습된 모델은 단순히 다수 클래스만 예측하는 쪽으로 학습되어 겉으로만 높은 정확도를 보일 수 있습니다.

위험 신호:

* 정확도는 높지만, 소수 클래스에 대한 정밀도(Precision)나 재현율(Recall)이 형편없습니다.
* 혼동 행렬(Confusion Matrix)을 보면 모델이 소수 클래스를 거의 예측하지 않습니다.

해결 방법:

* 클래스 가중치 또는 Focal Loss 사용: 소수 클래스에서 발생하는 실무에 더 큰 벌점을 줍니다. Focal Loss는 쉬운 샘플의 비중을 낮추고 오분류된 어려운 샘플에 집중하게 하여 소수 클래스가 묻히는 것을 방지합니다.
* 리샘플링: 소수 클래스를 오버샘플링하거나 다수 클래스를 언더샘플링합니다. 오버샘플링은 데이터가 부족할 때 안전하지만 과적합에 주의해야 합니다.
* 층화 추출(Stratified Sampling): 훈련, 검증, 테스트 세트를 나눌 때 각 세트가 원래의 클래스 비율을 그대로 유지하도록 분할하세요.

## 분포 변화 (Distribution Shifts)

훈련 데이터와 테스트 데이터가 서로 다른 실험실, 종, 시퀀싱 프로토콜 또는 영상 장비에서 올 수 있습니다. 이러한 변화는 모델이 일반적인 생물학적 원리가 아닌 특정 데이터셋에 국한된 특징을 학습하게 만듭니다.

위험 신호:

* 검증 성능은 뛰어나지만 실제 현장 데이터나 외부 데이터셋에 적용하면 성능이 급락합니다.
* 데이터셋 라벨(예: 실험실 ID, 배치 번호)을 예측하도록 모델을 학습시켰을 때 성능이 지나치게 잘 나옵니다.

감지 및 수정 방법:

* PCA나 UMAP을 통해 임베딩을 시각화하고 데이터 출처별로 색상을 칠해 클러스터링이 발생하는지 확인하세요.
* 필요한 경우 배치 효과 교정(Batch effect correction)이나 도메인 적응(Domain adaptation) 메서드를 사용하세요.
* 서로 다른 출처의 데이터를 섞을 때는 주의를 기울이고, 새로운 환경에 대한 일반화 능력을 명시적으로 테스트하세요.

## 생물학 특유의 함정들

생물학 데이터에서 반복적으로 발생하는 몇 가지 버그와 오류의 원인들입니다.

* 버전 관리 문제: 게놈 빌드(GRCh37 vs GRCh38), 유전자 ID 버전 등이 섞이지 않도록 주의하세요. 파이프라인 전체가 동일한 버전을 사용하는지 확인해야 합니다.
* 데이터 통합의 어려움: 서로 다른 출처의 데이터를 합칠 때 식별자(ID) 불일치, 파일 형식 차이, 측정 단위 불일치(예: Read counts vs TPM)가 발생할 수 있습니다.
* 생물학적 이질성: 유럽계 샘플로 학습된 모델은 다른 인종에게 일반화되지 않을 수 있습니다. 암 세포주 데이터로 만든 모델이 정상 일차 세포(Primary cells)에서 실패할 수도 있습니다. 학습 데이터의 범위를 항상 고려하세요.
* 모호하거나 부드러운 라벨: 생물학적 범주는 칼로 자르듯 나누어지지 않습니다. 세포 유형은 점진적으로 변할 수 있고 단백질 결합은 이진 값이 아닌 연속적인 점수입니다. 성능의 한계가 모델 실패가 아닌 라벨 자체의 모호성 때문일 수 있습니다.
* 실험 노이즈: 품질 지표를 사용하여 저품질 데이터를 필터링하거나, 실험 복제본(Replicates)을 활용하여 불확실성을 수량화하고 노이즈를 줄이세요.
* 배치 효과(Batch Effects): 실험 조건, 시약 제조 단위 등의 차이가 실제 생물학적 신호보다 강력하게 나타날 수 있습니다. 데이터를 시각화하여 배치끼리 뭉쳐 있는지 확인하세요.

# 일반적인 모델 관련 문제

모든 훈련 실패가 나쁜 데이터 때문에 발생하는 것은 아닙니다. 때로는 모델 자체가 문제일 수도 있습니다. 이 섹션에서는 과적합부터 기울기 불안정성까지, 모델 훈련 중에 발생하는 일반적인 문제들을 살펴봅니다.

Google의 Deep Learning Tuning Playbook처럼 더 상세한 디버깅 가이드도 시중에 많이 나와 있습니다. 여기서는 가장 일반적이고 실용적인 실패 사례들을 요약하고, 이를 효율적으로 식별하고 해결하는 방법에 집중하겠습니다.

## 과적합 및 저조한 일반화 (Overfitting and Poor Generalization)

과적합은 딥러닝에서 가장 흔히 발생하는 문제 중 하나입니다. 심층 신경망은 대개 용량이 크며, 일반화에 대한 내장된 개념 없이 오로지 훈련 손실을 최소화하도록 설계됩니다. 그 결과 훈련 데이터에서는 우수한 성능을 보이지만, 처음 보는 데이터에서는 성능이 떨어지는 경우가 많습니다.

다행히 과적합을 줄이는 데 도움이 되는 검증된 정규화 기법들이 있습니다.

* 드롭아웃(Dropout): 훈련 중 네트워크 유닛을 무작위로 비활성화하여 특정 경로에만 지나치게 의존하는 것을 방지합니다.
* 가중치 감소(Weight decay): 가중치가 커질수록 벌점(L1 또는 L2 정규화)을 부여하여 일반화가 더 잘되는 단순한 모델을 유도합니다.
* 조기 종료(Early stopping): 훈련 손실이 계속 떨어지더라도 검증 성능이 저하되기 시작하면 훈련을 중단합니다.
* 데이터 증강(Data augmentation): 이미지 회전/반전이나 서열 지터링/크롭 등 의미 있는 변환을 적용하여 데이터셋을 확장합니다.
* 앙상블(Ensembling): 서로 다른 시드나 데이터 분할로 훈련된 여러 모델의 예측을 결합합니다. 동일한 아키텍처를 앙상블하는 것만으로도 견고함을 크게 높일 수 있습니다.

또한 검증 또는 테스트 데이터가 훈련 데이터와 근본적으로 다른지 확인하세요. 분포 자체가 변했다면 모델이 과적합된 것이 아니라, 한 번도 학습하지 못한 데이터를 마주한 것일 수 있습니다.

## 기울기 소실 또는 폭주 (Vanishing or Exploding Gradients)

기울기 값이 0에 가까워지는 소실 현상이나, 지나치게 커지는 폭주 현상은 훈련을 심각하게 방해합니다. 다행히 이러한 문제는 비교적 쉽게 감지할 수 있습니다.

기울기를 모니터링하는 간단한 방법은 기울기의 L2 노름(유클리드 노름)을 계산하는 것입니다. 이는 기울기의 전반적인 크기를 하나의 스칼라 값으로 요약해 줍니다. 훈련 중 손실값과 함께 이 값을 기록하세요.

Flax에서 기울기의 L2 노름을 계산하는 방법은 다음과 같습니다.

```python
@jax.jit
def compute_gradients_l2_norm(grads):
    """기울기의 L2 노름을 계산합니다."""
    grads_flat = jax.tree_util.tree_leaves(grads) # 평탄화
    return jnp.sqrt(sum([jnp.sum(jnp.square(g)) for g in grads_flat]))

# 훈련 단계 내 사용 예시:
loss, grads = jax.value_and_grad(loss_fn)(state.params)
grad_norm = compute_gradients_l2_norm(grads)

```

훈련 과정에서 `grad_norm`을 기록하고 시각화하여 기울기 동작을 점검하세요.

* `grad_norm`이 0에 가깝다면 기울기가 소실되고 있을 가능성이 큽니다.
* 이 값이 급격히 커지거나 불규칙하게 튀면 기울기 폭주 현상일 수 있습니다.

일반적인 해결책은 다음과 같습니다.

* 학습률을 낮추거나 학습률 스케줄러를 사용합니다.
* 활성화 함수에 따라 적절한 가중치 초기화(Xavier 또는 He 초기화)를 사용합니다.
* 배치 정규화(Batch normalization)나 레이어 정규화(Layer normalization)를 통해 기울기 흐름을 안정화합니다.
* 잔차 연결(Residual connections)을 추가하여 깊은 네트워크에서도 기울기가 손실 없이 전파되도록 돕습니다.
* 기울기 클리핑(Gradient clipping): 극단적인 값을 억제하여 불안정성을 방지하는 투박하지만 효과적인 도구입니다.

```python
def clip_gradients(grads, threshold):
    """기울기를 클리핑합니다."""
    return jax.tree_map(lambda g: jnp.clip(g, -threshold, threshold), grads)

```

## 훈련 불안정성 (Training Instability)

훈련 불안정성은 불규칙한 손실값, 검증 손실의 갑작스러운 급증, 또는 완전한 발산(Divergence, 손실이 NaN이 되거나 무한히 진동함)으로 나타납니다.

훈련 불안정성의 주요 원인은 다음과 같습니다.

* 너무 높은 학습률: 옵티마이저가 최솟값을 지나치게 되어 불안정해집니다. 학습률을 낮추거나 작은 값에서 시작해 서서히 높이는 워밍업 스케줄을 사용하세요.
* 비적응형 옵티마이저 사용: Adam, RMSProp 같은 적응형 옵티마이저는 파라미터별로 학습률을 조정하므로 더 견고합니다. 일반적인 SGD는 더 세심한 튜닝이 필요합니다.
* 부적절한 배치 사이즈: 너무 작은 배치는 기울기 추정치에 노이즈를 만들어 훈련을 불안정하게 합니다. 초기 디버깅 단계에서는 하드웨어가 허용하는 한 충분히 큰 배치를 사용하세요.
* 활성화 값 폭주(Activation blowup): 네트워크가 깊어질수록 중간 활성화 값이 지나치게 커질 수 있습니다. 배치 정규화 등을 적용하여 활성화 값이 일정한 범위를 유지하도록 하세요.


# 저조한 모델 성능 (Poor Model Performance)

모델 훈련도 잘 되고 데이터셋도 문제없으며 과적합도 해결했습니다. 모든 것이 정상처럼 보이지만, 단지 모델 성능이 기대만큼 좋지 않은 상황입니다.

## 성능 기준 설정하기 (How Well Should You Do?)

성능을 판단하려면 문맥이 필요합니다. 다음 기준을 통해 기대치를 고정하세요.

* 무작위 성능: 단순히 무작위로 추측한다면 어떤 결과가 나올까요? 회귀 문제라면 훈련 세트의 평균이나 중앙값만 계속 예측했을 때보다 성능이 나은지 확인하세요.
* 베이스라인 모델: 단순한 선형 회귀나 로지스틱 회귀를 시도해 보세요. 딥러닝 모델이 이보다 성능이 낮다면 무언가 잘못된 것입니다.
* 기존 연구 모델: 동일한 과제를 다룬 다른 연구자들의 리포트를 확인하세요. 단, 공개된 지표가 항상 신뢰할 수 있는 것은 아니므로 주의해야 합니다.
* 인간의 성능: 전문가라면 이 과제를 얼마나 잘 수행할까요? 이는 기대치를 교정하는 데 도움을 줍니다.
* 실험 복제본(Experimental replicates): 생물학적 측정치에는 항상 노이즈가 있습니다. 두 실험 복제본 사이의 상관관계가 0.85라면, 모델 성능이 이 수치를 넘어서기는 어렵습니다. 생물학 자체보다 모델이 더 일관되기를 기대하지 마세요.

## 저조한 모델 성능 해결하기

보편적인 정답은 없지만, 다음 전략을 통해 해결의 실마리를 찾을 수 있습니다.

* 데이터 품질 재점검: 모델 문제의 상당수는 데이터 문제입니다. 모델이 틀리는 특정 사례를 파헤쳐 노이즈나 라벨링 오류를 찾아보세요. 필요하다면 도메인 전문가의 조언을 구하세요.
* 오류 분석(Error analysis): 모델이 잘하는 부분과 계속 실패하는 부분은 어디인가요? 특정 클래스나 경계 사례에서만 실수가 잦다면 누락된 특징이나 잘못된 가정이 있을 수 있습니다.
* 데이터 추가: 모델이 과소적합(Underfitting) 상태이거나 희귀 사례로 힘들어한다면 더 많은 데이터가 필요합니다. 데이터 규모에 따른 성능 변화를 관찰하세요.
* 하이퍼파라미터 튜닝: 학습률, 배치 사이즈, 모델 깊이, 정규화 강도 등 핵심 파이프라인 설정을 조정하세요.
* 전이 학습(Transfer learning) 시도: 유사한 데이터셋으로 사전 훈련된 모델을 시작점으로 사용하세요. 이는 성능을 높이는 매우 강력한 방법입니다.

# 마치며

생물학 분야에서 딥러닝을 적용하는 것은 어렵습니다. 데이터는 지저분하고, 목표는 모호하며, 모델을 훈련하는 과정은 까다롭습니다. 하지만 바로 그 점이 이 일을 흥미롭게 만듭니다. 모델은 예상치 못한 방식으로 실패하고, 치명적인 버그를 작성하거나 데이터에서 거대한 오류를 발견할 수도 있습니다. 하지만 호기심을 잃지 않고, 모듈화된 방식을 유지하며, 의심스러울 때는 단순화하고 인내심을 가진다면 결국 길을 찾을 것입니다. 게놈을 해독하든, 단백질 구조를 예측하든, 혹은 현미경 영상을 해석하든, 이 책이 여러분이 더 자신 있게 연구에 임하고 그 과정을 조금이나마 즐기는 데 도움이 되었기를 바랍니다. 행운을 빕니다.

# Reference

[^1]: Charles Ravarani and Natasha Latysheva, Deep Learning for Biology: Harness AI to Solve Real-World Biology Problems, O’Reilly, 2025